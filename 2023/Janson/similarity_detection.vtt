WEBVTT

00:00.000 --> 00:10.800
So, it's nice to see a nice crowd after two years of pandemic.

00:10.800 --> 00:11.800
You're beautiful.

00:11.800 --> 00:20.680
So, today we're going to talk about similarity detection and how we use it in integrity.

00:20.680 --> 00:29.680
Sustainability as a way to ensure that the website is a safe place, that people just

00:29.680 --> 00:33.840
maintain an integral place.

00:33.840 --> 00:36.120
The outline of the presentation is as follows.

00:36.120 --> 00:41.620
We're going to outline the problem, then how we use automation and similarity detection

00:41.620 --> 00:45.120
in order to achieve what we want.

00:45.120 --> 00:50.120
The current technology that we use for images, which is the vector search, then we're going

00:50.120 --> 00:58.280
to discuss in depth what is the actual technology, the vector embedding makes possible to transform

00:58.280 --> 01:01.360
a picture into an element of search.

01:01.360 --> 01:08.600
The current platform offering the metaput is a disposal to allow other people to crowdsource

01:08.600 --> 01:12.840
all of their findings into a centralized place.

01:12.840 --> 01:20.080
And last but not least, what we have of OpenM3 that you can install, you can deploy in your

01:20.080 --> 01:28.320
own site to benefit all these technological findings.

01:28.320 --> 01:34.260
The problem is that any big platform bears the responsibility to ensure it's a safe place

01:34.260 --> 01:35.260
to serve.

01:35.260 --> 01:40.660
No matter what also law says that you have to make sure whatever the user posts, you

01:40.660 --> 01:47.460
are ultimately responsible to make sure that everybody is just not exposed to things that

01:47.460 --> 01:52.080
would violate your community guidelines.

01:52.080 --> 01:57.520
Meta has almost 3 billion users, it's like the last word population.

01:57.520 --> 02:03.240
And although the vast majority of our users follow rules, some fringe bad actors will

02:03.240 --> 02:04.520
always be present.

02:04.520 --> 02:13.800
And at that scale, fringe means tens of millions of bad person creating a lot of problems.

02:13.800 --> 02:21.760
And when I mean issues, problems, I mean child exploitation, imageries, non-consensual, intimate

02:21.760 --> 02:27.440
imagery, which is a way to say revenge porn, all sexual exploitation, people forced to

02:27.440 --> 02:36.400
perform sexual acts in front of camera against the real, terrorism, violence, whatever.

02:36.400 --> 02:43.220
And just to give you a couple of numbers, Meta publishes a transparency report quarterly

02:43.220 --> 02:48.880
about what we do to ensure that the platform stays safe.

02:48.880 --> 02:57.560
And on the second quarter of 2022, we removed 38 million of adult sexual exploitation pieces

02:57.560 --> 02:59.320
of content taken down.

02:59.320 --> 03:03.800
And it's just for this category, child exploitation is not so huge, thank God, but also there

03:03.800 --> 03:07.240
are other like violence, terrorism and stuff.

03:07.240 --> 03:12.720
That accounted for the 0.04% of youth content worldwide.

03:12.720 --> 03:20.680
And in case you were asking, 97% of this content was proactively taken off, even before people

03:20.680 --> 03:23.880
could even see it.

03:23.880 --> 03:29.320
The remaining 2.8% is user reports, like AI found this.

03:29.320 --> 03:34.440
And we take that down also, and we also add to the data banks just to make sure that we

03:34.440 --> 03:36.880
are not forgetting about that.

03:36.880 --> 03:40.320
Sometimes there are false positives because it's just unavoidable.

03:40.320 --> 03:48.360
And half million was restored upon user appeal, and we restore accounts and mostly accounts

03:48.360 --> 03:51.080
and the pictures that we are banned for.

03:51.080 --> 03:56.000
It goes by itself that the sheer volume of content, the huge scale, the problem we are

03:56.000 --> 04:04.840
facing requires both automation and also human review to ensure either accuracy, both accuracy

04:04.840 --> 04:06.560
and also consistency.

04:06.560 --> 04:12.120
Because it will be a problem if we had 1 million people clicking and making decisions, and

04:12.120 --> 04:16.400
what is valid for one is not for the other.

04:16.400 --> 04:22.440
And we cannot just employ automation, because otherwise we will have this very powerful

04:22.440 --> 04:27.800
site decapitating everybody, also innocent users.

04:27.800 --> 04:33.960
So the role of automation and similarity detection, the thing is that a lot of the

04:33.960 --> 04:38.360
things that happen online are things that are being repeated.

04:38.360 --> 04:43.720
So are things that already occurred in the past, like people posting a picture of some

04:43.720 --> 04:50.240
shooting, some mass shooting, for example, like the buffalo or the Christchurch, gets

04:50.240 --> 04:54.800
taken down, and 10 more accounts spawn and post the same things.

04:54.800 --> 05:04.240
So it's very efficient to reason in terms of let's just redo the things that we already

05:04.240 --> 05:06.600
found that have worked.

05:06.600 --> 05:11.680
We employ automation to scale, of course, handle the scale of the problem, and to consistently

05:11.680 --> 05:17.280
repeat a decision that a human reviewer has already vatted in the past.

05:17.280 --> 05:24.080
So we tie a content to a decision, a violating content to a decision, let's act upon this.

05:24.080 --> 05:27.840
And we tie the decision to the actions.

05:27.840 --> 05:33.200
Let's just repeat this action every time we meet a piece of content that triggered this

05:33.200 --> 05:34.680
same decision.

05:34.680 --> 05:38.720
We do that for videos, for pictures, and also for text.

05:38.720 --> 05:44.400
Today we'll be mostly talking about images because the techniques for video and pictures

05:44.400 --> 05:46.360
are somewhat very similar.

05:46.360 --> 05:53.320
Text has a completely different array of techniques that will not be presented today.

05:53.320 --> 05:59.480
So a way to, if you want to achieve a similarity at action, you have to come up with a way

05:59.480 --> 06:02.080
to achieve similarity first.

06:02.080 --> 06:05.280
So how do we compare to pictures?

06:05.280 --> 06:09.440
Of course, we are not doing pixel by pixel comparison.

06:09.440 --> 06:11.600
We want to be much faster.

06:11.600 --> 06:17.040
A way to do that is just, okay, let's just MD5 hash all the pictures, or SHA1 all the

06:17.040 --> 06:22.480
pictures, and then we store them somewhere in an indexing system.

06:22.480 --> 06:29.760
And whenever a new picture comes in, we just recreate the hash, and if it matches, we just

06:29.760 --> 06:30.760
ban, right?

06:30.760 --> 06:36.400
Well, that doesn't work very well because the cryptographic hashes are not resistant

06:36.400 --> 06:44.600
to resizing rotation, one pixel alteration, all the hash changes altogether.

06:44.600 --> 06:51.720
Instead we can really benefit from local hashing because it allows for similarity measurement.

06:51.720 --> 06:59.040
Like you change slightly one piece, one portion of the image, and the hash changes a little,

06:59.040 --> 07:00.040
but not completely.

07:00.040 --> 07:06.240
Then you can reason in terms of distance between two hashes.

07:06.240 --> 07:11.360
So you have to find a way to turn an image into a vector, and then you perform a vector

07:11.360 --> 07:12.360
search.

07:12.360 --> 07:18.480
Whenever two vectors are very, very close beyond a certain threshold, then it's probably

07:18.480 --> 07:19.760
a match.

07:19.760 --> 07:24.020
And just in case if you're asking, this is your base as the architecture.

07:24.020 --> 07:29.000
You have more or less all the architectures shared these four stages.

07:29.000 --> 07:35.440
Observation, an image has been generated, usually push-shaven-like, user uploaded something.

07:35.440 --> 07:42.360
Then you have the representation phase in which you hash the image to a compact representation.

07:42.360 --> 07:46.400
If you're indexing, you store data into your index.

07:46.400 --> 07:52.000
Instead if you are at inference time, like an event, someone uploaded something, you

07:52.000 --> 07:55.200
search the index that you've built with representation.

07:55.200 --> 08:01.320
In case you have a match, you action upon what you decide what to do with the match

08:01.320 --> 08:03.120
you got.

08:03.120 --> 08:07.120
Usually the idea is that this is very close to an image that I already see in the past

08:07.120 --> 08:10.200
that was abandoned and also the account was taken down.

08:10.200 --> 08:14.480
Do the same to this user.

08:14.480 --> 08:19.240
So first free piece of content.

08:19.240 --> 08:26.800
Facebook has released a library, which is FICE, the Facebook I similarity search library.

08:26.800 --> 08:34.040
It's a library to do similarity search over a vector of dense vectors, so vector floats

08:34.040 --> 08:36.400
or integers, for example.

08:36.400 --> 08:39.760
You can think about it like a C++ version of Lucene.

08:39.760 --> 08:45.240
So you index stuff, puts that in a very big space, and you can search in this space very

08:45.240 --> 08:46.240
fast.

08:46.240 --> 08:51.720
It supports CUDA, so you can use your GPUs to search.

08:51.720 --> 08:54.680
It's basically an index on steroids.

08:54.680 --> 08:57.960
And it's C++, but it has Python bindings available.

08:57.960 --> 09:00.120
And it scales almost nearly.

09:00.120 --> 09:08.720
You can really index 100 millions of pieces on a single machine and it just handles them

09:08.720 --> 09:09.720
really.

09:09.720 --> 09:11.560
It doesn't need to saturate all the memory.

09:11.560 --> 09:16.660
So it has a very good optimization properties that makes it very good too.

09:16.660 --> 09:21.440
And you can go and download that on GitHub.

09:21.440 --> 09:27.920
Today we are also mostly referring to with the perceptual hashing.

09:27.920 --> 09:31.520
This means that we are reasoning in terms of colors.

09:31.520 --> 09:33.220
Colors and images, shapes.

09:33.220 --> 09:36.720
We are not reasoning about what's happening inside the image.

09:36.720 --> 09:43.240
That sees the semantic hashing, which we are not going to talk about today.

09:43.240 --> 09:46.600
Perceptible hashing just captures visual similarities.

09:46.600 --> 09:52.160
It's very nice for a use case because it exactly does its job.

09:52.160 --> 09:58.960
So you might think that we are all talking about machine learning systems that come up

09:58.960 --> 10:03.160
with very clever representations about our pictures.

10:03.160 --> 10:07.200
And I'm asking, do we really need a convnet for that?

10:07.200 --> 10:09.440
Do we really need to employ GPUs?

10:09.440 --> 10:13.260
You already said that it's some CUDA, so perhaps that's a nice hint.

10:13.260 --> 10:15.120
But absolutely not.

10:15.120 --> 10:18.400
Most of this technology is like hashing technology.

10:18.400 --> 10:23.600
So they just compute and represent a mathematical transformation over the image.

10:23.600 --> 10:24.600
It's really fast.

10:24.600 --> 10:25.600
And it's really cheap.

10:25.600 --> 10:29.100
And it can be executed almost everywhere.

10:29.100 --> 10:35.240
So a little bit of history, the first very notable example.

10:35.240 --> 10:39.440
It comes from a source that nobody would have thought about.

10:39.440 --> 10:42.400
It's Microsoft in 2009.

10:42.400 --> 10:45.400
Microsoft invents photo DNA.

10:45.400 --> 10:50.640
Photo DNA is the first algorithm employed in fight against exploitive images of children.

10:50.640 --> 10:54.520
So pedagornography.

10:54.520 --> 11:04.040
It transforms a picture into an ash of 144 unsigned integers on 8-bit representation.

11:04.040 --> 11:05.680
It's proprietary.

11:05.680 --> 11:15.760
So Microsoft licenses this to any nonprofit or an organization that wants to fight exploitive

11:15.760 --> 11:16.760
images of children.

11:16.760 --> 11:19.520
It gives you a license you can use for that and nothing else.

11:19.520 --> 11:23.040
But I cannot disclose the details of how that works.

11:23.040 --> 11:25.640
It can be used only for that.

11:25.640 --> 11:30.440
But Microsoft donated the photo DNA to the national center for the missing and exploited

11:30.440 --> 11:31.440
children, the NACMAQ.

11:31.440 --> 11:39.200
It's this American nonprofit that basically acts as a coordination center in the global

11:39.200 --> 11:42.040
fight against this phenomenon.

11:42.040 --> 11:47.880
And shares this library with anyone that wants to integrate.

11:47.880 --> 11:50.040
This I cannot talk about how this works.

11:50.040 --> 11:54.520
This is the only moment in which I will say something like that.

11:54.520 --> 12:00.120
But we can talk about a resource counterpart that almost ten years later Facebook releases

12:00.120 --> 12:01.120
PDQ.

12:01.120 --> 12:08.240
PDQ stands for Perceptual Algorithm using Discrete Cosine Transform and gives a quality

12:08.240 --> 12:09.240
metric.

12:09.240 --> 12:10.240
It's a very, very bad acronym.

12:10.240 --> 12:13.440
But we need a three-letter acronym.

12:13.440 --> 12:17.960
It creates a 256-bit hash.

12:17.960 --> 12:20.680
It's hamming distance to compute distance.

12:20.680 --> 12:21.960
It's really fast.

12:21.960 --> 12:27.640
The compute overhead is negligible compared to discrete.

12:27.640 --> 12:30.940
Can tolerate some level of adversity.

12:30.940 --> 12:35.800
This means that you change the image because you want to fool the systems in that this

12:35.800 --> 12:38.600
image is not something which is well known.

12:38.600 --> 12:43.880
PDQ can resist a little to this manipulation but not to all of them.

12:43.880 --> 12:48.880
It's used in stopncii.org.

12:48.880 --> 12:54.120
It's a website where people in case you have a fight with your experience and is threatening

12:54.120 --> 12:59.080
to publish your intimate imagery, you go to stopncii.org.

12:59.080 --> 13:02.960
You upload your intimate imagery.

13:02.960 --> 13:05.160
Fingerprints get taken.

13:05.160 --> 13:08.280
Original images get deleted right away, of course.

13:08.280 --> 13:15.920
And these fingerprints are shared with partners that, okay, if I am going to see these fingerprints

13:15.920 --> 13:19.640
in my website, my platform, I'm going to take them down.

13:19.640 --> 13:24.600
So it's a crowdsource effort and uses PDQ for images.

13:24.600 --> 13:26.400
How does that work?

13:26.400 --> 13:33.720
So PDQ hashing is optionally scaled down to a square image.

13:33.720 --> 13:35.840
Then you compute the luminance.

13:35.840 --> 13:42.240
Luminance is the idea that you take the pixel that contributes most in the RGB channel.

13:42.240 --> 13:45.080
Instead of putting black and white, you use the luminance.

13:45.080 --> 13:47.360
It's just another procedure.

13:47.360 --> 13:51.400
And the idea is that the luminance gives you better information about what was the channel

13:51.400 --> 13:58.240
that was contributing most to the color or to the light in that place.

13:58.240 --> 14:03.960
Then you down sample to 64 times 64 using a blur filter.

14:03.960 --> 14:09.200
And the idea of the blur filter, the tent filter, is that it gets the most significant

14:09.200 --> 14:12.000
value in that region.

14:12.000 --> 14:16.400
Because if you keep convulating a pixel with your neighborhood, what you will have in the

14:16.400 --> 14:19.960
end will be the highest value.

14:19.960 --> 14:26.600
So you obtain a representation which is compact and retains the most significant information.

14:26.600 --> 14:31.360
Then you divide the images in 16 times 16 boxes, each one by 4 pixels.

14:31.360 --> 14:35.200
You calculate a discrete causing transform of each box.

14:35.200 --> 14:40.720
The discrete causing transform, so the box is the 4-bar color there.

14:40.720 --> 14:47.120
You see that the grid with a lot of wobbly images, that is a discrete causing transform.

14:47.120 --> 14:54.660
The idea is that any image, any signal can be represented as a sum of causing signals.

14:54.660 --> 14:57.520
You only take the signal, the most significant one.

14:57.520 --> 14:59.920
So it's a form of compression, actually.

14:59.920 --> 15:09.120
And you take the most significant coefficient for the biggest causing you have.

15:09.120 --> 15:12.040
And then you calculate if the median is above a certain value.

15:12.040 --> 15:13.820
Then it's 1, otherwise it's 0.

15:13.820 --> 15:20.880
So you get this 256 in array of 010101 in case this pixel were a high luminance or a

15:20.880 --> 15:23.240
low luminance.

15:23.240 --> 15:27.600
The DCT provides a spectral hashing property.

15:27.600 --> 15:31.720
What is the point in the images that contributes more or less?

15:31.720 --> 15:36.840
You have an hashing space which is 2 to the power of 1 to 28 because it's half the hashes,

15:36.840 --> 15:41.200
because half is always 0, half is always 1.

15:41.200 --> 15:47.800
To search, you just do a vector search against what you've just created.

15:47.800 --> 15:53.120
In case we want, we can use partially the same technology to do video hashing.

15:53.120 --> 15:57.880
And this is another, it comes in almost the same paper.

15:57.880 --> 16:00.440
The TNK is a temporary matching kernel.

16:00.440 --> 16:10.600
It's a way to use the PDQ creation to do a video similarity detection algorithm.

16:10.600 --> 16:12.920
It produces fixed length video hashes.

16:12.920 --> 16:20.560
So your hash stays the same length, which is like a 256 kilobyte, if I'm wrong.

16:20.560 --> 16:25.720
Even if your video lasts for 3 hours or 30 seconds, it just produces a fixed length.

16:25.720 --> 16:27.560
So it's really nice.

16:27.560 --> 16:30.560
What you do is that you resample a video to 15 frames.

16:30.560 --> 16:35.280
Then you compute the PDQ without the 01 quantization.

16:35.280 --> 16:36.880
So you keep the float numbers.

16:36.880 --> 16:40.320
That's why it's called PDQF, PDQ float.

16:40.320 --> 16:47.920
And then you compute the average of the old descriptors that you have within various periods

16:47.920 --> 16:49.560
of the cuisine and scene.

16:49.560 --> 16:52.360
Why we add the cuisine curves?

16:52.360 --> 17:00.000
Because a cuisine or a scene adds this wobbly movement that tells you whether a frame is

17:00.000 --> 17:06.480
before or later in the near surroundings, the near neighborhood of the frames.

17:06.480 --> 17:10.760
So in case you have like 10 pictures, you add this cuisine signal.

17:10.760 --> 17:16.460
You know this picture is before this one because you see the cuisine curve which is going up

17:16.460 --> 17:17.600
and going down.

17:17.600 --> 17:25.360
And it's a nice uniqueness fingerprinting time signature algorithm to add a cuisine.

17:25.360 --> 17:31.080
So you compute the average of all the frames, the PDQF for all the frames, with various

17:31.080 --> 17:33.160
periods, various scene and cuisine.

17:33.160 --> 17:35.040
And then you pack them all together.

17:35.040 --> 17:38.380
And you have these like five or six averages.

17:38.380 --> 17:42.800
And that's your PDQF embedding.

17:42.800 --> 17:49.000
Everything is just you compare first the vector zero, which is the average of all the frames

17:49.000 --> 17:51.880
and doesn't retain the temporal signature.

17:51.880 --> 17:57.280
Then if there is a match, you compare also all the other vectors at different periods,

17:57.280 --> 18:01.160
which are level two action as the time signature.

18:01.160 --> 18:04.560
And so you can really be sure that the videos are really the same.

18:04.560 --> 18:09.600
Because if you find the same averages with the same periods, it must be the same video.

18:09.600 --> 18:13.720
It's nice that it's resistant to resampling because you always resample.

18:13.720 --> 18:18.000
So in some way, if you vary the frame rate, the video will change.

18:18.000 --> 18:19.760
And MD5 hash will change.

18:19.760 --> 18:22.120
But this one is not full.

18:22.120 --> 18:28.680
Hashing is really slow because you have to do a transcoding of all the videos first.

18:28.680 --> 18:33.440
And then you have to read all the frames and compute the PDQ for every frame.

18:33.440 --> 18:36.840
But search is actually very fast.

18:36.840 --> 18:39.920
Another nice hashing technique that we have is the video MD5.

18:39.920 --> 18:43.600
I said that we will not be using a crypto-ashers highlight.

18:43.600 --> 18:46.440
We use crypto-ashers, but just for videos.

18:46.440 --> 18:50.960
This because if you take MD5 of video and find exact copies, it's really cheap in this

18:50.960 --> 18:51.960
way.

18:51.960 --> 18:58.400
A lot of actors just post unmodified, repost unmodified content.

18:58.400 --> 19:04.320
They're not going really through the hassle of the do-or-encoding just to try to fool

19:04.320 --> 19:05.320
the systems.

19:05.320 --> 19:07.280
Just try to repost again.

19:07.280 --> 19:09.760
So the MD5 actually works.

19:09.760 --> 19:14.600
And it can be done with vector search if we use the bytes for the MD5 algorithm.

19:14.600 --> 19:20.880
And it's used widely in stopnci.org also.

19:20.880 --> 19:27.440
In 2022, Facebook has released the video PDQ, which is a different algorithm from the former

19:27.440 --> 19:28.840
one.

19:28.840 --> 19:33.880
Hashing is we hash every frame to a PDQ asher, and we just pack the list.

19:33.880 --> 19:36.120
It's much bigger.

19:36.120 --> 19:39.680
It's not slower than the other one.

19:39.680 --> 19:46.180
But it has a nice property that we just have to search for individual frames.

19:46.180 --> 19:50.140
So we treat the problem as a back-of-word approach.

19:50.140 --> 19:55.320
So we just put all these frames inside an index library.

19:55.320 --> 20:00.360
Then we search, and we take all the candidates, and we do a pairwise comparison.

20:00.360 --> 20:05.480
If the pairwise comparison is successful beyond a certain threshold, then it's a match.

20:05.480 --> 20:12.080
And also this you get for free, and it's released along with the PDQ, along with the TMK, PDQF.

20:12.080 --> 20:20.080
All this is available inside the Facebook research GitHub repository.

20:20.080 --> 20:22.840
What do you do once you have these hashes?

20:22.840 --> 20:25.800
So your platform is computing the hashes.

20:25.800 --> 20:30.500
Well, it's the first time that you see this content, but perhaps older actors have already

20:30.500 --> 20:32.680
seen this content too.

20:32.680 --> 20:36.520
Well, you upload them to the threat exchange platform.

20:36.520 --> 20:42.400
NACMAQ shares the PDNA hashes I told you with all companies that are asking for them.

20:42.400 --> 20:48.160
So can you please tell me where this picture that someone uploaded is a matching NACMAQ?

20:48.160 --> 20:52.440
So I already know that this is something I should call the law enforcement.

20:52.440 --> 20:59.400
Data does the equivalent, but for the PDQ, because it has much less friction to adopt

20:59.400 --> 21:02.640
PDQ compared to the PDNA.

21:02.640 --> 21:06.280
There's a team, the Internet Safety Engineering, that builds and operates all these services

21:06.280 --> 21:15.960
where anyone can upload fingerprints, and so you can crowdsource a big graph of matches.

21:15.960 --> 21:23.680
The rest API to access and post new data has multi-language clients, uses PDQ, and users

21:23.680 --> 21:25.040
can also download the data.

21:25.040 --> 21:27.800
You're not forced to stay online, stay connected.

21:27.800 --> 21:33.080
You can just request for a dump of the database, and you can search it.

21:33.080 --> 21:40.040
And you find all the data and all the APIs at the GitHub page.

21:40.040 --> 21:49.720
In 2020, Facebook also has released its most advanced algorithm to spot similar images,

21:49.720 --> 21:52.880
the SIM SearchNet++.

21:52.880 --> 22:01.960
This is an error network, and it is capable of facing adversarial manipulation that the

22:01.960 --> 22:05.920
other embeddings just are not able to.

22:05.920 --> 22:12.760
Unfortunately, SIM SearchNet is proprietary, so I cannot really talk about that.

22:12.760 --> 22:21.800
But we have a cousin product, SSCD, the SIM Search Copy Detection, or Simulagty Search

22:21.800 --> 22:25.720
Copy Detection, which is open source and free.

22:25.720 --> 22:27.600
So I can really talk about that.

22:27.600 --> 22:34.680
They are somewhat related in some technological principles, so I can really talk about this.

22:34.680 --> 22:39.520
So this is a PyTorch-based model.

22:39.520 --> 22:46.200
So the problem that this state-of-the-art product is trying to solve is what happens

22:46.200 --> 22:54.880
if I take a picture and I put a caption on it, alternating so many pixels everywhere.

22:54.880 --> 23:00.720
A PDQ or a PDNA hash will be altered dramatically.

23:00.720 --> 23:07.920
Is there anything we can do to teach a computer to just ignore all the captions, all the rotations,

23:07.920 --> 23:11.640
all the jitters, all the cropping of the image?

23:11.640 --> 23:12.640
Yes, there is.

23:12.640 --> 23:17.280
A person is able to do that, so we can teach a computer to do that too.

23:17.280 --> 23:19.200
So models and code are available.

23:19.200 --> 23:24.040
What is now available is the training data that we use to create a model, of course.

23:24.040 --> 23:31.720
For those which are into deep learning, it's a ResNet-50 convolative neural network.

23:31.720 --> 23:37.320
And the novelty of the approach is that it's based on our MAC vocabularies.

23:37.320 --> 23:44.120
A regional MAC, for those of you who know how a convolative network works, raise your

23:44.120 --> 23:45.120
hand.

23:45.120 --> 23:47.620
Okay, fine, very good.

23:47.620 --> 23:53.800
So it's a network for the others that looks at the image, looks at portions of the image.

23:53.800 --> 23:59.680
Each neuron looks at a different portion, and then they pass what they have understood

23:59.680 --> 24:04.800
to a higher level series of neurons, the higher and the higher and the higher, until the last

24:04.800 --> 24:10.640
layer of neurons has a very wide overview of the whole picture.

24:10.640 --> 24:17.240
In this case, we are using the maximum activation of all the channels that we have.

24:17.240 --> 24:24.960
So we take note which are the regions of our Carnaug maps for every different channel,

24:24.960 --> 24:28.840
which across all channels have the maximum activation.

24:28.840 --> 24:34.800
If you have 10 channels, and that region across all the different channels, all of them, you

24:34.800 --> 24:36.480
have a maximum activation.

24:36.480 --> 24:39.320
That means that that area is an area of interest.

24:39.320 --> 24:45.660
So we use these areas of interest as a word in a vocabulary.

24:45.660 --> 24:52.220
So exactly when you do the cosine similarity search for documents, you take all the words,

24:52.220 --> 24:57.760
you index all the words, you say these documents as these words, so it's like a vector of words.

24:57.760 --> 25:05.080
And then we try to see which are the vectors that have the most words in common, and put

25:05.080 --> 25:07.160
in the same place.

25:07.160 --> 25:10.700
We do the same things, but for portions of the image.

25:10.700 --> 25:13.120
So we use the R-MAX.

25:13.120 --> 25:17.280
The idea is that it's a self-supervised system also.

25:17.280 --> 25:25.240
So it means that it's trained to recognize augmented input, and it's trained to match

25:25.240 --> 25:28.320
an input to its augmented version.

25:28.320 --> 25:30.720
So what we do is that we take the training set.

25:30.720 --> 25:32.240
We repeat a lot of augmentation.

25:32.240 --> 25:37.880
We add the captions, the random, we rotate, we flip, we alter the colors.

25:37.880 --> 25:46.560
For example, if you do one degree of whitening, you make the image brighter, which is you

25:46.560 --> 25:49.440
add plus one to all the pixels in the image.

25:49.440 --> 25:51.520
You are altering all the pixels.

25:51.520 --> 25:56.760
But in this case, the PDQ ash is capable of understanding the difference.

25:56.760 --> 26:01.240
That's a very weak form of adversarial attack, because the PDQ just computes the difference

26:01.240 --> 26:03.520
between regions, so it's not going to be fooled.

26:03.520 --> 26:09.280
But you can be much more violent and put just a spot color somewhere, and PDQ is going to

26:09.280 --> 26:10.280
be fooled by that.

26:10.280 --> 26:12.440
Then you do through the CNN.

26:12.440 --> 26:19.480
You do a thing called gem pool, which means that you do a generative mean pooling, a generalization

26:19.480 --> 26:22.840
of the average pooling, in case you were wondering.

26:22.840 --> 26:30.400
Then you go, and at the end, you use entropy-oriented loss function.

26:30.400 --> 26:39.360
This means that we want to encourage the network to spread the representation of training data

26:39.360 --> 26:46.880
along all different places, because we want to maximize the distance between all the training

26:46.880 --> 26:48.140
examples in the training set.

26:48.140 --> 26:52.440
So you get a nice uniform search space.

26:52.440 --> 26:57.520
When you add in Frankenstein, you do the same with CNN, and then you obtain a vector, which

26:57.520 --> 26:59.760
is a representation of an image.

26:59.760 --> 27:06.680
And the idea is that there is a distance that you can compute between the data set of the

27:06.680 --> 27:08.440
reference images.

27:08.440 --> 27:14.960
Of course, you can subtract a background data set that was used generally to augment the

27:14.960 --> 27:15.960
images.

27:15.960 --> 27:21.120
But in this case, what you obtain in the end is that the score of the augmented image is

27:21.120 --> 27:27.400
almost the same of the non-augmented version, because it just learns to ignore the places

27:27.400 --> 27:30.480
which are not organic in the image.

27:30.480 --> 27:34.400
And SSCD is freely available.

27:34.400 --> 27:37.120
You can download that and start playing.

27:37.120 --> 27:42.600
You find both code and models, as I already said, but not the training data.

27:42.600 --> 27:47.160
And by the way, Facebook has also announced an image similarity challenge.

27:47.160 --> 27:52.320
You have to determine whether a query image is a modified copy of any image in a reference

27:52.320 --> 27:53.840
corpus of one million.

27:53.840 --> 28:01.160
This is very similar to the Netflix recommendation challenge, where you had to recommend the

28:01.160 --> 28:05.520
movies and you had to beat Netflix algorithm.

28:05.520 --> 28:10.640
And this image similarity challenge and also the meta-IE video similarity challenge, which

28:10.640 --> 28:14.520
is two tracks.

28:14.520 --> 28:18.880
Generate a useful vector representation for a video.

28:18.880 --> 28:24.000
And also try to find a reference video into this very big corpus.

28:24.000 --> 28:26.480
And you don't have to only find a video.

28:26.480 --> 28:36.240
You have to find a clip, so a super portion of a video, into a very big corpus.

28:36.240 --> 28:43.480
And last but not least, since the last part of a donor is the TASTEAR-1, we have your

28:43.480 --> 28:49.520
turnkey open source solution that you can install in your own premise.

28:49.520 --> 28:51.680
The HASHR-MATCHR actioner.

28:51.680 --> 28:57.720
HMA is an open source turnkey safety solution.

28:57.720 --> 29:03.520
So you just download it, install it, and it starts working right away.

29:03.520 --> 29:09.920
What it does is that it scans the images that you want to push towards it.

29:09.920 --> 29:16.320
It has an index that is updated with all the hashes coming from thread exchange, but also

29:16.320 --> 29:18.400
from yours.

29:18.400 --> 29:26.200
And it's able to say, to bind banks, verticals of violations.

29:26.200 --> 29:29.700
You might have non-severe violation or very severe violation.

29:29.700 --> 29:34.720
You might decide that for non-severe violation, you just delete the content and send a warning.

29:34.720 --> 29:41.120
Or for high severity violation, you just immediately delete the content, shut down the account

29:41.120 --> 29:45.960
of the poster, and you also signal it to the law enforcement.

29:45.960 --> 29:47.960
You can do that also.

29:47.960 --> 29:53.680
And you can configure actions in a backend that are tied to the content that you want

29:53.680 --> 29:58.440
to bank into your HMA platform.

29:58.440 --> 30:03.480
Can pull violating seats from Facebook thread exchange API and works on AWS only, because

30:03.480 --> 30:12.300
we wanted to make a very easy to use thing and also something that doesn't really mix

30:12.300 --> 30:13.800
your bill higher.

30:13.800 --> 30:17.640
So we built it on AWS lambda.

30:17.640 --> 30:19.800
So it doesn't cost anything until it runs.

30:19.800 --> 30:24.720
It runs, spawns lambda instance, and then goes down, and you only pay for the seconds

30:24.720 --> 30:27.240
that it actually runs.

30:27.240 --> 30:28.720
But it's very fast.

30:28.720 --> 30:33.600
And there's a Terraform module available, thanks to the lovely folks of Internet Safety

30:33.600 --> 30:34.600
Engineering.

30:34.600 --> 30:37.520
This is how you deploy data.

30:37.520 --> 30:43.320
Your infra, you co-locate HMA to your platform.

30:43.320 --> 30:49.200
For example, you might own a platform where people have a chat or people post pictures.

30:49.200 --> 30:55.160
Whenever new content comes, the web server asks the Azure, have you seen this?

30:55.160 --> 30:57.480
Then the Azure goes to Matcher.

30:57.480 --> 31:01.000
Azure goes to the index and says, do I know this?

31:01.000 --> 31:09.200
And in case there's a match, the actioner module will just tell your, you have to define

31:09.200 --> 31:11.800
a callback API in your own platform.

31:11.800 --> 31:17.960
Like whenever the actioner calls, you are killing this content in your own backend.

31:17.960 --> 31:24.360
Of course, you can fetch from external API new content from a fact exchange platform.

31:24.360 --> 31:30.180
So wrapping up, automation is necessary to be effective.

31:30.180 --> 31:35.440
But you will lose precision, of course, because automation doesn't really think.

31:35.440 --> 31:38.960
It just does whatever you have configured blindly.

31:38.960 --> 31:44.400
Human support is always needed for appeals and also to establish the ground through.

31:44.400 --> 31:47.480
So what is actually violating, what is not.

31:47.480 --> 31:52.360
Do expect false positive, because they will happen.

31:52.360 --> 31:59.240
You should put in place an appeal process to allow your users to restore the content.

31:59.240 --> 32:07.160
PDQ, VWQ, MT5 and SSCD will provide you with a way to obtain compact representation of

32:07.160 --> 32:11.440
high dimensional content like pictures and videos.

32:11.440 --> 32:18.800
HMA provides you with a turnkey solution you can install on your premise and search and

32:18.800 --> 32:24.280
enforce your integrity policies at your platform.

32:24.280 --> 32:29.080
And third exchange provides you with a platform for exchanging representation with other

32:29.080 --> 32:34.720
big actors like Meta itself, for example.

32:34.720 --> 32:35.800
That was all from me.

32:35.800 --> 32:46.160
Thank you very much for listening.

32:46.160 --> 32:55.120
Any question?

32:55.120 --> 33:00.440
You mentioned that for the challenge, I think.

33:00.440 --> 33:05.280
So you mentioned that for the challenge, finding a clip of a video.

33:05.280 --> 33:08.600
Can PDQ do that, actually?

33:08.600 --> 33:11.240
You can hear me.

33:11.240 --> 33:19.240
So can PDQ find clips of videos of...

33:19.240 --> 33:20.880
That's my question, actually.

33:20.880 --> 33:23.880
So you should...

33:23.880 --> 33:29.460
You say perhaps I heard about YouTube whether it is something that already does.

33:29.460 --> 33:33.200
Like if the challenge is to find the clips of videos...

33:33.200 --> 33:34.200
Yeah.

33:34.200 --> 33:46.920
In general, it's possible, of course, and the video PDQ algorithms will ask every frame.

33:46.920 --> 33:54.560
So in case you send a very small sub portion of a video, you will have like 100 frames,

33:54.560 --> 33:58.600
for example, then these 100 frames will be treated as a bag of words.

33:58.600 --> 34:00.280
You search the index.

34:00.280 --> 34:03.920
You find the video that contains all of these words.

34:03.920 --> 34:11.200
So you have a match of all your query frames inside the index at the very long video that

34:11.200 --> 34:12.200
has it.

34:12.200 --> 34:13.780
And so it's a match.

34:13.780 --> 34:14.780
That's how we do.

34:14.780 --> 34:19.360
Of course, there are more clever ways to do that.

34:19.360 --> 34:20.880
Thanks.

34:20.880 --> 34:23.000
Hello.

34:23.000 --> 34:28.200
Not a technical question, but let's see.

34:28.200 --> 34:35.720
I was thinking that if you're using such a system to try to prevent digital crimes and

34:35.720 --> 34:42.720
such things like that, from an ethical perspective, I was just wondering that you...

34:42.720 --> 34:47.640
I suppose you have such images to compare them.

34:47.640 --> 34:49.640
And how do you process those?

34:49.640 --> 34:53.120
How do you make the decisions whether...

34:53.120 --> 34:57.480
So I repeat the question.

34:57.480 --> 35:02.320
From the ethical perspective, the idea is that, of course, we have to see the images

35:02.320 --> 35:06.000
in order to be able to know what's happening, right?

35:06.000 --> 35:07.000
Was it the question?

35:07.000 --> 35:08.000
Yeah, see.

35:08.000 --> 35:12.080
And of course, you have to save them and, I don't know, process them.

35:12.080 --> 35:14.540
And how do you handle this?

35:14.540 --> 35:19.960
So this is not the kind of question that I really can answer because it is related to

35:19.960 --> 35:22.440
internal procedures.

35:22.440 --> 35:30.320
But if we have to compute the fingerprint of an image, there must be one second in which

35:30.320 --> 35:35.160
the image is on our servers.

35:35.160 --> 35:41.720
Since the agencies like NACMEC, they share ashes.

35:41.720 --> 35:45.480
So you might have an ash for which you don't have a picture.

35:45.480 --> 35:50.840
And you have to trust that these ashes coming from a trusted source that has already vetted

35:50.840 --> 35:54.560
whether this ash is a nasty stuff or not.

35:54.560 --> 36:01.400
That's how we actually avoid sanctioning heavily innocent people.

36:01.400 --> 36:05.600
So there is a collaboration with the trusted entities for this.

36:05.600 --> 36:10.600
When you receive those from an external agent, if those images are on your platform, you

36:10.600 --> 36:14.560
already know what you've seen.

36:14.560 --> 36:16.220
Thank you.

36:16.220 --> 36:19.100
Can you hear me despite the mask?

36:19.100 --> 36:20.100
Can you hear me?

36:20.100 --> 36:22.240
Thank you.

36:22.240 --> 36:27.640
So I have a question, but first I have a thanks because I have worked in this kind of thing

36:27.640 --> 36:31.960
and NACMEC doesn't share any useful data.

36:31.960 --> 36:36.080
IWF doesn't share any useful data.

36:36.080 --> 36:38.960
FAROS doesn't share any useful data.

36:38.960 --> 36:42.920
So I will definitely take a look at the threat exchange platform and hope that it's much

36:42.920 --> 36:44.280
more useful.

36:44.280 --> 36:45.520
And thanks for that.

36:45.520 --> 36:49.480
No, I have a question anyway.

36:49.480 --> 36:56.620
If I was an attacker, I could download data from the threat exchange platform and try

36:56.620 --> 37:02.880
and run as many filters automatically until I find something that is not matched by PDQ,

37:02.880 --> 37:06.080
video PDQ, et cetera.

37:06.080 --> 37:07.560
What's the way to counter that?

37:07.560 --> 37:12.920
Oh, you're asking whether adversarial attacks are possible on PDQ.

37:12.920 --> 37:14.680
Yeah, of course.

37:14.680 --> 37:19.520
PDQ is a very naive algorithm that just detects the patches of colors.

37:19.520 --> 37:24.440
It is actually possible to create adversarial attacks.

37:24.440 --> 37:34.400
Just if you think that you alter many pixels in the image and perceptually for us doesn't

37:34.400 --> 37:43.560
change anything, but you might end up changing the most relevant pictures for the DCT algorithm.

37:43.560 --> 37:51.160
It will create a completely different hashing in the end.

37:51.160 --> 37:58.280
Also someone has demonstrated an attack, a reverse engineering attack on photo DNA, like

37:58.280 --> 38:05.080
from the project is called the ribosome.

38:05.080 --> 38:13.000
And it's a neural network that from hash reconstructs a very blurry picture.

38:13.000 --> 38:16.240
So it is actually possible to do that.

38:16.240 --> 38:21.520
But PDQ is a very simple and fast algorithm.

38:21.520 --> 38:29.080
If you really want to combat the seriously adversarial engineering, the things that are

38:29.080 --> 38:35.400
in the neural networks like SSCD because it contains so many relations to different parts

38:35.400 --> 38:38.000
of the images, it's much harder to fool.

38:38.000 --> 38:41.560
I'm not saying it's not impossible because of course it's possible.

38:41.560 --> 38:43.560
One or later someone will find a way.

38:43.560 --> 38:49.280
But it's the usual arms race between attackers and defenders.

38:49.280 --> 38:51.000
And it's no exception.

38:51.000 --> 38:54.000
Thank you for your question.

38:54.000 --> 38:55.000
Hello.

38:55.000 --> 38:57.920
First, thank you for the presentation.

38:57.920 --> 39:00.520
I think it's a very interesting topic.

39:00.520 --> 39:06.880
I wanted to link it because it's been a bit of a buzz the past few weeks.

39:06.880 --> 39:11.760
And the generative AI, especially chat GPT, was wondering if when you use that kind of

39:11.760 --> 39:17.360
algorithm and you scan an image, detect something, is there a level of confidence attached to

39:17.360 --> 39:18.360
the result?

39:18.360 --> 39:21.680
And can you detect when an image is potentially a fake?

39:21.680 --> 39:22.680
Or?

39:22.680 --> 39:27.800
There is a hard time because there's an echo, so I cannot really.

39:27.800 --> 39:30.000
Can you do it louder, please?

39:30.000 --> 39:31.600
It's hard to understand from here.

39:31.600 --> 39:32.600
Hello.

39:32.600 --> 39:33.600
Is it better?

39:33.600 --> 39:34.600
Okay.

39:34.600 --> 39:37.120
So thank you.

39:37.120 --> 39:41.080
I wanted to link to generative AI.

39:41.080 --> 39:47.160
And I was asking, so when you run that kind of algorithm to detect violence or child abuse

39:47.160 --> 39:53.760
or anything else, can you also attach a level of confidence in the response to explain whether

39:53.760 --> 39:58.320
it's, well, to define whether it's potentially fake picture?

39:58.320 --> 40:03.200
Or is there an extension to the algorithm where you can link with the generative AI?

40:03.200 --> 40:08.400
I'm not sure about the answer.

40:08.400 --> 40:14.280
So we can go for a beer and can explain more details.

40:14.280 --> 40:16.720
Let's see.

40:16.720 --> 40:20.400
Yeah, you have a question.

40:20.400 --> 40:21.400
Hi.

40:21.400 --> 40:22.520
Thank you for the talk.

40:22.520 --> 40:24.440
It was very interesting.

40:24.440 --> 40:25.560
One more question also.

40:25.560 --> 40:28.880
Do you run SSCD in production as well?

40:28.880 --> 40:31.240
The deep learning network?

40:31.240 --> 40:33.080
If we're using SSCD in production.

40:33.080 --> 40:34.080
Yes.

40:34.080 --> 40:35.520
Can I reply to this question?

40:35.520 --> 40:36.520
Okay.

40:36.520 --> 40:38.080
We use SimSearch.

40:38.080 --> 40:40.120
We use SimSearch Net++.

40:40.120 --> 40:41.120
Yes.

40:41.120 --> 40:46.480
We use this other one because we have written a blog post about this.

40:46.480 --> 40:49.680
So I can confirm that we use SimSearch Net++.

40:49.680 --> 40:52.960
We cannot confirm or deny about SSCD.

40:52.960 --> 40:53.960
That's okay.

40:53.960 --> 40:55.600
Those are related technologies.

40:55.600 --> 40:56.600
So I can...

40:56.600 --> 40:58.280
That's okay.

40:58.280 --> 41:02.200
What does the production stack for SimSearch Net++ look like?

41:02.200 --> 41:03.200
How do you serve it?

41:03.200 --> 41:05.840
It must be pretty hard to deal with the GPS.

41:05.840 --> 41:07.480
This is not a question that unfortunately...

41:07.480 --> 41:08.480
Okay.

41:08.480 --> 41:09.480
I'm sorry.

41:09.480 --> 41:10.480
I cannot talk about production setups.

41:10.480 --> 41:12.480
I'm sorry.

41:12.480 --> 41:14.560
Okay.

41:14.560 --> 41:15.560
Any question nearby?

41:15.560 --> 41:16.560
Thank you.

41:16.560 --> 41:21.560
But of course, you can imagine that we do not operate in the vacuum.

41:21.560 --> 41:30.320
So if you can think about how we serve results from an error network, it is something perhaps

41:30.320 --> 41:42.360
similar to what would you do if you would have to put behind an API a model.

41:42.360 --> 41:44.680
So I kind of have two questions.

41:44.680 --> 41:50.000
The first question is to what extent do...

41:50.000 --> 41:53.880
So I think there are potentially two problems.

41:53.880 --> 41:59.500
Intentional mismatches and unintentional mismatches.

41:59.500 --> 42:05.280
So situations where perhaps an image has been recompressed or has been cropped or is perhaps

42:05.280 --> 42:10.720
another image of the same situation versus situations where people have deliberately

42:10.720 --> 42:14.880
deformed the image to try and get around these kind of systems.

42:14.880 --> 42:15.880
So...

42:15.880 --> 42:21.380
Do you have any idea of how performant it is against the two scenarios of either accidental

42:21.380 --> 42:26.040
or unintentional mismatches versus intentionally trying to avoid it?

42:26.040 --> 42:32.240
So it is of course possible to have unintentional mismatches.

42:32.240 --> 42:42.440
And I've seen images that were adversarial engineered to give the same embedding.

42:42.440 --> 42:44.400
Those are absolutely possible.

42:44.400 --> 42:50.840
Again, in PDQ, PDNA and all the perceptual hashing, which is just a mathematical transformation.

42:50.840 --> 42:57.300
You just have to find a way where the input seems the same to the algorithm.

42:57.300 --> 43:02.560
For the neural network things, it depends.

43:02.560 --> 43:03.920
You can study the code.

43:03.920 --> 43:08.000
You can study how it's done.

43:08.000 --> 43:15.320
It is absolutely possible sooner or later because the adversarial attack on convnets

43:15.320 --> 43:16.760
are a reality.

43:16.760 --> 43:18.240
So it's absolutely possible.

43:18.240 --> 43:26.360
I've seen some mismatches, but usually two perceptual hashes.

43:26.360 --> 43:31.840
Usually the more I find a technique, the harder it is to attack, of course.

43:31.840 --> 43:36.520
Otherwise we just will stay with MD5 because it will be enough.

43:36.520 --> 43:37.520
Crops.

43:37.520 --> 43:40.400
PDQ is resistant to crops.

43:40.400 --> 43:46.360
SACD is very resistant to crops.

43:46.360 --> 43:51.680
If you have rotations, I believe also PDQ is resistant to rotations, like flips.

43:51.680 --> 43:58.320
But you cannot ask much more than that.

43:58.320 --> 43:59.320
Other questions?

43:59.320 --> 44:00.320
Yeah.

44:00.320 --> 44:06.680
Do you have any information about speed difference between SACD and PDQ?

44:06.680 --> 44:08.680
Yeah.

44:08.680 --> 44:17.160
So the question is whether I have some speed benchmarks for difference of performance between

44:17.160 --> 44:21.680
PDQ and SACD at inference time.

44:21.680 --> 44:28.640
PDQ is faster than your time to read the image from disk.

44:28.640 --> 44:30.520
So it's negligible.

44:30.520 --> 44:31.520
It will just compute.

44:31.520 --> 44:33.920
It's a mathematical transformation on the pixel.

44:33.920 --> 44:37.080
The neural network requires dedicated hardware.

44:37.080 --> 44:41.120
If you do that on CPU, it will take seconds.

44:41.120 --> 44:43.960
Also because the model, I think, is big enough.

44:43.960 --> 44:49.720
It's not as big as GPT, but it's a 50-level CNET.

44:49.720 --> 44:54.680
So it's, of course, lower and requires dedicated hardware.

44:54.680 --> 44:55.880
But it's more precise.

44:55.880 --> 45:02.760
It just finds, SACD finds anything that PDQ is able to find and much more.

45:02.760 --> 45:11.680
So in case if you are very curious about, sorry, if you are very conscious about I have

45:11.680 --> 45:17.480
to scan this stuff just to make sure they don't come from a ill source, you might want

45:17.480 --> 45:22.280
to set up an async process that will take more, but will just batch process all your

45:22.280 --> 45:23.280
stuff.

45:23.280 --> 45:30.240
If you need a super fast thing, PDQ will not really wait over your server.

45:30.240 --> 45:31.240
Thank you.

45:31.240 --> 45:33.240
Any other questions?

45:33.240 --> 45:34.240
Hi.

45:34.240 --> 45:44.880
First of all, great question from my former colleague, David, I think, down there.

45:44.880 --> 45:47.280
Not even looking this way.

45:47.280 --> 45:50.240
But what happens if you get a false positive match?

45:50.240 --> 45:53.040
What if you get a false positive match?

45:53.040 --> 45:58.560
How do you disregard that in the future without potentially disregarding a real match?

45:58.560 --> 46:05.240
So if we get a false positive match, how do we do to restore?

46:05.240 --> 46:06.240
How do you restore it?

46:06.240 --> 46:10.000
You mean in meta?

46:10.000 --> 46:11.000
Just anywhere.

46:11.000 --> 46:12.000
As a concert, what would you do?

46:12.000 --> 46:14.360
In meta, I cannot really say.

46:14.360 --> 46:21.160
With the Azure Matcher Actioner, you have the, you should provide a capability to your

46:21.160 --> 46:25.160
own platform for which you are soft-deleting the image.

46:25.160 --> 46:31.800
Because you have to provide a way, an API in your platform that HMA will call on, where

46:31.800 --> 46:34.560
you say, soft-delete this picture.

46:34.560 --> 46:39.560
So make it unavailable, but do not really delete it in case you want to appeal.

46:39.560 --> 46:46.360
So you need to provide, like, undelete and soft-delete and soft-delete.

46:46.360 --> 46:54.600
This is the simplest way, most effective way to deal with false positive in case, oops,

46:54.600 --> 46:57.520
I did a mistake, I want to restore the content.

46:57.520 --> 47:02.680
Sure, but if you have an image that someone wants to upload, say it's a popular image

47:02.680 --> 47:09.200
that a lot of people are going to upload, but it matches a pattern of another bad image,

47:09.200 --> 47:15.680
can you auto, is there a good way to make a more precise hash and exclude that and say

47:15.680 --> 47:18.880
this one is a false positive, it doesn't match what you think it does?

47:18.880 --> 47:20.960
So you don't have to keep undoing.

47:20.960 --> 47:21.960
Okay, fine.

47:21.960 --> 47:28.040
So, partly, if the image is popular, so we have many examples and we have many examples

47:28.040 --> 47:32.400
of an image which is not bad, then comes a bad image.

47:32.400 --> 47:36.200
Whether we can use the fact that it's very widespread to augment our position, is this

47:36.200 --> 47:38.200
the question?

47:38.200 --> 47:40.200
Okay.

47:40.200 --> 47:48.040
Well, really, there's nothing in this presentation that says these, because once you train the

47:48.040 --> 47:53.560
network is trained, you start serving and the network will give you the same answers

47:53.560 --> 47:56.360
to the same question, to the same query.

47:56.360 --> 48:02.880
PDQ or other mathematical, perceptual algorithm is just a mathematical function so will not

48:02.880 --> 48:05.040
change, there's nothing to train.

48:05.040 --> 48:12.960
So to change a deficiency of your model, you have to retrain.

48:12.960 --> 48:18.760
You can do a better retraining and sometimes models are retrained as anything which is

48:18.760 --> 48:20.800
still under maintenance.

48:20.800 --> 48:25.600
For example, we get new data, for example, and we might want to retrain as any other

48:25.600 --> 48:27.560
model for the spam filters.

48:27.560 --> 48:31.440
It's the same.

48:31.440 --> 48:36.960
Do we have more room for questions?

48:36.960 --> 48:37.960
I think it's done.

48:37.960 --> 48:38.960
Thank you so much.

48:38.960 --> 48:45.960
It would be a wonderful audience.
