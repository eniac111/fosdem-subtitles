1
0:00:00.000 --> 0:00:06.440
All right, so I'm Todd Gamblin.

2
0:00:06.440 --> 0:00:08.440
I'm from Lawrence Livermore National Laboratory.

3
0:00:08.440 --> 0:00:13.120
Normally I would give an intro of what Livermore is, but who's been hearing about Livermore

4
0:00:13.120 --> 0:00:14.360
in the news lately?

5
0:00:14.360 --> 0:00:17.840
Have people heard about the fusion ignition over in the US?

6
0:00:17.840 --> 0:00:18.840
That's our lab.

7
0:00:18.840 --> 0:00:21.040
So I'm from there.

8
0:00:21.040 --> 0:00:26.320
I work in the HPC area at Livermore, and so we have a big supercomputing center.

9
0:00:26.320 --> 0:00:30.480
And the HPC ecosystem is a pretty complex place.

10
0:00:30.480 --> 0:00:33.160
People distribute software mostly as source.

11
0:00:33.160 --> 0:00:36.440
You build lots of different variants of the package.

12
0:00:36.440 --> 0:00:39.640
Users typically don't have root on the machine when they install software, and so they're

13
0:00:39.640 --> 0:00:43.680
building from source in their home directory or installing something in their home directory.

14
0:00:43.680 --> 0:00:47.720
And you want the code to be optimized for fancy machines like these ones over here.

15
0:00:47.720 --> 0:00:52.960
So you're trying to build software that supports a really broad set of environments, including

16
0:00:52.960 --> 0:00:58.560
like Power, ARM, AMD, Intel, and then also GPU architectures.

17
0:00:58.560 --> 0:01:02.360
So things like NVIDIA and now AMD GPUs are showing up.

18
0:01:02.360 --> 0:01:05.560
We've even got a machine coming all out at Argonne.

19
0:01:05.560 --> 0:01:09.840
This is near Chicago with Intel Panavecchio GPUs.

20
0:01:09.840 --> 0:01:15.680
On top of all that, the ecosystem has C, C++, Fortran, Python, other languages, Lua, all

21
0:01:15.680 --> 0:01:17.600
linked together in the same app.

22
0:01:17.600 --> 0:01:21.620
And so we want a distribution that can support this type of environment.

23
0:01:21.620 --> 0:01:25.800
And so SPAC is a package manager that enables software distribution for HPC, given that

24
0:01:25.800 --> 0:01:28.600
set of constraints.

25
0:01:28.600 --> 0:01:32.640
Packages are not quite like the build specs that you would see in your standard RPM or

26
0:01:32.640 --> 0:01:33.940
deb-based distribution.

27
0:01:33.940 --> 0:01:38.800
They're really parameterized Python recipes for how to build that package on lots of different

28
0:01:38.800 --> 0:01:39.800
architectures.

29
0:01:39.800 --> 0:01:41.100
And it has a DSL for doing that.

30
0:01:41.100 --> 0:01:43.120
I'm not going to get into that today.

31
0:01:43.120 --> 0:01:47.800
But the end user can essentially take one package and install it lots of different ways.

32
0:01:47.800 --> 0:01:51.920
So you could say, I want to install HCF5 at a particular version.

33
0:01:51.920 --> 0:01:54.280
I want to install it with Clang, not GCC.

34
0:01:54.280 --> 0:01:57.560
I want to have the thread safe option on.

35
0:01:57.560 --> 0:02:00.440
Or I want to inject some flags in the build and have an entirely different version of

36
0:02:00.440 --> 0:02:02.920
it that's built with a different set of flags.

37
0:02:02.920 --> 0:02:04.920
Or that's targeted at a particular microarchitecture.

38
0:02:04.920 --> 0:02:07.640
Or that maybe uses a particular dependency.

39
0:02:07.640 --> 0:02:12.200
So you can build the same package with like two versions of MPI.

40
0:02:12.200 --> 0:02:16.720
So we're trying to provide the ease of use of mainstream tools with the flexibility needed

41
0:02:16.720 --> 0:02:19.680
for HPC so that we can get the performance that we want.

42
0:02:19.680 --> 0:02:21.280
And it builds from source.

43
0:02:21.280 --> 0:02:26.560
But you can also install relocatable build caches in SPAC, much like you would with,

44
0:02:26.560 --> 0:02:29.360
say, Nix or Geeks.

45
0:02:29.360 --> 0:02:31.960
They're not relocatable because they're not really targeting the sort of home directory

46
0:02:31.960 --> 0:02:32.960
use case.

47
0:02:32.960 --> 0:02:35.040
But it's the same sort of build cache model.

48
0:02:35.040 --> 0:02:37.840
It's not a typical binary distribution.

49
0:02:37.840 --> 0:02:41.540
The whole project has a fairly large community of contributors, or at least maybe not large

50
0:02:41.540 --> 0:02:43.240
by some of the other distribution standards.

51
0:02:43.240 --> 0:02:46.280
But we have 1,100-plus contributors.

52
0:02:46.280 --> 0:02:47.920
We maintain the core tool.

53
0:02:47.920 --> 0:02:50.200
And then there's a whole bunch of people who work on package recipes.

54
0:02:50.200 --> 0:02:53.700
So in some ways it looks a lot like Homebrew or a project like that.

55
0:02:53.700 --> 0:02:57.400
And then there's a whole bunch of infrastructure behind the scenes to keep all this working.

56
0:02:57.400 --> 0:03:00.820
And all these things together enable people to build lots of different software stacks.

57
0:03:00.820 --> 0:03:05.660
And so there's like an exascale, extreme scale software stack that's maintained by the US

58
0:03:05.660 --> 0:03:06.660
Exascale project.

59
0:03:06.660 --> 0:03:12.440
AWS has a stack that they use on their parallel cluster product internally, and also for users.

60
0:03:12.440 --> 0:03:14.360
Livermore has its internal software deployment.

61
0:03:14.360 --> 0:03:18.480
There's some math library stacks, viz tools, things like that.

62
0:03:18.480 --> 0:03:21.600
And every application really in HPC is kind of its own software stack.

63
0:03:21.600 --> 0:03:26.920
So you've heard about flat packs and snaps in the last session.

64
0:03:26.920 --> 0:03:33.120
Well really, you know, making apps more mindful of how their software is actually a distribution

65
0:03:33.120 --> 0:03:38.960
is something that we've been pushing for a long time within HPC.

66
0:03:38.960 --> 0:03:40.920
The GitHub is a pretty busy place.

67
0:03:40.920 --> 0:03:46.560
We merge 300 to 500 PRs per month, and it's like something like 411 commits or more.

68
0:03:46.560 --> 0:03:51.040
And so managing that is kind of painful.

69
0:03:51.040 --> 0:03:56.520
And we're trying very hard to reduce downstream work, which is actually difficult for a source-based

70
0:03:56.520 --> 0:03:58.240
distribution.

71
0:03:58.240 --> 0:04:01.600
If you think about how stack is structured, there's this main line develop branch that

72
0:04:01.600 --> 0:04:02.600
actually most people use.

73
0:04:02.600 --> 0:04:05.360
They'll just clone it straight from the repo, build from that, kind of like you do with

74
0:04:05.360 --> 0:04:07.600
mixed packages or something.

75
0:04:07.600 --> 0:04:09.640
External contributors contribute there.

76
0:04:09.640 --> 0:04:14.120
And we cut a release every once in a while where we stabilize the packages and keep them

77
0:04:14.120 --> 0:04:19.560
sort of fixed so that you don't have a lot of version churn in the repo.

78
0:04:19.560 --> 0:04:23.560
And then to actually integrate with the HPC facilities, all the places that are deploying

79
0:04:23.560 --> 0:04:29.640
supercomputers, we have this E4S software distribution where they end up doing a whole

80
0:04:29.640 --> 0:04:33.160
bunch of downstream integration at the site where they're basically building the whole

81
0:04:33.160 --> 0:04:36.520
thing from source, essentially in a new environment.

82
0:04:36.520 --> 0:04:39.680
And there's a whole lot of debugging that takes place there that we would really like

83
0:04:39.680 --> 0:04:42.200
to be able to move upstream.

84
0:04:42.200 --> 0:04:46.600
The applications likewise are, they are not necessarily using what the facility deploys.

85
0:04:46.600 --> 0:04:48.680
Some of them do, some of them don't.

86
0:04:48.680 --> 0:04:50.520
They pull from basically all of these places.

87
0:04:50.520 --> 0:04:54.080
They might get a math solver library from the facility.

88
0:04:54.080 --> 0:04:58.760
They might get something else installed from stack main line built the way that they want.

89
0:04:58.760 --> 0:05:02.960
And they may pull stuff off of release branches too, all to assemble an application and have

90
0:05:02.960 --> 0:05:04.200
it built.

91
0:05:04.200 --> 0:05:07.960
And so this is a lot of porting at the lowest end.

92
0:05:07.960 --> 0:05:12.320
And what we'd really like to do is take that software integration and move it upstream

93
0:05:12.320 --> 0:05:18.680
and get to a point where we can have these types of environments building in CI all the

94
0:05:18.680 --> 0:05:24.380
time in sort of a rolling release and do binary deploys on the supercomputers with actual

95
0:05:24.380 --> 0:05:25.920
optimized binaries.

96
0:05:25.920 --> 0:05:27.280
So that's what we're trying to get to.

97
0:05:27.280 --> 0:05:30.920
So we set out to make a binary distribution with a bunch of different goals.

98
0:05:30.920 --> 0:05:37.020
The main one and the one that's pretty key to our whole ecosystem is it has to be sustainable.

99
0:05:37.020 --> 0:05:39.320
We don't have that many maintainers.

100
0:05:39.320 --> 0:05:43.720
And they currently, their workflow is basically to work with people who are making contributions,

101
0:05:43.720 --> 0:05:47.240
on pull requests, help them get them merged and then move on to the next one.

102
0:05:47.240 --> 0:05:51.560
And we don't want them to have to sit around and babysit builds on say a release integration

103
0:05:51.560 --> 0:05:54.060
branch all the time.

104
0:05:54.060 --> 0:05:57.960
We want a rolling release because people do tend to use the develop branch.

105
0:05:57.960 --> 0:06:01.880
And so we want that to be up to date with pretty current binaries all the time.

106
0:06:01.880 --> 0:06:06.420
But some people do fix themselves to releases and so we want sort of snapshots for those

107
0:06:06.420 --> 0:06:08.040
releases as well.

108
0:06:08.040 --> 0:06:13.160
We need to be able to support at least eventually all the packages that are in SPAC and it still

109
0:06:13.160 --> 0:06:15.300
has to be source buildable around those binaries.

110
0:06:15.300 --> 0:06:19.160
So if you want to build a component and rely on binaries for some other component, we want

111
0:06:19.160 --> 0:06:20.520
to support that.

112
0:06:20.520 --> 0:06:23.580
And then finally, people trust sources.

113
0:06:23.580 --> 0:06:24.580
They can check some of them.

114
0:06:24.580 --> 0:06:25.580
You can download the tarball.

115
0:06:25.580 --> 0:06:29.560
We can usually check some of them except for when GitHub changes the hashes.

116
0:06:29.560 --> 0:06:34.200
But we want to ensure that the binaries that we're generating are just as trustworthy as

117
0:06:34.200 --> 0:06:35.200
the sources.

118
0:06:35.200 --> 0:06:39.280
So we've taken some steps to ensure that.

119
0:06:39.280 --> 0:06:44.200
So SPAC is a little different from your standard distro if you haven't gathered already.

120
0:06:44.200 --> 0:06:49.440
If you think about a traditional package manager, you have a sort of a recipe per configuration.

121
0:06:49.440 --> 0:06:52.320
And so that's like your RPM build spec or deb spec or whatever.

122
0:06:52.320 --> 0:06:56.080
It goes into a build farm and you produce packages at least for one platform in sort

123
0:06:56.080 --> 0:07:00.680
of a one to one relationship with those specs, actually.

124
0:07:00.680 --> 0:07:03.720
There's templating and things that goes on to reduce that.

125
0:07:03.720 --> 0:07:07.840
But you're typically maintaining sort of one software stack that gets updated over time.

126
0:07:07.840 --> 0:07:11.900
In SPAC what we're trying to do is we have these parameterized package recipes that go

127
0:07:11.900 --> 0:07:15.280
in the build farm but it's really the same recipe that's being used across different

128
0:07:15.280 --> 0:07:16.280
architectures.

129
0:07:16.280 --> 0:07:19.880
We force the contributors to work on the same package so that essentially you're modeling

130
0:07:19.880 --> 0:07:23.200
all the different ways the software can be used and we try to get a lot of reuse out

131
0:07:23.200 --> 0:07:25.560
of the recipes across platforms.

132
0:07:25.560 --> 0:07:28.920
Those go into the build farm and you can use the same recipes to produce optimized binaries

133
0:07:28.920 --> 0:07:29.920
for lots of different platforms.

134
0:07:29.920 --> 0:07:34.000
So you could get a graviton, ARM build, you could get a Skylake binary, you could get

135
0:07:34.000 --> 0:07:35.900
a GPU build and so on.

136
0:07:35.900 --> 0:07:40.220
And then you could do that for many different software stacks for different use cases.

137
0:07:40.220 --> 0:07:43.320
And then we want you to be able to build from source on top of that.

138
0:07:43.320 --> 0:07:45.420
So that's what we're trying to do.

139
0:07:45.420 --> 0:07:50.520
We put a CI architecture together that is sort of based around this.

140
0:07:50.520 --> 0:07:51.820
Like I said, we want to be sustainable.

141
0:07:51.820 --> 0:07:54.280
We want to maintain the workflow that we already have on the project.

142
0:07:54.280 --> 0:07:58.280
And so we want people, we want basically GitHub to be the center of the distribution.

143
0:07:58.280 --> 0:08:01.760
What goes into develop is really maintaining the distribution as well as contributing to

144
0:08:01.760 --> 0:08:02.760
the project.

145
0:08:02.760 --> 0:08:07.960
And so we have a bunch of infrastructure currently stood up in AWS to support this.

146
0:08:07.960 --> 0:08:12.360
So the binaries themselves and the sources are all distributed through S3 and CloudFront.

147
0:08:12.360 --> 0:08:17.600
We set up a big Kubernetes cluster to support auto scaling runners.

148
0:08:17.600 --> 0:08:21.800
And we're using high availability GitLab in there to drive the CI.

149
0:08:21.800 --> 0:08:26.000
GitLab may seem like a strange choice for maintaining a distribution, but the motivation

150
0:08:26.000 --> 0:08:30.200
behind that is really that all of the HPC centers also have internal GitLabs.

151
0:08:30.200 --> 0:08:32.480
And so do a lot of universities and other sites.

152
0:08:32.480 --> 0:08:37.360
And so the goal is really for all of this automation and tooling to be usable not just

153
0:08:37.360 --> 0:08:42.120
in the Cloud for the large distribution of SPAC, but also for people's personal software

154
0:08:42.120 --> 0:08:43.280
to interact locally.

155
0:08:43.280 --> 0:08:48.540
And so the idea is that we're generating GitLab CI configuration and you can use that either

156
0:08:48.540 --> 0:08:54.300
for this or internally or in an air gap network somewhere.

157
0:08:54.300 --> 0:08:59.000
So we're leveraging Carpenter on the back end for just in time instances for runner

158
0:08:59.000 --> 0:09:00.000
pools.

159
0:09:00.000 --> 0:09:01.480
That's a tool from AWS that's open source.

160
0:09:01.480 --> 0:09:04.400
You can find it on GitHub.

161
0:09:04.400 --> 0:09:08.880
It essentially lets you make requests for nodes with certain amounts of memory, certain

162
0:09:08.880 --> 0:09:11.100
target architectures and so on.

163
0:09:11.100 --> 0:09:14.640
And it manages containers on the instances for you on the back end and sort of moves

164
0:09:14.640 --> 0:09:22.560
work around so that you can have an efficient build pool in Kubernetes.

165
0:09:22.560 --> 0:09:27.620
We also have some bare metal runners at the University of Oregon with more exotic architectures

166
0:09:27.620 --> 0:09:29.780
that you can maybe find in the Cloud.

167
0:09:29.780 --> 0:09:33.760
So like there's an AMD MI200 GPU builder in there.

168
0:09:33.760 --> 0:09:36.360
There's A64 effects, which is what runs on Fugaku.

169
0:09:36.360 --> 0:09:41.280
It's the ARM architecture with vector instructions, Power9 and so on.

170
0:09:41.280 --> 0:09:46.120
And so we are able to do runs there for architectures that aren't supported in the Cloud.

171
0:09:46.120 --> 0:09:48.280
There's some monitoring thrown in.

172
0:09:48.280 --> 0:09:52.080
We haven't really leveraged it in a smart way yet, but we are collecting a lot of data

173
0:09:52.080 --> 0:09:53.360
about our builds.

174
0:09:53.360 --> 0:09:58.440
And then there's a bot that helps sort of coordinate between GitHub and GitLab.

175
0:09:58.440 --> 0:10:03.920
And so we have sort of a sync script that allows us to build off of forks and things

176
0:10:03.920 --> 0:10:07.360
like that in GitLab for this whole setup.

177
0:10:07.360 --> 0:10:12.080
So it's fairly custom, but at least the GitLab component is recyclable internally.

178
0:10:12.080 --> 0:10:15.200
And we would like to be able to support more runners in the future, like if maybe we want

179
0:10:15.200 --> 0:10:19.080
to work with Azure on their HPC setup and they want to provide runners for the project

180
0:10:19.080 --> 0:10:24.160
or if other universities and places want to provide runners, we want to leave that open.

181
0:10:24.160 --> 0:10:29.900
For maintaining the stacks themselves, we made it possible to sort of instantiate a

182
0:10:29.900 --> 0:10:31.880
new stack in a pull request.

183
0:10:31.880 --> 0:10:35.880
And so we have this directory full of the sort of 16 stacks that we currently build

184
0:10:35.880 --> 0:10:36.880
in CI.

185
0:10:36.880 --> 0:10:38.000
You can see them there.

186
0:10:38.000 --> 0:10:44.900
Each one of those is some targeted software stack for some type of machine or some group.

187
0:10:44.900 --> 0:10:49.780
Each of those contains sort of a YAML file with configuration for the stack in it.

188
0:10:49.780 --> 0:10:52.600
And so the YAML file itself is fairly simple.

189
0:10:52.600 --> 0:10:55.560
It has a list of packages that you want to build.

190
0:10:55.560 --> 0:10:58.280
And so this is the machine learning one for CUDA.

191
0:10:58.280 --> 0:11:02.160
Those are all the names of stack recipes that you're building here.

192
0:11:02.160 --> 0:11:04.040
And then some configuration up here.

193
0:11:04.040 --> 0:11:11.200
And so for this particular stack, you're saying I want to build for x86.64v3, which is AVX2.

194
0:11:11.200 --> 0:11:15.900
And I want to disable Rockum and enable CUDA, except on LLVM because there's some weird

195
0:11:15.900 --> 0:11:19.940
bug with the CUDA support there, at least in our stack.

196
0:11:19.940 --> 0:11:21.760
And so you can see it's fairly concise.

197
0:11:21.760 --> 0:11:22.840
You make a list of packages.

198
0:11:22.840 --> 0:11:25.400
You say here's the configuration I want.

199
0:11:25.400 --> 0:11:30.240
And you can go and take this thing and build a bunch of packages.

200
0:11:30.240 --> 0:11:34.000
We make it easy to change sort of low level stack wide parameters.

201
0:11:34.000 --> 0:11:38.940
So the parameterized packages in stack, you can tell it to build with a different compiler.

202
0:11:38.940 --> 0:11:46.280
And so we had essentially this large E4S stack with maybe 600 packages working in standard

203
0:11:46.280 --> 0:11:47.280
environments.

204
0:11:47.280 --> 0:11:49.480
We wanted to support the oneAPI compilers from Intel.

205
0:11:49.480 --> 0:11:52.760
And so that's Intel's new optimizing compilers.

206
0:11:52.760 --> 0:11:57.120
It is unlikely that anyone has ever run this much open source through a proprietary vendor

207
0:11:57.120 --> 0:11:58.120
compiler like that.

208
0:11:58.120 --> 0:11:59.920
But it is Clang based.

209
0:11:59.920 --> 0:12:03.680
And so we were able to throw oneAPI into the config by just saying here's where oneAPI

210
0:12:03.680 --> 0:12:08.960
lives and make all packages require oneAPI.

211
0:12:08.960 --> 0:12:12.880
And so the build system swaps in the oneAPI compiler through some wrappers that are at

212
0:12:12.880 --> 0:12:13.880
the lower level.

213
0:12:13.880 --> 0:12:17.320
And we were able to get that stack working in a week or two, despite the fact that we've

214
0:12:17.320 --> 0:12:20.520
never built a lot of these packages with oneAPI before.

215
0:12:20.520 --> 0:12:21.600
So I think that's actually pretty cool.

216
0:12:21.600 --> 0:12:26.200
It makes, in a lot of cases, it's not worth it to use a vendor compiler because there's

217
0:12:26.200 --> 0:12:29.880
so many bugs and issues with software that's never been built.

218
0:12:29.880 --> 0:12:34.880
But here we're just really throwing sort of a bunch of open source packages through.

219
0:12:34.880 --> 0:12:37.120
And it helped us communicate with Intel.

220
0:12:37.120 --> 0:12:40.440
We were able to say hey, here are bugs that we're seeing with your compiler.

221
0:12:40.440 --> 0:12:44.880
We can link you directly to the build log for the build that failed.

222
0:12:44.880 --> 0:12:48.960
And that helped them patch up the compiler and it continues to help them ensure that

223
0:12:48.960 --> 0:12:54.160
it can build everything it needs to.

224
0:12:54.160 --> 0:12:56.720
In SPAC you don't.

225
0:12:56.720 --> 0:12:59.640
So like I said, the recipes are these parametrized things.

226
0:12:59.640 --> 0:13:02.960
And so there's actually a solving step to these stacks.

227
0:13:02.960 --> 0:13:07.280
You saw sort of the requirements in the YAML file that said what I want to build.

228
0:13:07.280 --> 0:13:11.760
We run that through our packet solver to get sort of a fully resolved graph of all the

229
0:13:11.760 --> 0:13:14.400
things that need to be built in a stack.

230
0:13:14.400 --> 0:13:17.560
And then that is used to generate a get-lab CI YAML.

231
0:13:17.560 --> 0:13:22.080
And then for one of the problems that we have to solve there is mapping builds to runners.

232
0:13:22.080 --> 0:13:25.160
So once the whole thing is concrete, and we've said here's all the dependencies, these are

233
0:13:25.160 --> 0:13:29.760
all the exact build configurations we want to make, we have to say how that should be

234
0:13:29.760 --> 0:13:31.800
mapped to particular runners.

235
0:13:31.800 --> 0:13:36.000
And so we don't currently support things like cross builds.

236
0:13:36.000 --> 0:13:41.040
So if you want to build for AVX 512 or the more fancy vector instructions on newer Intel

237
0:13:41.040 --> 0:13:45.260
CPUs, you need to make sure that you get one of those CPUs in the build environment.

238
0:13:45.260 --> 0:13:49.720
And so we say, you know, if you match AVX 512, give me an AVX 512 runner.

239
0:13:49.720 --> 0:13:54.640
If you match one of these somewhat atrocious, hard to build packages up here like LLVM and

240
0:13:54.640 --> 0:13:59.160
PyTorch, give me a gigantic runner with lots of memory, things like that.

241
0:13:59.160 --> 0:14:02.280
And essentially what this is doing is it's just saying here's the package properties

242
0:14:02.280 --> 0:14:05.760
up at the top, here are the tags that should be on the runner, make sure that I get a runner

243
0:14:05.760 --> 0:14:08.600
with those capabilities.

244
0:14:08.600 --> 0:14:13.360
And we haven't got, you know, a schema for all the tags yet, but I think we could standardize

245
0:14:13.360 --> 0:14:18.080
this and make it easy for someone to plug in runners at their own site for this sort

246
0:14:18.080 --> 0:14:20.080
of thing.

247
0:14:20.080 --> 0:14:21.160
All right.

248
0:14:21.160 --> 0:14:26.560
So one of the things that we did here to ensure trust is, you know, we have essentially a

249
0:14:26.560 --> 0:14:30.040
build environment going on in pull requests.

250
0:14:30.040 --> 0:14:32.280
If you trust back, you're basically trusting the maintainers.

251
0:14:32.280 --> 0:14:36.840
We want to ensure that the binaries are, you know, things that are approved by the maintainers.

252
0:14:36.840 --> 0:14:42.000
And so we can't just distribute binaries that got built in pull requests.

253
0:14:42.000 --> 0:14:46.080
So when contributors to the package changes, we go and we have private buckets for every

254
0:14:46.080 --> 0:14:49.320
PR that we're supporting where we're doing the builds.

255
0:14:49.320 --> 0:14:51.160
The maintainers come along and say, oh, it worked.

256
0:14:51.160 --> 0:14:52.160
They review the code.

257
0:14:52.160 --> 0:14:55.920
And then they say, okay, we can merge that and rebuild everything on development sign.

258
0:14:55.920 --> 0:14:59.920
So essentially everything in the main release is getting built from only approved recipes.

259
0:14:59.920 --> 0:15:04.320
It's not using any binaries that were built in the PR.

260
0:15:04.320 --> 0:15:05.320
Okay.

261
0:15:05.320 --> 0:15:06.320
All right.

262
0:15:06.320 --> 0:15:11.840
The pull request integration, yeah, definitely makes things easy for, hold on, this is, yeah,

263
0:15:11.840 --> 0:15:12.840
for contributors.

264
0:15:12.840 --> 0:15:16.840
And, you know, we were able to take this system and announce our, you know, public binary

265
0:15:16.840 --> 0:15:21.720
cache last June with something like 4600 builds in CI.

266
0:15:21.720 --> 0:15:22.880
And so it's mostly easy for contributors.

267
0:15:22.880 --> 0:15:25.320
They get a status update on their pull request.

268
0:15:25.320 --> 0:15:26.320
And mostly easy for users.

269
0:15:26.320 --> 0:15:30.320
They can just say, hey, use the binary mirror.

270
0:15:30.320 --> 0:15:32.320
So there are some problems.

271
0:15:32.320 --> 0:15:36.280
One issue is that build caches are a lot different from RPMs and devs.

272
0:15:36.280 --> 0:15:39.720
In most distributions, you would have sort of a stable ABI for your build cache.

273
0:15:39.720 --> 0:15:42.440
Your rebuild package, you can throw it in the mix with the others.

274
0:15:42.440 --> 0:15:46.400
Here if you modify one package, you really do have to rebuild all the dependents.

275
0:15:46.400 --> 0:15:50.880
And so if you modify XZ here, then you have to build everything that depends on it again

276
0:15:50.880 --> 0:15:52.300
in the build cache.

277
0:15:52.300 --> 0:15:55.280
And so what that could mean is if you have a gigantic software stack like this one and

278
0:15:55.280 --> 0:15:59.680
you modify, say, package conf at the bottom of it, it can trigger, like, a massive rebuild

279
0:15:59.680 --> 0:16:02.280
of everything in the stack.

280
0:16:02.280 --> 0:16:05.760
And so that's one of the scalability problems that I think we're going to have to deal with

281
0:16:05.760 --> 0:16:09.800
in the long term is that you can get these really long-running pipelines.

282
0:16:09.800 --> 0:16:12.680
Packages like Visit and PyTorch and so on will build forever.

283
0:16:12.680 --> 0:16:16.360
And it frustrates contributors.

284
0:16:16.360 --> 0:16:21.680
The other sort of thing that happens is if you think about how the release works on develop,

285
0:16:21.680 --> 0:16:25.480
you're picking a commit every once in a while and building it.

286
0:16:25.480 --> 0:16:31.280
And if you have a PR that is sort of based behind the last develop build, that's okay.

287
0:16:31.280 --> 0:16:34.600
Although GitHub typically wants to merge that with head, which means that you'll build a

288
0:16:34.600 --> 0:16:37.600
lot of redundant things in your build environment.

289
0:16:37.600 --> 0:16:40.920
We can be picky and merge it with the last develop build to ensure that we get a lot

290
0:16:40.920 --> 0:16:42.840
of cache reuse in the build environment.

291
0:16:42.840 --> 0:16:47.720
But what that means is if we get a PR that's out ahead of the last develop build and say

292
0:16:47.720 --> 0:16:52.240
D up there is in progress, if you merge that second PR with D, you're basically going to

293
0:16:52.240 --> 0:16:55.980
be doing the same builds that D is doing but in a PR environment.

294
0:16:55.980 --> 0:17:01.500
And so if you have a bunch of those, you can, we've brought GitLab down before by accidentally

295
0:17:01.500 --> 0:17:07.880
building all of those PRs that are not caught up with the latest, or for which develop has

296
0:17:07.880 --> 0:17:09.760
not caught up with them.

297
0:17:09.760 --> 0:17:14.360
And so we have to be picky and hold back these guys until there's a build ahead of them so

298
0:17:14.360 --> 0:17:18.540
that we get enough reuse out of the cache to support this.

299
0:17:18.540 --> 0:17:23.800
So the other problem with long pipelines is that they, depending on how reliable your

300
0:17:23.800 --> 0:17:28.080
infrastructure is, the more things that you build in a pipeline, the more likely you already

301
0:17:28.080 --> 0:17:31.240
get a build failure somewhere.

302
0:17:31.240 --> 0:17:36.320
And so because we're building this cone of destruction in our pipelines, we are sort

303
0:17:36.320 --> 0:17:39.860
of subject to system failures happening in the pipeline somewhere.

304
0:17:39.860 --> 0:17:43.640
And so users have to kind of babysit and restart builds that have nothing to do with what they're

305
0:17:43.640 --> 0:17:44.940
contributing.

306
0:17:44.940 --> 0:17:48.800
So we're looking for ways that we could make that better.

307
0:17:48.800 --> 0:17:51.280
One issue that we have is consistency.

308
0:17:51.280 --> 0:17:56.140
So when you test on PRs, it's not always sufficient to ensure that your develop branch is working.

309
0:17:56.140 --> 0:18:01.520
So you may have this initial package state, a PR gets submitted, you test with new B.

310
0:18:01.520 --> 0:18:05.560
Another PR gets submitted, you test with new package C.

311
0:18:05.560 --> 0:18:09.640
If you take those and you don't require PRs to be up to date with develop, when they both

312
0:18:09.640 --> 0:18:14.860
get merged, the state that's in develop is something that you've never tested because

313
0:18:14.860 --> 0:18:18.680
you have basically new versions of those two packages together now.

314
0:18:18.680 --> 0:18:21.280
And so there are ways to get around this.

315
0:18:21.280 --> 0:18:22.680
One of them is merge queues.

316
0:18:22.680 --> 0:18:26.220
So we're looking at merge queues as a way to scale this pipeline out.

317
0:18:26.220 --> 0:18:33.000
They essentially allow you to have pull requests with a small amount of testing where you then

318
0:18:33.000 --> 0:18:36.480
enqueue them in your sort of merge queue up there.

319
0:18:36.480 --> 0:18:37.940
That's the gray stuff.

320
0:18:37.940 --> 0:18:41.120
And they are sort of serialized for commit to develop.

321
0:18:41.120 --> 0:18:45.880
If they succeed, then they're merged directly in a fast forward fashion.

322
0:18:45.880 --> 0:18:50.680
And then basically the full testing is only done on the merge queue.

323
0:18:50.680 --> 0:18:54.480
And you always are assured that the thing that you tested is the thing that gets merged

324
0:18:54.480 --> 0:18:55.640
into develop.

325
0:18:55.640 --> 0:19:01.040
So we're looking very much forward to GitHub making merge queue available in the next couple

326
0:19:01.040 --> 0:19:02.520
of weeks.

327
0:19:02.520 --> 0:19:06.400
The other thing we think that could do is allow us to sort of stage the work on PRs.

328
0:19:06.400 --> 0:19:09.340
So we're looking at ways we could scale this out.

329
0:19:09.340 --> 0:19:14.280
Right now for a relatively small number of packages, 4600, we're able to build these

330
0:19:14.280 --> 0:19:17.660
massive rebuilds on PRs.

331
0:19:17.660 --> 0:19:20.660
But we need to stage the CI to scale it out further.

332
0:19:20.660 --> 0:19:22.180
So that's what we're looking at now.

333
0:19:22.180 --> 0:19:27.920
We might build only the package or only the package and direct dependence on PRs and maybe

334
0:19:27.920 --> 0:19:31.040
phase how much work we do on the develop builds as well.

335
0:19:31.040 --> 0:19:35.280
But we do need to do a full build every once in a while so that there's a consistent state

336
0:19:35.280 --> 0:19:36.280
in the build cache.

337
0:19:36.280 --> 0:19:37.280
So that's where we're at.

338
0:19:37.280 --> 0:19:38.280
Thanks.

339
0:19:38.280 --> 0:19:58.440
We have, I think, one minute for questions.

340
0:19:58.440 --> 0:20:00.880
Thank you very much for the presentation.

341
0:20:00.880 --> 0:20:11.480
You mentioned quite a bit of other technologies like Nyx, Gweeks, DAB RPM.

342
0:20:11.480 --> 0:20:13.680
You could have mentioned Umbru as well.

343
0:20:13.680 --> 0:20:14.680
Maybe you did.

344
0:20:14.680 --> 0:20:19.440
And Docker, it feels like all these tools could help you.

345
0:20:19.440 --> 0:20:20.440
Yeah.

346
0:20:20.440 --> 0:20:23.840
It feels like you are building everything on your own.

347
0:20:23.840 --> 0:20:29.120
So is there a reason not to leverage any of these technologies?

348
0:20:29.120 --> 0:20:30.120
Which technologies do you mean?

349
0:20:30.120 --> 0:20:32.000
So we are leveraging a lot of technologies.

350
0:20:32.000 --> 0:20:34.040
I guess which ones do you think we should use?

351
0:20:34.040 --> 0:20:35.040
Nyx, for example.

352
0:20:35.040 --> 0:20:41.320
So we don't, so Nyx has essentially one version of everything in the mainline.

353
0:20:41.320 --> 0:20:46.600
And in the HPC environment, what we want you to be able to do is not build that one thing

354
0:20:46.600 --> 0:20:50.080
that's in the mainline, but to be able to build a one-off very easily.

355
0:20:50.080 --> 0:20:54.440
So the whole point of SPAC is, think of it as Nyx with a solver.

356
0:20:54.440 --> 0:20:57.880
It's Nyx where you can say, actually, no, build this version of this thing with this

357
0:20:57.880 --> 0:21:00.040
build option for that GPU.

358
0:21:00.040 --> 0:21:02.400
And it will take the recipe and reuse it for that purpose.

359
0:21:02.400 --> 0:21:05.320
Whereas in Nyx, it's much harder to have package variants like that.

360
0:21:05.320 --> 0:21:07.960
So that's really the power of SPAC.

361
0:21:07.960 --> 0:21:10.760
And so we're combinatorial, Nyx.

362
0:21:10.760 --> 0:21:11.760
You can think of it that way.

363
0:21:11.760 --> 0:21:17.320
Well, wouldn't you be able to leverage Nyx and describe all these differences instead

364
0:21:17.320 --> 0:21:19.160
of redoing it?

365
0:21:19.160 --> 0:21:20.160
No.

366
0:21:20.160 --> 0:21:31.640
The Nyx packages don't do that.

