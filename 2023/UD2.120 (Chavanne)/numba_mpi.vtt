WEBVTT

00:00.000 --> 00:10.360
Thank you for the opportunity to present our project, Namba NPI.

00:10.360 --> 00:12.760
Let me first acknowledge the co-authors.

00:12.760 --> 00:18.880
My name is Sylvester Arabas, and we are here with Alexi Bulenok and Kasper Derlatka from

00:18.880 --> 00:24.160
Yagilonian University in Krakow, Poland, Machin Manna from the same university contributed

00:24.160 --> 00:28.640
to this project, and we have also, we will be presenting some work from David Zwicker

00:28.640 --> 00:33.320
from Max Planck Institute for Dynamics and Self-Organization in GÃ¶ttingen.

00:33.320 --> 00:41.360
So let's start with a maybe controversial provocative question, Python and HPC, and let's

00:41.360 --> 00:48.360
try to look for answers to this question in a very respective, respected journal.

00:48.360 --> 00:53.560
So maybe you have some guesses what's written there.

00:53.560 --> 00:59.680
2019, in scripting languages such as Python, users type code into an interactive editor

00:59.680 --> 01:00.680
line by line.

01:00.680 --> 01:03.400
It doesn't sound like HPC.

01:03.400 --> 01:08.600
Next year, level of computational performance that Python simply couldn't deliver.

01:08.600 --> 01:11.240
Same year, same journal.

01:11.240 --> 01:16.320
Numpy runs on machines ranging from embedded devices to the world's largest supercomputers

01:16.320 --> 01:21.580
with performance approaching that of compiled languages.

01:21.580 --> 01:23.840
Same year, nature astronomy.

01:23.840 --> 01:27.400
Astronomers should avoid interpreted scripting languages such as Python.

01:27.400 --> 01:32.480
In principle, Numpy and Numpy can lead to an enormous increase in speed, but please reconsider

01:32.480 --> 01:36.080
teaching Python to university students.

01:36.080 --> 01:38.320
Same year, nature methods.

01:38.320 --> 01:44.000
Implementing new functionality into SciPy, Python is still the language of choice.

01:44.000 --> 01:50.000
Full tests should pass with the PyPy Just-in-Time compiler as of 1.0 SciPy.

01:50.000 --> 01:52.520
Are they talking about the same language?

01:52.520 --> 01:54.040
No.

01:54.040 --> 01:56.880
The left-hand side are papers about Rust and Julia.

01:56.880 --> 01:58.960
The right-hand side are papers about Python.

01:58.960 --> 01:59.960
Okay?

01:59.960 --> 02:01.860
So maybe that's the reason.

02:01.860 --> 02:08.920
So just to set the stage, let me present, I think, the way that is apt for thinking about

02:08.920 --> 02:10.000
Python.

02:10.000 --> 02:16.720
So Python as a language lacks any support for multidimensional arrays or number crunching

02:16.720 --> 02:21.680
because it leaves it to packages to be handled.

02:21.680 --> 02:27.680
Python also leaves it to implementations to actually interpret its syntax.

02:27.680 --> 02:32.280
And SciPy, of course, the major domain implementation, but it's not the only one.

02:32.280 --> 02:37.640
And actually, solutions exist that streamline, for example, Just-in-Time compilation of Python

02:37.640 --> 02:38.640
code.

02:38.640 --> 02:46.080
Moreover, Numpy, while de facto standard, is not the implementation of the Numpy API.

02:46.080 --> 02:52.760
And alternatives are embedded in Just-in-Time frameworks, Just-in-Time compilation frameworks,

02:52.760 --> 02:58.200
GPU frameworks for Python, and they leverage typing and concurrency.

02:58.200 --> 03:04.160
So probably here the highlight is that Python lets you glue these technologies together

03:04.160 --> 03:10.240
and package them together, leveraging some of the Python ecosystem and its popularity,

03:10.240 --> 03:11.820
etc.

03:11.820 --> 03:15.360
And probably, arguably, I would say that's an advantage.

03:15.360 --> 03:21.680
I'm not saying that please use Python for HPC instead of Julia, probably vice versa,

03:21.680 --> 03:22.680
actually.

03:22.680 --> 03:26.360
But still, it's an interesting question to see how it can perform.

03:26.360 --> 03:30.080
Okay, so let's check it.

03:30.080 --> 03:36.320
I will present a brief benchmark, a very tiny one that we have come up with in relation

03:36.320 --> 03:37.720
with this project.

03:37.720 --> 03:39.080
And it uses Numba.

03:39.080 --> 03:44.800
Numba is a Just-in-Time compiler that translates a subset of Python and Numpy into machine

03:44.800 --> 03:49.240
code that is compiled at runtime using LLVM.

03:49.240 --> 03:55.560
Okay, so here is the story about the super simple benchmark problem.

03:55.560 --> 03:58.320
It's related to numerical weather prediction.

03:58.320 --> 04:05.640
So you can imagine a grid of numbers representing weather here.

04:05.640 --> 04:09.920
And numerical weather prediction, or part of numerical weather prediction, the integration

04:09.920 --> 04:17.760
part involves solving equations for the hydrodynamics, that is the transport of such pattern in space

04:17.760 --> 04:22.760
and time, and of course, thermodynamics that tell you what's happening in the atmosphere.

04:22.760 --> 04:23.960
Super simplified picture.

04:23.960 --> 04:27.000
I'm not saying that's the whole story about NWP.

04:27.000 --> 04:33.480
But for benchmarking Numba, let's simplify it down to, in this case, two-dimensional

04:33.480 --> 04:34.480
simple problem.

04:34.480 --> 04:37.520
You have a grid, x, y, some signal.

04:37.520 --> 04:43.520
And if we look at just the transport problem, a partial differential equation for transport,

04:43.520 --> 04:49.880
we can see what happens if we move around such signal, which could be some humidity,

04:49.880 --> 04:51.800
temperature, or whatever in the atmosphere.

04:51.800 --> 04:55.440
Okay, so we have a sample problem.

04:55.440 --> 05:01.360
Here I'm showing results from a three-dimensional version of what was just shown.

05:01.360 --> 05:04.560
And let's start with the right-hand side plot.

05:04.560 --> 05:07.120
X axis, the size of the grid.

05:07.120 --> 05:09.840
So if it's eight, it means eight by eight.

05:09.840 --> 05:10.840
Super tiny.

05:10.840 --> 05:14.760
If it's 128, it's 128 by 128 by 128.

05:14.760 --> 05:18.120
And wall time per time step on the y axis.

05:18.120 --> 05:19.120
Green.

05:19.120 --> 05:24.600
C++ implementation of one particular algorithm for this kind of problems.

05:24.600 --> 05:25.600
Orange.

05:25.600 --> 05:27.400
PiMP data.

05:27.400 --> 05:32.480
The same algorithm numerically, but a Python implementation.

05:32.480 --> 05:42.560
So here you see that actually Numba just compiled version outperformed C++, maintaining even

05:42.560 --> 05:48.960
better scaling for the tiny matrices, but they are kind of irrelevant for the problem.

05:48.960 --> 05:53.080
And please note that in both cases we have used multi-threading.

05:53.080 --> 05:57.760
So here on the left-hand side, you can see actually on the x axis number of threads,

05:57.760 --> 06:00.760
y axis wall time per time step.

06:00.760 --> 06:04.080
And again, the green line is the C++ implementation.

06:04.080 --> 06:06.640
These two are two variants of the Python one.

06:06.640 --> 06:08.880
JIT compiled with Numba.

06:08.880 --> 06:11.280
Almost an order of magnitude faster execution.

06:11.280 --> 06:12.720
Five times faster.

06:12.720 --> 06:19.080
And what's probably most interesting for now is that when you compare with just setting

06:19.080 --> 06:26.960
the environment variable for Numba JIT to disabled, we jump more than two orders of

06:26.960 --> 06:31.280
magnitude up in wall time.

06:31.280 --> 06:40.040
So this is how Numba timing compares with plain Python timing.

06:40.040 --> 06:43.300
But there are two important things to be mentioned here.

06:43.300 --> 06:48.040
The Python package is written having Numba in mind.

06:48.040 --> 06:55.120
That is, everything is loop based, which is the reason why plain CPython with Numba performs

06:55.120 --> 06:56.120
badly.

06:56.120 --> 07:01.200
And this line is kind of irrelevant, just as a curiosity.

07:01.200 --> 07:06.320
On the other hand, the C++ version is kind of legacy based on Blitz++ library.

07:06.320 --> 07:10.520
Back then when it was developed, Eigen didn't have support for multiple dimensions.

07:10.520 --> 07:16.840
And it's object oriented processing which was reported and measured to be kind of five

07:16.840 --> 07:21.320
times slower than for some 77 for these kind of small domains.

07:21.320 --> 07:24.200
It's not the same for larger domains.

07:24.200 --> 07:28.680
Anyhow, we can achieve high performance with Python.

07:28.680 --> 07:31.080
But what if we need MPI?

07:31.080 --> 07:35.360
We need message passing in our code.

07:35.360 --> 07:36.960
How would we use it?

07:36.960 --> 07:42.700
Let's say we divide in a domain that can position spirit our domain in two parts.

07:42.700 --> 07:49.440
So the same problem, same setup, just half of the domain is computed by one process or

07:49.440 --> 07:59.200
node or anything that has distributed, has different memory addressing than another worker.

07:59.200 --> 08:00.680
So this is how we want to use it.

08:00.680 --> 08:02.400
Why we want to use MPI?

08:02.400 --> 08:07.640
Well, because despite expansion in parallel computation, both in the number of machines

08:07.640 --> 08:12.400
and the number of cores, no other parallel programming paradigm has replaced MPI.

08:12.400 --> 08:15.000
At least as of 2013.

08:15.000 --> 08:21.320
And already in 2013, people were writing that this is even though it's universally acknowledged

08:21.320 --> 08:24.320
that MPI is a crude way of programming these machines.

08:24.320 --> 08:27.000
Anyhow, still let's try it.

08:27.000 --> 08:29.000
And let's try it with Python.

08:29.000 --> 08:37.120
So here is a seven line snippet of code where we try to import number to get the JIT compilation

08:37.120 --> 08:38.760
of Python code.

08:38.760 --> 08:43.280
And then we use MPI for Py which is Python interface to MPI.

08:43.280 --> 08:44.280
What do we do?

08:44.280 --> 08:49.000
We find some number crunching routine and we try to use MPI from it and then we try

08:49.000 --> 08:51.360
to end JIT.

08:51.360 --> 08:58.840
End JIT means the highest performance variant of number JIT compilation.

08:58.840 --> 09:02.840
We try to JIT compile this function and straight ahead execute it.

09:02.840 --> 09:05.760
What happens?

09:05.760 --> 09:06.760
It doesn't work.

09:06.760 --> 09:14.640
It doesn't compile because Numba cannot determine type of MPI for Py.MPI.intrachom.

09:14.640 --> 09:16.440
Because it's a class.

09:16.440 --> 09:17.920
Classes do not work with Numba.

09:17.920 --> 09:22.120
At least not the ordinary Python classes.

09:22.120 --> 09:24.120
So something doesn't work.

09:24.120 --> 09:29.360
So the problem is that we have Numba which is one of the leading solutions to speed up

09:29.360 --> 09:30.360
Python.

09:30.360 --> 09:36.600
We have MPI which is clearly the standard for distributed memory parallelization.

09:36.600 --> 09:40.480
We try to work with them together but it doesn't work.

09:40.480 --> 09:43.760
So stack overflow.

09:43.760 --> 09:45.920
Let's duck, duck, go it.

09:45.920 --> 09:46.920
Nothing.

09:46.920 --> 09:47.920
Let's quantit.

09:47.920 --> 09:48.920
Nothing.

09:48.920 --> 09:51.480
Wrong search engine, right?

09:51.480 --> 09:54.200
Someone must have solved the problem.

09:54.200 --> 09:55.360
Nothing.

09:55.360 --> 09:58.280
Let's ask Numba guys and MPI for Py guys.

09:58.280 --> 10:00.920
June 2020.

10:00.920 --> 10:03.680
You will not be able to use MPI for Py's Python code.

10:03.680 --> 10:06.040
It was not designed for such low level usage.

10:06.040 --> 10:07.040
Okay.

10:07.040 --> 10:09.520
It's a Python.

10:09.520 --> 10:12.840
But I mean it must be doable, right?

10:12.840 --> 10:16.120
We have two established packages.

10:16.120 --> 10:19.040
The aim is kind of solid and makes sense.

10:19.040 --> 10:22.080
So it must be doable.

10:22.080 --> 10:28.400
And 30 months later, 120 comments later, 50 PRs later from five contributors on a totally

10:28.400 --> 10:33.400
unplanned site project, we are introducing Numba MPI.

10:33.400 --> 10:43.360
It's an open source kind of small Python project which allows you to let's jump here to the

10:43.360 --> 10:49.640
hello world example which allows you to use the Numba and JIT decorator on a function

10:49.640 --> 10:59.280
that involves rank, size or any other MPI API calls within the Python code.

10:59.280 --> 11:05.680
So now we cover size, rank, send, receive, broadcast, the API for Numba MPI is based

11:05.680 --> 11:07.360
on Numba.

11:07.360 --> 11:09.400
We have auto generated documentation.

11:09.400 --> 11:12.480
We are on PyPy and Conda Forge.

11:12.480 --> 11:15.600
Few words about how it's implemented.

11:15.600 --> 11:22.280
Essentially we start with C types built into Python to try to address the C API.

11:22.280 --> 11:26.800
There are some things related with passing addresses, memories, void pointers, et cetera.

11:26.800 --> 11:31.280
Not super interesting.

11:31.280 --> 11:36.600
Probably the key message here is that we are offering the send function that is already

11:36.600 --> 11:41.480
N JITed which means you can use it from other N JITed functions.

11:41.480 --> 11:46.560
We handle non-continuous arrays from Numba MPI so we try to be user friendly.

11:46.560 --> 11:51.120
We then call the underlying C function.

11:51.120 --> 11:53.560
Kind of that's all.

11:53.560 --> 11:57.880
Truly there is the key line number 30.

11:57.880 --> 11:58.880
This one.

11:58.880 --> 12:04.360
Well, that's nothing but in principle without it Numba optimizes out all our code.

12:04.360 --> 12:10.520
Anyhow, these are kind of things that you see when trying to implement such things.

12:10.520 --> 12:14.640
Unfortunately there are quite more of such hacks inside Numba MPI.

12:14.640 --> 12:20.840
The next slide is kind of thing that you prefer to never see but they cannot be unseen in

12:20.840 --> 12:22.760
a way if you work with it.

12:22.760 --> 12:28.960
So please just think of it as a picture of some problems that we have challenged and

12:28.960 --> 12:36.320
essentially wrote to Numba guys asking how can it be done and we got this kind of tools

12:36.320 --> 12:44.840
for handling void pointers from C types in Numba with Python, NumPy, et cetera.

12:44.840 --> 12:49.360
But well, that's you, Tilt Spy, and that's it.

12:49.360 --> 12:50.400
And it kind of works.

12:50.400 --> 12:51.680
And why do we know it works?

12:51.680 --> 12:52.800
Because we test it.

12:52.800 --> 12:58.560
And let me handle the mic to Alexey to tell you more about testing.

12:58.560 --> 13:01.600
Here it is.

13:01.600 --> 13:03.440
Thank you.

13:03.440 --> 13:05.440
Thank you.

13:05.440 --> 13:06.440
Okay.

13:06.440 --> 13:08.640
This works.

13:08.640 --> 13:18.160
So I'm going to tell you about the CI that we have set up for our project for Numba MPI.

13:18.160 --> 13:22.160
So the CI was set up on a...

13:22.160 --> 13:23.160
Sorry.

13:23.160 --> 13:24.160
The wrong arrow.

13:24.160 --> 13:31.920
The CI was set up at GitHub Actions, as I said.

13:31.920 --> 13:34.880
This is the screen of the workflow.

13:34.880 --> 13:40.040
We start from running the PDOC, pre-commit, and Pylint.

13:40.040 --> 13:47.640
PDOC is for automatic documentation generation, Pylint for static content analysis, and pre-commit

13:47.640 --> 13:49.200
for styling.

13:49.200 --> 13:53.200
After that, if these steps were successfully moved into the main part where we run our

13:53.200 --> 13:59.160
unit tests, this is the example...

13:59.160 --> 14:03.480
Not example, but actually the workflow file that we run.

14:03.480 --> 14:10.200
As you can see, when we run the CI against multiple systems, different Pylint versions

14:10.200 --> 14:13.120
and different MPI implementations.

14:13.120 --> 14:19.680
And here we should say a big thank you to MPI for PyTeam for providing setup MPI GitHub

14:19.680 --> 14:26.560
Action, because this has saved us a lot of time.

14:26.560 --> 14:29.240
Thank you MPI for Py.

14:29.240 --> 14:34.820
And as of operation systems and MPI implementations, we are running...

14:34.820 --> 14:41.720
In case of Linux, we are testing against OpenMPI, MPI-CH, and Intel-PI, Mac OS, MPI-MPI-NC,

14:41.720 --> 14:48.440
MPI-CH, and in case of Windows, it's of course MSMPI implementation.

14:48.440 --> 15:01.240
But when we are talking about MPI-CH, there is a problem that has recently occurred, namely

15:01.240 --> 15:07.920
starting from version four of MPI-CH, it fails for on a Ubuntu, on our CI, for PyTeam version

15:07.920 --> 15:10.680
less than 3.10.

15:10.680 --> 15:18.720
So if anyone has ideas how to fix it, please contact us, we will appreciate any help.

15:18.720 --> 15:26.760
Okay, so we are running the unit test on different systems and so on.

15:26.760 --> 15:29.080
Let's see the sample unit test.

15:29.080 --> 15:39.920
In this test, we are testing the logic of the wrapper, of the broadcast function of

15:39.920 --> 15:46.440
MPI, and the main thing that you should remember from this slide is that we are testing this

15:46.440 --> 15:56.440
function in plain Python implementation as well as JIT compiled by number.

15:56.440 --> 16:01.640
We have also set up an integration test.

16:01.640 --> 16:07.760
The integration test is in another project named by SuperDopplet-LES.

16:07.760 --> 16:15.760
And this is just a scheme of this test, we are starting from providing the initial conditions

16:15.760 --> 16:27.120
for the APDS solver, and these initial conditions are written to the HDF5 file.

16:27.120 --> 16:32.080
After that, we are running three runs.

16:32.080 --> 16:36.680
First one we run with only one process, the second one we have two processes, in the third

16:36.680 --> 16:43.160
we run the three processes, and in each we divide, well, in the first we don't divide

16:43.160 --> 16:50.080
the domain, but the other ones we divide the domain accordingly.

16:50.080 --> 16:57.840
And in the assert state we just compare the results and we want the results to be the

16:57.840 --> 17:00.760
same for different runs.

17:00.760 --> 17:10.080
And also these results are also written to HDF5 file.

17:10.080 --> 17:19.360
Interesting fact that everything works on Windows except installing HDF5.py package

17:19.360 --> 17:21.440
for concurrent file access.

17:21.440 --> 17:24.600
HD5.py package with enabled MPI.io.

17:24.600 --> 17:32.520
We have troubles setting up on Windows, but everything else works fine.

17:32.520 --> 17:40.040
And there is also an independent use case, the PyPD project that uses our library, our

17:40.040 --> 17:48.120
package, and it's not developed by us, so the reader is a user.

17:48.120 --> 17:52.480
And this is the Python package for solving partial differential equation, it focuses

17:52.480 --> 18:04.280
on finite differencing and PDEs are defined by, are provided as strings.

18:04.280 --> 18:09.200
And the solution strategy as follows, we start from partitioning the grid onto different

18:09.200 --> 18:18.600
nodes using number MPI after that we pass expressions using the simpy and compile the

18:18.600 --> 18:26.360
results using number and then we trade the PDE exchange in boundary information between

18:26.360 --> 18:31.400
the nodes using number MPI.

18:31.400 --> 18:32.400
Take home messages.

18:32.400 --> 18:38.960
There is a common mismatch between the Python language and Python ecosystem.

18:38.960 --> 18:45.080
We should remember that, okay, the language could be slow, but we also should consider

18:45.080 --> 18:52.640
the ecosystem around this language, the libraries that are available, the libraries that are

18:52.640 --> 18:56.960
available and probably different implementations.

18:56.960 --> 19:03.360
And Python has a range of global HPC solutions such as just in time compilation, GPU programming,

19:03.360 --> 19:06.480
multithreading and MPI.

19:06.480 --> 19:18.640
In case of number MPI, this is the package to glue the MPI with Python code.

19:18.640 --> 19:26.480
It is tested on CI, on GitHub actions.

19:26.480 --> 19:37.160
We are aiming for 100% unit test coverage and also there is also already the project

19:37.160 --> 19:43.280
that are dependent on this package.

19:43.280 --> 19:49.280
Here we can find the links for number MPI, the GitHub links and also the links to the

19:49.280 --> 19:52.240
packages at PyPy and Anaconda.

19:52.240 --> 19:56.280
And we also welcome contributions.

19:56.280 --> 20:03.120
The first two issues I have mentioned earlier and we also welcome encourage to provide the

20:03.120 --> 20:11.520
logo for number MPI as well as adding support for the other functions or we are also aiming

20:11.520 --> 20:21.400
for dropping dependency on MPI for Py in our project and also the plan is to benchmark

20:21.400 --> 20:25.160
performance of this package.

20:25.160 --> 20:27.960
And also we wanted to acknowledge funding.

20:27.960 --> 20:33.080
The project was funded by National Science Centre of Poland.

20:33.080 --> 20:35.080
Thank you for your attention.

20:35.080 --> 20:46.920
We now have time for questions.

20:46.920 --> 20:52.920
Thank you very much.

20:52.920 --> 20:56.960
Any questions?

20:56.960 --> 20:59.280
Question from an MPI expert.

20:59.280 --> 21:00.280
Thank you.

21:00.280 --> 21:01.280
Hello.

21:01.280 --> 21:02.600
Thank you for the talk.

21:02.600 --> 21:09.120
So the interface you are proposing is very close to the let's say C MPI interface.

21:09.120 --> 21:14.160
Let's say when you do a send you work with a buffer or do you try to provide a bit higher

21:14.160 --> 21:22.960
level interface for example serializing some Python object or it could be very useful.

21:22.960 --> 21:34.680
Yeah, the interface is as slim thin as possible probably.

21:34.680 --> 21:36.760
Very close to the C MPI.

21:36.760 --> 21:44.760
One of the reasons being that within Numba and JIT code, things like serialization might

21:44.760 --> 21:48.560
not be that easy to do.

21:48.560 --> 21:53.920
There is no problem in combining MPI for Py and Numba MPI in one code base.

21:53.920 --> 22:00.120
So when you are out of the JIT code you can use MPI for Py which has high level things

22:00.120 --> 22:01.560
like serialization, et cetera.

22:01.560 --> 22:10.200
So you can use it there but within LLVM compiled blocks you can use Numba MPI for simple send

22:10.200 --> 22:13.040
receive already used.

22:13.040 --> 22:16.800
Without higher level array functioning.

22:16.800 --> 22:27.680
Having said that we for example handle transparently noncontiguous slices of arrays, we also yeah

22:27.680 --> 22:33.440
there are some things that are higher level than C interface but in general we try to

22:33.440 --> 22:37.200
provide a wrapper around the C routines.

22:37.200 --> 22:42.760
Okay, thank you.

22:42.760 --> 22:50.040
Any other questions?

22:50.040 --> 22:51.640
Thanks for a great talk.

22:51.640 --> 22:53.720
It seems really interesting what you are working on.

22:53.720 --> 23:00.080
I have a couple of questions probably born out of ignorance but I just kind of wondered

23:00.080 --> 23:01.400
if you could help me with them.

23:01.400 --> 23:08.160
So firstly I was wondering why you went with making a separate package rather than sort

23:08.160 --> 23:14.080
of trying to build this functionality on top of MPI for Py.

23:14.080 --> 23:18.960
Would it have been possible to sort of add this feature of making things JIT compilable

23:18.960 --> 23:21.200
into MPI for Py?

23:21.200 --> 23:26.400
And secondly I was kind of wondering with the MPI I.O. thing that you were looking at

23:26.400 --> 23:34.080
with Windows, if that requires kind of concurrent file access from separate processes in Windows,

23:34.080 --> 23:37.360
is that just a complete no go for Windows?

23:37.360 --> 23:42.200
Because I understand that's something that Windows kernel doesn't support.

23:42.200 --> 23:43.200
Thank you.

23:43.200 --> 23:44.200
Thanks.

23:44.200 --> 23:46.400
Let me start from the second one.

23:46.400 --> 23:54.440
So here our well, essentially it's a fun fact that everything else worked for Windows.

23:54.440 --> 23:59.240
We do not really target Windows but it was nice to observe that all works.

23:59.240 --> 24:06.920
It's kind of one of these advantages of Python that you code and you don't really need to

24:06.920 --> 24:12.400
take too much care about the targeted platforms because the underlying packages are meant

24:12.400 --> 24:14.000
to work on all of them.

24:14.000 --> 24:17.960
And here everything works with Microsoft MPI.

24:17.960 --> 24:24.240
The only thing that actually was a problem for us was to even install H5.py on Windows

24:24.240 --> 24:25.560
with MPI support.

24:25.560 --> 24:32.640
So we don't really know what's the true bottleneck but even the documentation of H5.py suggests

24:32.640 --> 24:35.060
again trying.

24:35.060 --> 24:41.840
For the first question, why do we create, why do we develop a separate package instead

24:41.840 --> 24:45.880
of adding it on top of MPI 4.py?

24:45.880 --> 24:57.040
So I think even on the slide with the story of the package, there was a link to MPI 4.py

24:57.040 --> 25:03.080
issue, the bottom footnote, where we suggested would it be possible to add it?

25:03.080 --> 25:10.680
And in relation with the first question, so probably the scope, the goal of MPI 4.py is

25:10.680 --> 25:20.600
to provide very high level API for MPI in Python.

25:20.600 --> 25:26.440
So with discussing with the developers there, we realized that it's probably not within

25:26.440 --> 25:30.320
the scope of a very high level interface.

25:30.320 --> 25:36.440
So we started off with just, well, small separate project.

25:36.440 --> 25:38.360
But I mean, well, great idea.

25:38.360 --> 25:40.840
It could be glued together.

25:40.840 --> 25:47.040
We aim for dropping dependency on MPI 4.py which we now use just for some utility routine,

25:47.040 --> 25:52.360
not for the communication or nothing that is used by Numba.

25:52.360 --> 25:58.500
And probably that might be an advantage because you can eventually you should be able to install

25:58.500 --> 26:03.000
Numba MPI with very little other dependencies.

26:03.000 --> 26:06.380
And Numba MPI is written purely in Python.

26:06.380 --> 26:12.120
So installing it, you do not need to have any site on related C compiled code and you

26:12.120 --> 26:14.360
can do it quite easily.

26:14.360 --> 26:15.360
Okay.

26:15.360 --> 26:38.440
Thank you very much.
