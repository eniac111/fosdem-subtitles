1
0:00:00.000 --> 0:00:09.680
Okay, we're good to get started with one more MPI talk, but I think it's very different

2
0:00:09.680 --> 0:00:11.480
from compared to the others.

3
0:00:11.480 --> 0:00:12.480
Hopefully.

4
0:00:12.480 --> 0:00:14.480
Compiler-aided MPI correctness check.

5
0:00:14.480 --> 0:00:15.480
Yeah.

6
0:00:15.480 --> 0:00:16.480
Thank you.

7
0:00:16.480 --> 0:00:21.320
So my name is Alexander Hooke, and today I'm going to talk about basically the dynamic

8
0:00:21.320 --> 0:00:23.680
MPI correctness tool, which is called MAST.

9
0:00:23.680 --> 0:00:28.160
And in particular, I'm going to talk about the compiler extension, which is called type

10
0:00:28.160 --> 0:00:34.760
art, which is supposed to help with MPI type correctness checking.

11
0:00:34.760 --> 0:00:41.440
And first of all, as we heard before, the message passing interface is the de facto

12
0:00:41.440 --> 0:00:46.440
standard of distributed computations in the HPC world, right?

13
0:00:46.440 --> 0:00:52.060
And it defines a large set of communication routines and other stuff.

14
0:00:52.060 --> 0:00:58.200
And it's also designed for a heterogeneous cluster system where you have different platforms

15
0:00:58.200 --> 0:01:01.280
that communicate and compute something.

16
0:01:01.280 --> 0:01:07.600
However, in that sense, it's also very low-level interface where you have to specify a lot

17
0:01:07.600 --> 0:01:13.400
of stuff manually, and you can expect only a little error checking in general from the

18
0:01:13.400 --> 0:01:14.880
library itself.

19
0:01:14.880 --> 0:01:22.040
So the user is required for this simple MPI send operation to specify the data, which

20
0:01:22.040 --> 0:01:25.120
is transferred as a typeless void buffer.

21
0:01:25.120 --> 0:01:32.480
The user has to specify its data length of the buffer and the user and the type manually,

22
0:01:32.480 --> 0:01:37.800
and also the message envelope, so the destination of the message and the communicate and stuff

23
0:01:37.800 --> 0:01:39.640
like that has to be specified manually.

24
0:01:39.640 --> 0:01:45.040
So there's a lot of opportunity to commit a mistake, basically.

25
0:01:45.040 --> 0:01:49.640
And this is quite a question to you guys.

26
0:01:49.640 --> 0:01:56.960
If you look at the small code, try to figure out how many errors you can spot in this small

27
0:01:56.960 --> 0:02:02.960
example and just try to look at every corner, basically.

28
0:02:02.960 --> 0:02:08.640
And while I'm talking, I can also spoiler you that I'm going to show you every issue

29
0:02:08.640 --> 0:02:16.760
in this small example in a couple of seconds, so to speak.

30
0:02:16.760 --> 0:02:21.840
When I first looked at it, my colleague, Joachim, showed me I couldn't find the most simple

31
0:02:21.840 --> 0:02:26.760
one that was a bit crazy to me.

32
0:02:26.760 --> 0:02:32.440
Sometimes you don't see the forest in front of the trees.

33
0:02:32.440 --> 0:02:33.760
Okay.

34
0:02:33.760 --> 0:02:38.480
So the most basic one, we don't call MPI in it, right?

35
0:02:38.480 --> 0:02:40.800
That's usually an MPI application.

36
0:02:40.800 --> 0:02:46.680
That's the very first call you're supposed to do where you initialize the MPI environment.

37
0:02:46.680 --> 0:02:51.140
And then likewise, if you look at the end of the program, we do not call MPI finalize.

38
0:02:51.140 --> 0:02:54.040
So those are two simple mistakes.

39
0:02:54.040 --> 0:02:57.040
But then in total, we have eight issues.

40
0:02:57.040 --> 0:03:02.160
I don't know how many you found, and I'm also not going to talk about each one of them.

41
0:03:02.160 --> 0:03:09.920
But it's quite easy to, if you look at each individual issue, to kind of guess that it's

42
0:03:09.920 --> 0:03:13.680
... yeah, it can happen to you also.

43
0:03:13.680 --> 0:03:17.000
And those are the pointers where they are.

44
0:03:17.000 --> 0:03:22.880
And in particular, I want to talk about the receive-receive deadlock, where, for instance,

45
0:03:22.880 --> 0:03:28.920
two process waits on each other without being able to continue.

46
0:03:28.920 --> 0:03:33.400
You can argue that all those issues, except maybe the deadlock, could be found by the

47
0:03:33.400 --> 0:03:37.200
MPI library itself, but typically on HPC systems.

48
0:03:37.200 --> 0:03:41.120
The library does not do any checking for performance reasons.

49
0:03:41.120 --> 0:03:48.800
That's why many of these issues will not be... will cause maybe crashes for unknown reasons

50
0:03:48.800 --> 0:03:53.200
or just produce some strange results.

51
0:03:53.200 --> 0:04:01.000
Well, that's why the dynamic MPI correctness tool must have been developed in the past,

52
0:04:01.000 --> 0:04:08.360
which is a tool that, during one-time checks for issues and produces such reports where

53
0:04:08.360 --> 0:04:09.360
it finds some issues.

54
0:04:09.360 --> 0:04:16.200
And this is a report of the deadlock we have seen in the example code, where the message

55
0:04:16.200 --> 0:04:18.320
itself just describes there's a deadlock.

56
0:04:18.320 --> 0:04:28.080
In the bottom left, you can see a wait for graph, which just shows you which rank waits

57
0:04:28.080 --> 0:04:31.440
for another rank causing the deadlock.

58
0:04:31.440 --> 0:04:38.400
This helps you to kind of see where the deadlock occurs and why it occurs, and also, must can

59
0:04:38.400 --> 0:04:43.880
produce so-called call-second information, where you can see from main, beginning from

60
0:04:43.880 --> 0:04:49.200
main of the program to the basically origin of the deadlock.

61
0:04:49.200 --> 0:04:51.400
But this was omitted now.

62
0:04:51.400 --> 0:04:52.560
Okay.

63
0:04:52.560 --> 0:05:00.800
So to facilitate correctness checking for MPI, must uses a so-called distributed agent-based

64
0:05:00.800 --> 0:05:05.240
analysis, which means that you have your normal MPI application with four ranks, four processes

65
0:05:05.240 --> 0:05:10.320
that communicate as you would expect as the user wrote it.

66
0:05:10.320 --> 0:05:17.480
But must will also create an analysis network, which helps you to do local analysis, helps

67
0:05:17.480 --> 0:05:18.920
you to do distributed analysis.

68
0:05:18.920 --> 0:05:24.000
If you think about a deadlock, you need information for more than one process to figure out that

69
0:05:24.000 --> 0:05:26.440
there occurred a deadlock in your program.

70
0:05:26.440 --> 0:05:29.880
So must creates that completely transparent to the user.

71
0:05:29.880 --> 0:05:36.120
So you would use MPI, CommWorld, and any other communicator as normal.

72
0:05:36.120 --> 0:05:39.160
Must takes care of creating such a network.

73
0:05:39.160 --> 0:05:47.520
And also, what is maybe the focus of the talk today is the local analysis, where we look

74
0:05:47.520 --> 0:05:50.000
at process local checks.

75
0:05:50.000 --> 0:05:55.000
If you think about MPI type correctness of a cent operation, you can do a lot of stuff

76
0:05:55.000 --> 0:06:01.000
locally, or should do a lot of stuff locally, and this is the focus.

77
0:06:01.000 --> 0:06:06.240
So MPI type correctness, we focus basically on the buffer and the user-specified length

78
0:06:06.240 --> 0:06:12.400
and the user-specified MPI data type today.

79
0:06:12.400 --> 0:06:18.000
Must can already detect mismatches of, for instance, the send and receive communication

80
0:06:18.000 --> 0:06:24.280
pair, where must basically creates a so-called type map.

81
0:06:24.280 --> 0:06:29.160
It looks at the user-specified buffer size and the user-specified data type, and compares

82
0:06:29.160 --> 0:06:31.400
it to the corresponding receive operation.

83
0:06:31.400 --> 0:06:36.880
And if there's a mismatch, obviously there is going to be an issue, and must creates

84
0:06:36.880 --> 0:06:40.320
a report about that.

85
0:06:40.320 --> 0:06:44.360
This also, of course, works for collective communications, where you can make sure that

86
0:06:44.360 --> 0:06:53.160
all ranks call, for instance, a broadcast operation with the same data type.

87
0:06:53.160 --> 0:07:00.560
However, since must only intercepts MPI communication calls, MPI calls in general, it cannot look

88
0:07:00.560 --> 0:07:05.880
behind, like it cannot look what happens in user space, you know.

89
0:07:05.880 --> 0:07:11.840
So we cannot reason about the type of the void buffer data.

90
0:07:11.840 --> 0:07:20.000
And this is why we were motivated to create the tool type art, which is something that

91
0:07:20.000 --> 0:07:25.960
helps with basically figuring out what the memory location is that you put into your

92
0:07:25.960 --> 0:07:26.960
MPI calls.

93
0:07:26.960 --> 0:07:33.440
So, if you look at this small example on the right side, completely processed locally,

94
0:07:33.440 --> 0:07:36.640
there might be some memory allocation in that example.

95
0:07:36.640 --> 0:07:40.720
It's a double buffer that was allocated by malloc, let's say.

96
0:07:40.720 --> 0:07:48.080
And the question now becomes, how can we make sure that data, the data buffer, which is

97
0:07:48.080 --> 0:07:54.240
a void buffer, fits the user-specified buffer size, so is it off-length buffer size, and

98
0:07:54.240 --> 0:07:58.520
it also should be compatible with the MPI float type.

99
0:07:58.520 --> 0:08:04.680
And of course, we can already see that double and MPI float, there's a type mismatch, but

100
0:08:04.680 --> 0:08:11.120
must cannot answer such a question without further tooling, because it just intercepts

101
0:08:11.120 --> 0:08:12.920
MPI calls.

102
0:08:12.920 --> 0:08:13.920
Okay.

103
0:08:13.920 --> 0:08:19.520
So, to just show you that it's not an academic example, there's two well-known HPC benchmark

104
0:08:19.520 --> 0:08:27.320
codes which have some issues, so one was reported in the past by others where there's a broadcast

105
0:08:27.320 --> 0:08:32.160
operation, it uses a big int, which is an alias for a 64-bit data type.

106
0:08:32.160 --> 0:08:38.640
However, the user-specified an MPI int, which is a 32-bit data type for the broadcast operation,

107
0:08:38.640 --> 0:08:41.240
so there's an obvious mismatch.

108
0:08:41.240 --> 0:08:43.300
That could be a problem, likely.

109
0:08:43.300 --> 0:08:50.880
And also, for milk, there's an all-reduce operation where the user's passed in a struct

110
0:08:50.880 --> 0:08:57.480
with two float members, and it's interpreted as a float array of size two, which is benign,

111
0:08:57.480 --> 0:09:02.320
to be honest, but that could be a portability issue in the future, maybe.

112
0:09:02.320 --> 0:09:06.680
Depending on the platform, maybe there's padding or whatnot, maybe it's an illegal operation,

113
0:09:06.680 --> 0:09:10.200
so this could also be an issue in the future.

114
0:09:10.200 --> 0:09:15.480
Well, from a high-level point of view, how it does, it must work.

115
0:09:15.480 --> 0:09:21.240
Well, you have your MPI application, and during runtime, it intercepts all the MPI calls and

116
0:09:21.240 --> 0:09:26.400
collects all the states that it's needed for deadlock detection and so on.

117
0:09:26.400 --> 0:09:32.240
And we added type art, which looks at all those allocations that are passed to MPI calls

118
0:09:32.240 --> 0:09:38.760
for those local analysis of buffers, which is the compiler extension based on LLVM, so

119
0:09:38.760 --> 0:09:46.000
you compile your code with our extension, and the extension instruments all allocations,

120
0:09:46.000 --> 0:09:52.260
be it stack, be it heap, which are related to MPI calls, and we also provide a runtime,

121
0:09:52.260 --> 0:09:58.600
so during runtime, we get callbacks of the target application, all allocations, all three

122
0:09:58.600 --> 0:10:06.000
operations, so we have a state of the allocation of the memory, basically, in a target code.

123
0:10:06.000 --> 0:10:11.640
We also, of course, look at the allocations and pass out their type, so simple cases buffer,

124
0:10:11.640 --> 0:10:16.280
A is a double type, more complex cases would be structs or classes.

125
0:10:16.280 --> 0:10:22.080
We pass the serialized type information to our runtime, which then enables, of course,

126
0:10:22.080 --> 0:10:27.440
must to make queries, so for instance, for an MPI center operation, we give the type

127
0:10:27.440 --> 0:10:32.880
art runtime the buffer, the typeless buffer, and the runtime would return all the necessary

128
0:10:32.880 --> 0:10:38.120
type information to ensure type correctness of those buffer handles.

129
0:10:38.120 --> 0:10:40.960
This is the whole high level process behind it.

130
0:10:40.960 --> 0:10:49.460
And then if you take a look at an example of memory allocation, here's a small heap

131
0:10:49.460 --> 0:10:56.280
allocation of a float array, this all happens in LLVM IR, I'm just showing C-like code to

132
0:10:56.280 --> 0:11:04.120
make it easier to understand, we would add such a type art alloc callback, where we need

133
0:11:04.120 --> 0:11:10.140
the data pointer, of course, and then we need a so-called type ID, it's just a representation

134
0:11:10.140 --> 0:11:16.440
of what we allocated, that is later used for type checking, and of course we need the dynamic

135
0:11:16.440 --> 0:11:23.520
length of the allocated array to reason about where we are in the memory space, so to speak.

136
0:11:23.520 --> 0:11:28.600
Otherwise we handle stack and global allocations, for stack allocations, of course, we have

137
0:11:28.600 --> 0:11:34.080
to respect the automatic scope dependent lifetime properties, and for globals we just register

138
0:11:34.080 --> 0:11:40.040
once and then it exists at our runtime for the whole program duration.

139
0:11:40.040 --> 0:11:45.960
And of course, for performance reasons, you can imagine that the less callbacks the better,

140
0:11:45.960 --> 0:11:50.880
hence we try to filter out allocations where we can prove that I'm never part of an MPI

141
0:11:50.880 --> 0:11:56.840
call, and just never instrument those.

142
0:11:56.840 --> 0:12:06.200
This is basically possible on LLVM IR by data flow analysis, so in the function foo we have

143
0:12:06.200 --> 0:12:10.400
two stack allocations, and then we try to follow the data flow where we can see that

144
0:12:10.400 --> 0:12:16.280
A is passed to bar, and inside bar there's never any MPI call, so we can just say, okay,

145
0:12:16.280 --> 0:12:21.840
we do not need to instrument this, this is discarded.

146
0:12:21.840 --> 0:12:28.480
Likewise for FUBAR we can see that B is passed, if it's in another translation unit we would

147
0:12:28.480 --> 0:12:37.160
need to have a whole program view of the program, which we support, but other tools have to

148
0:12:37.160 --> 0:12:41.720
create such a call graph with those required information.

149
0:12:41.720 --> 0:12:48.440
Anyways, so also if we had this view, we can see FUBAR also does not call MPI, so both

150
0:12:48.440 --> 0:12:54.600
stack allocations don't need to be instrumented, which helps a lot with the performance.

151
0:12:54.600 --> 0:13:04.480
Okay, so the type ID, which is passed to the runtime for identification works as follows,

152
0:13:04.480 --> 0:13:10.120
built-in types are obviously known a priori, so we know the type layout, flow is four bytes,

153
0:13:10.120 --> 0:13:13.320
the tabular is eight bytes, depending on platform of course.

154
0:13:13.320 --> 0:13:20.560
For user defined types, which means structs, classes, and so on, we basically serialize

155
0:13:20.560 --> 0:13:26.480
it to a YAML file, and the corresponding type ID of course, so we can match those during

156
0:13:26.480 --> 0:13:34.080
runtime where we have the extent, how many members, offsets, byte offsets basically from

157
0:13:34.080 --> 0:13:40.320
the beginning of the struct, and also the subtypes all listed, which can then be used

158
0:13:40.320 --> 0:13:48.560
for making type queries about the layout and stuff like that.

159
0:13:48.560 --> 0:13:54.880
And then of course, must needs to have some API to figure out type correctness, and this

160
0:13:54.880 --> 0:14:05.120
is provided by our runtime, which has quite a few API functions, the most basic one would

161
0:14:05.120 --> 0:14:12.280
be this type ID, get type, where you put in the MPI buffer handle, and what we put out

162
0:14:12.280 --> 0:14:18.040
is the type ID and the error length, and then you can use the type ID subsequently, for

163
0:14:18.040 --> 0:14:23.320
instance in this call, where you put in the type ID and you get out the struct layout

164
0:14:23.320 --> 0:14:33.480
I just mentioned earlier, and this way you can assemble some iterative type checking,

165
0:14:33.480 --> 0:14:38.680
which is done in must.

166
0:14:38.680 --> 0:14:46.560
And then putting it all together, if you want to use our tooling, you would need to first

167
0:14:46.560 --> 0:14:54.480
of all compile your program with our provided compiler wrapper, which is a bash script,

168
0:14:54.480 --> 0:15:00.960
and does the bookkeeping required to introduce the instrumentation, the type out stuff, so

169
0:15:00.960 --> 0:15:04.880
you exchange your compiler, that's the first step, it's optional, you don't have to do

170
0:15:04.880 --> 0:15:11.640
it if you don't need this local type out checking, and then you would also need to replace your

171
0:15:11.640 --> 0:15:18.240
MPI exec or MPI run, depending on the system, with the must run, which also does some bookkeeping

172
0:15:18.240 --> 0:15:27.640
for must to execute the target code appropriately, spawn all the analysis agent based networking

173
0:15:27.640 --> 0:15:35.760
and so on, and then the program runs as normal, and must output file is generated with all

174
0:15:35.760 --> 0:15:44.920
issues found during execution of your program, and as a side note maybe, as I said, must

175
0:15:44.920 --> 0:15:49.400
does this agent based network, and in the most simple case for the distributed analysis,

176
0:15:49.400 --> 0:15:56.440
there's an additional process needed for the deadlock detection and so on, so for SLURM

177
0:15:56.440 --> 0:16:03.000
or whatnot you need to allocate an additional process, however you don't need to specify

178
0:16:03.000 --> 0:16:08.400
it in the must run stuff, it happens automatically in the background.

179
0:16:08.400 --> 0:16:16.160
Alright so that's it, if you look now at what the impact is of our tooling, well that's

180
0:16:16.160 --> 0:16:22.640
quite dependent as I kind of alluded to, how many callbacks you have, how many memory allocations

181
0:16:22.640 --> 0:16:28.840
you actually have to track, and how good we are at filtering them, so here's two examples,

182
0:16:28.840 --> 0:16:36.400
Lulish and Tachyon, which are again quite well known HPC benchmarking codes, and Lulish

183
0:16:36.400 --> 0:16:44.120
is quite favorable for our presentation because there's not many callbacks and hence our runtime

184
0:16:44.120 --> 0:16:52.680
impact is like quite non-existent so to speak, where you can see that this is compared to

185
0:16:52.680 --> 0:16:58.680
vanilla without any instrumentation, without our tooling, type-out almost has no impact,

186
0:16:58.680 --> 0:17:07.640
and then must and must with type-out analysis enabled has yeah almost no additional impact.

187
0:17:07.640 --> 0:17:11.880
For Tachyon the picture looks quite different as you can see, there's an overhead factor

188
0:17:11.880 --> 0:17:20.240
of about three using when you introduce type-out, this is because there's a lot of stack allocations

189
0:17:20.240 --> 0:17:25.400
that cannot filter, so we track a lot of stack allocations and the runtime impact is quite

190
0:17:25.400 --> 0:17:33.680
high, and this is reflected by those runtime and static instrumentation numbers, so first

191
0:17:33.680 --> 0:17:41.040
of all the above table here shows you during compilation what we instrument, so you can

192
0:17:41.040 --> 0:17:47.240
see that there are some heap free operations that we find an instrument, there's some stack

193
0:17:47.240 --> 0:17:53.960
allocation in the globals that we instrument, well of course those numbers do not represent

194
0:17:53.960 --> 0:18:00.400
the runtime numbers because heap and free operations sometimes are written in a way

195
0:18:00.400 --> 0:18:06.520
that they are like centralized in the program, that's why those numbers are not as high as

196
0:18:06.520 --> 0:18:08.120
you would expect.

197
0:18:08.120 --> 0:18:17.560
For stack allocations we find 54 and out of those 54 we can filter for Lulash at least

198
0:18:17.560 --> 0:18:23.680
21%, and globals are much easier to follow along the data flow in LLVM IR, so we can

199
0:18:23.680 --> 0:18:28.120
filter much more and much more effectively.

200
0:18:28.120 --> 0:18:33.640
Going to the runtime numbers which means that those are basically the number of callbacks

201
0:18:33.640 --> 0:18:43.280
that happen during our benchmarking, we can already see that the high overhead of which

202
0:18:43.280 --> 0:18:51.320
we observed in Taryon is to be explained by the almost 80 million stack allocation callbacks

203
0:18:51.320 --> 0:18:56.160
basically that we have to track during runtime, which is a lot of context switching and so

204
0:18:56.160 --> 0:19:03.920
on which is not good for the runtime.

205
0:19:03.920 --> 0:19:07.920
Alright, so this is already my conclusion.

206
0:19:07.920 --> 0:19:14.400
What we have done is basically with type art, must can now check all phases of the MPI communication

207
0:19:14.400 --> 0:19:16.240
with respect to type correctness.

208
0:19:16.240 --> 0:19:23.080
So the first phase that must can already do is this one, which is basically the message

209
0:19:23.080 --> 0:19:28.360
transfer that is checked against, however there is also the phase of message assembly

210
0:19:28.360 --> 0:19:34.920
where you go from the user process into the MPI process and you have to check this, and

211
0:19:34.920 --> 0:19:40.440
of course if you think about it you would also have to check the message disassembly

212
0:19:40.440 --> 0:19:48.400
where you go from the received data to your user program again.

213
0:19:48.400 --> 0:19:56.120
So type art enables these kind of local checks to ensure type correctness.

214
0:19:56.120 --> 0:20:05.600
Thank you very much.

215
0:20:05.600 --> 0:20:12.320
Any questions?

216
0:20:12.320 --> 0:20:13.320
Getting my exercise.

217
0:20:13.320 --> 0:20:26.760
Yeah, so I really liked your talk, I thought it was really interesting.

218
0:20:26.760 --> 0:20:31.280
So one thing I wanted to ask was like how does one get must?

219
0:20:31.280 --> 0:20:35.480
How do they install it?

220
0:20:35.480 --> 0:20:39.040
Is it available for distribution package managers or is it more that you have to compile it

221
0:20:39.040 --> 0:20:40.040
yourself?

222
0:20:40.040 --> 0:20:48.000
Good question, I think you have to compile it yourself, even on our HPC system.

223
0:20:48.000 --> 0:20:53.600
But it's not that tedious to compile, I think, maybe I'm biased.

224
0:20:53.600 --> 0:20:59.640
But just go to the website and there's a zip file, it includes every dependency that you

225
0:20:59.640 --> 0:21:05.040
need and I think the documentation is quite straightforward.

226
0:21:05.040 --> 0:21:10.820
You need of course maybe open MPI installed but not much more to be honest and then you

227
0:21:10.820 --> 0:21:14.000
should be good to go.

228
0:21:14.000 --> 0:21:21.240
I think it's CMAQ based, I don't know if you have problems with that but it should be straightforward

229
0:21:21.240 --> 0:21:22.840
to try it out.

230
0:21:22.840 --> 0:21:23.840
Thank you.

231
0:21:23.840 --> 0:21:26.560
Okay, another question there.

232
0:21:26.560 --> 0:21:30.760
On my way.

233
0:21:30.760 --> 0:21:45.960
So on the type analysis that you do, I mean if you look at malloc and it has like a typecast

234
0:21:45.960 --> 0:21:50.040
then you know what the type is but if it doesn't have a typecast, if you malloc into a void

235
0:21:50.040 --> 0:21:54.400
pointer and if the amount of bytes you are locating comes from some constant or macro

236
0:21:54.400 --> 0:22:00.240
or some argument, how far do you follow and if you can't see it, do you have a warning,

237
0:22:00.240 --> 0:22:03.080
do you crash?

238
0:22:03.080 --> 0:22:07.560
That's a good question and that's basically a fundamental problem, right?

239
0:22:07.560 --> 0:22:12.480
So we have to have some expectations of the program, right?

240
0:22:12.480 --> 0:22:21.360
So our expectation is that the malloc calls are typed, otherwise we would just track it

241
0:22:21.360 --> 0:22:24.840
as a chunk of bytes.

242
0:22:24.840 --> 0:22:35.520
And I think our analysis is quite forgiving so we would just look at okay, this is a chunk

243
0:22:35.520 --> 0:22:40.480
of bytes, it fits the buffer and this is fine.

244
0:22:40.480 --> 0:22:46.960
And miners beginning is inside and also if it's like aligned to the beginning of an element,

245
0:22:46.960 --> 0:22:47.960
right?

246
0:22:47.960 --> 0:22:49.840
Yes, you kind of lose that, right?

247
0:22:49.840 --> 0:22:57.160
If you just know it's a chunk of bytes then you kind of lose the alignment checks because

248
0:22:57.160 --> 0:23:05.920
if you have a malloc extract and then you do some pointer magic for your MPI buffer

249
0:23:05.920 --> 0:23:13.280
and you point between members in the padding area, only if typeart knows about the malloc

250
0:23:13.280 --> 0:23:20.200
extract it can of course warn that you are doing some illegal memory operations.

251
0:23:20.200 --> 0:23:31.160
If we just see a void pointer due to the typeless malloc then we have lost basically.

252
0:23:31.160 --> 0:23:33.160
Anyone else?

253
0:23:33.160 --> 0:23:41.240
Do you have any thoughts on using Rust which is a way more memory safe language than C

254
0:23:41.240 --> 0:23:42.240
and C++?

255
0:23:42.240 --> 0:23:44.760
Have you looked at it?

256
0:23:44.760 --> 0:23:48.960
Not really, not really, not yet.

257
0:23:48.960 --> 0:23:55.200
For now we have so much to do with the C and C++ world to support typing better, you know,

258
0:23:55.200 --> 0:24:01.920
to get more robustness and so on and not yet to be honest.

259
0:24:01.920 --> 0:24:04.920
Maybe all that work becomes irrelevant if Rust gets popular enough.

260
0:24:04.920 --> 0:24:11.800
I think in general maybe I'm completely like a newbie when it comes to Rust.

261
0:24:11.800 --> 0:24:16.040
I think the MPI support itself is still in the works.

262
0:24:16.040 --> 0:24:24.240
I read some papers about like generating bindings for MPI which are inherently type safe, not

263
0:24:24.240 --> 0:24:30.400
sure how that goes.

264
0:24:30.400 --> 0:24:36.360
I think everyone would be happy if Rust or some other type safe language becomes more

265
0:24:36.360 --> 0:24:40.320
used by people and then this kind of work is irrelevant.

266
0:24:40.320 --> 0:24:44.800
But while people still use C++ this is very relevant.

267
0:24:44.800 --> 0:24:49.760
That pays my bids.

268
0:24:49.760 --> 0:24:50.760
Thank you very much.

269
0:24:50.760 --> 0:25:15.360
Thank you.

