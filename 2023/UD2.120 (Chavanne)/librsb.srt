1
0:00:00.000 --> 0:00:16.880
We're going to start the next talk.

2
0:00:16.880 --> 0:00:19.080
If you're going to stay in the room, please take a seat.

3
0:00:19.080 --> 0:00:26.760
If you want to leave, please leave.

4
0:00:26.760 --> 0:00:38.120
Okay, so the next speaker is Mikaeli, who's going to talk about a universal sparse blast

5
0:00:38.120 --> 0:00:41.120
library.

6
0:00:41.120 --> 0:00:48.520
So, yes.

7
0:00:48.520 --> 0:00:55.520
At the core of many technical or scientific computing problems, we end up with reduced

8
0:00:55.520 --> 0:01:00.880
the problem to solving a system of linear equations.

9
0:01:00.880 --> 0:01:07.160
If the system of linear equations were a simple one, like a two by two one, the method of

10
0:01:07.160 --> 0:01:09.800
solving would be pretty simple.

11
0:01:09.800 --> 0:01:21.560
And in any case, it would involve representing the linear systems by the data structure of

12
0:01:21.560 --> 0:01:29.600
a matrix, so a table of symbols, or usually numbers, and a few vectors of numbers.

13
0:01:29.600 --> 0:01:35.240
So the matrix is the basic structure of scientific computing.

14
0:01:35.240 --> 0:01:44.520
In the case of such toys problems or school problems, we have exact direct solutions at

15
0:01:44.520 --> 0:01:48.680
our disposal, which works fine.

16
0:01:48.680 --> 0:01:58.080
However, once we go into the problems involving simulation of larger domains, so engineering

17
0:01:58.080 --> 0:02:08.680
problems, those linear systems to be solved become large, and also the methods that we

18
0:02:08.680 --> 0:02:14.440
use for smaller systems are not applicable here anymore, because the numerical stability

19
0:02:14.440 --> 0:02:21.720
of, let's say, toy problems or small problems, the stability is not here anymore.

20
0:02:21.720 --> 0:02:27.200
Simply with those methods, numbers, results, diverge.

21
0:02:27.200 --> 0:02:32.280
And the time to solution also increases more than exponentially.

22
0:02:32.280 --> 0:02:35.720
So they're simply infeasible and don't make sense.

23
0:02:35.720 --> 0:02:44.480
So it's a different, it was completely different techniques for large linear systems.

24
0:02:44.480 --> 0:02:56.040
So furthermore, if the systems were not only large, but also full of zeros in the matrices,

25
0:02:56.040 --> 0:03:01.360
so how do we call the systems, or how do we, what do we have to do it here?

26
0:03:01.360 --> 0:03:06.080
We have to do perhaps with sparse systems or sparse problems.

27
0:03:06.080 --> 0:03:08.260
This is the way we call it.

28
0:03:08.260 --> 0:03:17.000
So in this acoustics matrix, or matrix coming from acoustics, we observe that less than

29
0:03:17.000 --> 0:03:25.140
a half percent of each row on the average has a non-zero element.

30
0:03:25.140 --> 0:03:34.120
So we would call this sparse system, perhaps, so the system coming from this matrix.

31
0:03:34.120 --> 0:03:43.320
Indeed, usually we are happy with the definition of Jim Wilkinson, where we say a problem or

32
0:03:43.320 --> 0:03:45.840
a matrix is sparse.

33
0:03:45.840 --> 0:03:53.800
If we can, with our technology, which are technique, we can make use of the amount of

34
0:03:53.800 --> 0:03:56.720
zeros there to our advantage.

35
0:03:56.720 --> 0:03:57.720
So this is the definition.

36
0:03:57.720 --> 0:03:58.720
It's not about numbers.

37
0:03:58.720 --> 0:04:04.400
It's really about what we are able to do with the way the matrix looks like.

38
0:04:04.400 --> 0:04:13.240
So among the different matrices we can encounter, we could have matrices from a circuit simulation

39
0:04:13.240 --> 0:04:20.000
which look like this and have such clustered elements in them.

40
0:04:20.000 --> 0:04:26.040
Sometimes the elements are more clustered around the diagonal, like in this quantum

41
0:04:26.040 --> 0:04:30.640
chromodynamics matrix.

42
0:04:30.640 --> 0:04:34.880
Computational flow dynamics matrices are a bit more regular, you could say.

43
0:04:34.880 --> 0:04:39.560
So it means that you can exploit all of those matrices perhaps in different ways.

44
0:04:39.560 --> 0:04:41.600
This is why I'm showing you this gallery.

45
0:04:41.600 --> 0:04:49.980
This is another CFD, so computational flow dynamics matrix, a structural matrix, another

46
0:04:49.980 --> 0:04:54.520
material problem matrix, structural, and so on.

47
0:04:54.520 --> 0:04:56.040
This is also CFD1.

48
0:04:56.040 --> 0:05:04.400
So this was just to tell you that sparsity really is related to the technology we use

49
0:05:04.400 --> 0:05:07.500
to deal with it.

50
0:05:07.500 --> 0:05:14.160
So usually we are happy using iterative methods with sparse systems.

51
0:05:14.160 --> 0:05:20.560
Iterative methods because something is being iterated, so there is a loop, and with the

52
0:05:20.560 --> 0:05:28.000
most common methods, Krilov methods, the loop usually has a bottleneck, has a core operation,

53
0:05:28.000 --> 0:05:36.560
which is prominently multiplying the system matrix by one vector or many vectors.

54
0:05:36.560 --> 0:05:38.480
Depends a bit on the technique.

55
0:05:38.480 --> 0:05:40.360
There are several of them.

56
0:05:40.360 --> 0:05:46.600
Some of them showing a new octave implementation of such one iterative method.

57
0:05:46.600 --> 0:05:50.760
So there are two kernel operations or main operations.

58
0:05:50.760 --> 0:05:56.680
Multiplication of the matrix by many vectors or let's say another dense matrix, or the

59
0:05:56.680 --> 0:06:03.680
triangular solve, so the solving of matrices which are called preconditioner matrices,

60
0:06:03.680 --> 0:06:05.040
but are sparse.

61
0:06:05.040 --> 0:06:11.880
And these are the core operations which we are interested in, and I want to mention that

62
0:06:11.880 --> 0:06:20.240
those operations for the sparse matrix vector or multi-vector operation can have many variants.

63
0:06:20.240 --> 0:06:29.320
The variants can be on the matrix, which could be perhaps complex and hermitian and all symmetric.

64
0:06:29.320 --> 0:06:31.760
It doesn't have always to be square.

65
0:06:31.760 --> 0:06:38.800
It could be any rectangular, and perhaps it has already a unit diagonal, and we can exploit

66
0:06:38.800 --> 0:06:39.800
this.

67
0:06:39.800 --> 0:06:43.040
This is what I'm saying this.

68
0:06:43.040 --> 0:06:50.680
Many things change if the matrix has a different, has a complex number, so long complex numbers

69
0:06:50.680 --> 0:06:54.880
like a speaker before me spoke about.

70
0:06:54.880 --> 0:06:59.800
So that changes the balances in the performance profile here.

71
0:06:59.800 --> 0:07:01.160
And other things might change.

72
0:07:01.160 --> 0:07:06.040
And all of this have influence on the specific kernels.

73
0:07:06.040 --> 0:07:11.360
And if you think like Ludovic has spoken about the different variants that one might want

74
0:07:11.360 --> 0:07:16.520
to build over different architectures, you see that this is, you end up with code bloat

75
0:07:16.520 --> 0:07:20.440
if you really want to optimize each subcase.

76
0:07:20.440 --> 0:07:24.240
Also the operands have their own variants.

77
0:07:24.240 --> 0:07:28.840
So in the way the data are laid in the dense matrix.

78
0:07:28.840 --> 0:07:29.840
Yeah.

79
0:07:29.840 --> 0:07:37.040
And similarly for the triangular solve operation, there also you have different variants, which

80
0:07:37.040 --> 0:07:42.920
leads to a multitude of different kernels or ways you wish to write them, kernels of

81
0:07:42.920 --> 0:07:44.800
code.

82
0:07:44.800 --> 0:07:51.880
So this leads to a committee of people, end of the 90s, to meet together, mostly US people,

83
0:07:51.880 --> 0:08:00.880
but also with delegations from Europe, to standardize an API, which they called sparse

84
0:08:00.880 --> 0:08:07.960
blast, sparse basic linear algebra subroutines, to somehow just give an API to the different

85
0:08:07.960 --> 0:08:11.920
variations of the operations that I spoke about.

86
0:08:11.920 --> 0:08:15.560
It's mostly, it's not like full blast, if you know the dense blast, it's mostly about

87
0:08:15.560 --> 0:08:20.960
creating sparse matrices, destroying them, and doing a few operations, not only those

88
0:08:20.960 --> 0:08:24.040
but these are really the core operations.

89
0:08:24.040 --> 0:08:30.320
And they talked about C and Fortran because 20 years ago, 20 something years ago was the

90
0:08:30.320 --> 0:08:33.680
final document which they finalized.

91
0:08:33.680 --> 0:08:39.920
Now after 20 years we could say that, well, what they've wrote, especially in my opinion,

92
0:08:39.920 --> 0:08:47.640
is perfectly portable, allows some parallelization even if it's not specified at all.

93
0:08:47.640 --> 0:08:52.000
They didn't foresee extensions but it's possible.

94
0:08:52.000 --> 0:08:59.520
If you look at the API, you see that you can have extensions, so they're not blocked somehow.

95
0:08:59.520 --> 0:09:05.080
The namesake of sparse blast has been copied by every major vendor you can imagine.

96
0:09:05.080 --> 0:09:10.560
The set thing that each major vendor has completely violated their API, so they changed something

97
0:09:10.560 --> 0:09:15.200
in a slightly incompatible way, which is sad, it's simply sad.

98
0:09:15.200 --> 0:09:20.200
And the original sparse blast didn't think about the GPUs but actually in my experience

99
0:09:20.200 --> 0:09:25.440
looking at how people program codes, I see so much technical depth that I think you can

100
0:09:25.440 --> 0:09:34.720
do compromises and with small adaptions you could adapt the sparse blast to the GPU computations

101
0:09:34.720 --> 0:09:36.600
to some extent.

102
0:09:36.600 --> 0:09:40.600
So I think you can save this API to a good extent.

103
0:09:40.600 --> 0:09:44.220
And this is the reason why I'm here.

104
0:09:44.220 --> 0:09:49.080
So I wrote a library which respects the original sparse blast.

105
0:09:49.080 --> 0:09:55.040
So I see sparse blast program can look like this where you have annotation for the sparse

106
0:09:55.040 --> 0:10:02.000
blast operations, which is logical if you know a bit, so you can understand it a bit.

107
0:10:02.000 --> 0:10:09.180
And going in the direction of my library, it's centered around a data format, a sparse

108
0:10:09.180 --> 0:10:15.760
matrix format which I came up with, it's called recursive sparse blocks, because there is

109
0:10:15.760 --> 0:10:20.720
a recursive sub division, there are blocks which are sparse.

110
0:10:20.720 --> 0:10:27.080
And the reason, the motivation for this data structure is to not exclude the sparse blast

111
0:10:27.080 --> 0:10:28.080
operations.

112
0:10:28.080 --> 0:10:32.960
So I have made compromises in order to allow sparse blast operations to be there.

113
0:10:32.960 --> 0:10:36.680
I didn't want to preclude these operations.

114
0:10:36.680 --> 0:10:39.440
So it's a compromise.

115
0:10:39.440 --> 0:10:46.980
And the core idea here is to have, let's say, cache size blocks more or less, and a way

116
0:10:46.980 --> 0:10:52.520
to give each multi-thread, multi-core, core something to work with.

117
0:10:52.520 --> 0:10:55.080
So it's oriented towards multi-core.

118
0:10:55.080 --> 0:10:58.980
It's not for GPUs, or not at the moment at least.

119
0:10:58.980 --> 0:11:04.840
So the matrices which you have seen before with this data format, the data structure

120
0:11:04.840 --> 0:11:06.760
works a bit like this.

121
0:11:06.760 --> 0:11:12.160
The color is based on the population, on the amount of matrices are there.

122
0:11:12.160 --> 0:11:16.560
Then there is another code with other information.

123
0:11:16.560 --> 0:11:21.620
But this is just to tell you that the irregular aspect of those matrices is reflected also

124
0:11:21.620 --> 0:11:29.000
here to some extent.

125
0:11:29.000 --> 0:11:34.080
So the library itself wants to provide sparse blocks.

126
0:11:34.080 --> 0:11:41.360
So building blocks for iterative solvers is pretty compatible at the library.

127
0:11:41.360 --> 0:11:45.120
It works with C++, Fortune, Octave, and Python.

128
0:11:45.120 --> 0:11:46.680
I say it's quite compatible.

129
0:11:46.680 --> 0:11:51.740
So it uses, let's say, established technologies.

130
0:11:51.740 --> 0:11:54.720
And it's quite compatible also in the sense with your software.

131
0:11:54.720 --> 0:11:56.680
It doesn't require you to use too many.

132
0:11:56.680 --> 0:11:59.760
The only data structure which is custom is the matrix.

133
0:11:59.760 --> 0:12:06.040
You don't need extra data structures for vectors.

134
0:12:06.040 --> 0:12:11.920
And the program you saw before written in the sparse blocks for using RSB uses just

135
0:12:11.920 --> 0:12:14.800
one extra init and finalize function.

136
0:12:14.800 --> 0:12:17.360
So I really respect the API.

137
0:12:17.360 --> 0:12:21.360
But however, it's nice to write also the 15th standard.

138
0:12:21.360 --> 0:12:22.360
Or I'm joking.

139
0:12:22.360 --> 0:12:26.440
This is not the 15th standard, but just the internal API.

140
0:12:26.440 --> 0:12:32.440
So if you want, perhaps you can exploit the internal API of LibreSB, or not internal,

141
0:12:32.440 --> 0:12:34.120
but the native one.

142
0:12:34.120 --> 0:12:39.640
Please tell me when I'm at 10 minutes.

143
0:12:39.640 --> 0:12:44.760
Which is primarily in C. Then you have wrappers with C++.

144
0:12:44.760 --> 0:12:47.960
And there's also the Fortune 1.

145
0:12:47.960 --> 0:12:52.560
These are the native APIs.

146
0:12:52.560 --> 0:13:01.160
And what is specific about RSB is that the blocking is not so clear which blocking is

147
0:13:01.160 --> 0:13:02.160
best.

148
0:13:02.160 --> 0:13:09.920
Because, yeah, depending on how you block, you could have better or worse performance.

149
0:13:09.920 --> 0:13:17.520
And for this reason, there is an idea of using automated empirical optimization in this library.

150
0:13:17.520 --> 0:13:25.680
There is a special call, a function which you call where you invest time to ask the

151
0:13:25.680 --> 0:13:29.680
library to optimize a bit the data structure.

152
0:13:29.680 --> 0:13:36.960
So you sacrifice a minute, perhaps, for optimizing the data structure a bit.

153
0:13:36.960 --> 0:13:44.320
And you do this in the hope that the many hours which will be using this matrix afterwards

154
0:13:44.320 --> 0:13:49.800
will be, will profit, will be decreased thanks to the optimization.

155
0:13:49.800 --> 0:13:54.000
Because as I said, this is meant to be used for iterative methods.

156
0:13:54.000 --> 0:13:56.720
So you will be running this for many hours.

157
0:13:56.720 --> 0:14:01.520
And therefore, spending a few minutes in automated optimization, it's something that should pay

158
0:14:01.520 --> 0:14:02.520
off.

159
0:14:02.520 --> 0:14:03.520
No guarantee.

160
0:14:03.520 --> 0:14:04.520
But that's the idea.

161
0:14:04.520 --> 0:14:08.280
And that is usually how it's got.

162
0:14:08.280 --> 0:14:11.920
To give an idea, this C++ API is what you would expect.

163
0:14:11.920 --> 0:14:16.760
So there is a class templated on the type.

164
0:14:16.760 --> 0:14:18.600
So there is type safety here.

165
0:14:18.600 --> 0:14:20.000
What do you say?

166
0:14:20.000 --> 0:14:21.480
This is my library.

167
0:14:21.480 --> 0:14:22.760
Sorry, this is my matrix.

168
0:14:22.760 --> 0:14:28.160
These are my non-zeros because this is what we are representing here.

169
0:14:28.160 --> 0:14:38.460
We have flags, C-style flags for options like symmetry or asking for discarding zeros rather

170
0:14:38.460 --> 0:14:43.160
than keeping the zeros because sometimes you want to keep structural zeros for modifying

171
0:14:43.160 --> 0:14:44.360
them later, for instance.

172
0:14:44.360 --> 0:14:46.440
So you have many such options here.

173
0:14:46.440 --> 0:14:48.760
And this is why I'm showing this slide.

174
0:14:48.760 --> 0:14:52.520
I'll tell you that there are many options which I'm not showing you here.

175
0:14:52.520 --> 0:14:55.160
And the only data structure here is the RSB matrix.

176
0:14:55.160 --> 0:14:56.840
No other custom stuff.

177
0:14:56.840 --> 0:15:04.480
And you can enjoy the spam interface of C++ 20 that doesn't really force you to have any

178
0:15:04.480 --> 0:15:10.480
weird custom vector type apart from the standard C++ ones.

179
0:15:10.480 --> 0:15:18.920
If you want to use, for instance, GNU Octave and enjoy the multi-core speedup from LibreSB,

180
0:15:18.920 --> 0:15:28.460
you can use the sparse RSB plugin which I wrote which uses LibreSB pretty efficiently.

181
0:15:28.460 --> 0:15:35.440
So apart from a few conversions, it should have almost native performance.

182
0:15:35.440 --> 0:15:47.280
Similarly for Python, the PyRSB plugin for standalone package has an interface which

183
0:15:47.280 --> 0:15:52.900
is copied from CSR matrix.

184
0:15:52.900 --> 0:15:59.080
So you use it mostly the same way, but underneath, LibreSB runs.

185
0:15:59.080 --> 0:16:00.600
You don't see it.

186
0:16:00.600 --> 0:16:06.520
Or you see it if you ask it to use the auto-tuning routine.

187
0:16:06.520 --> 0:16:13.120
Because as I said, in all of those language implementations, you can also use all of the

188
0:16:13.120 --> 0:16:18.740
functionality of LibreSB which includes the auto-tuning also here in Octave.

189
0:16:18.740 --> 0:16:20.440
And I want to stress this.

190
0:16:20.440 --> 0:16:25.080
GNU Octave doesn't have multi-threaded sparse operations.

191
0:16:25.080 --> 0:16:26.660
With LibreSB, you can have them.

192
0:16:26.660 --> 0:16:30.560
Same for SciPy sparse.

193
0:16:30.560 --> 0:16:33.160
As far as I know, it's not multi-threaded.

194
0:16:33.160 --> 0:16:35.320
With LibreSB, you get it.

195
0:16:35.320 --> 0:16:44.680
LibreSB is by default is licensed as lesser GPL3, which means you can, as long as you

196
0:16:44.680 --> 0:16:52.320
don't modify it, you can distribute it with your proprietary code.

197
0:16:52.320 --> 0:16:55.680
If you modify it, well, it's more complicated.

198
0:16:55.680 --> 0:16:59.600
You have to release the modified version.

199
0:16:59.600 --> 0:17:06.600
The LibreSB library is if you want to learn to use it, it makes absolutely sense to use

200
0:17:06.600 --> 0:17:14.460
a packaged version from Debian Ubuntu or most of Linux distributions.

201
0:17:14.460 --> 0:17:19.040
Or if you use Windows and you can use Seguin.

202
0:17:19.040 --> 0:17:24.160
Or once you want the performance, I mean, you can just compile it by yourself because

203
0:17:24.160 --> 0:17:25.400
it's quite trivial.

204
0:17:25.400 --> 0:17:31.400
Or enjoy what our colleagues here from Spark and EasyBuild have done and use the packaged

205
0:17:31.400 --> 0:17:35.720
version from those distributions.

206
0:17:35.720 --> 0:17:43.240
And some people have written wrappers for Rust and Julia.

207
0:17:43.240 --> 0:17:47.840
I don't know these languages, so I didn't use them.

208
0:17:47.840 --> 0:17:50.720
I think the last one is like the entire API.

209
0:17:50.720 --> 0:17:58.000
I think Julia is more in Julia style, so it's just what is the core functionality is there,

210
0:17:58.000 --> 0:17:59.000
I think.

211
0:17:59.000 --> 0:18:00.000
Yeah.

212
0:18:00.000 --> 0:18:01.000
That was everything.

213
0:18:01.000 --> 0:18:06.000
I don't know how much time did I take.

214
0:18:06.000 --> 0:18:09.880
Oh, 50 minutes.

215
0:18:09.880 --> 0:18:17.480
Okay.

