1
0:00:00.000 --> 0:00:14.920
Okay, next lightning talk is Jan VerSkel.

2
0:00:14.920 --> 0:00:18.880
They're talking about multiple double arithmetic on GPUs.

3
0:00:18.880 --> 0:00:19.880
Thank you very much.

4
0:00:19.880 --> 0:00:25.360
Thank you very much, the organizers, for allowing me to speak here.

5
0:00:25.360 --> 0:00:36.080
So I will hope to talk about computations that I've been doing with multiple doubles.

6
0:00:36.080 --> 0:00:43.340
So the multiple doubles go back actually from the time when people, when the hardware was

7
0:00:43.340 --> 0:00:45.820
not yet supporting doubles.

8
0:00:45.820 --> 0:00:49.060
So this was the late 60s.

9
0:00:49.060 --> 0:00:51.720
So this is actually a similar idea.

10
0:00:51.720 --> 0:00:58.680
So you use the hardware arithmetic to extend your precision.

11
0:00:58.680 --> 0:01:00.600
It has a lot of advantages.

12
0:01:00.600 --> 0:01:06.420
So if you're used to working with complex arithmetic, then double double arithmetic has

13
0:01:06.420 --> 0:01:10.400
about the same intensity.

14
0:01:10.400 --> 0:01:17.160
So speaking of intensities, relative to the previous talk, where we were working with

15
0:01:17.160 --> 0:01:23.000
graphs, so in the previous talk I had the impression that everything was about graphs,

16
0:01:23.000 --> 0:01:24.800
and there was memory bound.

17
0:01:24.800 --> 0:01:27.480
My problems are compute bound.

18
0:01:27.480 --> 0:01:33.240
So I get really good arithmetic intensities.

19
0:01:33.240 --> 0:01:35.040
There are some disadvantages, of course.

20
0:01:35.040 --> 0:01:40.360
If you want to work with, say, 17 decimal places, you can't.

21
0:01:40.360 --> 0:01:46.240
Also if you want to work with truly infinite decimals, well, you can't either because you're

22
0:01:46.240 --> 0:01:53.200
still having your 11 bits of the exponent.

23
0:01:53.200 --> 0:01:57.300
Disadvantage might also be that you can still do numerical analysis.

24
0:01:57.300 --> 0:02:02.440
So this might be an advantage or disadvantage.

25
0:02:02.440 --> 0:02:06.040
I got into this by power series arithmetic.

26
0:02:06.040 --> 0:02:09.840
So this is about the EXP and the EPS.

27
0:02:09.840 --> 0:02:15.320
So when I started working with power series, I was using 1111111.

28
0:02:15.320 --> 0:02:17.200
And I know the binomial theorem.

29
0:02:17.200 --> 0:02:22.160
Well, I only knew it when I saw the numbers blowing up on me.

30
0:02:22.160 --> 0:02:24.880
So you know it when you don't know it.

31
0:02:24.880 --> 0:02:26.560
So here is a table.

32
0:02:26.560 --> 0:02:31.960
The exponential has a very nice development, nicely decaying.

33
0:02:31.960 --> 0:02:36.360
And if you multiply these exponentials, you don't have any blow up.

34
0:02:36.360 --> 0:02:43.520
However, the last coefficient, if you want to represent that, and you have to think about

35
0:02:43.520 --> 0:02:44.520
GPUs.

36
0:02:44.520 --> 0:02:48.640
The GPUs are actually quite happy if they can do things in groups of 32.

37
0:02:48.640 --> 0:02:56.640
So actually a 32 power series, an order 32 power series is actually still very small

38
0:02:56.640 --> 0:02:57.640
for GPUs.

39
0:02:57.640 --> 0:03:00.040
But there you have already to use quad doubles.

40
0:03:00.040 --> 0:03:09.920
Otherwise, your last coefficients, you can't represent it anymore.

41
0:03:09.920 --> 0:03:13.940
So I started working with the QD library.

42
0:03:13.940 --> 0:03:18.040
And then we were doing multi core.

43
0:03:18.040 --> 0:03:22.680
Me and my student, Gennady Joffee, and we looked at each other.

44
0:03:22.680 --> 0:03:24.720
Should we do this on the GPU?

45
0:03:24.720 --> 0:03:28.040
Should we write the entire library on the GPU?

46
0:03:28.040 --> 0:03:29.720
My student didn't really want to do it.

47
0:03:29.720 --> 0:03:31.200
And I didn't want to do it.

48
0:03:31.200 --> 0:03:33.800
But then we discovered GQD.

49
0:03:33.800 --> 0:03:35.720
And we used GQD.

50
0:03:35.720 --> 0:03:39.840
And the recent package that we are using is Compari.

51
0:03:39.840 --> 0:03:43.880
It's actually the only software I know that is named after a beverage.

52
0:03:43.880 --> 0:03:46.560
I don't know if that's a good sign or not.

53
0:03:46.560 --> 0:03:50.620
In my supermarket store in Chicago, I once saw Compari.

54
0:03:50.620 --> 0:03:52.040
But it's not my drink.

55
0:03:52.040 --> 0:03:58.040
So I didn't want to ruin the taste of using Compari.

56
0:03:58.040 --> 0:03:59.760
So I stayed off this.

57
0:03:59.760 --> 0:04:01.080
Compari is actually quite good.

58
0:04:01.080 --> 0:04:08.920
So because it allowed me to go to quad double and now also octo double.

59
0:04:08.920 --> 0:04:11.960
The numbers in this table are kind of good.

60
0:04:11.960 --> 0:04:15.840
Because I want to have really performance.

61
0:04:15.840 --> 0:04:18.000
But it also comes somehow misleading.

62
0:04:18.000 --> 0:04:24.320
Because as soon as you're using complex double double, everything becomes compute bound.

63
0:04:24.320 --> 0:04:30.320
And the problems that you have with memory transfer and all, you do a lot of arithmetic

64
0:04:30.320 --> 0:04:37.360
operations on a relatively small amount of data.

65
0:04:37.360 --> 0:04:40.360
I also like to do quality up.

66
0:04:40.360 --> 0:04:48.320
If you can afford the time for, say, a double precision calculation, well, you will see

67
0:04:48.320 --> 0:04:50.880
that everything is not really right.

68
0:04:50.880 --> 0:04:54.760
But then you can work, you can allow the same amount of time.

69
0:04:54.760 --> 0:04:59.760
And you quadruple the precision.

70
0:04:59.760 --> 0:05:02.560
So the 439 there.

71
0:05:02.560 --> 0:05:05.040
Think about one gigaflop.

72
0:05:05.040 --> 0:05:07.880
About two gigaflop.

73
0:05:07.880 --> 0:05:09.560
And then you go to teraflop.

74
0:05:09.560 --> 0:05:15.000
So the 439 is kind of if you have teraflop performance, it's like as if you would be

75
0:05:15.000 --> 0:05:19.080
doing this on a single core.

76
0:05:19.080 --> 0:05:22.920
So I mentioned the funding agencies at the very slides.

77
0:05:22.920 --> 0:05:26.040
I would like to have a hopper.

78
0:05:26.040 --> 0:05:31.000
But so for now, I have to deal with Pascal and Volta.

79
0:05:31.000 --> 0:05:34.160
And the last one is a gaming laptop.

80
0:05:34.160 --> 0:05:38.720
So which is also actually quite a powerful GPU.

81
0:05:38.720 --> 0:05:42.040
My first teraflop card was Kepler.

82
0:05:42.040 --> 0:05:47.960
And this last list of GPU actually gets there.

83
0:05:47.960 --> 0:05:49.960
Okay.

84
0:05:49.960 --> 0:05:54.480
If you think of a double-double, there is a double-two.

85
0:05:54.480 --> 0:05:57.480
And then for a quad-double, there is the double-four.

86
0:05:57.480 --> 0:06:00.520
So that was what the GQD was using.

87
0:06:00.520 --> 0:06:03.160
And that's very good for memory coalescing.

88
0:06:03.160 --> 0:06:09.040
But we actually got into trouble with the complex quad-double because there was no double-eight.

89
0:06:09.040 --> 0:06:15.640
So instead of working, if you work with a vector of quad-doubles, a vector of arrays

90
0:06:15.640 --> 0:06:20.360
of four length, you actually better use four vectors.

91
0:06:20.360 --> 0:06:25.760
The first one with the highest double, second double, third double, fourth double.

92
0:06:25.760 --> 0:06:29.040
So it's a little bit similar like working with power series.

93
0:06:29.040 --> 0:06:33.280
A power series is invertible if the leading coefficient is not zero.

94
0:06:33.280 --> 0:06:36.200
You can work with matrices of power series.

95
0:06:36.200 --> 0:06:37.280
But actually, that's not good.

96
0:06:37.280 --> 0:06:41.640
You should actually work with a series where the coefficients are matrices.

97
0:06:41.640 --> 0:06:43.840
Same idea here.

98
0:06:43.840 --> 0:06:47.760
QDLIP is a very good library still.

99
0:06:47.760 --> 0:06:48.940
It's quite complete.

100
0:06:48.940 --> 0:06:54.880
So I have extended the square root, for example, to octo-double position.

101
0:06:54.880 --> 0:06:56.480
Okay.

102
0:06:56.480 --> 0:07:01.800
So here is then my beginning.

103
0:07:01.800 --> 0:07:04.840
So I mentioned, so you saw this eight.

104
0:07:04.840 --> 0:07:11.880
So if you take a vector of random complex number, 64, then the norm is eight.

105
0:07:11.880 --> 0:07:12.920
It should be eight.

106
0:07:12.920 --> 0:07:16.480
So that's a really nice test property.

107
0:07:16.480 --> 0:07:20.440
If you work with GPUs, you actually define kernels.

108
0:07:20.440 --> 0:07:22.200
And kernels, the name says it itself.

109
0:07:22.200 --> 0:07:23.200
It should be small.

110
0:07:23.200 --> 0:07:24.800
So think small.

111
0:07:24.800 --> 0:07:28.560
And actually, this problem is a small problem, mathematically speaking.

112
0:07:28.560 --> 0:07:35.480
But it has all the richness and the complexity of all the problems that you will run into.

113
0:07:35.480 --> 0:07:39.220
You will have to study the prefix sum algorithm, for example.

114
0:07:39.220 --> 0:07:40.600
So that is needed.

115
0:07:40.600 --> 0:07:45.800
You also have to tune your software for large vectors, or for small vector.

116
0:07:45.800 --> 0:07:53.440
For small vectors, you can only have one block of threads that is active.

117
0:07:53.440 --> 0:07:56.680
The square root, well, it works with staggered.

118
0:07:56.680 --> 0:08:01.920
So you apply a Newton method.

119
0:08:01.920 --> 0:08:07.320
And then actually, this is where the dot comes in.

120
0:08:07.320 --> 0:08:15.600
So the nice thing about double-double, squat-doubles, that everything fits into registers.

121
0:08:15.600 --> 0:08:19.120
So it's also very good if you do multi-core.

122
0:08:19.120 --> 0:08:21.000
So you don't have to use the heap ever.

123
0:08:21.000 --> 0:08:25.560
But of course, when you get to complex squat-doubles, you have these eight arrays.

124
0:08:25.560 --> 0:08:29.160
If you do octo-doubles, so it doubles and doubles and doubles.

125
0:08:29.160 --> 0:08:34.880
So I have with my old graphics cards, they can no longer even compile the octo-doubles

126
0:08:34.880 --> 0:08:36.880
if you inline too much.

127
0:08:36.880 --> 0:08:45.960
So it's still very interesting that actually you have to tailor your kernels towards the

128
0:08:45.960 --> 0:08:48.760
precision levels.

129
0:08:48.760 --> 0:08:50.040
So here is my last slide.

130
0:08:50.040 --> 0:08:52.800
I did more than just norms.

131
0:08:52.800 --> 0:09:00.280
So we have teraflop performance when we evaluate polynomials and differentiate them.

132
0:09:00.280 --> 0:09:04.080
The QR, the blocked householder QR, is also wonderful.

133
0:09:04.080 --> 0:09:12.240
You get already teraflop performance with 1,000 in complex double-double.

134
0:09:12.240 --> 0:09:17.520
And then the last paper is where you try to combine these things by computing Taylor series

135
0:09:17.520 --> 0:09:24.680
for solutions of solution developments for polynomial systems.

136
0:09:24.680 --> 0:09:27.240
Newton's method is actually a quite nice operator.

137
0:09:27.240 --> 0:09:32.640
You start with a multivariate system where all the variables are linked to each other.

138
0:09:32.640 --> 0:09:38.880
And what Newton actually does, it spits out power series for each component.

139
0:09:38.880 --> 0:09:46.200
So actually it untangles all the non-linearities.

140
0:09:46.200 --> 0:09:47.600
So I listed the archive.

141
0:09:47.600 --> 0:09:52.140
So the IEEE puts things in a paywall, behind the paywall.

142
0:09:52.140 --> 0:09:55.200
So you have the archive versions there.

143
0:09:55.200 --> 0:09:56.640
And you're more than welcome.

144
0:09:56.640 --> 0:10:01.200
So the bottom line of the slide, I mean the conclusion actually is that all the software

145
0:10:01.200 --> 0:10:02.920
is free and open source.

146
0:10:02.920 --> 0:10:18.800
I'd have the GitHub handle there.

