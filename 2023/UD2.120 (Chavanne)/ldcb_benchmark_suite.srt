1
0:00:00.000 --> 0:00:06.280
Hello, HPC room.

2
0:00:06.280 --> 0:00:14.240
I work at CWI Amsterdam as a researcher and today I'm here on behalf of the IDBC.

3
0:00:14.240 --> 0:00:17.380
The IDBC stands for the linked data benchmark council.

4
0:00:17.380 --> 0:00:22.600
We are a non-profit company founded in 2012 and we design graph benchmarks and govern

5
0:00:22.600 --> 0:00:23.600
their use.

6
0:00:23.600 --> 0:00:28.160
Additionally, we do research on graph schemas and modern graph languages and everything

7
0:00:28.160 --> 0:00:32.440
we do is available under the Apache V2 license.

8
0:00:32.440 --> 0:00:36.200
Organisationally, IDBC consists of more than 20 companies.

9
0:00:36.200 --> 0:00:38.760
These are companies interested in graph data management.

10
0:00:38.760 --> 0:00:44.320
We have financial service providers, database vendors, cloud vendors, hardware vendors,

11
0:00:44.320 --> 0:00:49.000
and consultancy companies as well as individual contributors like me.

12
0:00:49.000 --> 0:00:54.480
So, we design benchmarks, the first one being the IDBC social network benchmark which targets

13
0:00:54.480 --> 0:00:56.640
database systems.

14
0:00:56.640 --> 0:01:00.520
Let's go through this benchmark by a series of examples.

15
0:01:00.520 --> 0:01:05.480
I will touch on data sets, queries and updates that we use in this benchmark.

16
0:01:05.480 --> 0:01:10.600
As the name social network benchmark suggests, we have a social network that consists of

17
0:01:10.600 --> 0:01:17.040
person nodes who know each other via a distribution that mimics the Facebook career social network.

18
0:01:17.040 --> 0:01:20.080
The content that these people create is messages.

19
0:01:20.080 --> 0:01:25.680
These form little tree-shaped subgraphs and are connected via author edges to the people.

20
0:01:25.680 --> 0:01:28.760
On this graph, we can run queries like the following.

21
0:01:28.760 --> 0:01:34.160
Let's have a given person enumerate their friends and their friends of friends, get

22
0:01:34.160 --> 0:01:38.680
the messages that these people created and then filter them based on some condition on

23
0:01:38.680 --> 0:01:39.680
their dates.

24
0:01:39.680 --> 0:01:44.440
So, a potential substitution could be on this graph that we are interested in this query

25
0:01:44.440 --> 0:01:47.680
for Bob and the date set on Saturday.

26
0:01:47.680 --> 0:01:53.080
And if we evaluate this query, we start with Bob, we traverse the nodes edges to Ada and

27
0:01:53.080 --> 0:01:56.360
Karl then continue to Finn, Eve and Dan.

28
0:01:56.360 --> 0:02:02.320
We move along the author edges and then finally we apply the filter condition which will cut

29
0:02:02.320 --> 0:02:06.840
message three and will leave us messages one, two and four.

30
0:02:06.840 --> 0:02:09.840
So obviously, social network is not a static environment.

31
0:02:09.840 --> 0:02:11.120
There are always changes.

32
0:02:11.120 --> 0:02:13.280
For example, people become friends.

33
0:02:13.280 --> 0:02:15.800
Even Jia may add each other as a friend.

34
0:02:15.800 --> 0:02:18.320
That will result in a new nose edge.

35
0:02:18.320 --> 0:02:19.480
That's simple enough.

36
0:02:19.480 --> 0:02:22.120
Jia can decide to create a message.

37
0:02:22.120 --> 0:02:24.800
This message will be replied to message M3.

38
0:02:24.800 --> 0:02:29.160
So we add a new node and connect it to the existing graph via two edges.

39
0:02:29.160 --> 0:02:31.640
The heavy hitting updates are the deletes.

40
0:02:31.640 --> 0:02:36.760
A person may decide to delete their account and that will result in a cascade of deletes.

41
0:02:36.760 --> 0:02:42.840
For example, if we remove the node, Eve, that will result in the removal of their direct

42
0:02:42.840 --> 0:02:45.120
edges or the messages they created.

43
0:02:45.120 --> 0:02:49.560
And in some social network, this will even trigger the deletion of all the message trees

44
0:02:49.560 --> 0:02:53.360
and of course all the edges that point to those messages.

45
0:02:53.360 --> 0:02:56.880
So this is quite a hard operation for systems to execute.

46
0:02:56.880 --> 0:03:02.920
It stresses their garbage collectors and it disallows certain append-only data structures.

47
0:03:02.920 --> 0:03:06.920
So if you want to weave these three components together, the data set, the queries and the

48
0:03:06.920 --> 0:03:11.600
updates, we need a benchmark driver that schedules the operations to be executable.

49
0:03:11.600 --> 0:03:16.100
It runs the updates and the queries concurrently and of course it collects the results.

50
0:03:16.100 --> 0:03:21.720
The system under test that we run the benchmark on is provided by our members who are the

51
0:03:21.720 --> 0:03:27.960
database vendors and we go to great lengths to allow as many candidate systems as possible.

52
0:03:27.960 --> 0:03:34.680
So graph databases, triple source and relational databases can all compete on this benchmark.

53
0:03:34.680 --> 0:03:39.760
Speaking of relational databases, some of you may think, is CQL sufficient to express

54
0:03:39.760 --> 0:03:40.760
these queries?

55
0:03:40.760 --> 0:03:42.900
And the answer is that in most cases it is.

56
0:03:42.900 --> 0:03:48.840
So the query that we have just seen can be formulated in a reasonably simple SQL query.

57
0:03:48.840 --> 0:03:53.800
It is a bit unwieldy but it is certainly doable and the performance will be okay.

58
0:03:53.800 --> 0:03:59.040
However, this being a graph benchmark, it lends itself quite naturally to other query

59
0:03:59.040 --> 0:04:00.040
languages.

60
0:04:00.040 --> 0:04:04.160
There are two new query languages that are going to be coming out and both of them adopted

61
0:04:04.160 --> 0:04:08.120
a visual graph syntax inspired by Neo4j's Cypher language.

62
0:04:08.120 --> 0:04:13.080
The first one is called SQL PGQ where PGQ stands for property graph queries.

63
0:04:13.080 --> 0:04:18.080
This will be released this summer and as you can see it's an extension to SQL so you can

64
0:04:18.080 --> 0:04:23.060
use select and from but it adds the graph table construct and the query can be formulated

65
0:04:23.060 --> 0:04:25.760
in a very concise and readable manner.

66
0:04:25.760 --> 0:04:30.240
There is GQL, the graph query language, which is a standalone language that is going to

67
0:04:30.240 --> 0:04:36.120
be released next year and it shares the same pattern matching language as SQL PGQ.

68
0:04:36.120 --> 0:04:41.120
So the social network benchmark has multiple workloads to cover the diverse challenges

69
0:04:41.120 --> 0:04:44.880
that are created by graph workloads.

70
0:04:44.880 --> 0:04:49.160
The first one, the older one is the social network benchmark interactive workload.

71
0:04:49.160 --> 0:04:53.380
This is transactional in nature and it has queries like the one I have shown before.

72
0:04:53.380 --> 0:04:57.160
So these queries typically start in one or two person nodes.

73
0:04:57.160 --> 0:04:59.020
They are not very heavy hitting.

74
0:04:59.020 --> 0:05:01.380
They only touch a limited amount of data.

75
0:05:01.380 --> 0:05:06.760
They have concurrent reads and updates and systems are competing on achieving high throughputs.

76
0:05:06.760 --> 0:05:10.240
So this benchmark has been around for a few years and we have seen actually very good

77
0:05:10.240 --> 0:05:11.240
results.

78
0:05:11.240 --> 0:05:16.240
In the last three years we witnessed an exponential increase in throughput starting from a little

79
0:05:16.240 --> 0:05:22.880
above 5,000 operations per second to almost 17,000 operations per second this year.

80
0:05:22.880 --> 0:05:27.360
Our newer benchmark is the social network benchmark business intelligence workload.

81
0:05:27.360 --> 0:05:32.280
This is analytical in nature and it has queries that touch on large portions of the data.

82
0:05:32.280 --> 0:05:37.840
For example, the query on this slide enumerates all triangles of friendships in a given country

83
0:05:37.840 --> 0:05:44.200
which can potentially reach billions of edges and is a very difficult computational problem.

84
0:05:44.200 --> 0:05:48.760
Systems here are allowed to do either a bulk or a concurrent update approach, but they

85
0:05:48.760 --> 0:05:54.120
should strive to get both a high throughput and low individual query run times.

86
0:05:54.120 --> 0:05:58.080
This benchmark being relatively new, we only have a single result, so it's a bit difficult

87
0:05:58.080 --> 0:05:59.840
to put it into context.

88
0:05:59.840 --> 0:06:02.140
But it allows me to highlight one thing.

89
0:06:02.140 --> 0:06:04.080
Many of our benchmarks use different CPUs.

90
0:06:04.080 --> 0:06:08.600
We actually have quite a healthy diversity in the CPUs.

91
0:06:08.600 --> 0:06:13.440
We have results with the AMD Epic Genoa, like this one achieved by TIGOGRAPH.

92
0:06:13.440 --> 0:06:19.200
We have results using IntagZ on Ice Lakes and the ETN 710s which use an ARM architecture.

93
0:06:19.200 --> 0:06:24.520
We have more and larger scale results expected this year and we are also quite interested

94
0:06:24.520 --> 0:06:29.480
in some graph and machine learning accelerators that are going to be released soon.

95
0:06:29.480 --> 0:06:31.560
Our benchmark process is quite involved.

96
0:06:31.560 --> 0:06:34.360
For each workload we release a specification.

97
0:06:34.360 --> 0:06:36.840
We have an academic paper that motivates the benchmark.

98
0:06:36.840 --> 0:06:41.400
We have data generators, pre-generated data sets, as well as a benchmark driver and at

99
0:06:41.400 --> 0:06:44.500
least two reference implementations.

100
0:06:44.500 --> 0:06:49.640
We do this because we have an auditing process that allows the vendors to implement this

101
0:06:49.640 --> 0:06:56.000
benchmark to actually go through rigorous tests and if they do so, they can claim that

102
0:06:56.000 --> 0:06:58.800
they have an official benchmark result.

103
0:06:58.800 --> 0:07:05.400
We trademark the term IDBC such that the vendors have to go through these hoops of auditing

104
0:07:05.400 --> 0:07:09.800
and we still allow researchers and developers to do unofficial benchmarks, but they have

105
0:07:09.800 --> 0:07:14.360
to say that this is not an official IDBC benchmark result.

106
0:07:14.360 --> 0:07:18.200
One other benchmark I would like to touch upon briefly is the Graphalytics benchmark.

107
0:07:18.200 --> 0:07:19.480
This costs a wider net.

108
0:07:19.480 --> 0:07:25.200
It targets graph databases, graph processing framework, embedded graph libraries like NetworkX

109
0:07:25.200 --> 0:07:26.600
and so on.

110
0:07:26.600 --> 0:07:29.480
This uses untyped, unattributed graphs.

111
0:07:29.480 --> 0:07:34.200
It's only the person knows person graphs of the social network benchmark or other unknown

112
0:07:34.200 --> 0:07:35.200
graphs like Graph500.

113
0:07:35.200 --> 0:07:37.400
We have six algorithms.

114
0:07:37.400 --> 0:07:41.780
Many of these are textbook algorithms like BFS which just reverses the graph from a given

115
0:07:41.780 --> 0:07:47.160
source node or we have PageRank which selects the most important nodes in the network.

116
0:07:47.160 --> 0:07:51.200
We also have clustering coefficient, community detection, connected components and shortest

117
0:07:51.200 --> 0:07:53.360
paths.

118
0:07:53.360 --> 0:07:54.960
This benchmark is a bit simpler to implement.

119
0:07:54.960 --> 0:07:57.600
We have a leader board that we update periodically.

120
0:07:57.600 --> 0:08:02.720
The next one is going to come out in Spring 2023, so talk to us if you're interested.

121
0:08:02.720 --> 0:08:07.320
So wrapping up, you should consider becoming an IDBC member because members can participate

122
0:08:07.320 --> 0:08:11.920
in the benchmark design and have a say in where we go, they can commission audits of

123
0:08:11.920 --> 0:08:17.040
their benchmarks and they can also gain early access to the ISO standard drafts, CEQA, PGQ

124
0:08:17.040 --> 0:08:18.760
and GQA that I have shown.

125
0:08:18.760 --> 0:08:23.240
It's free for individuals and has a yearly fee for companies.

126
0:08:23.240 --> 0:08:25.440
So to sum up, these are our three main benchmarks.

127
0:08:25.440 --> 0:08:28.200
We have other benchmarks and many future ideas.

128
0:08:28.200 --> 0:08:33.320
If you're interested, please reach out.

129
0:08:33.320 --> 0:08:42.600
Okay, again we have time for one question.

130
0:08:42.600 --> 0:08:51.800
Any questions for Gabor?

131
0:08:51.800 --> 0:08:53.160
This is a new big question.

132
0:08:53.160 --> 0:08:57.200
I'm not into graphs.

133
0:08:57.200 --> 0:09:08.120
Apart from advertisement optimisation, mass surveillance and perhaps content distribution,

134
0:09:08.120 --> 0:09:14.880
which I don't know if they're the major applications, but it's just what my naive mind comes with.

135
0:09:14.880 --> 0:09:20.120
What other applications are those benchmarks meant to optimise?

136
0:09:20.120 --> 0:09:26.480
So the big one this year is supply chain optimisation, like strengthening supply chains, ensuring

137
0:09:26.480 --> 0:09:31.400
that they are ethical, ensuring that they are not passing conflict zones.

138
0:09:31.400 --> 0:09:34.120
It's something that is very important these days.

139
0:09:34.120 --> 0:09:42.520
You can also track CO2 emissions and other aspects of labour and manufacturing.

140
0:09:42.520 --> 0:09:46.160
So that's certainly a big one and that's something that we have seen.

141
0:09:46.160 --> 0:09:52.160
And there are of course all the graphic problems like power grid, a lot of e-commerce programmes

142
0:09:52.160 --> 0:09:56.480
and financial fraud detection, which is going to be part of our financial benchmark this

143
0:09:56.480 --> 0:10:22.480
year.

