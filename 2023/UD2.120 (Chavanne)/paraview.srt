1
0:00:00.000 --> 0:00:01.000
All right.

2
0:00:01.000 --> 0:00:02.000
Good morning, everyone.

3
0:00:02.000 --> 0:00:03.000
Welcome to the HPC Dev Room.

4
0:00:03.000 --> 0:00:13.560
Thanks for being here so early in the morning, maybe not entirely sober.

5
0:00:13.560 --> 0:00:18.280
We'll let Nicolas get started with opening the Dev Room with a talk on ParaV.

6
0:00:18.280 --> 0:00:19.280
Thank you, Nicolas.

7
0:00:19.280 --> 0:00:20.280
Thank you.

8
0:00:20.280 --> 0:00:21.280
Hello, everyone.

9
0:00:21.280 --> 0:00:29.280
Thanks to be here early in the morning to begin this HPC day.

10
0:00:29.280 --> 0:00:33.440
Big thanks to everyone who organized this room.

11
0:00:33.440 --> 0:00:35.120
That's really great to be here.

12
0:00:35.120 --> 0:00:38.720
Thanks, Gennet, and all your team.

13
0:00:38.720 --> 0:00:40.720
About me, I'm Nicolas.

14
0:00:40.720 --> 0:00:47.200
I'm a developer and I have the chance to make my job about making first-code contributions.

15
0:00:47.200 --> 0:00:53.840
I'm working at Keterware, Europe, and I work mainly on ParaV, so developing the software

16
0:00:53.840 --> 0:00:57.280
but also interacting with the community.

17
0:00:57.280 --> 0:01:09.080
So ParaV is an end-user application that works for scientific data analysis and visualizations.

18
0:01:09.080 --> 0:01:16.040
We have an open community on the GitLab for the code and discourse for discussions.

19
0:01:16.040 --> 0:01:21.480
It's supported by Keterware, which is behind a VTK for visualization toolkit and to make

20
0:01:21.480 --> 0:01:26.320
you potentially already know about.

21
0:01:26.320 --> 0:01:31.920
So what do we do with ParaV for displaying an analysis data set?

22
0:01:31.920 --> 0:01:38.160
So it's mainly a 3D visualizer, but you've got some charts and spreadsheets and so on.

23
0:01:38.160 --> 0:01:40.400
It's also an internet for data processing.

24
0:01:40.400 --> 0:01:45.880
So we have a concept of filters to take an input and compute some stuff on it and get

25
0:01:45.880 --> 0:01:47.240
your outputs.

26
0:01:47.240 --> 0:01:51.200
So basically you can extract the data from the rest of your raw data.

27
0:01:51.200 --> 0:01:58.440
We also have some other modules like realistic rendering, so you can do your communications

28
0:01:58.440 --> 0:02:04.960
with directly your real data sets on that with just some kind of fake one.

29
0:02:04.960 --> 0:02:10.000
So here is some basic screen shot of the applications.

30
0:02:10.000 --> 0:02:12.320
Who is using ParaV?

31
0:02:12.320 --> 0:02:18.600
We cover its generic applications, so we cover a large range of domains like fluid dynamics

32
0:02:18.600 --> 0:02:23.680
so we can compute streamlines, particle tracking, and so on.

33
0:02:23.680 --> 0:02:28.120
We have a volume rendering that is really nice for medical applications with you have

34
0:02:28.120 --> 0:02:33.160
some 3D scan and you want to understand what happened inside.

35
0:02:33.160 --> 0:02:39.400
So that's just an infinite list where a lot of domains that can be covered, but here is

36
0:02:39.400 --> 0:02:42.400
the more well-known one.

37
0:02:42.400 --> 0:02:44.060
We do use ParaV.

38
0:02:44.060 --> 0:02:50.980
So as I said, that's an application, so basically the first way to learn is to use the GUI.

39
0:02:50.980 --> 0:02:54.880
So you click on buttons, you do some stuff, and you're happy.

40
0:02:54.880 --> 0:03:00.800
But you can also use the Python wrapping to write some scripts, and so you can run the

41
0:03:00.800 --> 0:03:05.660
script on batch processing without having to be behind the computer.

42
0:03:05.660 --> 0:03:11.080
It's also a framework because you can code your own extensions, your own derivated work

43
0:03:11.080 --> 0:03:19.040
from ParaV in the native C++ language, but also some features can be done in Python code.

44
0:03:19.040 --> 0:03:21.680
And it's all based on the visualization toolkit.

45
0:03:21.680 --> 0:03:27.920
I mean, all the hard work of processing the data and doing the rendering come from VTK.

46
0:03:27.920 --> 0:03:35.480
So as I say, that's also supported by Kitware, so I do work sometimes on VTK to have some

47
0:03:35.480 --> 0:03:39.680
bug fix or some small new features.

48
0:03:39.680 --> 0:03:45.760
Here we can run ParaV on which hardware is it possible to use it.

49
0:03:45.760 --> 0:03:50.960
Basically on your small, classical desktop, we have some official binaries you can try

50
0:03:50.960 --> 0:03:52.960
to download and just run.

51
0:03:52.960 --> 0:03:59.600
It should be, it's cross-platform, I didn't say, but you can run it on ISO, on proprietary,

52
0:03:59.600 --> 0:04:05.840
so it's like Mac or Windows, but it should be out of the box for Linux too.

53
0:04:05.840 --> 0:04:10.840
You can also build it, we have a large selection of build options depending on what you want

54
0:04:10.840 --> 0:04:11.920
to do exactly.

55
0:04:11.920 --> 0:04:17.560
You can enable or disable it, so if you want to have pure Python data distribution, parallelizations,

56
0:04:17.560 --> 0:04:20.480
or custom rendering.

57
0:04:20.480 --> 0:04:25.080
A lot of stuff, we have some documentation about it, and we can help you on the discourse

58
0:04:25.080 --> 0:04:31.480
if you are trying to achieve some specific build of it.

59
0:04:31.480 --> 0:04:34.200
And which kind of usage do we have of ParaView?

60
0:04:34.200 --> 0:04:38.360
So either research and industry are using it.

61
0:04:38.360 --> 0:04:48.920
For instance, recently, well, the supercomputing conference in the US, and they organized a

62
0:04:48.920 --> 0:04:54.000
service contest where people are asked to upload some nice videos made about their scientific

63
0:04:54.000 --> 0:05:00.200
analysis, and most of them are using ParaView for just the data processing, but also sometimes

64
0:05:00.200 --> 0:05:04.720
for the video generations and animate their data.

65
0:05:04.720 --> 0:05:09.920
So why all those people are using ParaView?

66
0:05:09.920 --> 0:05:15.800
Because ParaView does some stuff efficiently to process the data on their infrastructures,

67
0:05:15.800 --> 0:05:23.120
so that's what I want to talk in the next part of my talk.

68
0:05:23.120 --> 0:05:28.840
So what ParaView uses behind the hood to make it possible?

69
0:05:28.840 --> 0:05:31.480
So first we have a client-server architecture.

70
0:05:31.480 --> 0:05:38.240
So I already said that you can do it from GUI or from Python, but the GUI and Python

71
0:05:38.240 --> 0:05:44.960
are just two clients, so you can do exactly the same stuff with one or the other.

72
0:05:44.960 --> 0:05:51.400
There's no really limitation about choosing one or the second.

73
0:05:51.400 --> 0:05:58.800
You can run with remote server, and you can run in a distributed environment to your server.

74
0:05:58.800 --> 0:06:07.920
So, in that case, you can connect, you can either just run the server parts with a script

75
0:06:07.920 --> 0:06:13.720
analysis, but you can either connect your local clients to your decent server, and again

76
0:06:13.720 --> 0:06:20.720
using the graphical interface to do the stuff as if it was on your local machine, but instead

77
0:06:20.720 --> 0:06:27.880
it's the supercomputer or the remote architecture.

78
0:06:27.880 --> 0:06:30.400
At the bottom, two other modes that are available.

79
0:06:30.400 --> 0:06:37.480
Typically, if you have some graphic nodes on your server, you can use them for just

80
0:06:37.480 --> 0:06:43.800
the rendering part and stay the data management on the CPU nodes, and still you can connect

81
0:06:43.800 --> 0:06:49.960
your client on it to see what happens and to control from a graphical interface.

82
0:06:49.960 --> 0:06:53.440
And last mode, I will go back on it later.

83
0:06:53.440 --> 0:07:00.200
We have an in situ infrastructure, so basically your simulation can call an API that's fluid

84
0:07:00.200 --> 0:07:05.560
ParaView script analysis, and you can even connect with a graphical client to see time

85
0:07:05.560 --> 0:07:09.880
step per time step what has happened on your simulation.

86
0:07:09.880 --> 0:07:16.400
So, that's for the different mode of use for ParaView.

87
0:07:16.400 --> 0:07:25.160
So first, to run on HPC infrastructure, we implement data distributions for the analysis.

88
0:07:25.160 --> 0:07:28.040
So basically we rely on the MPI standards.

89
0:07:28.040 --> 0:07:36.080
So our readers are MPI-aware, so they can distribute the data over the wrong early in

90
0:07:36.080 --> 0:07:38.960
the process when you read your data on the disk.

91
0:07:38.960 --> 0:07:45.720
Then most of the features are okay to run just with their sub parts of data, but some

92
0:07:45.720 --> 0:07:50.280
other features need to know about the neighborhood to execute correctly.

93
0:07:50.280 --> 0:07:56.840
So for that, we support the concept of ghost cells where each rank know a little bit about

94
0:07:56.840 --> 0:08:00.200
the rank that is next to it.

95
0:08:00.200 --> 0:08:06.120
In that case, mainly we split the data geometrically, so a subset is really a geometric subset of

96
0:08:06.120 --> 0:08:15.680
your data, and different rank can know and communicate with the author for specific tasks.

97
0:08:15.680 --> 0:08:21.880
At least we have some filters, so what I call a filter is really something that the user

98
0:08:21.880 --> 0:08:26.160
can instantiate from the client and ask to process.

99
0:08:26.160 --> 0:08:36.160
So we can ensure load balancing by redistributing the data during the process.

100
0:08:36.160 --> 0:08:39.720
The visualizations can also be distributed over several ranks.

101
0:08:39.720 --> 0:08:45.600
So for that, we use an inner library that's called ICT, that's based on the MPI process

102
0:08:45.600 --> 0:08:46.600
to do that.

103
0:08:46.600 --> 0:08:53.080
And probably you support a different kind of model of rendering, so if you are dedicating

104
0:08:53.080 --> 0:08:58.680
on a node, you can, as I said, create a parallel tutorial just for the rendering part and connect

105
0:08:58.680 --> 0:09:01.240
it to the data server.

106
0:09:01.240 --> 0:09:09.240
You can have multiple GPUs per rank to do your rendering, but that's also possible locally

107
0:09:09.240 --> 0:09:15.600
if you have just one machine that has multiple GPUs, you can ask to do rendering on both

108
0:09:15.600 --> 0:09:22.000
symmetry and agility.

109
0:09:22.000 --> 0:09:29.800
Concerning the performances now, so that distributions is not a bug performance, it's just about

110
0:09:29.800 --> 0:09:35.360
running with too big data, so you cannot just run on your machine, that's a requirement

111
0:09:35.360 --> 0:09:44.080
when you have huge data to be able to distribute it over your computer or your supercomputer.

112
0:09:44.080 --> 0:09:48.560
Now we're talking a little bit about performance, because if you have big data, you also need

113
0:09:48.560 --> 0:09:54.760
to be performant on how you analyze it and how you are proceed with it.

114
0:09:54.760 --> 0:10:00.840
So for that, we have a thin layer for CPU parallelism, we call that a simple tool in

115
0:10:00.840 --> 0:10:12.640
our code base, the goal is to do code parallelizations for many for loop, and main purpose is that

116
0:10:12.640 --> 0:10:19.240
you can choose at build time and then at run time if you enable the OpenMP or TBB backends

117
0:10:19.240 --> 0:10:29.240
and if you don't want external live, you can also use the C++5 to do that.

118
0:10:29.240 --> 0:10:37.440
And so, as is just for instance, to parallelize a for loop or fill operations, it's really

119
0:10:37.440 --> 0:10:44.440
widely used in a lot of our algorithm, and you have some environment variables that can

120
0:10:44.440 --> 0:10:51.680
control the backend on some of the number of thread, the type of the thread pools or

121
0:10:51.680 --> 0:10:58.520
if you allow nested pools or so on, depending on your resources and backend.

122
0:10:58.520 --> 0:11:08.640
So it has some documentation on it, and we made some improvements last year about that.

123
0:11:08.640 --> 0:11:17.320
Still about performances, we also use as an optional dependency the VTKM projects that

124
0:11:17.320 --> 0:11:24.480
stand for somewhat some many core that is intended to be used on heterogeneous systems.

125
0:11:24.480 --> 0:11:32.240
So basically, when you want to have performance on supercomputer or even, you still need to

126
0:11:32.240 --> 0:11:37.760
be aware of the current technology and the state of the art, and as we saw in the past

127
0:11:37.760 --> 0:11:43.320
decades, a lot of new architectures emerging.

128
0:11:43.320 --> 0:11:52.240
We think about using a dedicated library to be able to use these new architectures.

129
0:11:52.240 --> 0:11:58.840
So with VTKM, the goal inside VTKM library, the goal is to split all operations into really

130
0:11:58.840 --> 0:12:06.720
atomic operations, and then at runtime, it can dispatch all that on the hardware you

131
0:12:06.720 --> 0:12:10.320
find on the back ends that are available.

132
0:12:10.320 --> 0:12:16.880
So with VTKM, you can use CUDA, OpenMP, GBB also to do the computation.

133
0:12:16.880 --> 0:12:23.320
This time with VTKM is not just accelerating some specific loop inside an algorithm, it's

134
0:12:23.320 --> 0:12:34.000
more about VTKM is reimplementing some whole algorithm like extracting ISO control also.

135
0:12:34.000 --> 0:12:42.200
And then we embed this into ParaView with some kind of wrapper to communicate with all

136
0:12:42.200 --> 0:12:44.840
VTKM works.

137
0:12:44.840 --> 0:12:54.320
So that's optional, that's enabled by default in the binaries we provide.

138
0:12:54.320 --> 0:13:00.000
Another point about performance, but that's really depending on the use case, on the data

139
0:13:00.000 --> 0:13:04.120
you are using is the in-situ also.

140
0:13:04.120 --> 0:13:08.720
Basically when you traditionally when you have your simulation, it dumps every time

141
0:13:08.720 --> 0:13:13.520
step or every n time step some data on the disk, and then to analyze you have to load

142
0:13:13.520 --> 0:13:17.760
back to data with post-processing tools.

143
0:13:17.760 --> 0:13:22.800
But that's at a cost of writing and reading from your disk.

144
0:13:22.800 --> 0:13:26.960
You should have the size on your disk, the whole side of the disk, so you should have

145
0:13:26.960 --> 0:13:34.440
big disk, and then it has really a cost in terms of time when you should write a full

146
0:13:34.440 --> 0:13:38.600
mesh of full data on disk and then read back with another process.

147
0:13:38.600 --> 0:13:45.000
So basically the goal of in-situ is to make the simulation communicate directly with the

148
0:13:45.000 --> 0:13:50.320
processing tools, and then the processing tools can wrap the memory in place and analyze

149
0:13:50.320 --> 0:13:57.760
directly in the RAM without writing on the disk to save some IO time.

150
0:13:57.760 --> 0:14:08.520
So in the context of ParaView, we have a standalone API that's called Catalysts that was recently

151
0:14:08.520 --> 0:14:15.240
released, although we make big improvements into Catalysts past years.

152
0:14:15.240 --> 0:14:23.320
And the goal of Catalysts is to have a really minimal API and a stable ABI, so you can choose

153
0:14:23.320 --> 0:14:27.880
end-to-end time, the implementation you want.

154
0:14:27.880 --> 0:14:34.020
And one other goal is to minimize the instrumentation you need to do in your simulation code directly,

155
0:14:34.020 --> 0:14:38.840
so it's really easy for simulation developers to understand the few key places where they

156
0:14:38.840 --> 0:14:42.200
have to put a new code to call our API.

157
0:14:42.200 --> 0:14:47.280
So here is a really basic example from one of the tutorials we have.

158
0:14:47.280 --> 0:14:51.600
You need to initialize, of course, and you need to call some method at each time step

159
0:14:51.600 --> 0:14:56.560
where you want the processing to happen.

160
0:14:56.560 --> 0:14:57.920
And finalizations.

161
0:14:57.920 --> 0:15:05.520
Of course, you still have to do a little layer to describe your data.

162
0:15:05.520 --> 0:15:14.480
For that, we do some sort of a party library to help us.

163
0:15:14.480 --> 0:15:22.160
So Catalysts is a standalone, is an independent project, no, independent of ParaView, but

164
0:15:22.160 --> 0:15:28.720
of course the first real implementation is an implementation for ParaView.

165
0:15:28.720 --> 0:15:31.160
So we...

166
0:15:31.160 --> 0:15:32.480
Sorry.

167
0:15:32.480 --> 0:15:41.760
So, yes, we implement Catalysts, so the back end, so you can run ParaView pipeline directly

168
0:15:41.760 --> 0:15:50.160
from your simulation each time step or when you call it.

169
0:15:50.160 --> 0:15:52.840
So how does it work?

170
0:15:52.840 --> 0:15:53.840
How do you...

171
0:15:53.840 --> 0:15:54.840
You can...

172
0:15:54.840 --> 0:16:00.520
The idea is that you are hard-cored the communication between your simulation and Catalysts through

173
0:16:00.520 --> 0:16:07.040
the API, but then the actual script that is executed, the actual pipeline and visualization

174
0:16:07.040 --> 0:16:14.280
you want to produce, it's all scriptable thanks to the Python wrapping of ParaView.

175
0:16:14.280 --> 0:16:20.560
You can even load some representative data in the graphical interface of ParaView, do

176
0:16:20.560 --> 0:16:26.400
some analysis, export this as a Python script and use this script to feed Catalysts and

177
0:16:26.400 --> 0:16:32.880
then when you run your simulation with Catalysts enabled, it will reuse the script you produced

178
0:16:32.880 --> 0:16:34.300
directly from the GUI.

179
0:16:34.300 --> 0:16:41.880
So people that are not at all developers still can do some stuff with Catalysts.

180
0:16:41.880 --> 0:16:46.440
And last point is that when you have a running simulation with the Catalysts pipeline on

181
0:16:46.440 --> 0:16:52.720
your dedicated server, you also can use the GUI to connect to this ParaView server and

182
0:16:52.720 --> 0:16:58.280
to see real time and get some screenshots of the visualization on the analysis that

183
0:16:58.280 --> 0:17:00.280
is proceeding on the server.

184
0:17:00.280 --> 0:17:06.640
So you can have a feedback, a time step per time step on what happened on the simulation.

185
0:17:06.640 --> 0:17:11.480
So if you see that something is diverging or going wrong, you can stop your simulation

186
0:17:11.480 --> 0:17:17.480
directly and you don't waste all the time before seeing that something went wrong and

187
0:17:17.480 --> 0:17:24.640
that you should tweak the parameter and start again.

188
0:17:24.640 --> 0:17:31.240
So yeah, I was quite faster than expected for me.

189
0:17:31.240 --> 0:17:39.280
So in the conclusions of to be able to run efficiently on supercomputer with ParaView,

190
0:17:39.280 --> 0:17:41.920
we implemented a client server mode.

191
0:17:41.920 --> 0:17:46.900
The server is MPI-aware and can be run on distributed environments.

192
0:17:46.900 --> 0:17:53.920
We are relying on old and well-known libraries such as implementation of MPI to do these

193
0:17:53.920 --> 0:18:06.560
distributions, but we are also really looking toward new library that can help us.

194
0:18:06.560 --> 0:18:13.760
We are able to integrate new library to do some performance analysis on new library that

195
0:18:13.760 --> 0:18:19.400
is aware of a new architecture of a supercomputer or new technology.

196
0:18:19.400 --> 0:18:25.400
That's okay, for instance, with VTKM or others.

197
0:18:25.400 --> 0:18:33.360
And we have this API to do institute that can save a lot of time and disk space.

198
0:18:33.360 --> 0:18:38.360
Just a slide to summarize the organize, so we have different kind of way to interact

199
0:18:38.360 --> 0:18:39.360
with ParaView.

200
0:18:39.360 --> 0:18:44.000
I dug with Python scripting, the catalyst in CT stuff.

201
0:18:44.000 --> 0:18:45.760
You can also build some custom one.

202
0:18:45.760 --> 0:18:50.000
We have some web example of clients.

203
0:18:50.000 --> 0:18:57.320
At the bottom, we have a list, a non-finite list of library on which we would like to do

204
0:18:57.320 --> 0:19:00.440
the effective work.

205
0:19:00.440 --> 0:19:06.520
So basically open GLMPI, open MP.

206
0:19:06.520 --> 0:19:11.120
Concerning roadmap, we have several improvements that are coming.

207
0:19:11.120 --> 0:19:18.680
First I talk about in situ, induction implementation, you have each rank that does the simulation

208
0:19:18.680 --> 0:19:22.280
does the co-processing work.

209
0:19:22.280 --> 0:19:25.120
So that's not always what is intended.

210
0:19:25.120 --> 0:19:30.880
You want to do the co-processing on other ranks, just because, for instance, you have

211
0:19:30.880 --> 0:19:36.040
dedicated rank for visualization, so you want to do all the processing on the visualization

212
0:19:36.040 --> 0:19:37.800
nodes.

213
0:19:37.800 --> 0:19:41.920
That's not possible just with the in situ implementation, but we have an in-transit

214
0:19:41.920 --> 0:19:52.160
implementation where the simulation can communicate with those different nodes, and the analysis

215
0:19:52.160 --> 0:19:59.440
can happen on other ranks than the simulation, so the simulation can go forward directly.

216
0:19:59.440 --> 0:20:05.800
We also use some new library, recently used a library called DIY that's here to do some

217
0:20:05.800 --> 0:20:07.200
wrapper.

218
0:20:07.200 --> 0:20:17.920
For us, we take it as a wrapper around the MPI, so DIY allows you to cut the data into

219
0:20:17.920 --> 0:20:24.800
different blocks, and then the at runtime DIY itself is aware to do, okay, I should

220
0:20:24.800 --> 0:20:34.360
put three blocks on each rank, or only one block, and it's just a new abstraction over

221
0:20:34.360 --> 0:20:38.160
cutting your data for distribution.

222
0:20:38.160 --> 0:20:46.600
We are also looking for better VTK on always better VTK integrations to be able to run

223
0:20:46.600 --> 0:20:52.080
on a lot of hardware, and something very cool that is really new, it's just in the development

224
0:20:52.080 --> 0:20:57.800
branch of VTK, so it's not in ParaView yet, that was merged, I think, one or two weeks

225
0:20:57.800 --> 0:21:05.080
ago, it's what we call implicit arrays, and basically it's really good for memory point

226
0:21:05.080 --> 0:21:09.520
of view, because it's some kind of use on memory.

227
0:21:09.520 --> 0:21:16.600
For now, in the ParaView process, your data is really an array in the memory, in your

228
0:21:16.600 --> 0:21:25.600
memory, so with the implicit array, we have some views, so you can implement an open pattern

229
0:21:25.600 --> 0:21:33.520
on it, for instance, when you do an ISO control of your data, you know that the resulting

230
0:21:33.520 --> 0:21:39.080
data will all have the same values, so if you want, if you, after the ISO control, you

231
0:21:39.080 --> 0:21:43.680
will have one million points, you will know that all the points will share the same value,

232
0:21:43.680 --> 0:21:49.760
for now it's one million times duplicate in your memory, so that's not efficient, with

233
0:21:49.760 --> 0:21:55.000
implicit array, you can only one time the value and say, okay, this should be an array

234
0:21:55.000 --> 0:22:02.080
of size one million, and the value you should return is this one, but you can imagine, as

235
0:22:02.080 --> 0:22:10.840
though, have a compressed array in your memory, and have an on the fly, uncompressed algorithm,

236
0:22:10.840 --> 0:22:18.000
just when you want to read your data, so it has a cost in terms of time of computations,

237
0:22:18.000 --> 0:22:24.560
but if you run out of memory with too huge data, that can be really great.

238
0:22:24.560 --> 0:22:33.640
Okay, I still can have a lot of things to say, but that was the end of what I put in

239
0:22:33.640 --> 0:22:38.400
the slide, so thanks for attending, it sounds to be here early in the morning, and if you

240
0:22:38.400 --> 0:22:43.760
have any questions, I think it will be the time, I put just a lot of resources at the

241
0:22:43.760 --> 0:22:48.360
end of the slide, so you can get it from the website of the forum.

242
0:22:48.360 --> 0:22:49.360
Thank you.

243
0:22:49.360 --> 0:22:50.360
Thanks everyone.

244
0:22:50.360 --> 0:22:57.080
Thank you very much, Nikolas.

245
0:22:57.080 --> 0:23:03.920
Do we have any questions?

246
0:23:03.920 --> 0:23:04.920
Thank you.

247
0:23:04.920 --> 0:23:08.240
In our group, we are happy users of ParaView.

248
0:23:08.240 --> 0:23:18.880
One thing that maybe I could add to a wish list, or some, well, maybe just for discussion,

249
0:23:18.880 --> 0:23:26.920
is that we have quite some headache when using ParaView on GitHub actions for multiple platforms,

250
0:23:26.920 --> 0:23:32.360
so like to set up environments for Linux, Mac and Windows with the same version of ParaView

251
0:23:32.360 --> 0:23:39.840
coupling with Python, just to be ready to use it, it's a bit of a headache, especially

252
0:23:39.840 --> 0:23:45.800
when you go to Windows and you need to download things, brew things, opt to get install things,

253
0:23:45.800 --> 0:23:48.120
and then they not necessarily work altogether.

254
0:23:48.120 --> 0:23:55.000
So wish list thing, GitHub actions, ParaView setup thing, unless it doesn't, it exists

255
0:23:55.000 --> 0:23:58.760
already, but I haven't found it.

256
0:23:58.760 --> 0:24:03.160
And the truth is, if there are no questions here.

257
0:24:03.160 --> 0:24:08.880
The use of ParaView and GitHub actions, so in like a continuous integration, limited

258
0:24:08.880 --> 0:24:09.880
environment I guess.

259
0:24:09.880 --> 0:24:10.880
It's a wish list, I think.

260
0:24:10.880 --> 0:24:12.880
Yeah, it's a wish list, yeah.

261
0:24:12.880 --> 0:24:20.840
Well, we don't use GitHub directly, we have the GitLab where you can find a lot of stuff

262
0:24:20.840 --> 0:24:30.920
where our CI and CD will produce nightly releases of ParaView through the GitLab.

263
0:24:30.920 --> 0:24:35.760
I don't know if I answered part of the question.

264
0:24:35.760 --> 0:24:39.800
So what kind of stuff are you doing with ParaView and GitHub actions?

265
0:24:39.800 --> 0:24:40.800
Is it rendering?

266
0:24:40.800 --> 0:24:43.800
Rendering with Python, yeah.

267
0:24:43.800 --> 0:24:48.160
The fact is, I don't really know about GitHub actions because I don't use GitHub anymore,

268
0:24:48.160 --> 0:24:56.560
so I don't see what you can do with that, that you should not able to do otherwise.

269
0:24:56.560 --> 0:24:57.560
Any other questions?

270
0:24:57.560 --> 0:25:01.600
There's a question on the chat.

271
0:25:01.600 --> 0:25:06.680
Yeah, there's a question on the chat.

272
0:25:06.680 --> 0:25:10.920
If I want to put catalyst in my simulation, what is the first step?

273
0:25:10.920 --> 0:25:13.560
Oh, sorry, if you want to use catalyst?

274
0:25:13.560 --> 0:25:16.000
What's the first step?

275
0:25:16.000 --> 0:25:20.800
We have some, I think we have some tutorials, an example in the code base of ParaView, we

276
0:25:20.800 --> 0:25:27.960
have some examples where there are some dummy simulations with just a main, so you can enter

277
0:25:27.960 --> 0:25:37.440
it from it to see how it is organized, and yeah, one first thing is to be able to know

278
0:25:37.440 --> 0:25:45.440
what do you want, which data do you want to send through catalyst, and where you can access

279
0:25:45.440 --> 0:25:55.160
it in your code, and then so at this time you have located the entry points from your

280
0:25:55.160 --> 0:26:01.800
simulation code, and then you will be able to start writing the small wrapper you need

281
0:26:01.800 --> 0:26:07.600
to wrap your data on the need to the actual API of ParaView.

282
0:26:07.600 --> 0:26:08.600
Thanks.

283
0:26:08.600 --> 0:26:09.600
Okay.

284
0:26:09.600 --> 0:26:12.000
Any other burning questions?

285
0:26:12.000 --> 0:26:17.400
Maybe one last, yeah, last question.

286
0:26:17.400 --> 0:26:18.400
Thank you for the talk.

287
0:26:18.400 --> 0:26:19.400
Very naive question.

288
0:26:19.400 --> 0:26:20.400
Awesome.

289
0:26:20.400 --> 0:26:21.400
Is it working?

290
0:26:21.400 --> 0:26:25.360
Is it working?

291
0:26:25.360 --> 0:26:29.920
A very naive question, because I know almost nothing about ParaView.

292
0:26:29.920 --> 0:26:34.840
You had many components there, one of them was the client that does the visualizations.

293
0:26:34.840 --> 0:26:40.080
Would it be possible at some point in the future to be like a web client, where you

294
0:26:40.080 --> 0:26:45.120
just log into the website and it just displays everything, or is it just, do you do the architecture

295
0:26:45.120 --> 0:26:49.000
or is it like super complicated to do it that way?

296
0:26:49.000 --> 0:26:58.880
So the question is, are we able to use a web client for ParaView?

297
0:26:58.880 --> 0:27:02.880
Just for the part that does the visualization, if that could be like a web.

298
0:27:02.880 --> 0:27:10.560
We have some web client for ParaView already, so we have a framework called Trame, that

299
0:27:10.560 --> 0:27:16.960
in turn need to connect to ParaView server, and then you build your own front end for

300
0:27:16.960 --> 0:27:25.040
these applications, so basically we don't have, you should build your own, but it can

301
0:27:25.040 --> 0:27:32.580
be, okay, I have a server, I open always this data, and the front end is just a 3D render

302
0:27:32.580 --> 0:27:33.580
view.

303
0:27:33.580 --> 0:27:37.440
That's already possible quite easily, I think, with the Trame framework.

304
0:27:37.440 --> 0:27:40.640
And Jupyter Notebooks also included, right?

305
0:27:40.640 --> 0:27:44.880
Jupyter Notebooks, I think I saw it on the user interface line.

306
0:27:44.880 --> 0:27:51.760
As we use intensively Python, we also make the step to be supported from a Jupyter Notebook,

307
0:27:51.760 --> 0:27:57.320
and we also have a plugin that allow you to control a ParaView GUI, so you can do some

308
0:27:57.320 --> 0:28:02.160
stuff in the Notebook, and if something goes wrong or you don't understand, you can launch

309
0:28:02.160 --> 0:28:06.640
a magic command run ParaView, that's open the ParaView client with all the current pipeline,

310
0:28:06.640 --> 0:28:11.640
and you can introspect in the GUI, and then you can go back to your Notebook.

311
0:28:11.640 --> 0:28:15.160
Okay, thank you very much, Nicholas.

312
0:28:15.160 --> 0:28:16.160
Thanks.

313
0:28:16.160 --> 0:28:17.160
Thank you.

314
0:28:17.160 --> 0:28:18.160
Thank you.

315
0:28:18.160 --> 0:28:19.160
Thank you.

316
0:28:19.160 --> 0:28:41.900
Thank you.

