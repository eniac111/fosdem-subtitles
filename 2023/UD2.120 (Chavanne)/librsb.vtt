WEBVTT

00:00.000 --> 00:16.880
We're going to start the next talk.

00:16.880 --> 00:19.080
If you're going to stay in the room, please take a seat.

00:19.080 --> 00:26.760
If you want to leave, please leave.

00:26.760 --> 00:38.120
Okay, so the next speaker is Mikaeli, who's going to talk about a universal sparse blast

00:38.120 --> 00:41.120
library.

00:41.120 --> 00:48.520
So, yes.

00:48.520 --> 00:55.520
At the core of many technical or scientific computing problems, we end up with reduced

00:55.520 --> 01:00.880
the problem to solving a system of linear equations.

01:00.880 --> 01:07.160
If the system of linear equations were a simple one, like a two by two one, the method of

01:07.160 --> 01:09.800
solving would be pretty simple.

01:09.800 --> 01:21.560
And in any case, it would involve representing the linear systems by the data structure of

01:21.560 --> 01:29.600
a matrix, so a table of symbols, or usually numbers, and a few vectors of numbers.

01:29.600 --> 01:35.240
So the matrix is the basic structure of scientific computing.

01:35.240 --> 01:44.520
In the case of such toys problems or school problems, we have exact direct solutions at

01:44.520 --> 01:48.680
our disposal, which works fine.

01:48.680 --> 01:58.080
However, once we go into the problems involving simulation of larger domains, so engineering

01:58.080 --> 02:08.680
problems, those linear systems to be solved become large, and also the methods that we

02:08.680 --> 02:14.440
use for smaller systems are not applicable here anymore, because the numerical stability

02:14.440 --> 02:21.720
of, let's say, toy problems or small problems, the stability is not here anymore.

02:21.720 --> 02:27.200
Simply with those methods, numbers, results, diverge.

02:27.200 --> 02:32.280
And the time to solution also increases more than exponentially.

02:32.280 --> 02:35.720
So they're simply infeasible and don't make sense.

02:35.720 --> 02:44.480
So it's a different, it was completely different techniques for large linear systems.

02:44.480 --> 02:56.040
So furthermore, if the systems were not only large, but also full of zeros in the matrices,

02:56.040 --> 03:01.360
so how do we call the systems, or how do we, what do we have to do it here?

03:01.360 --> 03:06.080
We have to do perhaps with sparse systems or sparse problems.

03:06.080 --> 03:08.260
This is the way we call it.

03:08.260 --> 03:17.000
So in this acoustics matrix, or matrix coming from acoustics, we observe that less than

03:17.000 --> 03:25.140
a half percent of each row on the average has a non-zero element.

03:25.140 --> 03:34.120
So we would call this sparse system, perhaps, so the system coming from this matrix.

03:34.120 --> 03:43.320
Indeed, usually we are happy with the definition of Jim Wilkinson, where we say a problem or

03:43.320 --> 03:45.840
a matrix is sparse.

03:45.840 --> 03:53.800
If we can, with our technology, which are technique, we can make use of the amount of

03:53.800 --> 03:56.720
zeros there to our advantage.

03:56.720 --> 03:57.720
So this is the definition.

03:57.720 --> 03:58.720
It's not about numbers.

03:58.720 --> 04:04.400
It's really about what we are able to do with the way the matrix looks like.

04:04.400 --> 04:13.240
So among the different matrices we can encounter, we could have matrices from a circuit simulation

04:13.240 --> 04:20.000
which look like this and have such clustered elements in them.

04:20.000 --> 04:26.040
Sometimes the elements are more clustered around the diagonal, like in this quantum

04:26.040 --> 04:30.640
chromodynamics matrix.

04:30.640 --> 04:34.880
Computational flow dynamics matrices are a bit more regular, you could say.

04:34.880 --> 04:39.560
So it means that you can exploit all of those matrices perhaps in different ways.

04:39.560 --> 04:41.600
This is why I'm showing you this gallery.

04:41.600 --> 04:49.980
This is another CFD, so computational flow dynamics matrix, a structural matrix, another

04:49.980 --> 04:54.520
material problem matrix, structural, and so on.

04:54.520 --> 04:56.040
This is also CFD1.

04:56.040 --> 05:04.400
So this was just to tell you that sparsity really is related to the technology we use

05:04.400 --> 05:07.500
to deal with it.

05:07.500 --> 05:14.160
So usually we are happy using iterative methods with sparse systems.

05:14.160 --> 05:20.560
Iterative methods because something is being iterated, so there is a loop, and with the

05:20.560 --> 05:28.000
most common methods, Krilov methods, the loop usually has a bottleneck, has a core operation,

05:28.000 --> 05:36.560
which is prominently multiplying the system matrix by one vector or many vectors.

05:36.560 --> 05:38.480
Depends a bit on the technique.

05:38.480 --> 05:40.360
There are several of them.

05:40.360 --> 05:46.600
Some of them showing a new octave implementation of such one iterative method.

05:46.600 --> 05:50.760
So there are two kernel operations or main operations.

05:50.760 --> 05:56.680
Multiplication of the matrix by many vectors or let's say another dense matrix, or the

05:56.680 --> 06:03.680
triangular solve, so the solving of matrices which are called preconditioner matrices,

06:03.680 --> 06:05.040
but are sparse.

06:05.040 --> 06:11.880
And these are the core operations which we are interested in, and I want to mention that

06:11.880 --> 06:20.240
those operations for the sparse matrix vector or multi-vector operation can have many variants.

06:20.240 --> 06:29.320
The variants can be on the matrix, which could be perhaps complex and hermitian and all symmetric.

06:29.320 --> 06:31.760
It doesn't have always to be square.

06:31.760 --> 06:38.800
It could be any rectangular, and perhaps it has already a unit diagonal, and we can exploit

06:38.800 --> 06:39.800
this.

06:39.800 --> 06:43.040
This is what I'm saying this.

06:43.040 --> 06:50.680
Many things change if the matrix has a different, has a complex number, so long complex numbers

06:50.680 --> 06:54.880
like a speaker before me spoke about.

06:54.880 --> 06:59.800
So that changes the balances in the performance profile here.

06:59.800 --> 07:01.160
And other things might change.

07:01.160 --> 07:06.040
And all of this have influence on the specific kernels.

07:06.040 --> 07:11.360
And if you think like Ludovic has spoken about the different variants that one might want

07:11.360 --> 07:16.520
to build over different architectures, you see that this is, you end up with code bloat

07:16.520 --> 07:20.440
if you really want to optimize each subcase.

07:20.440 --> 07:24.240
Also the operands have their own variants.

07:24.240 --> 07:28.840
So in the way the data are laid in the dense matrix.

07:28.840 --> 07:29.840
Yeah.

07:29.840 --> 07:37.040
And similarly for the triangular solve operation, there also you have different variants, which

07:37.040 --> 07:42.920
leads to a multitude of different kernels or ways you wish to write them, kernels of

07:42.920 --> 07:44.800
code.

07:44.800 --> 07:51.880
So this leads to a committee of people, end of the 90s, to meet together, mostly US people,

07:51.880 --> 08:00.880
but also with delegations from Europe, to standardize an API, which they called sparse

08:00.880 --> 08:07.960
blast, sparse basic linear algebra subroutines, to somehow just give an API to the different

08:07.960 --> 08:11.920
variations of the operations that I spoke about.

08:11.920 --> 08:15.560
It's mostly, it's not like full blast, if you know the dense blast, it's mostly about

08:15.560 --> 08:20.960
creating sparse matrices, destroying them, and doing a few operations, not only those

08:20.960 --> 08:24.040
but these are really the core operations.

08:24.040 --> 08:30.320
And they talked about C and Fortran because 20 years ago, 20 something years ago was the

08:30.320 --> 08:33.680
final document which they finalized.

08:33.680 --> 08:39.920
Now after 20 years we could say that, well, what they've wrote, especially in my opinion,

08:39.920 --> 08:47.640
is perfectly portable, allows some parallelization even if it's not specified at all.

08:47.640 --> 08:52.000
They didn't foresee extensions but it's possible.

08:52.000 --> 08:59.520
If you look at the API, you see that you can have extensions, so they're not blocked somehow.

08:59.520 --> 09:05.080
The namesake of sparse blast has been copied by every major vendor you can imagine.

09:05.080 --> 09:10.560
The set thing that each major vendor has completely violated their API, so they changed something

09:10.560 --> 09:15.200
in a slightly incompatible way, which is sad, it's simply sad.

09:15.200 --> 09:20.200
And the original sparse blast didn't think about the GPUs but actually in my experience

09:20.200 --> 09:25.440
looking at how people program codes, I see so much technical depth that I think you can

09:25.440 --> 09:34.720
do compromises and with small adaptions you could adapt the sparse blast to the GPU computations

09:34.720 --> 09:36.600
to some extent.

09:36.600 --> 09:40.600
So I think you can save this API to a good extent.

09:40.600 --> 09:44.220
And this is the reason why I'm here.

09:44.220 --> 09:49.080
So I wrote a library which respects the original sparse blast.

09:49.080 --> 09:55.040
So I see sparse blast program can look like this where you have annotation for the sparse

09:55.040 --> 10:02.000
blast operations, which is logical if you know a bit, so you can understand it a bit.

10:02.000 --> 10:09.180
And going in the direction of my library, it's centered around a data format, a sparse

10:09.180 --> 10:15.760
matrix format which I came up with, it's called recursive sparse blocks, because there is

10:15.760 --> 10:20.720
a recursive sub division, there are blocks which are sparse.

10:20.720 --> 10:27.080
And the reason, the motivation for this data structure is to not exclude the sparse blast

10:27.080 --> 10:28.080
operations.

10:28.080 --> 10:32.960
So I have made compromises in order to allow sparse blast operations to be there.

10:32.960 --> 10:36.680
I didn't want to preclude these operations.

10:36.680 --> 10:39.440
So it's a compromise.

10:39.440 --> 10:46.980
And the core idea here is to have, let's say, cache size blocks more or less, and a way

10:46.980 --> 10:52.520
to give each multi-thread, multi-core, core something to work with.

10:52.520 --> 10:55.080
So it's oriented towards multi-core.

10:55.080 --> 10:58.980
It's not for GPUs, or not at the moment at least.

10:58.980 --> 11:04.840
So the matrices which you have seen before with this data format, the data structure

11:04.840 --> 11:06.760
works a bit like this.

11:06.760 --> 11:12.160
The color is based on the population, on the amount of matrices are there.

11:12.160 --> 11:16.560
Then there is another code with other information.

11:16.560 --> 11:21.620
But this is just to tell you that the irregular aspect of those matrices is reflected also

11:21.620 --> 11:29.000
here to some extent.

11:29.000 --> 11:34.080
So the library itself wants to provide sparse blocks.

11:34.080 --> 11:41.360
So building blocks for iterative solvers is pretty compatible at the library.

11:41.360 --> 11:45.120
It works with C++, Fortune, Octave, and Python.

11:45.120 --> 11:46.680
I say it's quite compatible.

11:46.680 --> 11:51.740
So it uses, let's say, established technologies.

11:51.740 --> 11:54.720
And it's quite compatible also in the sense with your software.

11:54.720 --> 11:56.680
It doesn't require you to use too many.

11:56.680 --> 11:59.760
The only data structure which is custom is the matrix.

11:59.760 --> 12:06.040
You don't need extra data structures for vectors.

12:06.040 --> 12:11.920
And the program you saw before written in the sparse blocks for using RSB uses just

12:11.920 --> 12:14.800
one extra init and finalize function.

12:14.800 --> 12:17.360
So I really respect the API.

12:17.360 --> 12:21.360
But however, it's nice to write also the 15th standard.

12:21.360 --> 12:22.360
Or I'm joking.

12:22.360 --> 12:26.440
This is not the 15th standard, but just the internal API.

12:26.440 --> 12:32.440
So if you want, perhaps you can exploit the internal API of LibreSB, or not internal,

12:32.440 --> 12:34.120
but the native one.

12:34.120 --> 12:39.640
Please tell me when I'm at 10 minutes.

12:39.640 --> 12:44.760
Which is primarily in C. Then you have wrappers with C++.

12:44.760 --> 12:47.960
And there's also the Fortune 1.

12:47.960 --> 12:52.560
These are the native APIs.

12:52.560 --> 13:01.160
And what is specific about RSB is that the blocking is not so clear which blocking is

13:01.160 --> 13:02.160
best.

13:02.160 --> 13:09.920
Because, yeah, depending on how you block, you could have better or worse performance.

13:09.920 --> 13:17.520
And for this reason, there is an idea of using automated empirical optimization in this library.

13:17.520 --> 13:25.680
There is a special call, a function which you call where you invest time to ask the

13:25.680 --> 13:29.680
library to optimize a bit the data structure.

13:29.680 --> 13:36.960
So you sacrifice a minute, perhaps, for optimizing the data structure a bit.

13:36.960 --> 13:44.320
And you do this in the hope that the many hours which will be using this matrix afterwards

13:44.320 --> 13:49.800
will be, will profit, will be decreased thanks to the optimization.

13:49.800 --> 13:54.000
Because as I said, this is meant to be used for iterative methods.

13:54.000 --> 13:56.720
So you will be running this for many hours.

13:56.720 --> 14:01.520
And therefore, spending a few minutes in automated optimization, it's something that should pay

14:01.520 --> 14:02.520
off.

14:02.520 --> 14:03.520
No guarantee.

14:03.520 --> 14:04.520
But that's the idea.

14:04.520 --> 14:08.280
And that is usually how it's got.

14:08.280 --> 14:11.920
To give an idea, this C++ API is what you would expect.

14:11.920 --> 14:16.760
So there is a class templated on the type.

14:16.760 --> 14:18.600
So there is type safety here.

14:18.600 --> 14:20.000
What do you say?

14:20.000 --> 14:21.480
This is my library.

14:21.480 --> 14:22.760
Sorry, this is my matrix.

14:22.760 --> 14:28.160
These are my non-zeros because this is what we are representing here.

14:28.160 --> 14:38.460
We have flags, C-style flags for options like symmetry or asking for discarding zeros rather

14:38.460 --> 14:43.160
than keeping the zeros because sometimes you want to keep structural zeros for modifying

14:43.160 --> 14:44.360
them later, for instance.

14:44.360 --> 14:46.440
So you have many such options here.

14:46.440 --> 14:48.760
And this is why I'm showing this slide.

14:48.760 --> 14:52.520
I'll tell you that there are many options which I'm not showing you here.

14:52.520 --> 14:55.160
And the only data structure here is the RSB matrix.

14:55.160 --> 14:56.840
No other custom stuff.

14:56.840 --> 15:04.480
And you can enjoy the spam interface of C++ 20 that doesn't really force you to have any

15:04.480 --> 15:10.480
weird custom vector type apart from the standard C++ ones.

15:10.480 --> 15:18.920
If you want to use, for instance, GNU Octave and enjoy the multi-core speedup from LibreSB,

15:18.920 --> 15:28.460
you can use the sparse RSB plugin which I wrote which uses LibreSB pretty efficiently.

15:28.460 --> 15:35.440
So apart from a few conversions, it should have almost native performance.

15:35.440 --> 15:47.280
Similarly for Python, the PyRSB plugin for standalone package has an interface which

15:47.280 --> 15:52.900
is copied from CSR matrix.

15:52.900 --> 15:59.080
So you use it mostly the same way, but underneath, LibreSB runs.

15:59.080 --> 16:00.600
You don't see it.

16:00.600 --> 16:06.520
Or you see it if you ask it to use the auto-tuning routine.

16:06.520 --> 16:13.120
Because as I said, in all of those language implementations, you can also use all of the

16:13.120 --> 16:18.740
functionality of LibreSB which includes the auto-tuning also here in Octave.

16:18.740 --> 16:20.440
And I want to stress this.

16:20.440 --> 16:25.080
GNU Octave doesn't have multi-threaded sparse operations.

16:25.080 --> 16:26.660
With LibreSB, you can have them.

16:26.660 --> 16:30.560
Same for SciPy sparse.

16:30.560 --> 16:33.160
As far as I know, it's not multi-threaded.

16:33.160 --> 16:35.320
With LibreSB, you get it.

16:35.320 --> 16:44.680
LibreSB is by default is licensed as lesser GPL3, which means you can, as long as you

16:44.680 --> 16:52.320
don't modify it, you can distribute it with your proprietary code.

16:52.320 --> 16:55.680
If you modify it, well, it's more complicated.

16:55.680 --> 16:59.600
You have to release the modified version.

16:59.600 --> 17:06.600
The LibreSB library is if you want to learn to use it, it makes absolutely sense to use

17:06.600 --> 17:14.460
a packaged version from Debian Ubuntu or most of Linux distributions.

17:14.460 --> 17:19.040
Or if you use Windows and you can use Seguin.

17:19.040 --> 17:24.160
Or once you want the performance, I mean, you can just compile it by yourself because

17:24.160 --> 17:25.400
it's quite trivial.

17:25.400 --> 17:31.400
Or enjoy what our colleagues here from Spark and EasyBuild have done and use the packaged

17:31.400 --> 17:35.720
version from those distributions.

17:35.720 --> 17:43.240
And some people have written wrappers for Rust and Julia.

17:43.240 --> 17:47.840
I don't know these languages, so I didn't use them.

17:47.840 --> 17:50.720
I think the last one is like the entire API.

17:50.720 --> 17:58.000
I think Julia is more in Julia style, so it's just what is the core functionality is there,

17:58.000 --> 17:59.000
I think.

17:59.000 --> 18:00.000
Yeah.

18:00.000 --> 18:01.000
That was everything.

18:01.000 --> 18:06.000
I don't know how much time did I take.

18:06.000 --> 18:09.880
Oh, 50 minutes.

18:09.880 --> 18:17.480
Okay.
