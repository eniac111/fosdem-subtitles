WEBVTT

00:00.000 --> 00:29.700
Okay, next talk is Pablo.

00:29.700 --> 00:32.460
who was going to explain us how to set up

00:32.460 --> 00:34.500
Slurm client environments more easily.

00:36.300 --> 00:39.140
Good morning everybody, thanks for attending.

00:39.140 --> 00:42.240
My name is Pablo, I'm representing

00:42.240 --> 00:46.740
the Simplified Creation of Slurm Client Environment,

00:46.740 --> 00:48.400
especially for containers.

00:50.340 --> 00:52.300
Just a quick introduction about myself.

00:53.540 --> 00:58.540
I first started working with Slurm professionally at CERN

00:58.540 --> 01:00.420
for about five years.

01:00.420 --> 01:03.540
I was running the HPC clusters at CERN.

01:03.540 --> 01:07.100
That involved mostly Slurm, running Slurm.

01:07.100 --> 01:11.260
That's when I came up with the idea for this tool.

01:11.260 --> 01:15.500
Since about AOR nine months ago,

01:15.500 --> 01:17.420
I started working at EPFL,

01:17.420 --> 01:19.700
it's a university in Switzerland.

01:19.700 --> 01:22.980
And there I also, I'm in a team that manages

01:22.980 --> 01:25.780
the HPC clusters, and I'm also participating

01:25.780 --> 01:28.860
in the SKA project, hence the free account,

01:29.820 --> 01:32.780
where we do also things related to the HPC

01:32.780 --> 01:33.940
and HPC infrastructure.

01:37.440 --> 01:40.820
So just a brief introduction to Slurm

01:40.820 --> 01:43.860
in case anybody is not familiar with it.

01:44.820 --> 01:48.580
Slurm is basically both a resource manager

01:48.580 --> 01:52.740
and a job scheduler, meaning Slurm will manage

01:52.740 --> 01:57.260
your allocations, it will track which machines are used

01:57.260 --> 02:00.980
and allocate to which jobs and which users own,

02:00.980 --> 02:05.060
which CPUs and which nodes, et cetera.

02:05.060 --> 02:07.500
And it's also the job scheduler,

02:07.500 --> 02:10.340
meaning it will, when users submit jobs,

02:10.340 --> 02:12.020
you have your happy users over there,

02:12.020 --> 02:14.820
or hopefully people will be happy users.

02:14.820 --> 02:17.020
And they will be one-on-one on your cluster,

02:17.020 --> 02:18.380
so they make a job submission,

02:18.380 --> 02:23.380
usually writing a script that launches some workloads.

02:25.820 --> 02:28.380
And they will basically interact with Slurm,

02:28.380 --> 02:30.860
and Slurm will manage all of these job submissions.

02:30.860 --> 02:32.620
You won't just have one by one,

02:32.620 --> 02:35.220
you will have hundreds or even thousands of jobs

02:35.220 --> 02:38.220
that are scheduled to run on the infrastructure,

02:38.220 --> 02:41.740
and Slurm will manage the fees and the priorities

02:41.740 --> 02:44.460
and the accounting, et cetera.

02:44.460 --> 02:47.380
So basically it's a batch manager,

02:47.380 --> 02:49.740
but there's both resource managing

02:49.740 --> 02:51.900
and the scheduling of the jobs.

02:54.420 --> 02:56.380
Building a bit deeper into how Slurm works,

02:56.380 --> 02:59.340
because this is relevant for this talk,

02:59.340 --> 03:01.420
there's basically two main components,

03:01.420 --> 03:04.180
two demons that are most relevant,

03:04.180 --> 03:05.980
and those are the controller,

03:06.900 --> 03:08.860
which is called the Slurm CTLD,

03:08.860 --> 03:11.740
and then the demons that run on the worker nodes

03:11.740 --> 03:15.380
at the bottom, which is the Slurm VDL.

03:15.380 --> 03:17.860
And then you have other demons,

03:17.860 --> 03:21.260
like the Slurm VPL, Slurm SD, Slurm VPL.

03:21.260 --> 03:22.580
Those are not relevant for this talk,

03:22.580 --> 03:25.340
but we mostly focus on the part on the left here.

03:27.980 --> 03:30.300
So users and client tools,

03:30.300 --> 03:33.260
they basically interact with the controller

03:33.260 --> 03:34.740
over a Slurm protocol.

03:35.940 --> 03:37.420
There's nowadays Slurm SD,

03:37.420 --> 03:40.740
so you can also interact over the rest with some scripts,

03:40.740 --> 03:43.700
but mostly the older user lab tools,

03:43.700 --> 03:47.460
and mostly almost everything in the Slurm ecosystem

03:47.460 --> 03:49.420
just talks to the Slurm CTLD,

03:49.420 --> 03:53.140
and this controller handles the source of truth for Slurm,

03:53.140 --> 03:55.940
so it knows which resources are allocated where,

03:55.940 --> 03:57.500
it knows which jobs exist,

03:57.500 --> 03:59.620
it knows who the users are, et cetera.

04:00.980 --> 04:04.140
The controller talks to the Slurm demons,

04:04.140 --> 04:06.140
and talking to the nodes in the Slurm units

04:06.140 --> 04:07.860
are a charge of launching the jobs,

04:07.860 --> 04:11.180
so you clean up, setting up the C-groups of the jobs,

04:11.180 --> 04:12.020
whatever you have.

04:12.020 --> 04:15.940
Now, what's important here is to know that

04:15.940 --> 04:19.420
for all of this to work, you need at least two things.

04:19.420 --> 04:21.500
You need the Slurm config files,

04:21.500 --> 04:24.020
and they need to be in sync between the whole cluster,

04:24.020 --> 04:26.340
so you may have some differences,

04:26.340 --> 04:28.420
but mostly it should be the same.

04:35.060 --> 04:36.020
There was no audio online?

04:36.020 --> 04:36.860
Okay, oh.

04:36.860 --> 04:41.180
So, as I was saying,

04:41.180 --> 04:45.420
the Slurm CTLD handles the source of truth,

04:45.420 --> 04:49.540
the Slurm demons are in charge of launching the jobs,

04:49.540 --> 04:51.540
and the two important things are that

04:51.540 --> 04:55.060
you need the Slurm configuration files,

04:55.060 --> 04:56.220
it's mostly the Slurm comp file,

04:56.220 --> 04:58.540
but there's other files as well.

04:58.540 --> 05:01.340
Those need to be in sync in the whole cluster,

05:01.340 --> 05:03.940
and they need to be basically the same,

05:03.940 --> 05:06.300
they should have the same hash, ideally,

05:06.300 --> 05:08.580
and then you should also have a shared secret,

05:08.580 --> 05:11.700
so that nobody can, a rogue client,

05:11.700 --> 05:13.820
cannot just add a worker node to the cluster

05:13.820 --> 05:16.020
and start doing malicious things.

05:16.020 --> 05:18.780
So you have, usually it's a munch secret,

05:18.780 --> 05:21.260
it's called the demon called munch,

05:21.260 --> 05:25.060
and you have a shared secret as well for the whole cluster.

05:25.060 --> 05:27.140
And this fact is important,

05:27.140 --> 05:28.900
it's very relevant for this talk.

05:30.500 --> 05:32.500
Now, up to containers.

05:32.500 --> 05:34.660
So containers are increasingly becoming

05:34.660 --> 05:37.860
a super popular tool to run infrastructure

05:37.860 --> 05:41.820
for reproducibility, for automating deployments,

05:41.820 --> 05:46.100
and just in general, they're becoming super ubiquitous

05:46.100 --> 05:50.820
in our industry, and I think for good reasons.

05:53.260 --> 05:56.940
And they are, I think, very good use cases

05:56.940 --> 05:59.620
for using containers with Slurm.

06:01.220 --> 06:03.620
In this talk, I will focus on the use case

06:03.620 --> 06:05.900
where you use containers for,

06:05.900 --> 06:08.380
on the user and client side of things.

06:08.380 --> 06:11.500
So those tools that will talk to Slurm,

06:11.500 --> 06:14.860
to the controller mostly, to do things on the cluster.

06:14.860 --> 06:16.140
So this could be some automation

06:16.140 --> 06:19.340
that you have run to do whatever, for instance,

06:19.340 --> 06:22.140
you could use it for monitoring purposes,

06:22.140 --> 06:25.780
you could write a tool that does health checks

06:25.780 --> 06:28.380
on the cluster for accounting,

06:28.380 --> 06:31.780
where I've used it extensively for accounting as well,

06:31.780 --> 06:33.900
but also integration with other services, right?

06:33.900 --> 06:38.420
What if you want to connect the Jupyter Notebook with Slurm,

06:38.420 --> 06:40.620
you will end up with some tools

06:40.620 --> 06:42.420
that talk to the controller.

06:46.700 --> 06:50.340
Now, there are basically two scenarios

06:50.340 --> 06:53.980
in which you would use containers,

06:53.980 --> 06:56.060
you can use containers with Slurm.

06:56.060 --> 06:58.300
On the left, we have the local use case,

06:58.300 --> 07:01.100
that means, imagine you have a front-end mode,

07:01.100 --> 07:04.860
you have a machine that's configured where it uses SSH too,

07:04.860 --> 07:07.060
and from there, they can run the Slurm commands

07:07.060 --> 07:10.900
to launch jobs, to track their job usage, et cetera,

07:10.900 --> 07:13.300
it's conventionally called the front-end mode

07:13.300 --> 07:14.140
for the cluster.

07:15.020 --> 07:19.500
So if you just add the Slurm client container on that node,

07:19.500 --> 07:21.940
it's very simple, because you can just, as I said,

07:21.940 --> 07:26.180
you need a secret with Munch,

07:26.180 --> 07:27.540
and you need the config files,

07:27.540 --> 07:28.700
and that scenario is very simple

07:28.700 --> 07:30.020
because you can just do bind mounts,

07:30.020 --> 07:33.700
and you can access the Munch socket to talk to Slurm,

07:33.700 --> 07:37.220
and you bind mount the Slurm config directory,

07:37.220 --> 07:39.220
and you're done, basically.

07:39.220 --> 07:41.340
So that's sort of easy.

07:41.340 --> 07:46.340
However, what if you have, for the use case on the right,

07:46.460 --> 07:48.860
you have the distributed or remote use case,

07:48.860 --> 07:53.860
and in that case, you may run your Slurm client container

07:54.420 --> 07:57.260
in a different service, that's in a different network,

07:57.260 --> 08:01.140
or you may run it on Kubernetes, or somewhere else.

08:01.140 --> 08:03.700
In that case, you obviously can't just do the bind mounts,

08:03.700 --> 08:07.420
because you need to give it all those things.

08:08.740 --> 08:11.500
So you would have to give it all the Slurm config files,

08:11.500 --> 08:14.060
and somehow the Munch shared key,

08:14.060 --> 08:17.100
so that your external service can talk to your cluster,

08:17.100 --> 08:19.460
right, specifically to the Slurm controller.

08:22.260 --> 08:26.060
Now, this is an extraction from a Dockerfile,

08:26.060 --> 08:27.140
this is the Naive approach,

08:27.140 --> 08:30.980
this is how I started trying things, easy, right?

08:31.900 --> 08:33.980
You just take the Slurmconf,

08:33.980 --> 08:37.220
and you just copy it to the destination, right?

08:37.220 --> 08:39.340
And this will absolutely work.

08:39.340 --> 08:42.620
But I was not happy with this approach,

08:42.620 --> 08:44.780
because then you end up managing two copies

08:44.780 --> 08:47.500
of your Slurmconf, and I really like having

08:47.500 --> 08:49.380
a single source of truth for,

08:49.380 --> 08:50.940
when you do configuration management,

08:50.940 --> 08:52.820
and automation of your infrastructure,

08:52.820 --> 08:55.020
I really like having a single source of truth.

08:55.020 --> 08:57.940
And managing this in this way with containers

08:57.940 --> 09:01.420
is very fiddly, because it's very easy

09:01.420 --> 09:02.620
that you will forget to update it,

09:02.620 --> 09:04.500
or something that will fail to update it automatically,

09:04.500 --> 09:06.700
it's, eh, it's just not ideal,

09:06.700 --> 09:09.700
I didn't like this approach, but it will work, it will work.

09:11.660 --> 09:14.100
And some of you who know Slurm may say,

09:14.100 --> 09:16.300
oh, but Pablo, why wouldn't you just use

09:16.300 --> 09:18.100
Slurm's config-less feature?

09:18.100 --> 09:22.180
So Slurmconf-less is a new feature since Slurm 20 or so,

09:22.180 --> 09:25.900
that will basically allow a client

09:25.900 --> 09:28.740
to just pull the config files from Slurm.

09:28.740 --> 09:32.380
So the Slurm-D demons that run on the worker nodes,

09:32.380 --> 09:36.980
when they start, they will just grab the Slurm config files.

09:36.980 --> 09:40.340
So you can just remove the needs,

09:40.340 --> 09:43.020
so you can copy the Slurmconf file, right?

09:43.020 --> 09:45.340
Well, it's a trick question.

09:47.220 --> 09:51.620
Not necessarily, because then you need to run

09:51.620 --> 09:54.220
a Slurm-D demon in your container,

09:54.220 --> 09:57.100
and you also need the Munch demon.

09:57.100 --> 10:01.860
And it sounds easy, but it's really not, trust me.

10:01.860 --> 10:03.620
You will need to do a lot of hacks,

10:03.620 --> 10:08.620
this is an instruction from a container that I was creating,

10:08.940 --> 10:12.500
and you run in lots of awful, awful things.

10:12.500 --> 10:15.660
Like Slurm-Demon, the Slurm-Demon expects

10:15.660 --> 10:18.620
there's a release agent file to exist in the C group,

10:18.620 --> 10:21.900
and the containers, they just don't create it.

10:21.900 --> 10:24.260
If I tried it on Docker, I tried it on different

10:24.260 --> 10:27.100
Kubernetes versions, it just doesn't exist.

10:27.100 --> 10:29.340
I don't know why, I couldn't find out why,

10:29.340 --> 10:31.700
if anybody knows, please tell me.

10:31.700 --> 10:34.420
I googling around a font that could have been related

10:34.420 --> 10:37.020
to some privileged escalation issues.

10:37.020 --> 10:38.780
However, if you just remount the C groups,

10:38.780 --> 10:42.660
the file appears, so I'm not sure what's going on there.

10:43.660 --> 10:46.500
Another fun story is that, for instance,

10:46.500 --> 10:49.980
if you're using Kubernetes, Kubernetes likes to

10:49.980 --> 10:53.420
give a Simlink to your secrets,

10:53.420 --> 10:56.700
and Munch refuses to take the secret from a Simlink

10:56.700 --> 10:58.780
for security reasons, makes sense.

10:58.780 --> 11:01.380
So there's no notworks, you will need to put in hacks,

11:01.380 --> 11:03.340
and it's hacks on top of hacks on top of hacks

11:03.340 --> 11:05.580
just to run these two demons.

11:05.580 --> 11:09.260
And yeah, I was not very happy with this approach either.

11:10.540 --> 11:15.540
So basically I was faced with two options.

11:15.540 --> 11:17.020
When you're in rough with this situation,

11:17.020 --> 11:18.180
you're faced with two options.

11:18.180 --> 11:21.700
Either you basically do the first naive approach,

11:21.700 --> 11:24.300
where you just copy all the stuff into your Slurm container,

11:24.300 --> 11:27.620
you manage a copy of your Slurm config files,

11:27.620 --> 11:30.700
but as I said, if you wanna consist a single source

11:30.700 --> 11:34.100
of truth, this might not be ideal.

11:34.100 --> 11:36.740
You also need, of course, in the case of use case,

11:36.740 --> 11:38.700
unless you need Munch, and you need to supply

11:38.700 --> 11:42.100
the Munch key, or you can try the conflictless approach,

11:42.100 --> 11:44.180
but then you need to add Slurm D to your container,

11:44.180 --> 11:47.260
so it can pull via conflictless to your conflict files,

11:47.260 --> 11:48.860
but then anyway, you also need Munch,

11:48.860 --> 11:52.740
and you need to add the Munch key to your container somehow

11:52.740 --> 11:53.580
and managing secrets.

11:53.580 --> 11:54.580
I mean, if you're running Kubernetes,

11:54.580 --> 11:55.700
it might not be a big issue,

11:55.700 --> 11:57.140
or some other container manager,

11:57.140 --> 12:01.140
but you will still need to maintain all these extra demons

12:01.140 --> 12:05.380
with nasty hacks, and we don't always like

12:05.380 --> 12:09.340
all these having lots of hacks in our infrastructure.

12:09.340 --> 12:10.900
There's a third option, by the way,

12:10.900 --> 12:14.340
which is trying to go secretless.

12:14.340 --> 12:16.180
It doesn't work in combination with conflictless,

12:16.180 --> 12:18.940
where you try to use JSON web tokens,

12:18.940 --> 12:21.100
but it gives a lot of issues, it doesn't really work,

12:21.100 --> 12:23.980
I tried it, so I didn't include it here.

12:23.980 --> 12:26.700
Just mentioning it in case somebody thought about it.

12:29.220 --> 12:31.440
So Pablo, you talked about the bad and the ugly,

12:31.440 --> 12:34.180
what about the good, is there any good part to this?

12:34.180 --> 12:35.960
I'm glad you asked, yes.

12:35.960 --> 12:39.140
What if we had a single shot CLI tool

12:39.140 --> 12:43.020
that just a very simple tool that just was able

12:43.020 --> 12:44.900
to authenticate to the controller,

12:44.900 --> 12:47.420
either using Munch or JSON web tokens,

12:47.420 --> 12:48.980
which Slurm also supports,

12:48.980 --> 12:52.220
and just fetch the config files and then it's done.

12:53.460 --> 12:56.140
That's all you really want to do, right?

12:56.140 --> 13:00.620
Because then your tools, the Slurm tools can work

13:00.620 --> 13:02.180
because they have the Slurm config files,

13:02.180 --> 13:05.020
and just by having the JSON web token in your environment,

13:05.020 --> 13:09.380
you can just talk to the Slurm controller.

13:09.380 --> 13:12.100
And yeah, that's the tool that I wrote.

13:12.100 --> 13:14.920
It's a very simple tool, it just does exactly

13:14.920 --> 13:17.660
what I described there, and it's open source,

13:17.660 --> 13:22.660
you can find it on GitHub, I uploaded it in the past month.

13:23.860 --> 13:26.220
Fun story about this, as I said,

13:26.220 --> 13:28.580
I had the idea for this when I was back at CERN,

13:28.580 --> 13:32.140
I worked on this a year ago already,

13:32.140 --> 13:35.160
but then I somehow lost the source.

13:35.160 --> 13:36.960
I don't know what happened.

13:36.960 --> 13:40.300
Just before I left CERN, the source was just lost.

13:40.300 --> 13:43.540
I don't know why, I must have deleted it by accident,

13:43.540 --> 13:45.740
I don't know what happened.

13:45.740 --> 13:47.420
So after I left CERN, I kept in contact

13:47.420 --> 13:49.000
with my ex-colleagues and they were telling me

13:49.000 --> 13:50.640
that they wanted to do this integration

13:50.640 --> 13:55.640
between the SWAN, which is the, who here knows SWAN, anybody?

13:56.100 --> 13:58.020
Okay, one, two, three.

13:58.020 --> 14:00.980
Yeah, so it's the Jupyter notebook service for CERN,

14:00.980 --> 14:05.100
which also does analytics, and we wanted to connect it

14:05.100 --> 14:07.180
to SLURM, and we run into all these issues

14:07.180 --> 14:08.740
because this is a service that's exposed

14:08.740 --> 14:11.060
to the whole internet, so we didn't want to have

14:11.060 --> 14:13.780
the munch key for the SLURM cluster in the container,

14:13.780 --> 14:15.300
et cetera.

14:15.300 --> 14:18.660
Anyway, so then I left CERN and then, yeah,

14:18.660 --> 14:20.340
my colleagues were telling me, oh, it would have been

14:20.340 --> 14:22.980
so useful to have this, what a pity.

14:22.980 --> 14:26.740
And then a few months ago, I just, yeah,

14:26.740 --> 14:28.640
I just didn't like the fact that I had lost the source

14:28.640 --> 14:32.820
and all these days, I spent a couple days

14:32.820 --> 14:35.540
reverse engineering the SLURM protocol,

14:35.540 --> 14:39.180
and I just didn't like losing it, so I just rewrote it

14:39.180 --> 14:43.140
more properly in Python and just made it public.

14:43.140 --> 14:46.500
So if you're interested in making client containers

14:46.500 --> 14:49.700
like this, feel free to give it a try.

14:52.140 --> 14:53.440
It looks a bit like this.

14:55.420 --> 14:58.520
It's very simple, you can choose between munch

14:58.520 --> 15:02.460
or JWT, JSON Web Tokens Authentication.

15:02.460 --> 15:04.900
If you choose JWT, which is the most simple one,

15:04.900 --> 15:08.300
you just need an environment variable with a token,

15:09.220 --> 15:11.620
and you can tell it where you want to store

15:11.620 --> 15:16.620
the config files, and then you have verbosity as an option.

15:16.980 --> 15:21.080
So it's very simple, it has very little dependencies.

15:21.080 --> 15:26.080
So the tool talks several SLURM protocol versions,

15:31.840 --> 15:33.720
because with every major release,

15:33.720 --> 15:38.020
SLURM changes the protocol versions.

15:38.020 --> 15:40.740
So you can list them with minus L,

15:40.740 --> 15:43.800
and it will show you basically all the versions

15:43.800 --> 15:44.900
that it supports.

15:46.840 --> 15:49.680
And then, yeah, so imagine you have a SLURM Web Token

15:49.680 --> 15:53.160
in this variable, you can just tell it to do

15:53.160 --> 15:56.520
JSON Web Token Authentication with the server.

15:56.520 --> 15:58.720
It supports multiple controllers in case you have

15:58.720 --> 16:01.240
high availability set up in your SLURM clusters,

16:01.240 --> 16:04.360
so you can specify a list of servers that it will retry

16:04.360 --> 16:07.280
until it succeeds, and then you tell it the protocol version

16:07.280 --> 16:09.480
of the SLURM CTLD, because it needs to know

16:09.480 --> 16:12.280
what protocol it should talk.

16:12.280 --> 16:15.840
The protocol version negotiation, I think it doesn't exist

16:15.840 --> 16:17.560
in the SLURM protocol, so you have to tell it

16:17.560 --> 16:19.320
which version you want it to talk.

16:19.320 --> 16:21.360
And that's it, and then it will just download

16:21.360 --> 16:25.920
the SLURM config files and happy days for your containers.

16:28.600 --> 16:31.560
Conclusions, I think I'm ahead of time.

16:33.840 --> 16:37.240
So this tool called straw, it can simplify the cost

16:37.240 --> 16:41.700
of creating and maintaining your SLURM client containers.

16:41.700 --> 16:43.560
It can also increase the security because you don't need

16:43.560 --> 16:47.000
to put the munch key everywhere where you're running

16:47.000 --> 16:50.680
your client containers, JSON Web Tokens, Sophist.

16:51.880 --> 16:56.840
Caveats, caveats, I think this tool should not exist.

16:58.720 --> 17:01.560
Because ideally this would be supported upstream.

17:01.560 --> 17:06.560
So if anybody has any influence on SkedMD SLURM development,

17:08.280 --> 17:10.600
yeah, I think it would be nice if we had this

17:10.600 --> 17:11.760
built in into SLURM.

17:11.760 --> 17:16.760
And then the second caveat is that the JSON Web Tokens,

17:18.560 --> 17:20.920
they need to build, the token needs to be associated

17:20.920 --> 17:23.520
with either, with a SLURM user, basically.

17:24.640 --> 17:28.200
So ideally, you would be able to just generate

17:28.200 --> 17:31.760
a JSON Web Token for a user that's gonna run

17:31.760 --> 17:35.060
on the SLURM cluster, and then if the secret

17:35.060 --> 17:37.480
for some reason is exposed, you've only exposed

17:37.480 --> 17:40.600
the JSON Web Token of a single user.

17:40.600 --> 17:44.440
However, this is a limitation built into the SLURM,

17:44.440 --> 17:45.820
into SLURM, basically.

17:46.820 --> 17:49.920
You cannot pull over the protocol of the SLURM config file

17:49.920 --> 17:53.600
unless the token belongs to the SLURM user or to root.

17:54.540 --> 17:56.100
Still, I think it's an improvement over having

17:56.100 --> 17:59.320
your munch key available everywhere.

17:59.320 --> 18:01.440
And yeah, feel free to try it out.

18:02.760 --> 18:06.120
That was it, I'm happy to answer any questions you might have.

18:06.120 --> 18:11.120
Thank you very much, Pablo.

18:16.400 --> 18:17.560
Time for questions.

18:19.480 --> 18:23.480
So what kind of clients do need the config file?

18:23.480 --> 18:25.660
Could you do everything over REST nowadays?

18:25.660 --> 18:29.560
Like, is it still necessary to use the config file?

18:29.560 --> 18:33.720
Yes, so anything that wants to run S run, S batch,

18:33.720 --> 18:37.080
S queue, S info, for instance, if you have the Jupyter

18:37.080 --> 18:40.040
notebook plugins, they will run just on those commands.

18:40.040 --> 18:43.200
Or if you wanna run a client that uses PySLURM, for instance,

18:44.260 --> 18:47.200
or any library, really, anything that uses libslurm

18:47.200 --> 18:51.360
underneath will automatically read the config files, right?

18:51.360 --> 18:56.280
So of course, you can write your own client,

18:56.280 --> 18:58.360
handwritten from scratch, that just interacts

18:58.360 --> 19:01.160
with the SLURM RESTD to do stuff.

19:01.160 --> 19:05.320
Yes, but you cannot leverage all the existing

19:05.320 --> 19:10.320
user client tools, and the libslurm, PySLURM, et cetera.

19:10.840 --> 19:15.160
So if you wanna create a Python tool, for instance,

19:15.160 --> 19:18.040
that leverages PySLURM, this would be,

19:18.960 --> 19:20.160
I think, a good solution.

19:21.760 --> 19:23.760
I think SLURM does have a REST API,

19:23.760 --> 19:26.240
but it's considered very insecure.

19:26.240 --> 19:29.040
So even the documentation tells you don't use this.

19:29.040 --> 19:33.760
Maybe, I just didn't understand, like, for a long time now,

19:33.760 --> 19:36.560
why you, everyone needs the config file, right?

19:36.560 --> 19:38.600
I mean, why does it need to be in sync?

19:38.600 --> 19:40.560
Like, couldn't they just exchange the information

19:40.560 --> 19:42.760
over the protocols, and just say, like,

19:42.760 --> 19:44.400
this is your SLURM server?

19:44.400 --> 19:45.760
Yeah, that's the config-less feature,

19:45.760 --> 19:47.080
that's the config-less feature, essentially.

19:47.080 --> 19:48.940
Yeah, but the config-less feature just downloads

19:48.940 --> 19:51.520
the config, look next, like, config-less, okay.

19:51.520 --> 19:52.360
Yes.

19:52.360 --> 19:54.920
Download the config, don't need the config beforehand.

19:56.160 --> 19:58.240
It's like serverless, there's always a server somewhere.

19:58.240 --> 20:00.240
Yes, yeah, exactly.

20:01.080 --> 20:03.760
So, that's just how SLURM works.

20:03.760 --> 20:04.600
Yeah.

20:04.600 --> 20:05.440
Yep.

20:11.760 --> 20:13.040
So, I'm still a little confused

20:13.040 --> 20:14.600
about the SLURM client container.

20:14.600 --> 20:16.120
So the container is an application

20:16.120 --> 20:17.840
on the actual SLURM client,

20:17.840 --> 20:20.120
because you have to document, in the SLURM Conf,

20:20.120 --> 20:22.120
you have to sort of say what your clients are,

20:22.120 --> 20:24.720
so that the scheduler can intelligently

20:24.720 --> 20:27.000
decide how to schedule jobs, right?

20:27.000 --> 20:27.880
I'm missing something.

20:27.880 --> 20:29.760
No, you don't really need to declare

20:29.760 --> 20:32.240
all the clients for SLURM.

20:32.240 --> 20:35.200
You just need to declare the worker nodes

20:35.200 --> 20:38.880
that are part of it, but you can have any,

20:38.880 --> 20:40.360
I mean, it depends on how you've configured it.

20:40.360 --> 20:42.240
You can limit it, you can limit in SLURM

20:42.240 --> 20:44.560
which clients are allowed to connect,

20:44.560 --> 20:45.760
but you don't have to.

20:45.760 --> 20:48.880
So you could just, but even if you do,

20:48.880 --> 20:51.580
you will need this, because you will,

20:51.580 --> 20:54.420
even if you authorize a host name to connect as a client,

20:54.420 --> 20:57.680
it will need to have the munch key

20:57.680 --> 21:00.240
and the SLURM Conf files, et cetera.

21:00.240 --> 21:01.960
Does this answer your question, or are you?

21:01.960 --> 21:04.880
Well, no, so when you, in the SLURM.conf,

21:04.880 --> 21:07.440
you sort of detail what your positions are,

21:07.440 --> 21:10.000
and you have to kind of tell it what the capabilities are

21:10.000 --> 21:11.440
of your clients, of your SLURM clients, right?

21:11.440 --> 21:13.440
So that SLURM can decide how to schedule jobs.

21:13.440 --> 21:15.240
I'm missing something cool about it.

21:15.240 --> 21:17.480
Well, I think you're thinking about the compute nodes.

21:17.480 --> 21:18.560
Yeah, I am.

21:18.560 --> 21:21.280
Yeah, the node names, part of the SLURM Conf.

21:21.280 --> 21:23.160
Right, so the containers run on the compute nodes.

21:23.160 --> 21:24.840
No, the containers would be,

21:24.840 --> 21:28.240
let me go back to one of the slides where,

21:28.240 --> 21:33.240
so you're thinking maybe about the compute nodes,

21:33.360 --> 21:35.640
each of which runs a SLURM D daemon,

21:35.640 --> 21:37.400
and those you have to declare.

21:37.400 --> 21:39.280
Yes, I think in 2023, by the way,

21:39.280 --> 21:42.360
you will be able to dynamically spawn compute nodes,

21:42.360 --> 21:45.520
but that's future.

21:46.520 --> 21:49.080
What I'm talking about is all the users and client tools

21:49.080 --> 21:52.600
that connect to the controller to run SQS info,

21:52.600 --> 21:54.920
like when you use SLURM, and you,

21:54.920 --> 21:59.520
so if you had some tooling that you automated

21:59.520 --> 22:03.240
to gather metrics from SLURM, or yeah,

22:03.240 --> 22:05.080
a Jupyter Notebook service, for instance,

22:05.080 --> 22:08.520
that connects to your cluster that wants to launch jobs,

22:08.520 --> 22:10.880
that wants to run as batch SQ, whatever,

22:10.880 --> 22:12.720
that's in that domain.

22:12.720 --> 22:15.760
Yeah, I mean, the newest version of Werewolf runs containers

22:15.760 --> 22:17.400
on my back for the stream.

22:17.400 --> 22:18.240
Sorry.

22:19.440 --> 22:20.840
I think the newest version of Werewolf

22:20.840 --> 22:23.960
is set up to run containers on the SLURM clients, right?

22:23.960 --> 22:26.960
It's sort of, you're actually launching containers

22:26.960 --> 22:29.000
as applications, so that was kind of.

22:29.000 --> 22:30.720
That's on the compute nodes.

22:30.720 --> 22:31.960
On the compute nodes, yes.

22:31.960 --> 22:33.760
Yeah, yeah, that's the compute nodes.

22:37.600 --> 22:39.120
Thank you for your talk.

22:39.120 --> 22:40.120
So I have a question.

22:40.120 --> 22:44.000
You are telling that you can pull the configuration

22:44.000 --> 22:48.240
with your tool, but there are many, fine,

22:48.240 --> 22:51.440
you can't pull with configless, for example,

22:51.440 --> 22:54.360
all the Spunk plugins, or I think,

22:54.360 --> 22:56.440
to apologize, you can pull it, but various,

22:56.440 --> 22:59.040
like I said, Spunk plugins and so on.

22:59.040 --> 23:01.800
So how do you manage this kind of config file

23:01.800 --> 23:05.160
that are not ended by default by SLURM?

23:05.160 --> 23:06.120
Right, that's correct.

23:06.120 --> 23:08.520
So when you use the configless feature,

23:08.520 --> 23:12.040
it will download the Serm Conf, the Cgroup Conf,

23:12.040 --> 23:14.120
a lot of config files, but it will not download

23:14.120 --> 23:17.040
your plugin files.

23:17.040 --> 23:18.440
But I think those are usually not needed

23:18.440 --> 23:20.760
if you're running a client, because those are just,

23:20.760 --> 23:24.200
usually just needed for the Slurm D-demons,

23:24.200 --> 23:25.840
for the worker nodes.

23:25.840 --> 23:27.440
Like the epilogue, the prologue,

23:27.440 --> 23:29.720
you mean all of those plugins, scripts, right?

23:29.720 --> 23:30.840
The authentication plugins.

23:30.840 --> 23:32.960
Those are usually needed by the Slurm D-demon,

23:32.960 --> 23:35.080
but if you're just writing a client,

23:35.960 --> 23:39.240
but say you're automating something with PySlurm

23:39.240 --> 23:41.000
to interact with it, you don't need those files.

23:41.000 --> 23:43.540
And Slurm will happily, you can happily run,

23:43.540 --> 23:46.020
Sinfo is run as batch, or SQ.

23:46.020 --> 23:47.600
You can happily run all of those commands

23:47.600 --> 23:49.400
without those files.

23:49.400 --> 23:52.360
Yeah, okay, so if I just summarize,

23:52.360 --> 23:54.640
the ID is just to create some front-end nodes,

23:54.640 --> 23:57.640
but not to really work nodes.

23:57.640 --> 23:59.020
That's right.

23:59.020 --> 23:59.860
So,

24:01.940 --> 24:06.940
so if you wanna use configless to set up a front-end node,

24:07.000 --> 24:09.220
you might need those files from somewhere else.

24:09.220 --> 24:12.080
But if you're just creating a container

24:12.080 --> 24:14.400
to just interact with Slurm and send Slurm commands,

24:14.400 --> 24:16.120
you don't need them, basically.

24:17.120 --> 24:20.760
Because the plugin files are usually the,

24:20.760 --> 24:23.560
yeah, the epilogue prologue for the Slurm D

24:23.560 --> 24:24.960
or the Slurm CTLD,

24:25.880 --> 24:30.880
and that's not what these Slurm client containers are about.

24:33.040 --> 24:35.160
So short answer, you usually don't need them.

24:35.160 --> 24:40.160
Hello, thank you for the talk.

24:44.920 --> 24:49.920
I'm wondering, in huge institutions like InCERN or EPFL,

24:50.160 --> 24:55.160
would you run your own forked or patched Slurm

24:55.540 --> 25:00.540
so you could fix maybe the authentication privileges?

25:01.520 --> 25:02.360
Yes.

25:02.360 --> 25:03.840
Or is it just not done because it's a...

25:03.840 --> 25:06.280
I've never carried any Slurm patches, to be honest.

25:06.280 --> 25:08.880
I've always, both at Slurm and EPFL,

25:08.880 --> 25:11.160
we just use Slurm out of the box.

25:11.160 --> 25:13.420
It works well enough for our use cases.

25:14.660 --> 25:16.400
It is true that you could, for instance,

25:16.400 --> 25:19.400
do a patch to enable finer,

25:19.400 --> 25:22.040
good on the similarity for the permissions.

25:22.040 --> 25:23.880
For instance, you could enable any user

25:23.880 --> 25:25.240
to pull the config file.

25:25.240 --> 25:27.400
That would be a nice patch.

25:27.400 --> 25:28.800
We don't do it.

25:28.800 --> 25:29.640
Okay, thank you.

25:29.640 --> 25:32.480
Okay.

25:32.480 --> 25:34.440
We have time for one short question.

25:35.720 --> 25:36.560
Hi, thanks.

25:36.560 --> 25:38.320
We actually are very interested in this

25:38.320 --> 25:42.160
because we are applying,

25:42.160 --> 25:44.360
we have a Jupyter hub frontend

25:44.360 --> 25:48.240
that actually talks to Slurm cluster through SSH

25:48.240 --> 25:50.040
because we don't want to install all that stuff

25:50.040 --> 25:52.880
like the Mange and the full Slurm deployment

25:52.880 --> 25:55.260
into the Jupyter hub host.

25:55.260 --> 25:57.640
And I'm wondering, how does it talk, actually,

25:57.640 --> 25:58.480
to Slurm control?

25:58.480 --> 26:01.080
Is Slurm control always listening to any,

26:03.680 --> 26:06.120
any of hosts that will talk to it?

26:06.120 --> 26:08.520
Or is there any restrictions to who is connecting

26:08.520 --> 26:10.320
to the Slurm control daemon?

26:12.240 --> 26:15.600
So there's an alloc nodes setting in the Slurmconf,

26:15.600 --> 26:19.080
I believe, which will allow you to restrict

26:19.080 --> 26:22.680
from which nodes you can allocate resources.

26:22.680 --> 26:24.760
So you can limit it.

26:24.760 --> 26:26.480
However, if you don't have that,

26:26.480 --> 26:30.040
I think Slurm will happily accept anything

26:30.040 --> 26:32.080
because if you have the shared secret,

26:32.080 --> 26:34.280
it's considered good enough.

26:34.280 --> 26:35.120
Okay.

26:35.120 --> 26:36.120
Or a valid JSON web token.

26:36.120 --> 26:36.960
Okay.

26:36.960 --> 26:37.780
Yeah.

26:37.780 --> 26:39.040
Thank you.

26:39.040 --> 26:40.200
Thank you very much, Pablo.

26:40.200 --> 26:41.040
Thanks.

26:41.040 --> 26:41.880
Thanks.

26:41.880 --> 26:42.720
Thanks.

26:42.720 --> 26:43.560
Thanks.

26:43.560 --> 26:44.400
Thanks.

26:44.400 --> 27:01.360
Okay, thanks everybody very much.
