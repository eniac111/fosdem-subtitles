WEBVTT

00:00.000 --> 00:12.240
Okay, so we're switching topics a bit away from MPI to something at least a little bit

00:12.240 --> 00:15.440
different, link time call graph analysis.

00:15.440 --> 00:18.760
All right, thank you.

00:18.760 --> 00:25.100
So yeah, we're going to talk about user guided program presentation approach and especially

00:25.100 --> 00:31.840
a link time call graph analysis extension to that project which kind of makes it more

00:31.840 --> 00:34.000
usable.

00:34.000 --> 00:38.560
So to give some background on this work, I'm involved in a project called ExaFoam that

00:38.560 --> 00:44.600
deals with the OpenFoam Computational Fluid Dynamics Toolbox and that's a very complex

00:44.600 --> 00:46.960
code quite large.

00:46.960 --> 00:51.840
And the goal here is to improve the performance of OpenFoam for Hqc systems, especially for

00:51.840 --> 00:54.280
the exascale error.

00:54.280 --> 01:00.640
And one of the things we do is develop empirical performance models.

01:00.640 --> 01:04.640
And for that we have developed a workflow, how we do that.

01:04.640 --> 01:11.040
We start with several measurements where we get an initial overview and identify hotspots,

01:11.040 --> 01:17.280
then based on these hotspots do an analysis of the critical kernels and finally we can

01:17.280 --> 01:26.280
do the empirical modeling in order to find scalability bugs and predict performance

01:26.280 --> 01:27.920
when scaling out.

01:27.920 --> 01:32.360
And so especially for the focus measurements and the modeling, you need quite accurate

01:32.360 --> 01:37.520
and reliable methods to be sure that the data you collect is right and you have the right

01:37.520 --> 01:39.840
level of detail.

01:39.840 --> 01:43.080
And what we use for that is code instrumentation.

01:43.080 --> 01:48.040
And just to give the background, I want to give an example.

01:48.040 --> 01:54.120
For example, in Hqc and Clang you have the F instrument functions flag, which does a

01:54.120 --> 02:03.000
very basic instrumentation where if you activate that flag, you insert these, or the compiler

02:03.000 --> 02:10.240
inserts these function enter and exit probes, which are then at runtime interface with a

02:10.240 --> 02:17.400
profiling tool which records runtime or more involved metrics, maybe performance counters,

02:17.400 --> 02:19.560
stuff like that.

02:19.560 --> 02:26.360
And the big problem with this approach, especially compared to other mechanisms like sampling,

02:26.360 --> 02:31.800
is that it can increase the run times by orders of magnitude if you're not careful, if you

02:31.800 --> 02:34.960
just instrument every function.

02:34.960 --> 02:40.120
So you have to have some kind of selection mechanism in place to prevent that.

02:40.120 --> 02:46.960
And sort of the method that's most commonly used is to use profile-based filtering, either

02:46.960 --> 02:54.480
manual, where you look at the profile that is generated by the instrumented measurement,

02:54.480 --> 03:00.520
and then determine which functions might maybe, for example, very small or called very often

03:00.520 --> 03:02.280
and don't do much work.

03:02.280 --> 03:04.680
So that can be filtered out.

03:04.680 --> 03:10.560
And there are also tools like Scopee, which can help in generating these filters.

03:10.560 --> 03:25.040
However, this is only like on a purely profile base, and there's no calling context involved

03:25.040 --> 03:26.480
in the decision.

03:26.480 --> 03:32.000
So there are some other call graph-based approaches where you have a static call graph that is

03:32.000 --> 03:35.000
then used to make the selection.

03:35.000 --> 03:42.480
A 100 tool, there's Pira, which does automatic iterative refinement.

03:42.480 --> 03:49.440
So we start with a static selection based on the statically collected call graph, and

03:49.440 --> 03:56.560
then iteratively compile a program with instrumentation, execute it, measure it, and then look at the

03:56.560 --> 04:01.620
profile in order to determine what to filter out.

04:01.620 --> 04:06.360
And then the tool we're working on is the Scopee tool, which is short for Compiler-assisted

04:06.360 --> 04:08.560
performance instrumentation.

04:08.560 --> 04:15.040
And that one is more focused on allowing the user to specify what he wants to measure.

04:15.040 --> 04:23.300
So you can say, okay, I'm interested in MPI communication, so that is what I want to measure.

04:23.300 --> 04:27.820
So here's a very high-level overview of how Scopee works.

04:27.820 --> 04:34.600
So you have your source code, and that is put into the static analysis in order to generate

04:34.600 --> 04:36.720
the call graph.

04:36.720 --> 04:42.520
And then at the same time, the user specifies the measurement objective in form of a selection

04:42.520 --> 04:43.520
spec.

04:43.520 --> 04:47.000
We have a simple DSL for that.

04:47.000 --> 04:52.120
And this together is then fed into the Copee tool in order to generate the instrumentation

04:52.120 --> 04:57.760
configuration, which is hopefully low overhead.

04:57.760 --> 04:59.160
And yeah.

04:59.160 --> 05:01.720
So let's consider an example for that.

05:01.720 --> 05:05.920
So we might have the following scenario.

05:05.920 --> 05:09.360
So I want to record all call paths that contain MPI communication.

05:09.360 --> 05:13.320
Additionally, I want to measure functions that contain loops with at least ten floating

05:13.320 --> 05:19.320
point operations, and I don't care about system headers or inline functions.

05:19.320 --> 05:23.560
And this is how this looks as a Copee spec.

05:23.560 --> 05:32.560
I won't get into the details of the syntax, but you can combine different selection modules,

05:32.560 --> 05:39.800
where you start with the set of all functions and then sort of here select inline functions

05:39.800 --> 05:42.920
or system header functions.

05:42.920 --> 05:50.320
And then you can combine them to, in the end, produce one set that you want to instrument.

05:50.320 --> 05:57.160
And for this particular example, this reduces the number of instrumented functions by 74%

05:57.160 --> 06:01.720
for a test case.

06:01.720 --> 06:07.280
So the choreograph is sort of the base data structure that we use for all the analysis

06:07.280 --> 06:09.360
and for the selection.

06:09.360 --> 06:16.320
And this is currently generated based on source level by a tool called MetaCg.

06:16.320 --> 06:21.760
And this can be a bit cumbersome for very complex applications such as OpenFoam, because

06:21.760 --> 06:28.720
you need sort of a separate source code analysis step, and that can be difficult to set up,

06:28.720 --> 06:32.480
especially if there's like shared library dependencies, stuff like that.

06:32.480 --> 06:34.680
So it can be tricky.

06:34.680 --> 06:40.280
And so we looked into how we can maybe generate a call graph at different stages.

06:40.280 --> 06:46.960
And so this is what Tim's going to talk about, how we can generate a call graph at different

06:46.960 --> 06:48.680
program stages.

06:48.680 --> 06:55.600
And then we introduced the cage compiler plugin, where we, yeah, evaluate how this can be done

06:55.600 --> 06:59.920
at link time.

06:59.920 --> 07:06.920
So, well, thank you.

07:06.920 --> 07:10.520
So we were interested in the whole program call graphs.

07:10.520 --> 07:14.600
This is because copy, the tool that does our instrumentation, needs to be aware of every

07:14.600 --> 07:19.800
function inside our call graph, which means that it's necessary to have a whole program

07:19.800 --> 07:21.080
view.

07:21.080 --> 07:27.680
And the tools that were used like MetaCg were able to create a call graph, and they have

07:27.680 --> 07:31.840
the distinct advantage of being able to annotate Meta data to this call graph.

07:31.840 --> 07:34.500
This is where the name MetaCg came from.

07:34.500 --> 07:40.320
And this means that we not only can answer structural questions, like we want to instrument

07:40.320 --> 07:46.840
every function that eventually call path down to some MPI call, but we can also answer an

07:46.840 --> 07:48.560
instrument based on Meta data.

07:48.560 --> 07:54.400
For example, the loop depth specification or floating point operations.

07:54.400 --> 07:58.240
And yes, there were multiple possible ways to generate a call graph.

07:58.240 --> 07:59.360
One of them is source code.

07:59.360 --> 08:03.800
One of them is the intermediate representation that is basically part of every compiler,

08:03.800 --> 08:10.520
especially the LLVM compiled pipeline, and machine code at the very last.

08:10.520 --> 08:16.160
So the basic idea is, well, we have the source code, let's do call graph generation on the

08:16.160 --> 08:19.240
source code, which is relatively easy.

08:19.240 --> 08:22.660
MetaCg is doing it on a source code basis.

08:22.660 --> 08:27.080
But this means as you feed every single source code file into MetaCg, this means that your

08:27.080 --> 08:32.140
view of the program is limited to one translation unit at a time.

08:32.140 --> 08:37.280
So you then need to merge those parts, call graphs of every translation unit together

08:37.280 --> 08:42.420
to generate this overview of the whole program that you need.

08:42.420 --> 08:46.080
The information you then gather from your profiling maps very cleanly back to your source

08:46.080 --> 08:52.240
code, because once you find a function, well, it's named the same way in your source code.

08:52.240 --> 08:56.600
But on the other hand, what you write in your source code is not necessarily what's actually

08:56.600 --> 08:58.640
executed on the machine, right?

08:58.640 --> 09:02.440
Because there might be code transformation, constant propagation, dead code elimination,

09:02.440 --> 09:07.520
inlining, and this is actually something we want to be aware of if we are doing instrumentation,

09:07.520 --> 09:11.200
not that we want to instrument a function that doesn't exist anymore.

09:11.200 --> 09:16.560
Also, this merging of the translation units means that the user is involved.

09:16.560 --> 09:22.060
The user currently has to specify when he uses MetaCg which source code translation

09:22.060 --> 09:28.240
units belong to one target, and then manually has to tell MetaCg these functions are all

09:28.240 --> 09:32.120
to be merged, please generate the call graph for that.

09:32.120 --> 09:35.680
The user might not perfectly emulate the linker behavior.

09:35.680 --> 09:41.640
So there are different resolution types that a linker might choose if there are samely

09:41.640 --> 09:48.640
named structs or classes, and dependent on how you are implementing your merging process,

09:48.640 --> 09:52.880
you might have slight differences between your call graph that you generated and that

09:52.880 --> 09:55.580
what the linker will eventually do.

09:55.580 --> 10:00.440
So the other extreme would be, well, let's do it on the compiled machine code then.

10:00.440 --> 10:04.180
Reverse engineering tools like radar or GDR are able to generate call graphs and binary

10:04.180 --> 10:09.400
data just fine, and those have the very distinct advantages that this is actually what is run

10:09.400 --> 10:10.400
on the machine.

10:10.400 --> 10:12.960
There are no code transformation left.

10:12.960 --> 10:19.760
You have the advantage of being able to see machine code optimization passes if they are

10:19.760 --> 10:22.620
influencing the generated call graph.

10:22.620 --> 10:26.800
But on the other hand, a lot of information, the metadata that we also would like to be

10:26.800 --> 10:31.160
able to instrument based upon are lost as soon as we go down to machine code.

10:31.160 --> 10:36.960
Inlining already happened, so there is no function annotated with please inline anymore.

10:36.960 --> 10:40.480
Also pointer type information as we heard in the talk earlier gets lost as soon as we

10:40.480 --> 10:46.000
go down to machine type, and constness is also something that is more to be inferred

10:46.000 --> 10:49.740
than actually stated once we go down to machine code.

10:49.740 --> 10:55.280
And so we decided the best of both worlds is probably the LLVM IR because it's a heavily

10:55.280 --> 10:58.120
annotated representation.

10:58.120 --> 11:03.360
It is close enough to what will run on the machine that we have the ability to observe

11:03.360 --> 11:04.560
the code transformation.

11:04.560 --> 11:11.520
We are able to give more specific estimates on what the actual cost of a function might

11:11.520 --> 11:16.440
be, because we have more clear way of tracking, for example, instruction counts, floating

11:16.440 --> 11:19.200
ops and integer ops.

11:19.200 --> 11:23.760
On the other hand, it's also close enough to what the user actually wrote because we

11:23.760 --> 11:28.160
are not down on the machine code yet, and we can figure out the inlining stuff, the

11:28.160 --> 11:32.800
constants, the virtual functions, we can get type information in the IR.

11:32.800 --> 11:38.000
And if we do it at link time, we are not even limited to the translation unit by translation

11:38.000 --> 11:42.000
unit scope, that source code based approaches are.

11:42.000 --> 11:46.380
So if you have your pretty default compile pipeline, you have your source code which

11:46.380 --> 11:51.040
builds a translation unit, gets fed into the compiler, which outputs intermediate representation,

11:51.040 --> 11:56.920
and then the optimizer runs there and multiple source code translation unit optimized IR

11:56.920 --> 12:02.880
modules are fed into the linker, and we can do our call graph analysis inside the linker,

12:02.880 --> 12:09.100
solved our translation unit problem, and are able to have all our information ready.

12:09.100 --> 12:11.640
So to do this, we developed the CAGE plugin.

12:11.640 --> 12:18.920
CAGE stands for call graph embedding LLVM plugin, and it basically generates a call

12:18.920 --> 12:25.760
graph using some of LLVM tools, does some annotation, virtual function call analysis,

12:25.760 --> 12:29.760
and this can run as part of the optimizer in the pipeline, but it can also run as part

12:29.760 --> 12:36.920
of the LLVM linker, also as a plugin for which we use a slightly modified version of the

12:36.920 --> 12:44.640
LLVM linker, but the basic logic of running plugins in the LLVM linker was there.

12:44.640 --> 12:49.080
So then we do our Vtable analysis, our metadata annotation, because it's all available to

12:49.080 --> 12:54.840
us, and then we embed the result into the binary, which enables dynamic augmentation,

12:54.840 --> 12:57.000
and I will come to that one later.

12:57.000 --> 13:01.880
So at link time, we are doing static analysis basically, and as I already mentioned, we

13:01.880 --> 13:05.140
split our information in basically two types.

13:05.140 --> 13:10.240
One of them is structural information, like call hierarchies, call paths, call depth,

13:10.240 --> 13:16.040
the number of children, how deep we are in our path, and you also have virtual function

13:16.040 --> 13:20.920
calls, which are mostly structural, because once you have virtual and polymorphic calls,

13:20.920 --> 13:26.840
you have a set of functions that are probably being called by that pointer, and so we can

13:26.840 --> 13:31.880
narrow down the possibilities of which functions are called, but we cannot actually statically

13:31.880 --> 13:34.600
figure out this function is getting called.

13:34.600 --> 13:39.280
So it's slightly metadata based, but it's also mostly structural, and on the metadata

13:39.280 --> 13:44.340
information side, we have instruction composition, so we can determine what is the relation between

13:44.340 --> 13:48.720
arithmetic operations and memory operations, for example, or we can generate local and

13:48.720 --> 13:56.480
global loop depth estimates, and then we have inlining information, which is metadata,

13:56.480 --> 14:01.920
because inlining is not like a must-do for a compiler, just because you have specified

14:01.920 --> 14:07.560
inlining for a function doesn't mean the compiler will actually inline the function, so it's

14:07.560 --> 14:13.720
partly structural information and partly metadata, so you see there's no clear line between those,

14:13.720 --> 14:21.200
they blur at some points, but we can represent all those in the metadata annotated call graph,

14:21.200 --> 14:24.160
and if you remember, we were able to do dynamic augmentation.

14:24.160 --> 14:26.000
Well, what is dynamic augmentation?

14:26.000 --> 14:31.880
If you remember, each object contains the call graph that we generated, which means

14:31.880 --> 14:38.160
that the call graphs can be aggregated at runtime if a shared library is loaded, because

14:38.160 --> 14:42.080
even if you are at link time, even if you can see all the statically linked objects,

14:42.080 --> 14:48.360
all the translation units that belong to your target, your binary might load a shared library,

14:48.360 --> 14:51.040
which then, well, you're unaware of.

14:51.040 --> 14:57.040
So the idea is, as soon as the main executable is loaded, it passes its embedded call graph

14:57.040 --> 15:02.780
on startup to a runtime collecting library, and then the main executable can load whatever

15:02.780 --> 15:09.800
shared object it wants, and if this shared object also contains a call graph, then this

15:09.800 --> 15:14.680
runtime collector gets passed this call graph on the first load of the shared object and

15:14.680 --> 15:19.920
can aggregate it, like merging, so we're basically back to the translation unit by translation

15:19.920 --> 15:27.040
unit based approach, but now we're doing shared library on binary and executable based merging,

15:27.040 --> 15:31.880
and then we attach all this data together to one really big whole program call graph,

15:31.880 --> 15:36.400
now including shared objects, and then we can export this, for example, and pass it

15:36.400 --> 15:40.600
back to Kapi for some further refinement of the instrumentation.

15:40.600 --> 15:44.240
All right.

15:44.240 --> 15:48.000
Thanks.

15:48.000 --> 15:56.720
So to put it all together, for Kapi, we have the call graph analysis approach Tim just

15:56.720 --> 16:02.920
explained, so for each object file we have the call graph analysis and then the embedding,

16:02.920 --> 16:10.840
and then on the runtime side we can sort of merge the main executable call graph with

16:10.840 --> 16:16.040
the shared libraries as they are loaded, and we defer the instrumentation using dynamic

16:16.040 --> 16:26.040
instrumentation approach in order to sort of apply that selection dynamically, and yeah,

16:26.040 --> 16:28.200
that's how it works.

16:28.200 --> 16:35.160
So to summarize, we are developing the Kapi tool for instrumentation selection based on

16:35.160 --> 16:41.560
call graph analysis, and we have explored this cage plugin for generating this call

16:41.560 --> 16:47.760
graph information at link time, which allows whole program visibility and this dynamic

16:47.760 --> 16:55.800
instrumentation, and together with Kapi we can use the embedded call graph to run the

16:55.800 --> 17:04.600
selection at runtime, and thereby improving Kapi to make the compilation process and the

17:04.600 --> 17:07.760
analysis process more streamlined.

17:07.760 --> 17:13.680
And this is sort of an active development, so at this point we don't have a very detailed

17:13.680 --> 17:20.000
evaluation about performance and stuff like that, so there's some concerns, for example,

17:20.000 --> 17:27.200
when you go to very big programs and do LTO, there might be performance problems, so there

17:27.200 --> 17:37.680
might be more work to make it viable in that regard, but yeah, it works well in a prototype

17:37.680 --> 17:38.680
version.

17:38.680 --> 17:39.680
Thank you.

17:39.680 --> 17:40.680
Any questions?

17:40.680 --> 18:05.200
Perhaps I was a bit distracted, so I have two questions.

18:05.200 --> 18:17.640
If you can comment on Lambda functions, OMP sections of the code, not specific constructs,

18:17.640 --> 18:26.400
and instruction cache, if you can comment how those are handled, those three aspects, let's

18:26.400 --> 18:27.400
say.

18:27.400 --> 18:34.280
So when it comes to OpenMP, we don't at the moment have any specific OpenMP profiling

18:34.280 --> 18:41.440
interface that we target, so this might be something that is probably useful in the future,

18:41.440 --> 18:49.240
but for now we just do the basic function implementation and select on that.

18:49.240 --> 18:51.040
Then the other point was...

18:51.040 --> 18:53.640
Sorry, could you repeat?

18:53.640 --> 18:55.640
Lambda's in case.

18:55.640 --> 18:59.440
So do you want to comment on that?

18:59.440 --> 19:05.760
So Lambda's in caches, so caching, no, there is no logic to handle caching in any way,

19:05.760 --> 19:11.600
and regarding Lambda's in OpenMP, if you're talking about profiling, you've got your answer

19:11.600 --> 19:19.000
right there, but if you're talking about generating call graphs in which Lambda's and OpenMP runtime

19:19.000 --> 19:24.760
calls are available, then the call graph will actually figure out that there are OpenMP

19:24.760 --> 19:33.440
runtime calls and will correctly, if I remember correctly, will figure out that this function

19:33.440 --> 19:38.400
calls back to the OpenMP runtime, because once you're in IR, the runtime is actually

19:38.400 --> 19:44.000
carved out, all the pragmas were removed from the actual source code, so we are aware of

19:44.000 --> 19:48.640
OpenMP, but we are not using that information currently for any profiling, but you could

19:48.640 --> 19:55.440
do metadata-based copy selection with it, so every call path that eventually leads to

19:55.440 --> 20:03.080
an OpenMP runtime call would be a valid instrumentation using copy.

20:03.080 --> 20:06.680
Just that the question of the instruction cache is whether...

20:06.680 --> 20:12.680
This is my ignorance, but are you introducing a lot of new instructions here in the code

20:12.680 --> 20:15.480
or in the code that is being read?

20:15.480 --> 20:16.800
Oh, I see.

20:16.800 --> 20:19.520
I was thinking whether too much data ends up.

20:19.520 --> 20:22.800
Maybe I'm wrong and I didn't understand well.

20:22.800 --> 20:25.880
This is more of a performance-related question, right?

20:25.880 --> 20:31.160
Yes, of course, you introduce a new instruction whenever a shared library is loaded, because

20:31.160 --> 20:37.800
we then add instructions that pass the call graph back to our runtime collecting facility,

20:37.800 --> 20:42.920
and we also introduce instructions because we are using a profiling approach, which are

20:42.920 --> 20:46.960
instruction calls, so yeah, we are impeding the instruction fetching, instruction caching

20:46.960 --> 20:52.560
flow, which is also why profiling has rather high overhead compared to sampling approaches,

20:52.560 --> 21:00.840
for example, but as Sebastian told, we have not really extensively profiled our application,

21:00.840 --> 21:08.920
quite ironic, so we are not aware how much the benefit or the impact actually would be.

21:08.920 --> 21:17.880
So, in your slide, you say you have a fork of LLD.

21:17.880 --> 21:22.480
The obvious question is what's stopping you from upstreaming this?

21:22.480 --> 21:31.000
So yes, we have a fork of LLD that just basically exposes the flag load new pass manager plugin

21:31.000 --> 21:34.160
and you pass the plugin and it does the rest.

21:34.160 --> 21:38.600
What's currently holding us back from upstreaming is it's not very well developed.

21:38.600 --> 21:44.000
It was coupled together in half a week or something, and there already is an open merge

21:44.000 --> 21:50.880
request and fabricator that implements this exact functionality for, if I remember correctly,

21:50.880 --> 22:00.200
LLVM9, which was abandoned for a year until it was totally abandoned and closed, and so

22:00.200 --> 22:05.800
we didn't actually figure out how to make this more interesting.

22:05.800 --> 22:08.680
We'll take that as a signal.

22:08.680 --> 22:10.800
People just move jobs or whatever.

22:10.800 --> 22:13.040
So try it again and bash people.

22:13.040 --> 22:16.520
I can help you with that as well, find the right people to get this, because it seems

22:16.520 --> 22:18.960
like a simple and obvious thing to have.

22:18.960 --> 22:21.920
It isn't actually that hard.

22:21.920 --> 22:27.240
Apparently there wasn't much interest in the community back in 2020.

22:27.240 --> 22:28.240
That's too long ago.

22:28.240 --> 22:29.240
Just try again.

22:29.240 --> 22:40.720
Then we will polish it a little much, and then we will hopefully get this one upstream.

22:40.720 --> 22:47.360
Any other questions?

22:47.360 --> 22:48.360
Thank you very much.

22:48.360 --> 23:00.360
Aye.
