WEBVTT

00:00.000 --> 00:01.000
All right.

00:01.000 --> 00:02.000
Good morning, everyone.

00:02.000 --> 00:03.000
Welcome to the HPC Dev Room.

00:03.000 --> 00:13.560
Thanks for being here so early in the morning, maybe not entirely sober.

00:13.560 --> 00:18.280
We'll let Nicolas get started with opening the Dev Room with a talk on ParaV.

00:18.280 --> 00:19.280
Thank you, Nicolas.

00:19.280 --> 00:20.280
Thank you.

00:20.280 --> 00:21.280
Hello, everyone.

00:21.280 --> 00:29.280
Thanks to be here early in the morning to begin this HPC day.

00:29.280 --> 00:33.440
Big thanks to everyone who organized this room.

00:33.440 --> 00:35.120
That's really great to be here.

00:35.120 --> 00:38.720
Thanks, Gennet, and all your team.

00:38.720 --> 00:40.720
About me, I'm Nicolas.

00:40.720 --> 00:47.200
I'm a developer and I have the chance to make my job about making first-code contributions.

00:47.200 --> 00:53.840
I'm working at Keterware, Europe, and I work mainly on ParaV, so developing the software

00:53.840 --> 00:57.280
but also interacting with the community.

00:57.280 --> 01:09.080
So ParaV is an end-user application that works for scientific data analysis and visualizations.

01:09.080 --> 01:16.040
We have an open community on the GitLab for the code and discourse for discussions.

01:16.040 --> 01:21.480
It's supported by Keterware, which is behind a VTK for visualization toolkit and to make

01:21.480 --> 01:26.320
you potentially already know about.

01:26.320 --> 01:31.920
So what do we do with ParaV for displaying an analysis data set?

01:31.920 --> 01:38.160
So it's mainly a 3D visualizer, but you've got some charts and spreadsheets and so on.

01:38.160 --> 01:40.400
It's also an internet for data processing.

01:40.400 --> 01:45.880
So we have a concept of filters to take an input and compute some stuff on it and get

01:45.880 --> 01:47.240
your outputs.

01:47.240 --> 01:51.200
So basically you can extract the data from the rest of your raw data.

01:51.200 --> 01:58.440
We also have some other modules like realistic rendering, so you can do your communications

01:58.440 --> 02:04.960
with directly your real data sets on that with just some kind of fake one.

02:04.960 --> 02:10.000
So here is some basic screen shot of the applications.

02:10.000 --> 02:12.320
Who is using ParaV?

02:12.320 --> 02:18.600
We cover its generic applications, so we cover a large range of domains like fluid dynamics

02:18.600 --> 02:23.680
so we can compute streamlines, particle tracking, and so on.

02:23.680 --> 02:28.120
We have a volume rendering that is really nice for medical applications with you have

02:28.120 --> 02:33.160
some 3D scan and you want to understand what happened inside.

02:33.160 --> 02:39.400
So that's just an infinite list where a lot of domains that can be covered, but here is

02:39.400 --> 02:42.400
the more well-known one.

02:42.400 --> 02:44.060
We do use ParaV.

02:44.060 --> 02:50.980
So as I said, that's an application, so basically the first way to learn is to use the GUI.

02:50.980 --> 02:54.880
So you click on buttons, you do some stuff, and you're happy.

02:54.880 --> 03:00.800
But you can also use the Python wrapping to write some scripts, and so you can run the

03:00.800 --> 03:05.660
script on batch processing without having to be behind the computer.

03:05.660 --> 03:11.080
It's also a framework because you can code your own extensions, your own derivated work

03:11.080 --> 03:19.040
from ParaV in the native C++ language, but also some features can be done in Python code.

03:19.040 --> 03:21.680
And it's all based on the visualization toolkit.

03:21.680 --> 03:27.920
I mean, all the hard work of processing the data and doing the rendering come from VTK.

03:27.920 --> 03:35.480
So as I say, that's also supported by Kitware, so I do work sometimes on VTK to have some

03:35.480 --> 03:39.680
bug fix or some small new features.

03:39.680 --> 03:45.760
Here we can run ParaV on which hardware is it possible to use it.

03:45.760 --> 03:50.960
Basically on your small, classical desktop, we have some official binaries you can try

03:50.960 --> 03:52.960
to download and just run.

03:52.960 --> 03:59.600
It should be, it's cross-platform, I didn't say, but you can run it on ISO, on proprietary,

03:59.600 --> 04:05.840
so it's like Mac or Windows, but it should be out of the box for Linux too.

04:05.840 --> 04:10.840
You can also build it, we have a large selection of build options depending on what you want

04:10.840 --> 04:11.920
to do exactly.

04:11.920 --> 04:17.560
You can enable or disable it, so if you want to have pure Python data distribution, parallelizations,

04:17.560 --> 04:20.480
or custom rendering.

04:20.480 --> 04:25.080
A lot of stuff, we have some documentation about it, and we can help you on the discourse

04:25.080 --> 04:31.480
if you are trying to achieve some specific build of it.

04:31.480 --> 04:34.200
And which kind of usage do we have of ParaView?

04:34.200 --> 04:38.360
So either research and industry are using it.

04:38.360 --> 04:48.920
For instance, recently, well, the supercomputing conference in the US, and they organized a

04:48.920 --> 04:54.000
service contest where people are asked to upload some nice videos made about their scientific

04:54.000 --> 05:00.200
analysis, and most of them are using ParaView for just the data processing, but also sometimes

05:00.200 --> 05:04.720
for the video generations and animate their data.

05:04.720 --> 05:09.920
So why all those people are using ParaView?

05:09.920 --> 05:15.800
Because ParaView does some stuff efficiently to process the data on their infrastructures,

05:15.800 --> 05:23.120
so that's what I want to talk in the next part of my talk.

05:23.120 --> 05:28.840
So what ParaView uses behind the hood to make it possible?

05:28.840 --> 05:31.480
So first we have a client-server architecture.

05:31.480 --> 05:38.240
So I already said that you can do it from GUI or from Python, but the GUI and Python

05:38.240 --> 05:44.960
are just two clients, so you can do exactly the same stuff with one or the other.

05:44.960 --> 05:51.400
There's no really limitation about choosing one or the second.

05:51.400 --> 05:58.800
You can run with remote server, and you can run in a distributed environment to your server.

05:58.800 --> 06:07.920
So, in that case, you can connect, you can either just run the server parts with a script

06:07.920 --> 06:13.720
analysis, but you can either connect your local clients to your decent server, and again

06:13.720 --> 06:20.720
using the graphical interface to do the stuff as if it was on your local machine, but instead

06:20.720 --> 06:27.880
it's the supercomputer or the remote architecture.

06:27.880 --> 06:30.400
At the bottom, two other modes that are available.

06:30.400 --> 06:37.480
Typically, if you have some graphic nodes on your server, you can use them for just

06:37.480 --> 06:43.800
the rendering part and stay the data management on the CPU nodes, and still you can connect

06:43.800 --> 06:49.960
your client on it to see what happens and to control from a graphical interface.

06:49.960 --> 06:53.440
And last mode, I will go back on it later.

06:53.440 --> 07:00.200
We have an in situ infrastructure, so basically your simulation can call an API that's fluid

07:00.200 --> 07:05.560
ParaView script analysis, and you can even connect with a graphical client to see time

07:05.560 --> 07:09.880
step per time step what has happened on your simulation.

07:09.880 --> 07:16.400
So, that's for the different mode of use for ParaView.

07:16.400 --> 07:25.160
So first, to run on HPC infrastructure, we implement data distributions for the analysis.

07:25.160 --> 07:28.040
So basically we rely on the MPI standards.

07:28.040 --> 07:36.080
So our readers are MPI-aware, so they can distribute the data over the wrong early in

07:36.080 --> 07:38.960
the process when you read your data on the disk.

07:38.960 --> 07:45.720
Then most of the features are okay to run just with their sub parts of data, but some

07:45.720 --> 07:50.280
other features need to know about the neighborhood to execute correctly.

07:50.280 --> 07:56.840
So for that, we support the concept of ghost cells where each rank know a little bit about

07:56.840 --> 08:00.200
the rank that is next to it.

08:00.200 --> 08:06.120
In that case, mainly we split the data geometrically, so a subset is really a geometric subset of

08:06.120 --> 08:15.680
your data, and different rank can know and communicate with the author for specific tasks.

08:15.680 --> 08:21.880
At least we have some filters, so what I call a filter is really something that the user

08:21.880 --> 08:26.160
can instantiate from the client and ask to process.

08:26.160 --> 08:36.160
So we can ensure load balancing by redistributing the data during the process.

08:36.160 --> 08:39.720
The visualizations can also be distributed over several ranks.

08:39.720 --> 08:45.600
So for that, we use an inner library that's called ICT, that's based on the MPI process

08:45.600 --> 08:46.600
to do that.

08:46.600 --> 08:53.080
And probably you support a different kind of model of rendering, so if you are dedicating

08:53.080 --> 08:58.680
on a node, you can, as I said, create a parallel tutorial just for the rendering part and connect

08:58.680 --> 09:01.240
it to the data server.

09:01.240 --> 09:09.240
You can have multiple GPUs per rank to do your rendering, but that's also possible locally

09:09.240 --> 09:15.600
if you have just one machine that has multiple GPUs, you can ask to do rendering on both

09:15.600 --> 09:22.000
symmetry and agility.

09:22.000 --> 09:29.800
Concerning the performances now, so that distributions is not a bug performance, it's just about

09:29.800 --> 09:35.360
running with too big data, so you cannot just run on your machine, that's a requirement

09:35.360 --> 09:44.080
when you have huge data to be able to distribute it over your computer or your supercomputer.

09:44.080 --> 09:48.560
Now we're talking a little bit about performance, because if you have big data, you also need

09:48.560 --> 09:54.760
to be performant on how you analyze it and how you are proceed with it.

09:54.760 --> 10:00.840
So for that, we have a thin layer for CPU parallelism, we call that a simple tool in

10:00.840 --> 10:12.640
our code base, the goal is to do code parallelizations for many for loop, and main purpose is that

10:12.640 --> 10:19.240
you can choose at build time and then at run time if you enable the OpenMP or TBB backends

10:19.240 --> 10:29.240
and if you don't want external live, you can also use the C++5 to do that.

10:29.240 --> 10:37.440
And so, as is just for instance, to parallelize a for loop or fill operations, it's really

10:37.440 --> 10:44.440
widely used in a lot of our algorithm, and you have some environment variables that can

10:44.440 --> 10:51.680
control the backend on some of the number of thread, the type of the thread pools or

10:51.680 --> 10:58.520
if you allow nested pools or so on, depending on your resources and backend.

10:58.520 --> 11:08.640
So it has some documentation on it, and we made some improvements last year about that.

11:08.640 --> 11:17.320
Still about performances, we also use as an optional dependency the VTKM projects that

11:17.320 --> 11:24.480
stand for somewhat some many core that is intended to be used on heterogeneous systems.

11:24.480 --> 11:32.240
So basically, when you want to have performance on supercomputer or even, you still need to

11:32.240 --> 11:37.760
be aware of the current technology and the state of the art, and as we saw in the past

11:37.760 --> 11:43.320
decades, a lot of new architectures emerging.

11:43.320 --> 11:52.240
We think about using a dedicated library to be able to use these new architectures.

11:52.240 --> 11:58.840
So with VTKM, the goal inside VTKM library, the goal is to split all operations into really

11:58.840 --> 12:06.720
atomic operations, and then at runtime, it can dispatch all that on the hardware you

12:06.720 --> 12:10.320
find on the back ends that are available.

12:10.320 --> 12:16.880
So with VTKM, you can use CUDA, OpenMP, GBB also to do the computation.

12:16.880 --> 12:23.320
This time with VTKM is not just accelerating some specific loop inside an algorithm, it's

12:23.320 --> 12:34.000
more about VTKM is reimplementing some whole algorithm like extracting ISO control also.

12:34.000 --> 12:42.200
And then we embed this into ParaView with some kind of wrapper to communicate with all

12:42.200 --> 12:44.840
VTKM works.

12:44.840 --> 12:54.320
So that's optional, that's enabled by default in the binaries we provide.

12:54.320 --> 13:00.000
Another point about performance, but that's really depending on the use case, on the data

13:00.000 --> 13:04.120
you are using is the in-situ also.

13:04.120 --> 13:08.720
Basically when you traditionally when you have your simulation, it dumps every time

13:08.720 --> 13:13.520
step or every n time step some data on the disk, and then to analyze you have to load

13:13.520 --> 13:17.760
back to data with post-processing tools.

13:17.760 --> 13:22.800
But that's at a cost of writing and reading from your disk.

13:22.800 --> 13:26.960
You should have the size on your disk, the whole side of the disk, so you should have

13:26.960 --> 13:34.440
big disk, and then it has really a cost in terms of time when you should write a full

13:34.440 --> 13:38.600
mesh of full data on disk and then read back with another process.

13:38.600 --> 13:45.000
So basically the goal of in-situ is to make the simulation communicate directly with the

13:45.000 --> 13:50.320
processing tools, and then the processing tools can wrap the memory in place and analyze

13:50.320 --> 13:57.760
directly in the RAM without writing on the disk to save some IO time.

13:57.760 --> 14:08.520
So in the context of ParaView, we have a standalone API that's called Catalysts that was recently

14:08.520 --> 14:15.240
released, although we make big improvements into Catalysts past years.

14:15.240 --> 14:23.320
And the goal of Catalysts is to have a really minimal API and a stable ABI, so you can choose

14:23.320 --> 14:27.880
end-to-end time, the implementation you want.

14:27.880 --> 14:34.020
And one other goal is to minimize the instrumentation you need to do in your simulation code directly,

14:34.020 --> 14:38.840
so it's really easy for simulation developers to understand the few key places where they

14:38.840 --> 14:42.200
have to put a new code to call our API.

14:42.200 --> 14:47.280
So here is a really basic example from one of the tutorials we have.

14:47.280 --> 14:51.600
You need to initialize, of course, and you need to call some method at each time step

14:51.600 --> 14:56.560
where you want the processing to happen.

14:56.560 --> 14:57.920
And finalizations.

14:57.920 --> 15:05.520
Of course, you still have to do a little layer to describe your data.

15:05.520 --> 15:14.480
For that, we do some sort of a party library to help us.

15:14.480 --> 15:22.160
So Catalysts is a standalone, is an independent project, no, independent of ParaView, but

15:22.160 --> 15:28.720
of course the first real implementation is an implementation for ParaView.

15:28.720 --> 15:31.160
So we...

15:31.160 --> 15:32.480
Sorry.

15:32.480 --> 15:41.760
So, yes, we implement Catalysts, so the back end, so you can run ParaView pipeline directly

15:41.760 --> 15:50.160
from your simulation each time step or when you call it.

15:50.160 --> 15:52.840
So how does it work?

15:52.840 --> 15:53.840
How do you...

15:53.840 --> 15:54.840
You can...

15:54.840 --> 16:00.520
The idea is that you are hard-cored the communication between your simulation and Catalysts through

16:00.520 --> 16:07.040
the API, but then the actual script that is executed, the actual pipeline and visualization

16:07.040 --> 16:14.280
you want to produce, it's all scriptable thanks to the Python wrapping of ParaView.

16:14.280 --> 16:20.560
You can even load some representative data in the graphical interface of ParaView, do

16:20.560 --> 16:26.400
some analysis, export this as a Python script and use this script to feed Catalysts and

16:26.400 --> 16:32.880
then when you run your simulation with Catalysts enabled, it will reuse the script you produced

16:32.880 --> 16:34.300
directly from the GUI.

16:34.300 --> 16:41.880
So people that are not at all developers still can do some stuff with Catalysts.

16:41.880 --> 16:46.440
And last point is that when you have a running simulation with the Catalysts pipeline on

16:46.440 --> 16:52.720
your dedicated server, you also can use the GUI to connect to this ParaView server and

16:52.720 --> 16:58.280
to see real time and get some screenshots of the visualization on the analysis that

16:58.280 --> 17:00.280
is proceeding on the server.

17:00.280 --> 17:06.640
So you can have a feedback, a time step per time step on what happened on the simulation.

17:06.640 --> 17:11.480
So if you see that something is diverging or going wrong, you can stop your simulation

17:11.480 --> 17:17.480
directly and you don't waste all the time before seeing that something went wrong and

17:17.480 --> 17:24.640
that you should tweak the parameter and start again.

17:24.640 --> 17:31.240
So yeah, I was quite faster than expected for me.

17:31.240 --> 17:39.280
So in the conclusions of to be able to run efficiently on supercomputer with ParaView,

17:39.280 --> 17:41.920
we implemented a client server mode.

17:41.920 --> 17:46.900
The server is MPI-aware and can be run on distributed environments.

17:46.900 --> 17:53.920
We are relying on old and well-known libraries such as implementation of MPI to do these

17:53.920 --> 18:06.560
distributions, but we are also really looking toward new library that can help us.

18:06.560 --> 18:13.760
We are able to integrate new library to do some performance analysis on new library that

18:13.760 --> 18:19.400
is aware of a new architecture of a supercomputer or new technology.

18:19.400 --> 18:25.400
That's okay, for instance, with VTKM or others.

18:25.400 --> 18:33.360
And we have this API to do institute that can save a lot of time and disk space.

18:33.360 --> 18:38.360
Just a slide to summarize the organize, so we have different kind of way to interact

18:38.360 --> 18:39.360
with ParaView.

18:39.360 --> 18:44.000
I dug with Python scripting, the catalyst in CT stuff.

18:44.000 --> 18:45.760
You can also build some custom one.

18:45.760 --> 18:50.000
We have some web example of clients.

18:50.000 --> 18:57.320
At the bottom, we have a list, a non-finite list of library on which we would like to do

18:57.320 --> 19:00.440
the effective work.

19:00.440 --> 19:06.520
So basically open GLMPI, open MP.

19:06.520 --> 19:11.120
Concerning roadmap, we have several improvements that are coming.

19:11.120 --> 19:18.680
First I talk about in situ, induction implementation, you have each rank that does the simulation

19:18.680 --> 19:22.280
does the co-processing work.

19:22.280 --> 19:25.120
So that's not always what is intended.

19:25.120 --> 19:30.880
You want to do the co-processing on other ranks, just because, for instance, you have

19:30.880 --> 19:36.040
dedicated rank for visualization, so you want to do all the processing on the visualization

19:36.040 --> 19:37.800
nodes.

19:37.800 --> 19:41.920
That's not possible just with the in situ implementation, but we have an in-transit

19:41.920 --> 19:52.160
implementation where the simulation can communicate with those different nodes, and the analysis

19:52.160 --> 19:59.440
can happen on other ranks than the simulation, so the simulation can go forward directly.

19:59.440 --> 20:05.800
We also use some new library, recently used a library called DIY that's here to do some

20:05.800 --> 20:07.200
wrapper.

20:07.200 --> 20:17.920
For us, we take it as a wrapper around the MPI, so DIY allows you to cut the data into

20:17.920 --> 20:24.800
different blocks, and then the at runtime DIY itself is aware to do, okay, I should

20:24.800 --> 20:34.360
put three blocks on each rank, or only one block, and it's just a new abstraction over

20:34.360 --> 20:38.160
cutting your data for distribution.

20:38.160 --> 20:46.600
We are also looking for better VTK on always better VTK integrations to be able to run

20:46.600 --> 20:52.080
on a lot of hardware, and something very cool that is really new, it's just in the development

20:52.080 --> 20:57.800
branch of VTK, so it's not in ParaView yet, that was merged, I think, one or two weeks

20:57.800 --> 21:05.080
ago, it's what we call implicit arrays, and basically it's really good for memory point

21:05.080 --> 21:09.520
of view, because it's some kind of use on memory.

21:09.520 --> 21:16.600
For now, in the ParaView process, your data is really an array in the memory, in your

21:16.600 --> 21:25.600
memory, so with the implicit array, we have some views, so you can implement an open pattern

21:25.600 --> 21:33.520
on it, for instance, when you do an ISO control of your data, you know that the resulting

21:33.520 --> 21:39.080
data will all have the same values, so if you want, if you, after the ISO control, you

21:39.080 --> 21:43.680
will have one million points, you will know that all the points will share the same value,

21:43.680 --> 21:49.760
for now it's one million times duplicate in your memory, so that's not efficient, with

21:49.760 --> 21:55.000
implicit array, you can only one time the value and say, okay, this should be an array

21:55.000 --> 22:02.080
of size one million, and the value you should return is this one, but you can imagine, as

22:02.080 --> 22:10.840
though, have a compressed array in your memory, and have an on the fly, uncompressed algorithm,

22:10.840 --> 22:18.000
just when you want to read your data, so it has a cost in terms of time of computations,

22:18.000 --> 22:24.560
but if you run out of memory with too huge data, that can be really great.

22:24.560 --> 22:33.640
Okay, I still can have a lot of things to say, but that was the end of what I put in

22:33.640 --> 22:38.400
the slide, so thanks for attending, it sounds to be here early in the morning, and if you

22:38.400 --> 22:43.760
have any questions, I think it will be the time, I put just a lot of resources at the

22:43.760 --> 22:48.360
end of the slide, so you can get it from the website of the forum.

22:48.360 --> 22:49.360
Thank you.

22:49.360 --> 22:50.360
Thanks everyone.

22:50.360 --> 22:57.080
Thank you very much, Nikolas.

22:57.080 --> 23:03.920
Do we have any questions?

23:03.920 --> 23:04.920
Thank you.

23:04.920 --> 23:08.240
In our group, we are happy users of ParaView.

23:08.240 --> 23:18.880
One thing that maybe I could add to a wish list, or some, well, maybe just for discussion,

23:18.880 --> 23:26.920
is that we have quite some headache when using ParaView on GitHub actions for multiple platforms,

23:26.920 --> 23:32.360
so like to set up environments for Linux, Mac and Windows with the same version of ParaView

23:32.360 --> 23:39.840
coupling with Python, just to be ready to use it, it's a bit of a headache, especially

23:39.840 --> 23:45.800
when you go to Windows and you need to download things, brew things, opt to get install things,

23:45.800 --> 23:48.120
and then they not necessarily work altogether.

23:48.120 --> 23:55.000
So wish list thing, GitHub actions, ParaView setup thing, unless it doesn't, it exists

23:55.000 --> 23:58.760
already, but I haven't found it.

23:58.760 --> 24:03.160
And the truth is, if there are no questions here.

24:03.160 --> 24:08.880
The use of ParaView and GitHub actions, so in like a continuous integration, limited

24:08.880 --> 24:09.880
environment I guess.

24:09.880 --> 24:10.880
It's a wish list, I think.

24:10.880 --> 24:12.880
Yeah, it's a wish list, yeah.

24:12.880 --> 24:20.840
Well, we don't use GitHub directly, we have the GitLab where you can find a lot of stuff

24:20.840 --> 24:30.920
where our CI and CD will produce nightly releases of ParaView through the GitLab.

24:30.920 --> 24:35.760
I don't know if I answered part of the question.

24:35.760 --> 24:39.800
So what kind of stuff are you doing with ParaView and GitHub actions?

24:39.800 --> 24:40.800
Is it rendering?

24:40.800 --> 24:43.800
Rendering with Python, yeah.

24:43.800 --> 24:48.160
The fact is, I don't really know about GitHub actions because I don't use GitHub anymore,

24:48.160 --> 24:56.560
so I don't see what you can do with that, that you should not able to do otherwise.

24:56.560 --> 24:57.560
Any other questions?

24:57.560 --> 25:01.600
There's a question on the chat.

25:01.600 --> 25:06.680
Yeah, there's a question on the chat.

25:06.680 --> 25:10.920
If I want to put catalyst in my simulation, what is the first step?

25:10.920 --> 25:13.560
Oh, sorry, if you want to use catalyst?

25:13.560 --> 25:16.000
What's the first step?

25:16.000 --> 25:20.800
We have some, I think we have some tutorials, an example in the code base of ParaView, we

25:20.800 --> 25:27.960
have some examples where there are some dummy simulations with just a main, so you can enter

25:27.960 --> 25:37.440
it from it to see how it is organized, and yeah, one first thing is to be able to know

25:37.440 --> 25:45.440
what do you want, which data do you want to send through catalyst, and where you can access

25:45.440 --> 25:55.160
it in your code, and then so at this time you have located the entry points from your

25:55.160 --> 26:01.800
simulation code, and then you will be able to start writing the small wrapper you need

26:01.800 --> 26:07.600
to wrap your data on the need to the actual API of ParaView.

26:07.600 --> 26:08.600
Thanks.

26:08.600 --> 26:09.600
Okay.

26:09.600 --> 26:12.000
Any other burning questions?

26:12.000 --> 26:17.400
Maybe one last, yeah, last question.

26:17.400 --> 26:18.400
Thank you for the talk.

26:18.400 --> 26:19.400
Very naive question.

26:19.400 --> 26:20.400
Awesome.

26:20.400 --> 26:21.400
Is it working?

26:21.400 --> 26:25.360
Is it working?

26:25.360 --> 26:29.920
A very naive question, because I know almost nothing about ParaView.

26:29.920 --> 26:34.840
You had many components there, one of them was the client that does the visualizations.

26:34.840 --> 26:40.080
Would it be possible at some point in the future to be like a web client, where you

26:40.080 --> 26:45.120
just log into the website and it just displays everything, or is it just, do you do the architecture

26:45.120 --> 26:49.000
or is it like super complicated to do it that way?

26:49.000 --> 26:58.880
So the question is, are we able to use a web client for ParaView?

26:58.880 --> 27:02.880
Just for the part that does the visualization, if that could be like a web.

27:02.880 --> 27:10.560
We have some web client for ParaView already, so we have a framework called Trame, that

27:10.560 --> 27:16.960
in turn need to connect to ParaView server, and then you build your own front end for

27:16.960 --> 27:25.040
these applications, so basically we don't have, you should build your own, but it can

27:25.040 --> 27:32.580
be, okay, I have a server, I open always this data, and the front end is just a 3D render

27:32.580 --> 27:33.580
view.

27:33.580 --> 27:37.440
That's already possible quite easily, I think, with the Trame framework.

27:37.440 --> 27:40.640
And Jupyter Notebooks also included, right?

27:40.640 --> 27:44.880
Jupyter Notebooks, I think I saw it on the user interface line.

27:44.880 --> 27:51.760
As we use intensively Python, we also make the step to be supported from a Jupyter Notebook,

27:51.760 --> 27:57.320
and we also have a plugin that allow you to control a ParaView GUI, so you can do some

27:57.320 --> 28:02.160
stuff in the Notebook, and if something goes wrong or you don't understand, you can launch

28:02.160 --> 28:06.640
a magic command run ParaView, that's open the ParaView client with all the current pipeline,

28:06.640 --> 28:11.640
and you can introspect in the GUI, and then you can go back to your Notebook.

28:11.640 --> 28:15.160
Okay, thank you very much, Nicholas.

28:15.160 --> 28:16.160
Thanks.

28:16.160 --> 28:17.160
Thank you.

28:17.160 --> 28:18.160
Thank you.

28:18.160 --> 28:19.160
Thank you.

28:19.160 --> 28:41.900
Thank you.
