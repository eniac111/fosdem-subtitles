1
0:00:00.000 --> 0:00:12.240
Okay, so we're switching topics a bit away from MPI to something at least a little bit

2
0:00:12.240 --> 0:00:15.440
different, link time call graph analysis.

3
0:00:15.440 --> 0:00:18.760
All right, thank you.

4
0:00:18.760 --> 0:00:25.100
So yeah, we're going to talk about user guided program presentation approach and especially

5
0:00:25.100 --> 0:00:31.840
a link time call graph analysis extension to that project which kind of makes it more

6
0:00:31.840 --> 0:00:34.000
usable.

7
0:00:34.000 --> 0:00:38.560
So to give some background on this work, I'm involved in a project called ExaFoam that

8
0:00:38.560 --> 0:00:44.600
deals with the OpenFoam Computational Fluid Dynamics Toolbox and that's a very complex

9
0:00:44.600 --> 0:00:46.960
code quite large.

10
0:00:46.960 --> 0:00:51.840
And the goal here is to improve the performance of OpenFoam for Hqc systems, especially for

11
0:00:51.840 --> 0:00:54.280
the exascale error.

12
0:00:54.280 --> 0:01:00.640
And one of the things we do is develop empirical performance models.

13
0:01:00.640 --> 0:01:04.640
And for that we have developed a workflow, how we do that.

14
0:01:04.640 --> 0:01:11.040
We start with several measurements where we get an initial overview and identify hotspots,

15
0:01:11.040 --> 0:01:17.280
then based on these hotspots do an analysis of the critical kernels and finally we can

16
0:01:17.280 --> 0:01:26.280
do the empirical modeling in order to find scalability bugs and predict performance

17
0:01:26.280 --> 0:01:27.920
when scaling out.

18
0:01:27.920 --> 0:01:32.360
And so especially for the focus measurements and the modeling, you need quite accurate

19
0:01:32.360 --> 0:01:37.520
and reliable methods to be sure that the data you collect is right and you have the right

20
0:01:37.520 --> 0:01:39.840
level of detail.

21
0:01:39.840 --> 0:01:43.080
And what we use for that is code instrumentation.

22
0:01:43.080 --> 0:01:48.040
And just to give the background, I want to give an example.

23
0:01:48.040 --> 0:01:54.120
For example, in Hqc and Clang you have the F instrument functions flag, which does a

24
0:01:54.120 --> 0:02:03.000
very basic instrumentation where if you activate that flag, you insert these, or the compiler

25
0:02:03.000 --> 0:02:10.240
inserts these function enter and exit probes, which are then at runtime interface with a

26
0:02:10.240 --> 0:02:17.400
profiling tool which records runtime or more involved metrics, maybe performance counters,

27
0:02:17.400 --> 0:02:19.560
stuff like that.

28
0:02:19.560 --> 0:02:26.360
And the big problem with this approach, especially compared to other mechanisms like sampling,

29
0:02:26.360 --> 0:02:31.800
is that it can increase the run times by orders of magnitude if you're not careful, if you

30
0:02:31.800 --> 0:02:34.960
just instrument every function.

31
0:02:34.960 --> 0:02:40.120
So you have to have some kind of selection mechanism in place to prevent that.

32
0:02:40.120 --> 0:02:46.960
And sort of the method that's most commonly used is to use profile-based filtering, either

33
0:02:46.960 --> 0:02:54.480
manual, where you look at the profile that is generated by the instrumented measurement,

34
0:02:54.480 --> 0:03:00.520
and then determine which functions might maybe, for example, very small or called very often

35
0:03:00.520 --> 0:03:02.280
and don't do much work.

36
0:03:02.280 --> 0:03:04.680
So that can be filtered out.

37
0:03:04.680 --> 0:03:10.560
And there are also tools like Scopee, which can help in generating these filters.

38
0:03:10.560 --> 0:03:25.040
However, this is only like on a purely profile base, and there's no calling context involved

39
0:03:25.040 --> 0:03:26.480
in the decision.

40
0:03:26.480 --> 0:03:32.000
So there are some other call graph-based approaches where you have a static call graph that is

41
0:03:32.000 --> 0:03:35.000
then used to make the selection.

42
0:03:35.000 --> 0:03:42.480
A 100 tool, there's Pira, which does automatic iterative refinement.

43
0:03:42.480 --> 0:03:49.440
So we start with a static selection based on the statically collected call graph, and

44
0:03:49.440 --> 0:03:56.560
then iteratively compile a program with instrumentation, execute it, measure it, and then look at the

45
0:03:56.560 --> 0:04:01.620
profile in order to determine what to filter out.

46
0:04:01.620 --> 0:04:06.360
And then the tool we're working on is the Scopee tool, which is short for Compiler-assisted

47
0:04:06.360 --> 0:04:08.560
performance instrumentation.

48
0:04:08.560 --> 0:04:15.040
And that one is more focused on allowing the user to specify what he wants to measure.

49
0:04:15.040 --> 0:04:23.300
So you can say, okay, I'm interested in MPI communication, so that is what I want to measure.

50
0:04:23.300 --> 0:04:27.820
So here's a very high-level overview of how Scopee works.

51
0:04:27.820 --> 0:04:34.600
So you have your source code, and that is put into the static analysis in order to generate

52
0:04:34.600 --> 0:04:36.720
the call graph.

53
0:04:36.720 --> 0:04:42.520
And then at the same time, the user specifies the measurement objective in form of a selection

54
0:04:42.520 --> 0:04:43.520
spec.

55
0:04:43.520 --> 0:04:47.000
We have a simple DSL for that.

56
0:04:47.000 --> 0:04:52.120
And this together is then fed into the Copee tool in order to generate the instrumentation

57
0:04:52.120 --> 0:04:57.760
configuration, which is hopefully low overhead.

58
0:04:57.760 --> 0:04:59.160
And yeah.

59
0:04:59.160 --> 0:05:01.720
So let's consider an example for that.

60
0:05:01.720 --> 0:05:05.920
So we might have the following scenario.

61
0:05:05.920 --> 0:05:09.360
So I want to record all call paths that contain MPI communication.

62
0:05:09.360 --> 0:05:13.320
Additionally, I want to measure functions that contain loops with at least ten floating

63
0:05:13.320 --> 0:05:19.320
point operations, and I don't care about system headers or inline functions.

64
0:05:19.320 --> 0:05:23.560
And this is how this looks as a Copee spec.

65
0:05:23.560 --> 0:05:32.560
I won't get into the details of the syntax, but you can combine different selection modules,

66
0:05:32.560 --> 0:05:39.800
where you start with the set of all functions and then sort of here select inline functions

67
0:05:39.800 --> 0:05:42.920
or system header functions.

68
0:05:42.920 --> 0:05:50.320
And then you can combine them to, in the end, produce one set that you want to instrument.

69
0:05:50.320 --> 0:05:57.160
And for this particular example, this reduces the number of instrumented functions by 74%

70
0:05:57.160 --> 0:06:01.720
for a test case.

71
0:06:01.720 --> 0:06:07.280
So the choreograph is sort of the base data structure that we use for all the analysis

72
0:06:07.280 --> 0:06:09.360
and for the selection.

73
0:06:09.360 --> 0:06:16.320
And this is currently generated based on source level by a tool called MetaCg.

74
0:06:16.320 --> 0:06:21.760
And this can be a bit cumbersome for very complex applications such as OpenFoam, because

75
0:06:21.760 --> 0:06:28.720
you need sort of a separate source code analysis step, and that can be difficult to set up,

76
0:06:28.720 --> 0:06:32.480
especially if there's like shared library dependencies, stuff like that.

77
0:06:32.480 --> 0:06:34.680
So it can be tricky.

78
0:06:34.680 --> 0:06:40.280
And so we looked into how we can maybe generate a call graph at different stages.

79
0:06:40.280 --> 0:06:46.960
And so this is what Tim's going to talk about, how we can generate a call graph at different

80
0:06:46.960 --> 0:06:48.680
program stages.

81
0:06:48.680 --> 0:06:55.600
And then we introduced the cage compiler plugin, where we, yeah, evaluate how this can be done

82
0:06:55.600 --> 0:06:59.920
at link time.

83
0:06:59.920 --> 0:07:06.920
So, well, thank you.

84
0:07:06.920 --> 0:07:10.520
So we were interested in the whole program call graphs.

85
0:07:10.520 --> 0:07:14.600
This is because copy, the tool that does our instrumentation, needs to be aware of every

86
0:07:14.600 --> 0:07:19.800
function inside our call graph, which means that it's necessary to have a whole program

87
0:07:19.800 --> 0:07:21.080
view.

88
0:07:21.080 --> 0:07:27.680
And the tools that were used like MetaCg were able to create a call graph, and they have

89
0:07:27.680 --> 0:07:31.840
the distinct advantage of being able to annotate Meta data to this call graph.

90
0:07:31.840 --> 0:07:34.500
This is where the name MetaCg came from.

91
0:07:34.500 --> 0:07:40.320
And this means that we not only can answer structural questions, like we want to instrument

92
0:07:40.320 --> 0:07:46.840
every function that eventually call path down to some MPI call, but we can also answer an

93
0:07:46.840 --> 0:07:48.560
instrument based on Meta data.

94
0:07:48.560 --> 0:07:54.400
For example, the loop depth specification or floating point operations.

95
0:07:54.400 --> 0:07:58.240
And yes, there were multiple possible ways to generate a call graph.

96
0:07:58.240 --> 0:07:59.360
One of them is source code.

97
0:07:59.360 --> 0:08:03.800
One of them is the intermediate representation that is basically part of every compiler,

98
0:08:03.800 --> 0:08:10.520
especially the LLVM compiled pipeline, and machine code at the very last.

99
0:08:10.520 --> 0:08:16.160
So the basic idea is, well, we have the source code, let's do call graph generation on the

100
0:08:16.160 --> 0:08:19.240
source code, which is relatively easy.

101
0:08:19.240 --> 0:08:22.660
MetaCg is doing it on a source code basis.

102
0:08:22.660 --> 0:08:27.080
But this means as you feed every single source code file into MetaCg, this means that your

103
0:08:27.080 --> 0:08:32.140
view of the program is limited to one translation unit at a time.

104
0:08:32.140 --> 0:08:37.280
So you then need to merge those parts, call graphs of every translation unit together

105
0:08:37.280 --> 0:08:42.420
to generate this overview of the whole program that you need.

106
0:08:42.420 --> 0:08:46.080
The information you then gather from your profiling maps very cleanly back to your source

107
0:08:46.080 --> 0:08:52.240
code, because once you find a function, well, it's named the same way in your source code.

108
0:08:52.240 --> 0:08:56.600
But on the other hand, what you write in your source code is not necessarily what's actually

109
0:08:56.600 --> 0:08:58.640
executed on the machine, right?

110
0:08:58.640 --> 0:09:02.440
Because there might be code transformation, constant propagation, dead code elimination,

111
0:09:02.440 --> 0:09:07.520
inlining, and this is actually something we want to be aware of if we are doing instrumentation,

112
0:09:07.520 --> 0:09:11.200
not that we want to instrument a function that doesn't exist anymore.

113
0:09:11.200 --> 0:09:16.560
Also, this merging of the translation units means that the user is involved.

114
0:09:16.560 --> 0:09:22.060
The user currently has to specify when he uses MetaCg which source code translation

115
0:09:22.060 --> 0:09:28.240
units belong to one target, and then manually has to tell MetaCg these functions are all

116
0:09:28.240 --> 0:09:32.120
to be merged, please generate the call graph for that.

117
0:09:32.120 --> 0:09:35.680
The user might not perfectly emulate the linker behavior.

118
0:09:35.680 --> 0:09:41.640
So there are different resolution types that a linker might choose if there are samely

119
0:09:41.640 --> 0:09:48.640
named structs or classes, and dependent on how you are implementing your merging process,

120
0:09:48.640 --> 0:09:52.880
you might have slight differences between your call graph that you generated and that

121
0:09:52.880 --> 0:09:55.580
what the linker will eventually do.

122
0:09:55.580 --> 0:10:00.440
So the other extreme would be, well, let's do it on the compiled machine code then.

123
0:10:00.440 --> 0:10:04.180
Reverse engineering tools like radar or GDR are able to generate call graphs and binary

124
0:10:04.180 --> 0:10:09.400
data just fine, and those have the very distinct advantages that this is actually what is run

125
0:10:09.400 --> 0:10:10.400
on the machine.

126
0:10:10.400 --> 0:10:12.960
There are no code transformation left.

127
0:10:12.960 --> 0:10:19.760
You have the advantage of being able to see machine code optimization passes if they are

128
0:10:19.760 --> 0:10:22.620
influencing the generated call graph.

129
0:10:22.620 --> 0:10:26.800
But on the other hand, a lot of information, the metadata that we also would like to be

130
0:10:26.800 --> 0:10:31.160
able to instrument based upon are lost as soon as we go down to machine code.

131
0:10:31.160 --> 0:10:36.960
Inlining already happened, so there is no function annotated with please inline anymore.

132
0:10:36.960 --> 0:10:40.480
Also pointer type information as we heard in the talk earlier gets lost as soon as we

133
0:10:40.480 --> 0:10:46.000
go down to machine type, and constness is also something that is more to be inferred

134
0:10:46.000 --> 0:10:49.740
than actually stated once we go down to machine code.

135
0:10:49.740 --> 0:10:55.280
And so we decided the best of both worlds is probably the LLVM IR because it's a heavily

136
0:10:55.280 --> 0:10:58.120
annotated representation.

137
0:10:58.120 --> 0:11:03.360
It is close enough to what will run on the machine that we have the ability to observe

138
0:11:03.360 --> 0:11:04.560
the code transformation.

139
0:11:04.560 --> 0:11:11.520
We are able to give more specific estimates on what the actual cost of a function might

140
0:11:11.520 --> 0:11:16.440
be, because we have more clear way of tracking, for example, instruction counts, floating

141
0:11:16.440 --> 0:11:19.200
ops and integer ops.

142
0:11:19.200 --> 0:11:23.760
On the other hand, it's also close enough to what the user actually wrote because we

143
0:11:23.760 --> 0:11:28.160
are not down on the machine code yet, and we can figure out the inlining stuff, the

144
0:11:28.160 --> 0:11:32.800
constants, the virtual functions, we can get type information in the IR.

145
0:11:32.800 --> 0:11:38.000
And if we do it at link time, we are not even limited to the translation unit by translation

146
0:11:38.000 --> 0:11:42.000
unit scope, that source code based approaches are.

147
0:11:42.000 --> 0:11:46.380
So if you have your pretty default compile pipeline, you have your source code which

148
0:11:46.380 --> 0:11:51.040
builds a translation unit, gets fed into the compiler, which outputs intermediate representation,

149
0:11:51.040 --> 0:11:56.920
and then the optimizer runs there and multiple source code translation unit optimized IR

150
0:11:56.920 --> 0:12:02.880
modules are fed into the linker, and we can do our call graph analysis inside the linker,

151
0:12:02.880 --> 0:12:09.100
solved our translation unit problem, and are able to have all our information ready.

152
0:12:09.100 --> 0:12:11.640
So to do this, we developed the CAGE plugin.

153
0:12:11.640 --> 0:12:18.920
CAGE stands for call graph embedding LLVM plugin, and it basically generates a call

154
0:12:18.920 --> 0:12:25.760
graph using some of LLVM tools, does some annotation, virtual function call analysis,

155
0:12:25.760 --> 0:12:29.760
and this can run as part of the optimizer in the pipeline, but it can also run as part

156
0:12:29.760 --> 0:12:36.920
of the LLVM linker, also as a plugin for which we use a slightly modified version of the

157
0:12:36.920 --> 0:12:44.640
LLVM linker, but the basic logic of running plugins in the LLVM linker was there.

158
0:12:44.640 --> 0:12:49.080
So then we do our Vtable analysis, our metadata annotation, because it's all available to

159
0:12:49.080 --> 0:12:54.840
us, and then we embed the result into the binary, which enables dynamic augmentation,

160
0:12:54.840 --> 0:12:57.000
and I will come to that one later.

161
0:12:57.000 --> 0:13:01.880
So at link time, we are doing static analysis basically, and as I already mentioned, we

162
0:13:01.880 --> 0:13:05.140
split our information in basically two types.

163
0:13:05.140 --> 0:13:10.240
One of them is structural information, like call hierarchies, call paths, call depth,

164
0:13:10.240 --> 0:13:16.040
the number of children, how deep we are in our path, and you also have virtual function

165
0:13:16.040 --> 0:13:20.920
calls, which are mostly structural, because once you have virtual and polymorphic calls,

166
0:13:20.920 --> 0:13:26.840
you have a set of functions that are probably being called by that pointer, and so we can

167
0:13:26.840 --> 0:13:31.880
narrow down the possibilities of which functions are called, but we cannot actually statically

168
0:13:31.880 --> 0:13:34.600
figure out this function is getting called.

169
0:13:34.600 --> 0:13:39.280
So it's slightly metadata based, but it's also mostly structural, and on the metadata

170
0:13:39.280 --> 0:13:44.340
information side, we have instruction composition, so we can determine what is the relation between

171
0:13:44.340 --> 0:13:48.720
arithmetic operations and memory operations, for example, or we can generate local and

172
0:13:48.720 --> 0:13:56.480
global loop depth estimates, and then we have inlining information, which is metadata,

173
0:13:56.480 --> 0:14:01.920
because inlining is not like a must-do for a compiler, just because you have specified

174
0:14:01.920 --> 0:14:07.560
inlining for a function doesn't mean the compiler will actually inline the function, so it's

175
0:14:07.560 --> 0:14:13.720
partly structural information and partly metadata, so you see there's no clear line between those,

176
0:14:13.720 --> 0:14:21.200
they blur at some points, but we can represent all those in the metadata annotated call graph,

177
0:14:21.200 --> 0:14:24.160
and if you remember, we were able to do dynamic augmentation.

178
0:14:24.160 --> 0:14:26.000
Well, what is dynamic augmentation?

179
0:14:26.000 --> 0:14:31.880
If you remember, each object contains the call graph that we generated, which means

180
0:14:31.880 --> 0:14:38.160
that the call graphs can be aggregated at runtime if a shared library is loaded, because

181
0:14:38.160 --> 0:14:42.080
even if you are at link time, even if you can see all the statically linked objects,

182
0:14:42.080 --> 0:14:48.360
all the translation units that belong to your target, your binary might load a shared library,

183
0:14:48.360 --> 0:14:51.040
which then, well, you're unaware of.

184
0:14:51.040 --> 0:14:57.040
So the idea is, as soon as the main executable is loaded, it passes its embedded call graph

185
0:14:57.040 --> 0:15:02.780
on startup to a runtime collecting library, and then the main executable can load whatever

186
0:15:02.780 --> 0:15:09.800
shared object it wants, and if this shared object also contains a call graph, then this

187
0:15:09.800 --> 0:15:14.680
runtime collector gets passed this call graph on the first load of the shared object and

188
0:15:14.680 --> 0:15:19.920
can aggregate it, like merging, so we're basically back to the translation unit by translation

189
0:15:19.920 --> 0:15:27.040
unit based approach, but now we're doing shared library on binary and executable based merging,

190
0:15:27.040 --> 0:15:31.880
and then we attach all this data together to one really big whole program call graph,

191
0:15:31.880 --> 0:15:36.400
now including shared objects, and then we can export this, for example, and pass it

192
0:15:36.400 --> 0:15:40.600
back to Kapi for some further refinement of the instrumentation.

193
0:15:40.600 --> 0:15:44.240
All right.

194
0:15:44.240 --> 0:15:48.000
Thanks.

195
0:15:48.000 --> 0:15:56.720
So to put it all together, for Kapi, we have the call graph analysis approach Tim just

196
0:15:56.720 --> 0:16:02.920
explained, so for each object file we have the call graph analysis and then the embedding,

197
0:16:02.920 --> 0:16:10.840
and then on the runtime side we can sort of merge the main executable call graph with

198
0:16:10.840 --> 0:16:16.040
the shared libraries as they are loaded, and we defer the instrumentation using dynamic

199
0:16:16.040 --> 0:16:26.040
instrumentation approach in order to sort of apply that selection dynamically, and yeah,

200
0:16:26.040 --> 0:16:28.200
that's how it works.

201
0:16:28.200 --> 0:16:35.160
So to summarize, we are developing the Kapi tool for instrumentation selection based on

202
0:16:35.160 --> 0:16:41.560
call graph analysis, and we have explored this cage plugin for generating this call

203
0:16:41.560 --> 0:16:47.760
graph information at link time, which allows whole program visibility and this dynamic

204
0:16:47.760 --> 0:16:55.800
instrumentation, and together with Kapi we can use the embedded call graph to run the

205
0:16:55.800 --> 0:17:04.600
selection at runtime, and thereby improving Kapi to make the compilation process and the

206
0:17:04.600 --> 0:17:07.760
analysis process more streamlined.

207
0:17:07.760 --> 0:17:13.680
And this is sort of an active development, so at this point we don't have a very detailed

208
0:17:13.680 --> 0:17:20.000
evaluation about performance and stuff like that, so there's some concerns, for example,

209
0:17:20.000 --> 0:17:27.200
when you go to very big programs and do LTO, there might be performance problems, so there

210
0:17:27.200 --> 0:17:37.680
might be more work to make it viable in that regard, but yeah, it works well in a prototype

211
0:17:37.680 --> 0:17:38.680
version.

212
0:17:38.680 --> 0:17:39.680
Thank you.

213
0:17:39.680 --> 0:17:40.680
Any questions?

214
0:17:40.680 --> 0:18:05.200
Perhaps I was a bit distracted, so I have two questions.

215
0:18:05.200 --> 0:18:17.640
If you can comment on Lambda functions, OMP sections of the code, not specific constructs,

216
0:18:17.640 --> 0:18:26.400
and instruction cache, if you can comment how those are handled, those three aspects, let's

217
0:18:26.400 --> 0:18:27.400
say.

218
0:18:27.400 --> 0:18:34.280
So when it comes to OpenMP, we don't at the moment have any specific OpenMP profiling

219
0:18:34.280 --> 0:18:41.440
interface that we target, so this might be something that is probably useful in the future,

220
0:18:41.440 --> 0:18:49.240
but for now we just do the basic function implementation and select on that.

221
0:18:49.240 --> 0:18:51.040
Then the other point was...

222
0:18:51.040 --> 0:18:53.640
Sorry, could you repeat?

223
0:18:53.640 --> 0:18:55.640
Lambda's in case.

224
0:18:55.640 --> 0:18:59.440
So do you want to comment on that?

225
0:18:59.440 --> 0:19:05.760
So Lambda's in caches, so caching, no, there is no logic to handle caching in any way,

226
0:19:05.760 --> 0:19:11.600
and regarding Lambda's in OpenMP, if you're talking about profiling, you've got your answer

227
0:19:11.600 --> 0:19:19.000
right there, but if you're talking about generating call graphs in which Lambda's and OpenMP runtime

228
0:19:19.000 --> 0:19:24.760
calls are available, then the call graph will actually figure out that there are OpenMP

229
0:19:24.760 --> 0:19:33.440
runtime calls and will correctly, if I remember correctly, will figure out that this function

230
0:19:33.440 --> 0:19:38.400
calls back to the OpenMP runtime, because once you're in IR, the runtime is actually

231
0:19:38.400 --> 0:19:44.000
carved out, all the pragmas were removed from the actual source code, so we are aware of

232
0:19:44.000 --> 0:19:48.640
OpenMP, but we are not using that information currently for any profiling, but you could

233
0:19:48.640 --> 0:19:55.440
do metadata-based copy selection with it, so every call path that eventually leads to

234
0:19:55.440 --> 0:20:03.080
an OpenMP runtime call would be a valid instrumentation using copy.

235
0:20:03.080 --> 0:20:06.680
Just that the question of the instruction cache is whether...

236
0:20:06.680 --> 0:20:12.680
This is my ignorance, but are you introducing a lot of new instructions here in the code

237
0:20:12.680 --> 0:20:15.480
or in the code that is being read?

238
0:20:15.480 --> 0:20:16.800
Oh, I see.

239
0:20:16.800 --> 0:20:19.520
I was thinking whether too much data ends up.

240
0:20:19.520 --> 0:20:22.800
Maybe I'm wrong and I didn't understand well.

241
0:20:22.800 --> 0:20:25.880
This is more of a performance-related question, right?

242
0:20:25.880 --> 0:20:31.160
Yes, of course, you introduce a new instruction whenever a shared library is loaded, because

243
0:20:31.160 --> 0:20:37.800
we then add instructions that pass the call graph back to our runtime collecting facility,

244
0:20:37.800 --> 0:20:42.920
and we also introduce instructions because we are using a profiling approach, which are

245
0:20:42.920 --> 0:20:46.960
instruction calls, so yeah, we are impeding the instruction fetching, instruction caching

246
0:20:46.960 --> 0:20:52.560
flow, which is also why profiling has rather high overhead compared to sampling approaches,

247
0:20:52.560 --> 0:21:00.840
for example, but as Sebastian told, we have not really extensively profiled our application,

248
0:21:00.840 --> 0:21:08.920
quite ironic, so we are not aware how much the benefit or the impact actually would be.

249
0:21:08.920 --> 0:21:17.880
So, in your slide, you say you have a fork of LLD.

250
0:21:17.880 --> 0:21:22.480
The obvious question is what's stopping you from upstreaming this?

251
0:21:22.480 --> 0:21:31.000
So yes, we have a fork of LLD that just basically exposes the flag load new pass manager plugin

252
0:21:31.000 --> 0:21:34.160
and you pass the plugin and it does the rest.

253
0:21:34.160 --> 0:21:38.600
What's currently holding us back from upstreaming is it's not very well developed.

254
0:21:38.600 --> 0:21:44.000
It was coupled together in half a week or something, and there already is an open merge

255
0:21:44.000 --> 0:21:50.880
request and fabricator that implements this exact functionality for, if I remember correctly,

256
0:21:50.880 --> 0:22:00.200
LLVM9, which was abandoned for a year until it was totally abandoned and closed, and so

257
0:22:00.200 --> 0:22:05.800
we didn't actually figure out how to make this more interesting.

258
0:22:05.800 --> 0:22:08.680
We'll take that as a signal.

259
0:22:08.680 --> 0:22:10.800
People just move jobs or whatever.

260
0:22:10.800 --> 0:22:13.040
So try it again and bash people.

261
0:22:13.040 --> 0:22:16.520
I can help you with that as well, find the right people to get this, because it seems

262
0:22:16.520 --> 0:22:18.960
like a simple and obvious thing to have.

263
0:22:18.960 --> 0:22:21.920
It isn't actually that hard.

264
0:22:21.920 --> 0:22:27.240
Apparently there wasn't much interest in the community back in 2020.

265
0:22:27.240 --> 0:22:28.240
That's too long ago.

266
0:22:28.240 --> 0:22:29.240
Just try again.

267
0:22:29.240 --> 0:22:40.720
Then we will polish it a little much, and then we will hopefully get this one upstream.

268
0:22:40.720 --> 0:22:47.360
Any other questions?

269
0:22:47.360 --> 0:22:48.360
Thank you very much.

270
0:22:48.360 --> 0:23:00.360
Aye.

