WEBVTT

00:00.000 --> 00:14.920
Okay, next lightning talk is Jan VerSkel.

00:14.920 --> 00:18.880
They're talking about multiple double arithmetic on GPUs.

00:18.880 --> 00:19.880
Thank you very much.

00:19.880 --> 00:25.360
Thank you very much, the organizers, for allowing me to speak here.

00:25.360 --> 00:36.080
So I will hope to talk about computations that I've been doing with multiple doubles.

00:36.080 --> 00:43.340
So the multiple doubles go back actually from the time when people, when the hardware was

00:43.340 --> 00:45.820
not yet supporting doubles.

00:45.820 --> 00:49.060
So this was the late 60s.

00:49.060 --> 00:51.720
So this is actually a similar idea.

00:51.720 --> 00:58.680
So you use the hardware arithmetic to extend your precision.

00:58.680 --> 01:00.600
It has a lot of advantages.

01:00.600 --> 01:06.420
So if you're used to working with complex arithmetic, then double double arithmetic has

01:06.420 --> 01:10.400
about the same intensity.

01:10.400 --> 01:17.160
So speaking of intensities, relative to the previous talk, where we were working with

01:17.160 --> 01:23.000
graphs, so in the previous talk I had the impression that everything was about graphs,

01:23.000 --> 01:24.800
and there was memory bound.

01:24.800 --> 01:27.480
My problems are compute bound.

01:27.480 --> 01:33.240
So I get really good arithmetic intensities.

01:33.240 --> 01:35.040
There are some disadvantages, of course.

01:35.040 --> 01:40.360
If you want to work with, say, 17 decimal places, you can't.

01:40.360 --> 01:46.240
Also if you want to work with truly infinite decimals, well, you can't either because you're

01:46.240 --> 01:53.200
still having your 11 bits of the exponent.

01:53.200 --> 01:57.300
Disadvantage might also be that you can still do numerical analysis.

01:57.300 --> 02:02.440
So this might be an advantage or disadvantage.

02:02.440 --> 02:06.040
I got into this by power series arithmetic.

02:06.040 --> 02:09.840
So this is about the EXP and the EPS.

02:09.840 --> 02:15.320
So when I started working with power series, I was using 1111111.

02:15.320 --> 02:17.200
And I know the binomial theorem.

02:17.200 --> 02:22.160
Well, I only knew it when I saw the numbers blowing up on me.

02:22.160 --> 02:24.880
So you know it when you don't know it.

02:24.880 --> 02:26.560
So here is a table.

02:26.560 --> 02:31.960
The exponential has a very nice development, nicely decaying.

02:31.960 --> 02:36.360
And if you multiply these exponentials, you don't have any blow up.

02:36.360 --> 02:43.520
However, the last coefficient, if you want to represent that, and you have to think about

02:43.520 --> 02:44.520
GPUs.

02:44.520 --> 02:48.640
The GPUs are actually quite happy if they can do things in groups of 32.

02:48.640 --> 02:56.640
So actually a 32 power series, an order 32 power series is actually still very small

02:56.640 --> 02:57.640
for GPUs.

02:57.640 --> 03:00.040
But there you have already to use quad doubles.

03:00.040 --> 03:09.920
Otherwise, your last coefficients, you can't represent it anymore.

03:09.920 --> 03:13.940
So I started working with the QD library.

03:13.940 --> 03:18.040
And then we were doing multi core.

03:18.040 --> 03:22.680
Me and my student, Gennady Joffee, and we looked at each other.

03:22.680 --> 03:24.720
Should we do this on the GPU?

03:24.720 --> 03:28.040
Should we write the entire library on the GPU?

03:28.040 --> 03:29.720
My student didn't really want to do it.

03:29.720 --> 03:31.200
And I didn't want to do it.

03:31.200 --> 03:33.800
But then we discovered GQD.

03:33.800 --> 03:35.720
And we used GQD.

03:35.720 --> 03:39.840
And the recent package that we are using is Compari.

03:39.840 --> 03:43.880
It's actually the only software I know that is named after a beverage.

03:43.880 --> 03:46.560
I don't know if that's a good sign or not.

03:46.560 --> 03:50.620
In my supermarket store in Chicago, I once saw Compari.

03:50.620 --> 03:52.040
But it's not my drink.

03:52.040 --> 03:58.040
So I didn't want to ruin the taste of using Compari.

03:58.040 --> 03:59.760
So I stayed off this.

03:59.760 --> 04:01.080
Compari is actually quite good.

04:01.080 --> 04:08.920
So because it allowed me to go to quad double and now also octo double.

04:08.920 --> 04:11.960
The numbers in this table are kind of good.

04:11.960 --> 04:15.840
Because I want to have really performance.

04:15.840 --> 04:18.000
But it also comes somehow misleading.

04:18.000 --> 04:24.320
Because as soon as you're using complex double double, everything becomes compute bound.

04:24.320 --> 04:30.320
And the problems that you have with memory transfer and all, you do a lot of arithmetic

04:30.320 --> 04:37.360
operations on a relatively small amount of data.

04:37.360 --> 04:40.360
I also like to do quality up.

04:40.360 --> 04:48.320
If you can afford the time for, say, a double precision calculation, well, you will see

04:48.320 --> 04:50.880
that everything is not really right.

04:50.880 --> 04:54.760
But then you can work, you can allow the same amount of time.

04:54.760 --> 04:59.760
And you quadruple the precision.

04:59.760 --> 05:02.560
So the 439 there.

05:02.560 --> 05:05.040
Think about one gigaflop.

05:05.040 --> 05:07.880
About two gigaflop.

05:07.880 --> 05:09.560
And then you go to teraflop.

05:09.560 --> 05:15.000
So the 439 is kind of if you have teraflop performance, it's like as if you would be

05:15.000 --> 05:19.080
doing this on a single core.

05:19.080 --> 05:22.920
So I mentioned the funding agencies at the very slides.

05:22.920 --> 05:26.040
I would like to have a hopper.

05:26.040 --> 05:31.000
But so for now, I have to deal with Pascal and Volta.

05:31.000 --> 05:34.160
And the last one is a gaming laptop.

05:34.160 --> 05:38.720
So which is also actually quite a powerful GPU.

05:38.720 --> 05:42.040
My first teraflop card was Kepler.

05:42.040 --> 05:47.960
And this last list of GPU actually gets there.

05:47.960 --> 05:49.960
Okay.

05:49.960 --> 05:54.480
If you think of a double-double, there is a double-two.

05:54.480 --> 05:57.480
And then for a quad-double, there is the double-four.

05:57.480 --> 06:00.520
So that was what the GQD was using.

06:00.520 --> 06:03.160
And that's very good for memory coalescing.

06:03.160 --> 06:09.040
But we actually got into trouble with the complex quad-double because there was no double-eight.

06:09.040 --> 06:15.640
So instead of working, if you work with a vector of quad-doubles, a vector of arrays

06:15.640 --> 06:20.360
of four length, you actually better use four vectors.

06:20.360 --> 06:25.760
The first one with the highest double, second double, third double, fourth double.

06:25.760 --> 06:29.040
So it's a little bit similar like working with power series.

06:29.040 --> 06:33.280
A power series is invertible if the leading coefficient is not zero.

06:33.280 --> 06:36.200
You can work with matrices of power series.

06:36.200 --> 06:37.280
But actually, that's not good.

06:37.280 --> 06:41.640
You should actually work with a series where the coefficients are matrices.

06:41.640 --> 06:43.840
Same idea here.

06:43.840 --> 06:47.760
QDLIP is a very good library still.

06:47.760 --> 06:48.940
It's quite complete.

06:48.940 --> 06:54.880
So I have extended the square root, for example, to octo-double position.

06:54.880 --> 06:56.480
Okay.

06:56.480 --> 07:01.800
So here is then my beginning.

07:01.800 --> 07:04.840
So I mentioned, so you saw this eight.

07:04.840 --> 07:11.880
So if you take a vector of random complex number, 64, then the norm is eight.

07:11.880 --> 07:12.920
It should be eight.

07:12.920 --> 07:16.480
So that's a really nice test property.

07:16.480 --> 07:20.440
If you work with GPUs, you actually define kernels.

07:20.440 --> 07:22.200
And kernels, the name says it itself.

07:22.200 --> 07:23.200
It should be small.

07:23.200 --> 07:24.800
So think small.

07:24.800 --> 07:28.560
And actually, this problem is a small problem, mathematically speaking.

07:28.560 --> 07:35.480
But it has all the richness and the complexity of all the problems that you will run into.

07:35.480 --> 07:39.220
You will have to study the prefix sum algorithm, for example.

07:39.220 --> 07:40.600
So that is needed.

07:40.600 --> 07:45.800
You also have to tune your software for large vectors, or for small vector.

07:45.800 --> 07:53.440
For small vectors, you can only have one block of threads that is active.

07:53.440 --> 07:56.680
The square root, well, it works with staggered.

07:56.680 --> 08:01.920
So you apply a Newton method.

08:01.920 --> 08:07.320
And then actually, this is where the dot comes in.

08:07.320 --> 08:15.600
So the nice thing about double-double, squat-doubles, that everything fits into registers.

08:15.600 --> 08:19.120
So it's also very good if you do multi-core.

08:19.120 --> 08:21.000
So you don't have to use the heap ever.

08:21.000 --> 08:25.560
But of course, when you get to complex squat-doubles, you have these eight arrays.

08:25.560 --> 08:29.160
If you do octo-doubles, so it doubles and doubles and doubles.

08:29.160 --> 08:34.880
So I have with my old graphics cards, they can no longer even compile the octo-doubles

08:34.880 --> 08:36.880
if you inline too much.

08:36.880 --> 08:45.960
So it's still very interesting that actually you have to tailor your kernels towards the

08:45.960 --> 08:48.760
precision levels.

08:48.760 --> 08:50.040
So here is my last slide.

08:50.040 --> 08:52.800
I did more than just norms.

08:52.800 --> 09:00.280
So we have teraflop performance when we evaluate polynomials and differentiate them.

09:00.280 --> 09:04.080
The QR, the blocked householder QR, is also wonderful.

09:04.080 --> 09:12.240
You get already teraflop performance with 1,000 in complex double-double.

09:12.240 --> 09:17.520
And then the last paper is where you try to combine these things by computing Taylor series

09:17.520 --> 09:24.680
for solutions of solution developments for polynomial systems.

09:24.680 --> 09:27.240
Newton's method is actually a quite nice operator.

09:27.240 --> 09:32.640
You start with a multivariate system where all the variables are linked to each other.

09:32.640 --> 09:38.880
And what Newton actually does, it spits out power series for each component.

09:38.880 --> 09:46.200
So actually it untangles all the non-linearities.

09:46.200 --> 09:47.600
So I listed the archive.

09:47.600 --> 09:52.140
So the IEEE puts things in a paywall, behind the paywall.

09:52.140 --> 09:55.200
So you have the archive versions there.

09:55.200 --> 09:56.640
And you're more than welcome.

09:56.640 --> 10:01.200
So the bottom line of the slide, I mean the conclusion actually is that all the software

10:01.200 --> 10:02.920
is free and open source.

10:02.920 --> 10:18.800
I'd have the GitHub handle there.
