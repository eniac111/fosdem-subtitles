1
0:00:00.000 --> 0:00:10.360
Thank you for the opportunity to present our project, Namba NPI.

2
0:00:10.360 --> 0:00:12.760
Let me first acknowledge the co-authors.

3
0:00:12.760 --> 0:00:18.880
My name is Sylvester Arabas, and we are here with Alexi Bulenok and Kasper Derlatka from

4
0:00:18.880 --> 0:00:24.160
Yagilonian University in Krakow, Poland, Machin Manna from the same university contributed

5
0:00:24.160 --> 0:00:28.640
to this project, and we have also, we will be presenting some work from David Zwicker

6
0:00:28.640 --> 0:00:33.320
from Max Planck Institute for Dynamics and Self-Organization in GÃ¶ttingen.

7
0:00:33.320 --> 0:00:41.360
So let's start with a maybe controversial provocative question, Python and HPC, and let's

8
0:00:41.360 --> 0:00:48.360
try to look for answers to this question in a very respective, respected journal.

9
0:00:48.360 --> 0:00:53.560
So maybe you have some guesses what's written there.

10
0:00:53.560 --> 0:00:59.680
2019, in scripting languages such as Python, users type code into an interactive editor

11
0:00:59.680 --> 0:01:00.680
line by line.

12
0:01:00.680 --> 0:01:03.400
It doesn't sound like HPC.

13
0:01:03.400 --> 0:01:08.600
Next year, level of computational performance that Python simply couldn't deliver.

14
0:01:08.600 --> 0:01:11.240
Same year, same journal.

15
0:01:11.240 --> 0:01:16.320
Numpy runs on machines ranging from embedded devices to the world's largest supercomputers

16
0:01:16.320 --> 0:01:21.580
with performance approaching that of compiled languages.

17
0:01:21.580 --> 0:01:23.840
Same year, nature astronomy.

18
0:01:23.840 --> 0:01:27.400
Astronomers should avoid interpreted scripting languages such as Python.

19
0:01:27.400 --> 0:01:32.480
In principle, Numpy and Numpy can lead to an enormous increase in speed, but please reconsider

20
0:01:32.480 --> 0:01:36.080
teaching Python to university students.

21
0:01:36.080 --> 0:01:38.320
Same year, nature methods.

22
0:01:38.320 --> 0:01:44.000
Implementing new functionality into SciPy, Python is still the language of choice.

23
0:01:44.000 --> 0:01:50.000
Full tests should pass with the PyPy Just-in-Time compiler as of 1.0 SciPy.

24
0:01:50.000 --> 0:01:52.520
Are they talking about the same language?

25
0:01:52.520 --> 0:01:54.040
No.

26
0:01:54.040 --> 0:01:56.880
The left-hand side are papers about Rust and Julia.

27
0:01:56.880 --> 0:01:58.960
The right-hand side are papers about Python.

28
0:01:58.960 --> 0:01:59.960
Okay?

29
0:01:59.960 --> 0:02:01.860
So maybe that's the reason.

30
0:02:01.860 --> 0:02:08.920
So just to set the stage, let me present, I think, the way that is apt for thinking about

31
0:02:08.920 --> 0:02:10.000
Python.

32
0:02:10.000 --> 0:02:16.720
So Python as a language lacks any support for multidimensional arrays or number crunching

33
0:02:16.720 --> 0:02:21.680
because it leaves it to packages to be handled.

34
0:02:21.680 --> 0:02:27.680
Python also leaves it to implementations to actually interpret its syntax.

35
0:02:27.680 --> 0:02:32.280
And SciPy, of course, the major domain implementation, but it's not the only one.

36
0:02:32.280 --> 0:02:37.640
And actually, solutions exist that streamline, for example, Just-in-Time compilation of Python

37
0:02:37.640 --> 0:02:38.640
code.

38
0:02:38.640 --> 0:02:46.080
Moreover, Numpy, while de facto standard, is not the implementation of the Numpy API.

39
0:02:46.080 --> 0:02:52.760
And alternatives are embedded in Just-in-Time frameworks, Just-in-Time compilation frameworks,

40
0:02:52.760 --> 0:02:58.200
GPU frameworks for Python, and they leverage typing and concurrency.

41
0:02:58.200 --> 0:03:04.160
So probably here the highlight is that Python lets you glue these technologies together

42
0:03:04.160 --> 0:03:10.240
and package them together, leveraging some of the Python ecosystem and its popularity,

43
0:03:10.240 --> 0:03:11.820
etc.

44
0:03:11.820 --> 0:03:15.360
And probably, arguably, I would say that's an advantage.

45
0:03:15.360 --> 0:03:21.680
I'm not saying that please use Python for HPC instead of Julia, probably vice versa,

46
0:03:21.680 --> 0:03:22.680
actually.

47
0:03:22.680 --> 0:03:26.360
But still, it's an interesting question to see how it can perform.

48
0:03:26.360 --> 0:03:30.080
Okay, so let's check it.

49
0:03:30.080 --> 0:03:36.320
I will present a brief benchmark, a very tiny one that we have come up with in relation

50
0:03:36.320 --> 0:03:37.720
with this project.

51
0:03:37.720 --> 0:03:39.080
And it uses Numba.

52
0:03:39.080 --> 0:03:44.800
Numba is a Just-in-Time compiler that translates a subset of Python and Numpy into machine

53
0:03:44.800 --> 0:03:49.240
code that is compiled at runtime using LLVM.

54
0:03:49.240 --> 0:03:55.560
Okay, so here is the story about the super simple benchmark problem.

55
0:03:55.560 --> 0:03:58.320
It's related to numerical weather prediction.

56
0:03:58.320 --> 0:04:05.640
So you can imagine a grid of numbers representing weather here.

57
0:04:05.640 --> 0:04:09.920
And numerical weather prediction, or part of numerical weather prediction, the integration

58
0:04:09.920 --> 0:04:17.760
part involves solving equations for the hydrodynamics, that is the transport of such pattern in space

59
0:04:17.760 --> 0:04:22.760
and time, and of course, thermodynamics that tell you what's happening in the atmosphere.

60
0:04:22.760 --> 0:04:23.960
Super simplified picture.

61
0:04:23.960 --> 0:04:27.000
I'm not saying that's the whole story about NWP.

62
0:04:27.000 --> 0:04:33.480
But for benchmarking Numba, let's simplify it down to, in this case, two-dimensional

63
0:04:33.480 --> 0:04:34.480
simple problem.

64
0:04:34.480 --> 0:04:37.520
You have a grid, x, y, some signal.

65
0:04:37.520 --> 0:04:43.520
And if we look at just the transport problem, a partial differential equation for transport,

66
0:04:43.520 --> 0:04:49.880
we can see what happens if we move around such signal, which could be some humidity,

67
0:04:49.880 --> 0:04:51.800
temperature, or whatever in the atmosphere.

68
0:04:51.800 --> 0:04:55.440
Okay, so we have a sample problem.

69
0:04:55.440 --> 0:05:01.360
Here I'm showing results from a three-dimensional version of what was just shown.

70
0:05:01.360 --> 0:05:04.560
And let's start with the right-hand side plot.

71
0:05:04.560 --> 0:05:07.120
X axis, the size of the grid.

72
0:05:07.120 --> 0:05:09.840
So if it's eight, it means eight by eight.

73
0:05:09.840 --> 0:05:10.840
Super tiny.

74
0:05:10.840 --> 0:05:14.760
If it's 128, it's 128 by 128 by 128.

75
0:05:14.760 --> 0:05:18.120
And wall time per time step on the y axis.

76
0:05:18.120 --> 0:05:19.120
Green.

77
0:05:19.120 --> 0:05:24.600
C++ implementation of one particular algorithm for this kind of problems.

78
0:05:24.600 --> 0:05:25.600
Orange.

79
0:05:25.600 --> 0:05:27.400
PiMP data.

80
0:05:27.400 --> 0:05:32.480
The same algorithm numerically, but a Python implementation.

81
0:05:32.480 --> 0:05:42.560
So here you see that actually Numba just compiled version outperformed C++, maintaining even

82
0:05:42.560 --> 0:05:48.960
better scaling for the tiny matrices, but they are kind of irrelevant for the problem.

83
0:05:48.960 --> 0:05:53.080
And please note that in both cases we have used multi-threading.

84
0:05:53.080 --> 0:05:57.760
So here on the left-hand side, you can see actually on the x axis number of threads,

85
0:05:57.760 --> 0:06:00.760
y axis wall time per time step.

86
0:06:00.760 --> 0:06:04.080
And again, the green line is the C++ implementation.

87
0:06:04.080 --> 0:06:06.640
These two are two variants of the Python one.

88
0:06:06.640 --> 0:06:08.880
JIT compiled with Numba.

89
0:06:08.880 --> 0:06:11.280
Almost an order of magnitude faster execution.

90
0:06:11.280 --> 0:06:12.720
Five times faster.

91
0:06:12.720 --> 0:06:19.080
And what's probably most interesting for now is that when you compare with just setting

92
0:06:19.080 --> 0:06:26.960
the environment variable for Numba JIT to disabled, we jump more than two orders of

93
0:06:26.960 --> 0:06:31.280
magnitude up in wall time.

94
0:06:31.280 --> 0:06:40.040
So this is how Numba timing compares with plain Python timing.

95
0:06:40.040 --> 0:06:43.300
But there are two important things to be mentioned here.

96
0:06:43.300 --> 0:06:48.040
The Python package is written having Numba in mind.

97
0:06:48.040 --> 0:06:55.120
That is, everything is loop based, which is the reason why plain CPython with Numba performs

98
0:06:55.120 --> 0:06:56.120
badly.

99
0:06:56.120 --> 0:07:01.200
And this line is kind of irrelevant, just as a curiosity.

100
0:07:01.200 --> 0:07:06.320
On the other hand, the C++ version is kind of legacy based on Blitz++ library.

101
0:07:06.320 --> 0:07:10.520
Back then when it was developed, Eigen didn't have support for multiple dimensions.

102
0:07:10.520 --> 0:07:16.840
And it's object oriented processing which was reported and measured to be kind of five

103
0:07:16.840 --> 0:07:21.320
times slower than for some 77 for these kind of small domains.

104
0:07:21.320 --> 0:07:24.200
It's not the same for larger domains.

105
0:07:24.200 --> 0:07:28.680
Anyhow, we can achieve high performance with Python.

106
0:07:28.680 --> 0:07:31.080
But what if we need MPI?

107
0:07:31.080 --> 0:07:35.360
We need message passing in our code.

108
0:07:35.360 --> 0:07:36.960
How would we use it?

109
0:07:36.960 --> 0:07:42.700
Let's say we divide in a domain that can position spirit our domain in two parts.

110
0:07:42.700 --> 0:07:49.440
So the same problem, same setup, just half of the domain is computed by one process or

111
0:07:49.440 --> 0:07:59.200
node or anything that has distributed, has different memory addressing than another worker.

112
0:07:59.200 --> 0:08:00.680
So this is how we want to use it.

113
0:08:00.680 --> 0:08:02.400
Why we want to use MPI?

114
0:08:02.400 --> 0:08:07.640
Well, because despite expansion in parallel computation, both in the number of machines

115
0:08:07.640 --> 0:08:12.400
and the number of cores, no other parallel programming paradigm has replaced MPI.

116
0:08:12.400 --> 0:08:15.000
At least as of 2013.

117
0:08:15.000 --> 0:08:21.320
And already in 2013, people were writing that this is even though it's universally acknowledged

118
0:08:21.320 --> 0:08:24.320
that MPI is a crude way of programming these machines.

119
0:08:24.320 --> 0:08:27.000
Anyhow, still let's try it.

120
0:08:27.000 --> 0:08:29.000
And let's try it with Python.

121
0:08:29.000 --> 0:08:37.120
So here is a seven line snippet of code where we try to import number to get the JIT compilation

122
0:08:37.120 --> 0:08:38.760
of Python code.

123
0:08:38.760 --> 0:08:43.280
And then we use MPI for Py which is Python interface to MPI.

124
0:08:43.280 --> 0:08:44.280
What do we do?

125
0:08:44.280 --> 0:08:49.000
We find some number crunching routine and we try to use MPI from it and then we try

126
0:08:49.000 --> 0:08:51.360
to end JIT.

127
0:08:51.360 --> 0:08:58.840
End JIT means the highest performance variant of number JIT compilation.

128
0:08:58.840 --> 0:09:02.840
We try to JIT compile this function and straight ahead execute it.

129
0:09:02.840 --> 0:09:05.760
What happens?

130
0:09:05.760 --> 0:09:06.760
It doesn't work.

131
0:09:06.760 --> 0:09:14.640
It doesn't compile because Numba cannot determine type of MPI for Py.MPI.intrachom.

132
0:09:14.640 --> 0:09:16.440
Because it's a class.

133
0:09:16.440 --> 0:09:17.920
Classes do not work with Numba.

134
0:09:17.920 --> 0:09:22.120
At least not the ordinary Python classes.

135
0:09:22.120 --> 0:09:24.120
So something doesn't work.

136
0:09:24.120 --> 0:09:29.360
So the problem is that we have Numba which is one of the leading solutions to speed up

137
0:09:29.360 --> 0:09:30.360
Python.

138
0:09:30.360 --> 0:09:36.600
We have MPI which is clearly the standard for distributed memory parallelization.

139
0:09:36.600 --> 0:09:40.480
We try to work with them together but it doesn't work.

140
0:09:40.480 --> 0:09:43.760
So stack overflow.

141
0:09:43.760 --> 0:09:45.920
Let's duck, duck, go it.

142
0:09:45.920 --> 0:09:46.920
Nothing.

143
0:09:46.920 --> 0:09:47.920
Let's quantit.

144
0:09:47.920 --> 0:09:48.920
Nothing.

145
0:09:48.920 --> 0:09:51.480
Wrong search engine, right?

146
0:09:51.480 --> 0:09:54.200
Someone must have solved the problem.

147
0:09:54.200 --> 0:09:55.360
Nothing.

148
0:09:55.360 --> 0:09:58.280
Let's ask Numba guys and MPI for Py guys.

149
0:09:58.280 --> 0:10:00.920
June 2020.

150
0:10:00.920 --> 0:10:03.680
You will not be able to use MPI for Py's Python code.

151
0:10:03.680 --> 0:10:06.040
It was not designed for such low level usage.

152
0:10:06.040 --> 0:10:07.040
Okay.

153
0:10:07.040 --> 0:10:09.520
It's a Python.

154
0:10:09.520 --> 0:10:12.840
But I mean it must be doable, right?

155
0:10:12.840 --> 0:10:16.120
We have two established packages.

156
0:10:16.120 --> 0:10:19.040
The aim is kind of solid and makes sense.

157
0:10:19.040 --> 0:10:22.080
So it must be doable.

158
0:10:22.080 --> 0:10:28.400
And 30 months later, 120 comments later, 50 PRs later from five contributors on a totally

159
0:10:28.400 --> 0:10:33.400
unplanned site project, we are introducing Numba MPI.

160
0:10:33.400 --> 0:10:43.360
It's an open source kind of small Python project which allows you to let's jump here to the

161
0:10:43.360 --> 0:10:49.640
hello world example which allows you to use the Numba and JIT decorator on a function

162
0:10:49.640 --> 0:10:59.280
that involves rank, size or any other MPI API calls within the Python code.

163
0:10:59.280 --> 0:11:05.680
So now we cover size, rank, send, receive, broadcast, the API for Numba MPI is based

164
0:11:05.680 --> 0:11:07.360
on Numba.

165
0:11:07.360 --> 0:11:09.400
We have auto generated documentation.

166
0:11:09.400 --> 0:11:12.480
We are on PyPy and Conda Forge.

167
0:11:12.480 --> 0:11:15.600
Few words about how it's implemented.

168
0:11:15.600 --> 0:11:22.280
Essentially we start with C types built into Python to try to address the C API.

169
0:11:22.280 --> 0:11:26.800
There are some things related with passing addresses, memories, void pointers, et cetera.

170
0:11:26.800 --> 0:11:31.280
Not super interesting.

171
0:11:31.280 --> 0:11:36.600
Probably the key message here is that we are offering the send function that is already

172
0:11:36.600 --> 0:11:41.480
N JITed which means you can use it from other N JITed functions.

173
0:11:41.480 --> 0:11:46.560
We handle non-continuous arrays from Numba MPI so we try to be user friendly.

174
0:11:46.560 --> 0:11:51.120
We then call the underlying C function.

175
0:11:51.120 --> 0:11:53.560
Kind of that's all.

176
0:11:53.560 --> 0:11:57.880
Truly there is the key line number 30.

177
0:11:57.880 --> 0:11:58.880
This one.

178
0:11:58.880 --> 0:12:04.360
Well, that's nothing but in principle without it Numba optimizes out all our code.

179
0:12:04.360 --> 0:12:10.520
Anyhow, these are kind of things that you see when trying to implement such things.

180
0:12:10.520 --> 0:12:14.640
Unfortunately there are quite more of such hacks inside Numba MPI.

181
0:12:14.640 --> 0:12:20.840
The next slide is kind of thing that you prefer to never see but they cannot be unseen in

182
0:12:20.840 --> 0:12:22.760
a way if you work with it.

183
0:12:22.760 --> 0:12:28.960
So please just think of it as a picture of some problems that we have challenged and

184
0:12:28.960 --> 0:12:36.320
essentially wrote to Numba guys asking how can it be done and we got this kind of tools

185
0:12:36.320 --> 0:12:44.840
for handling void pointers from C types in Numba with Python, NumPy, et cetera.

186
0:12:44.840 --> 0:12:49.360
But well, that's you, Tilt Spy, and that's it.

187
0:12:49.360 --> 0:12:50.400
And it kind of works.

188
0:12:50.400 --> 0:12:51.680
And why do we know it works?

189
0:12:51.680 --> 0:12:52.800
Because we test it.

190
0:12:52.800 --> 0:12:58.560
And let me handle the mic to Alexey to tell you more about testing.

191
0:12:58.560 --> 0:13:01.600
Here it is.

192
0:13:01.600 --> 0:13:03.440
Thank you.

193
0:13:03.440 --> 0:13:05.440
Thank you.

194
0:13:05.440 --> 0:13:06.440
Okay.

195
0:13:06.440 --> 0:13:08.640
This works.

196
0:13:08.640 --> 0:13:18.160
So I'm going to tell you about the CI that we have set up for our project for Numba MPI.

197
0:13:18.160 --> 0:13:22.160
So the CI was set up on a...

198
0:13:22.160 --> 0:13:23.160
Sorry.

199
0:13:23.160 --> 0:13:24.160
The wrong arrow.

200
0:13:24.160 --> 0:13:31.920
The CI was set up at GitHub Actions, as I said.

201
0:13:31.920 --> 0:13:34.880
This is the screen of the workflow.

202
0:13:34.880 --> 0:13:40.040
We start from running the PDOC, pre-commit, and Pylint.

203
0:13:40.040 --> 0:13:47.640
PDOC is for automatic documentation generation, Pylint for static content analysis, and pre-commit

204
0:13:47.640 --> 0:13:49.200
for styling.

205
0:13:49.200 --> 0:13:53.200
After that, if these steps were successfully moved into the main part where we run our

206
0:13:53.200 --> 0:13:59.160
unit tests, this is the example...

207
0:13:59.160 --> 0:14:03.480
Not example, but actually the workflow file that we run.

208
0:14:03.480 --> 0:14:10.200
As you can see, when we run the CI against multiple systems, different Pylint versions

209
0:14:10.200 --> 0:14:13.120
and different MPI implementations.

210
0:14:13.120 --> 0:14:19.680
And here we should say a big thank you to MPI for PyTeam for providing setup MPI GitHub

211
0:14:19.680 --> 0:14:26.560
Action, because this has saved us a lot of time.

212
0:14:26.560 --> 0:14:29.240
Thank you MPI for Py.

213
0:14:29.240 --> 0:14:34.820
And as of operation systems and MPI implementations, we are running...

214
0:14:34.820 --> 0:14:41.720
In case of Linux, we are testing against OpenMPI, MPI-CH, and Intel-PI, Mac OS, MPI-MPI-NC,

215
0:14:41.720 --> 0:14:48.440
MPI-CH, and in case of Windows, it's of course MSMPI implementation.

216
0:14:48.440 --> 0:15:01.240
But when we are talking about MPI-CH, there is a problem that has recently occurred, namely

217
0:15:01.240 --> 0:15:07.920
starting from version four of MPI-CH, it fails for on a Ubuntu, on our CI, for PyTeam version

218
0:15:07.920 --> 0:15:10.680
less than 3.10.

219
0:15:10.680 --> 0:15:18.720
So if anyone has ideas how to fix it, please contact us, we will appreciate any help.

220
0:15:18.720 --> 0:15:26.760
Okay, so we are running the unit test on different systems and so on.

221
0:15:26.760 --> 0:15:29.080
Let's see the sample unit test.

222
0:15:29.080 --> 0:15:39.920
In this test, we are testing the logic of the wrapper, of the broadcast function of

223
0:15:39.920 --> 0:15:46.440
MPI, and the main thing that you should remember from this slide is that we are testing this

224
0:15:46.440 --> 0:15:56.440
function in plain Python implementation as well as JIT compiled by number.

225
0:15:56.440 --> 0:16:01.640
We have also set up an integration test.

226
0:16:01.640 --> 0:16:07.760
The integration test is in another project named by SuperDopplet-LES.

227
0:16:07.760 --> 0:16:15.760
And this is just a scheme of this test, we are starting from providing the initial conditions

228
0:16:15.760 --> 0:16:27.120
for the APDS solver, and these initial conditions are written to the HDF5 file.

229
0:16:27.120 --> 0:16:32.080
After that, we are running three runs.

230
0:16:32.080 --> 0:16:36.680
First one we run with only one process, the second one we have two processes, in the third

231
0:16:36.680 --> 0:16:43.160
we run the three processes, and in each we divide, well, in the first we don't divide

232
0:16:43.160 --> 0:16:50.080
the domain, but the other ones we divide the domain accordingly.

233
0:16:50.080 --> 0:16:57.840
And in the assert state we just compare the results and we want the results to be the

234
0:16:57.840 --> 0:17:00.760
same for different runs.

235
0:17:00.760 --> 0:17:10.080
And also these results are also written to HDF5 file.

236
0:17:10.080 --> 0:17:19.360
Interesting fact that everything works on Windows except installing HDF5.py package

237
0:17:19.360 --> 0:17:21.440
for concurrent file access.

238
0:17:21.440 --> 0:17:24.600
HD5.py package with enabled MPI.io.

239
0:17:24.600 --> 0:17:32.520
We have troubles setting up on Windows, but everything else works fine.

240
0:17:32.520 --> 0:17:40.040
And there is also an independent use case, the PyPD project that uses our library, our

241
0:17:40.040 --> 0:17:48.120
package, and it's not developed by us, so the reader is a user.

242
0:17:48.120 --> 0:17:52.480
And this is the Python package for solving partial differential equation, it focuses

243
0:17:52.480 --> 0:18:04.280
on finite differencing and PDEs are defined by, are provided as strings.

244
0:18:04.280 --> 0:18:09.200
And the solution strategy as follows, we start from partitioning the grid onto different

245
0:18:09.200 --> 0:18:18.600
nodes using number MPI after that we pass expressions using the simpy and compile the

246
0:18:18.600 --> 0:18:26.360
results using number and then we trade the PDE exchange in boundary information between

247
0:18:26.360 --> 0:18:31.400
the nodes using number MPI.

248
0:18:31.400 --> 0:18:32.400
Take home messages.

249
0:18:32.400 --> 0:18:38.960
There is a common mismatch between the Python language and Python ecosystem.

250
0:18:38.960 --> 0:18:45.080
We should remember that, okay, the language could be slow, but we also should consider

251
0:18:45.080 --> 0:18:52.640
the ecosystem around this language, the libraries that are available, the libraries that are

252
0:18:52.640 --> 0:18:56.960
available and probably different implementations.

253
0:18:56.960 --> 0:19:03.360
And Python has a range of global HPC solutions such as just in time compilation, GPU programming,

254
0:19:03.360 --> 0:19:06.480
multithreading and MPI.

255
0:19:06.480 --> 0:19:18.640
In case of number MPI, this is the package to glue the MPI with Python code.

256
0:19:18.640 --> 0:19:26.480
It is tested on CI, on GitHub actions.

257
0:19:26.480 --> 0:19:37.160
We are aiming for 100% unit test coverage and also there is also already the project

258
0:19:37.160 --> 0:19:43.280
that are dependent on this package.

259
0:19:43.280 --> 0:19:49.280
Here we can find the links for number MPI, the GitHub links and also the links to the

260
0:19:49.280 --> 0:19:52.240
packages at PyPy and Anaconda.

261
0:19:52.240 --> 0:19:56.280
And we also welcome contributions.

262
0:19:56.280 --> 0:20:03.120
The first two issues I have mentioned earlier and we also welcome encourage to provide the

263
0:20:03.120 --> 0:20:11.520
logo for number MPI as well as adding support for the other functions or we are also aiming

264
0:20:11.520 --> 0:20:21.400
for dropping dependency on MPI for Py in our project and also the plan is to benchmark

265
0:20:21.400 --> 0:20:25.160
performance of this package.

266
0:20:25.160 --> 0:20:27.960
And also we wanted to acknowledge funding.

267
0:20:27.960 --> 0:20:33.080
The project was funded by National Science Centre of Poland.

268
0:20:33.080 --> 0:20:35.080
Thank you for your attention.

269
0:20:35.080 --> 0:20:46.920
We now have time for questions.

270
0:20:46.920 --> 0:20:52.920
Thank you very much.

271
0:20:52.920 --> 0:20:56.960
Any questions?

272
0:20:56.960 --> 0:20:59.280
Question from an MPI expert.

273
0:20:59.280 --> 0:21:00.280
Thank you.

274
0:21:00.280 --> 0:21:01.280
Hello.

275
0:21:01.280 --> 0:21:02.600
Thank you for the talk.

276
0:21:02.600 --> 0:21:09.120
So the interface you are proposing is very close to the let's say C MPI interface.

277
0:21:09.120 --> 0:21:14.160
Let's say when you do a send you work with a buffer or do you try to provide a bit higher

278
0:21:14.160 --> 0:21:22.960
level interface for example serializing some Python object or it could be very useful.

279
0:21:22.960 --> 0:21:34.680
Yeah, the interface is as slim thin as possible probably.

280
0:21:34.680 --> 0:21:36.760
Very close to the C MPI.

281
0:21:36.760 --> 0:21:44.760
One of the reasons being that within Numba and JIT code, things like serialization might

282
0:21:44.760 --> 0:21:48.560
not be that easy to do.

283
0:21:48.560 --> 0:21:53.920
There is no problem in combining MPI for Py and Numba MPI in one code base.

284
0:21:53.920 --> 0:22:00.120
So when you are out of the JIT code you can use MPI for Py which has high level things

285
0:22:00.120 --> 0:22:01.560
like serialization, et cetera.

286
0:22:01.560 --> 0:22:10.200
So you can use it there but within LLVM compiled blocks you can use Numba MPI for simple send

287
0:22:10.200 --> 0:22:13.040
receive already used.

288
0:22:13.040 --> 0:22:16.800
Without higher level array functioning.

289
0:22:16.800 --> 0:22:27.680
Having said that we for example handle transparently noncontiguous slices of arrays, we also yeah

290
0:22:27.680 --> 0:22:33.440
there are some things that are higher level than C interface but in general we try to

291
0:22:33.440 --> 0:22:37.200
provide a wrapper around the C routines.

292
0:22:37.200 --> 0:22:42.760
Okay, thank you.

293
0:22:42.760 --> 0:22:50.040
Any other questions?

294
0:22:50.040 --> 0:22:51.640
Thanks for a great talk.

295
0:22:51.640 --> 0:22:53.720
It seems really interesting what you are working on.

296
0:22:53.720 --> 0:23:00.080
I have a couple of questions probably born out of ignorance but I just kind of wondered

297
0:23:00.080 --> 0:23:01.400
if you could help me with them.

298
0:23:01.400 --> 0:23:08.160
So firstly I was wondering why you went with making a separate package rather than sort

299
0:23:08.160 --> 0:23:14.080
of trying to build this functionality on top of MPI for Py.

300
0:23:14.080 --> 0:23:18.960
Would it have been possible to sort of add this feature of making things JIT compilable

301
0:23:18.960 --> 0:23:21.200
into MPI for Py?

302
0:23:21.200 --> 0:23:26.400
And secondly I was kind of wondering with the MPI I.O. thing that you were looking at

303
0:23:26.400 --> 0:23:34.080
with Windows, if that requires kind of concurrent file access from separate processes in Windows,

304
0:23:34.080 --> 0:23:37.360
is that just a complete no go for Windows?

305
0:23:37.360 --> 0:23:42.200
Because I understand that's something that Windows kernel doesn't support.

306
0:23:42.200 --> 0:23:43.200
Thank you.

307
0:23:43.200 --> 0:23:44.200
Thanks.

308
0:23:44.200 --> 0:23:46.400
Let me start from the second one.

309
0:23:46.400 --> 0:23:54.440
So here our well, essentially it's a fun fact that everything else worked for Windows.

310
0:23:54.440 --> 0:23:59.240
We do not really target Windows but it was nice to observe that all works.

311
0:23:59.240 --> 0:24:06.920
It's kind of one of these advantages of Python that you code and you don't really need to

312
0:24:06.920 --> 0:24:12.400
take too much care about the targeted platforms because the underlying packages are meant

313
0:24:12.400 --> 0:24:14.000
to work on all of them.

314
0:24:14.000 --> 0:24:17.960
And here everything works with Microsoft MPI.

315
0:24:17.960 --> 0:24:24.240
The only thing that actually was a problem for us was to even install H5.py on Windows

316
0:24:24.240 --> 0:24:25.560
with MPI support.

317
0:24:25.560 --> 0:24:32.640
So we don't really know what's the true bottleneck but even the documentation of H5.py suggests

318
0:24:32.640 --> 0:24:35.060
again trying.

319
0:24:35.060 --> 0:24:41.840
For the first question, why do we create, why do we develop a separate package instead

320
0:24:41.840 --> 0:24:45.880
of adding it on top of MPI 4.py?

321
0:24:45.880 --> 0:24:57.040
So I think even on the slide with the story of the package, there was a link to MPI 4.py

322
0:24:57.040 --> 0:25:03.080
issue, the bottom footnote, where we suggested would it be possible to add it?

323
0:25:03.080 --> 0:25:10.680
And in relation with the first question, so probably the scope, the goal of MPI 4.py is

324
0:25:10.680 --> 0:25:20.600
to provide very high level API for MPI in Python.

325
0:25:20.600 --> 0:25:26.440
So with discussing with the developers there, we realized that it's probably not within

326
0:25:26.440 --> 0:25:30.320
the scope of a very high level interface.

327
0:25:30.320 --> 0:25:36.440
So we started off with just, well, small separate project.

328
0:25:36.440 --> 0:25:38.360
But I mean, well, great idea.

329
0:25:38.360 --> 0:25:40.840
It could be glued together.

330
0:25:40.840 --> 0:25:47.040
We aim for dropping dependency on MPI 4.py which we now use just for some utility routine,

331
0:25:47.040 --> 0:25:52.360
not for the communication or nothing that is used by Numba.

332
0:25:52.360 --> 0:25:58.500
And probably that might be an advantage because you can eventually you should be able to install

333
0:25:58.500 --> 0:26:03.000
Numba MPI with very little other dependencies.

334
0:26:03.000 --> 0:26:06.380
And Numba MPI is written purely in Python.

335
0:26:06.380 --> 0:26:12.120
So installing it, you do not need to have any site on related C compiled code and you

336
0:26:12.120 --> 0:26:14.360
can do it quite easily.

337
0:26:14.360 --> 0:26:15.360
Okay.

338
0:26:15.360 --> 0:26:38.440
Thank you very much.

