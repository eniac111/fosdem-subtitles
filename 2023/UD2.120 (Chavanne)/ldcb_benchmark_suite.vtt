WEBVTT

00:00.000 --> 00:06.280
Hello, HPC room.

00:06.280 --> 00:14.240
I work at CWI Amsterdam as a researcher and today I'm here on behalf of the IDBC.

00:14.240 --> 00:17.380
The IDBC stands for the linked data benchmark council.

00:17.380 --> 00:22.600
We are a non-profit company founded in 2012 and we design graph benchmarks and govern

00:22.600 --> 00:23.600
their use.

00:23.600 --> 00:28.160
Additionally, we do research on graph schemas and modern graph languages and everything

00:28.160 --> 00:32.440
we do is available under the Apache V2 license.

00:32.440 --> 00:36.200
Organisationally, IDBC consists of more than 20 companies.

00:36.200 --> 00:38.760
These are companies interested in graph data management.

00:38.760 --> 00:44.320
We have financial service providers, database vendors, cloud vendors, hardware vendors,

00:44.320 --> 00:49.000
and consultancy companies as well as individual contributors like me.

00:49.000 --> 00:54.480
So, we design benchmarks, the first one being the IDBC social network benchmark which targets

00:54.480 --> 00:56.640
database systems.

00:56.640 --> 01:00.520
Let's go through this benchmark by a series of examples.

01:00.520 --> 01:05.480
I will touch on data sets, queries and updates that we use in this benchmark.

01:05.480 --> 01:10.600
As the name social network benchmark suggests, we have a social network that consists of

01:10.600 --> 01:17.040
person nodes who know each other via a distribution that mimics the Facebook career social network.

01:17.040 --> 01:20.080
The content that these people create is messages.

01:20.080 --> 01:25.680
These form little tree-shaped subgraphs and are connected via author edges to the people.

01:25.680 --> 01:28.760
On this graph, we can run queries like the following.

01:28.760 --> 01:34.160
Let's have a given person enumerate their friends and their friends of friends, get

01:34.160 --> 01:38.680
the messages that these people created and then filter them based on some condition on

01:38.680 --> 01:39.680
their dates.

01:39.680 --> 01:44.440
So, a potential substitution could be on this graph that we are interested in this query

01:44.440 --> 01:47.680
for Bob and the date set on Saturday.

01:47.680 --> 01:53.080
And if we evaluate this query, we start with Bob, we traverse the nodes edges to Ada and

01:53.080 --> 01:56.360
Karl then continue to Finn, Eve and Dan.

01:56.360 --> 02:02.320
We move along the author edges and then finally we apply the filter condition which will cut

02:02.320 --> 02:06.840
message three and will leave us messages one, two and four.

02:06.840 --> 02:09.840
So obviously, social network is not a static environment.

02:09.840 --> 02:11.120
There are always changes.

02:11.120 --> 02:13.280
For example, people become friends.

02:13.280 --> 02:15.800
Even Jia may add each other as a friend.

02:15.800 --> 02:18.320
That will result in a new nose edge.

02:18.320 --> 02:19.480
That's simple enough.

02:19.480 --> 02:22.120
Jia can decide to create a message.

02:22.120 --> 02:24.800
This message will be replied to message M3.

02:24.800 --> 02:29.160
So we add a new node and connect it to the existing graph via two edges.

02:29.160 --> 02:31.640
The heavy hitting updates are the deletes.

02:31.640 --> 02:36.760
A person may decide to delete their account and that will result in a cascade of deletes.

02:36.760 --> 02:42.840
For example, if we remove the node, Eve, that will result in the removal of their direct

02:42.840 --> 02:45.120
edges or the messages they created.

02:45.120 --> 02:49.560
And in some social network, this will even trigger the deletion of all the message trees

02:49.560 --> 02:53.360
and of course all the edges that point to those messages.

02:53.360 --> 02:56.880
So this is quite a hard operation for systems to execute.

02:56.880 --> 03:02.920
It stresses their garbage collectors and it disallows certain append-only data structures.

03:02.920 --> 03:06.920
So if you want to weave these three components together, the data set, the queries and the

03:06.920 --> 03:11.600
updates, we need a benchmark driver that schedules the operations to be executable.

03:11.600 --> 03:16.100
It runs the updates and the queries concurrently and of course it collects the results.

03:16.100 --> 03:21.720
The system under test that we run the benchmark on is provided by our members who are the

03:21.720 --> 03:27.960
database vendors and we go to great lengths to allow as many candidate systems as possible.

03:27.960 --> 03:34.680
So graph databases, triple source and relational databases can all compete on this benchmark.

03:34.680 --> 03:39.760
Speaking of relational databases, some of you may think, is CQL sufficient to express

03:39.760 --> 03:40.760
these queries?

03:40.760 --> 03:42.900
And the answer is that in most cases it is.

03:42.900 --> 03:48.840
So the query that we have just seen can be formulated in a reasonably simple SQL query.

03:48.840 --> 03:53.800
It is a bit unwieldy but it is certainly doable and the performance will be okay.

03:53.800 --> 03:59.040
However, this being a graph benchmark, it lends itself quite naturally to other query

03:59.040 --> 04:00.040
languages.

04:00.040 --> 04:04.160
There are two new query languages that are going to be coming out and both of them adopted

04:04.160 --> 04:08.120
a visual graph syntax inspired by Neo4j's Cypher language.

04:08.120 --> 04:13.080
The first one is called SQL PGQ where PGQ stands for property graph queries.

04:13.080 --> 04:18.080
This will be released this summer and as you can see it's an extension to SQL so you can

04:18.080 --> 04:23.060
use select and from but it adds the graph table construct and the query can be formulated

04:23.060 --> 04:25.760
in a very concise and readable manner.

04:25.760 --> 04:30.240
There is GQL, the graph query language, which is a standalone language that is going to

04:30.240 --> 04:36.120
be released next year and it shares the same pattern matching language as SQL PGQ.

04:36.120 --> 04:41.120
So the social network benchmark has multiple workloads to cover the diverse challenges

04:41.120 --> 04:44.880
that are created by graph workloads.

04:44.880 --> 04:49.160
The first one, the older one is the social network benchmark interactive workload.

04:49.160 --> 04:53.380
This is transactional in nature and it has queries like the one I have shown before.

04:53.380 --> 04:57.160
So these queries typically start in one or two person nodes.

04:57.160 --> 04:59.020
They are not very heavy hitting.

04:59.020 --> 05:01.380
They only touch a limited amount of data.

05:01.380 --> 05:06.760
They have concurrent reads and updates and systems are competing on achieving high throughputs.

05:06.760 --> 05:10.240
So this benchmark has been around for a few years and we have seen actually very good

05:10.240 --> 05:11.240
results.

05:11.240 --> 05:16.240
In the last three years we witnessed an exponential increase in throughput starting from a little

05:16.240 --> 05:22.880
above 5,000 operations per second to almost 17,000 operations per second this year.

05:22.880 --> 05:27.360
Our newer benchmark is the social network benchmark business intelligence workload.

05:27.360 --> 05:32.280
This is analytical in nature and it has queries that touch on large portions of the data.

05:32.280 --> 05:37.840
For example, the query on this slide enumerates all triangles of friendships in a given country

05:37.840 --> 05:44.200
which can potentially reach billions of edges and is a very difficult computational problem.

05:44.200 --> 05:48.760
Systems here are allowed to do either a bulk or a concurrent update approach, but they

05:48.760 --> 05:54.120
should strive to get both a high throughput and low individual query run times.

05:54.120 --> 05:58.080
This benchmark being relatively new, we only have a single result, so it's a bit difficult

05:58.080 --> 05:59.840
to put it into context.

05:59.840 --> 06:02.140
But it allows me to highlight one thing.

06:02.140 --> 06:04.080
Many of our benchmarks use different CPUs.

06:04.080 --> 06:08.600
We actually have quite a healthy diversity in the CPUs.

06:08.600 --> 06:13.440
We have results with the AMD Epic Genoa, like this one achieved by TIGOGRAPH.

06:13.440 --> 06:19.200
We have results using IntagZ on Ice Lakes and the ETN 710s which use an ARM architecture.

06:19.200 --> 06:24.520
We have more and larger scale results expected this year and we are also quite interested

06:24.520 --> 06:29.480
in some graph and machine learning accelerators that are going to be released soon.

06:29.480 --> 06:31.560
Our benchmark process is quite involved.

06:31.560 --> 06:34.360
For each workload we release a specification.

06:34.360 --> 06:36.840
We have an academic paper that motivates the benchmark.

06:36.840 --> 06:41.400
We have data generators, pre-generated data sets, as well as a benchmark driver and at

06:41.400 --> 06:44.500
least two reference implementations.

06:44.500 --> 06:49.640
We do this because we have an auditing process that allows the vendors to implement this

06:49.640 --> 06:56.000
benchmark to actually go through rigorous tests and if they do so, they can claim that

06:56.000 --> 06:58.800
they have an official benchmark result.

06:58.800 --> 07:05.400
We trademark the term IDBC such that the vendors have to go through these hoops of auditing

07:05.400 --> 07:09.800
and we still allow researchers and developers to do unofficial benchmarks, but they have

07:09.800 --> 07:14.360
to say that this is not an official IDBC benchmark result.

07:14.360 --> 07:18.200
One other benchmark I would like to touch upon briefly is the Graphalytics benchmark.

07:18.200 --> 07:19.480
This costs a wider net.

07:19.480 --> 07:25.200
It targets graph databases, graph processing framework, embedded graph libraries like NetworkX

07:25.200 --> 07:26.600
and so on.

07:26.600 --> 07:29.480
This uses untyped, unattributed graphs.

07:29.480 --> 07:34.200
It's only the person knows person graphs of the social network benchmark or other unknown

07:34.200 --> 07:35.200
graphs like Graph500.

07:35.200 --> 07:37.400
We have six algorithms.

07:37.400 --> 07:41.780
Many of these are textbook algorithms like BFS which just reverses the graph from a given

07:41.780 --> 07:47.160
source node or we have PageRank which selects the most important nodes in the network.

07:47.160 --> 07:51.200
We also have clustering coefficient, community detection, connected components and shortest

07:51.200 --> 07:53.360
paths.

07:53.360 --> 07:54.960
This benchmark is a bit simpler to implement.

07:54.960 --> 07:57.600
We have a leader board that we update periodically.

07:57.600 --> 08:02.720
The next one is going to come out in Spring 2023, so talk to us if you're interested.

08:02.720 --> 08:07.320
So wrapping up, you should consider becoming an IDBC member because members can participate

08:07.320 --> 08:11.920
in the benchmark design and have a say in where we go, they can commission audits of

08:11.920 --> 08:17.040
their benchmarks and they can also gain early access to the ISO standard drafts, CEQA, PGQ

08:17.040 --> 08:18.760
and GQA that I have shown.

08:18.760 --> 08:23.240
It's free for individuals and has a yearly fee for companies.

08:23.240 --> 08:25.440
So to sum up, these are our three main benchmarks.

08:25.440 --> 08:28.200
We have other benchmarks and many future ideas.

08:28.200 --> 08:33.320
If you're interested, please reach out.

08:33.320 --> 08:42.600
Okay, again we have time for one question.

08:42.600 --> 08:51.800
Any questions for Gabor?

08:51.800 --> 08:53.160
This is a new big question.

08:53.160 --> 08:57.200
I'm not into graphs.

08:57.200 --> 09:08.120
Apart from advertisement optimisation, mass surveillance and perhaps content distribution,

09:08.120 --> 09:14.880
which I don't know if they're the major applications, but it's just what my naive mind comes with.

09:14.880 --> 09:20.120
What other applications are those benchmarks meant to optimise?

09:20.120 --> 09:26.480
So the big one this year is supply chain optimisation, like strengthening supply chains, ensuring

09:26.480 --> 09:31.400
that they are ethical, ensuring that they are not passing conflict zones.

09:31.400 --> 09:34.120
It's something that is very important these days.

09:34.120 --> 09:42.520
You can also track CO2 emissions and other aspects of labour and manufacturing.

09:42.520 --> 09:46.160
So that's certainly a big one and that's something that we have seen.

09:46.160 --> 09:52.160
And there are of course all the graphic problems like power grid, a lot of e-commerce programmes

09:52.160 --> 09:56.480
and financial fraud detection, which is going to be part of our financial benchmark this

09:56.480 --> 10:22.480
year.
