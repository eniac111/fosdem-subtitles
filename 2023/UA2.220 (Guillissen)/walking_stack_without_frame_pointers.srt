1
0:00:00.000 --> 0:00:10.840
I'm going to talk about walking native stacks in BPF without frame pointers.

2
0:00:10.840 --> 0:00:11.840
Thanks.

3
0:00:11.840 --> 0:00:13.840
So, yeah.

4
0:00:13.840 --> 0:00:18.560
So, my name is Vaishali.

5
0:00:18.560 --> 0:00:24.920
I work at Polu-Simmons as a software engineer, mostly on profiling and eBPF related stuff.

6
0:00:24.920 --> 0:00:31.200
And before that I have worked in different corner subsystems as part of my job.

7
0:00:31.200 --> 0:00:32.200
My name is Javier.

8
0:00:32.200 --> 0:00:36.200
And I've been working at Polu-Simmons for a year, mostly working on native stack unwinding.

9
0:00:36.200 --> 0:00:41.640
And before I was working on web reliability and dev tooling at Facebook.

10
0:00:41.640 --> 0:00:44.840
So before we get into the talk, let's talk about the agenda.

11
0:00:44.840 --> 0:00:49.200
So we'll first address the first question, which is always being asked, that why there

12
0:00:49.200 --> 0:00:53.920
is a need for a DWARF-based stack walker in eBPF.

13
0:00:53.920 --> 0:00:59.120
And we will briefly go through the design of our stack walker, we'll also go from how

14
0:00:59.120 --> 0:01:02.280
we went from the prototype to making it production ready.

15
0:01:02.280 --> 0:01:06.920
And then what were a bunch of the learnings so far, especially when we are interacting

16
0:01:06.920 --> 0:01:09.600
with the eBPF subsystem of the kernel.

17
0:01:09.600 --> 0:01:13.000
And then our future plans.

18
0:01:13.000 --> 0:01:16.520
So as I said, we work on the production profilers.

19
0:01:16.520 --> 0:01:22.240
Generally sampling profilers collect the stack traces at particular intervals and attaches

20
0:01:22.240 --> 0:01:24.080
values to it.

21
0:01:24.080 --> 0:01:29.480
Note that profilers generally need both user application stacks and kernel stacks.

22
0:01:29.480 --> 0:01:33.120
Stack walking is part of the process for collecting the stack traces.

23
0:01:33.120 --> 0:01:38.680
In simple words, it involves iterating over all the stack frames and collecting the return

24
0:01:38.680 --> 0:01:39.680
addresses.

25
0:01:39.680 --> 0:01:48.320
Historically, there has been a dedicated register to store the value of it in both x86 and ARM.

26
0:01:48.320 --> 0:01:56.560
Although it has fallen victim of some of the compiler optimisations, so most of the runtime

27
0:01:56.560 --> 0:01:58.040
actually severs it.

28
0:01:58.040 --> 0:02:02.480
It's called frame pointer, as many of you have heard of it.

29
0:02:02.480 --> 0:02:09.400
And when we don't have the frame pointer, walking the stack becomes like magnitude of

30
0:02:09.400 --> 0:02:12.160
like a lengthy process.

31
0:02:12.160 --> 0:02:16.720
So instead of involving a couple of memory accesses per frame, which is quite fast,

32
0:02:16.720 --> 0:02:20.800
we will have to do more work in the stack walkers.

33
0:02:20.800 --> 0:02:24.040
The stack walking is also common practice in debuggers, right?

34
0:02:24.040 --> 0:02:30.160
So what's the current state of the world with respect to frame pointers?

35
0:02:30.160 --> 0:02:34.000
So it's not a problem for especially hyperscalers.

36
0:02:34.000 --> 0:02:38.880
As you may know, in production, they are always running the applications which has the frame

37
0:02:38.880 --> 0:02:44.600
pointers unable, because whenever they have to inspect the incidents, getting faster and

38
0:02:44.600 --> 0:02:48.120
the reliable stack registers must.

39
0:02:48.120 --> 0:02:59.480
Go runtime enables the frame pointers since go 1.7 in x86 and 1.12 in ARM64.

40
0:02:59.480 --> 0:03:04.520
Mac OS is always compiled with frame pointers.

41
0:03:04.520 --> 0:03:10.200
There's also an amazing work going on for the compact frame format.

42
0:03:10.200 --> 0:03:17.480
It's called simple frame format, and there has been like support being added in the tool

43
0:03:17.480 --> 0:03:24.240
chains, and now there is also like a mailing list discussion going on in the kernel about

44
0:03:24.240 --> 0:03:31.640
having an unwinded stack walker, sorry, a stack walker for unwinding the user stacks.

45
0:03:31.640 --> 0:03:34.000
But the thing is that we want it now.

46
0:03:34.000 --> 0:03:36.520
We want to support all the distros.

47
0:03:36.520 --> 0:03:39.240
We want to support all the run times.

48
0:03:39.240 --> 0:03:45.640
And the one thing that comes across a lot of this is dwarf, and that's why we are using

49
0:03:45.640 --> 0:03:46.640
it.

50
0:03:46.640 --> 0:03:48.720
So where does it come from?

51
0:03:48.720 --> 0:03:55.120
So like some of you might be wondering about like the exceptions in C++ or, for example,

52
0:03:55.120 --> 0:04:01.560
Rust tool chain which is like always disabling the frame pointers, but when you use the panic,

53
0:04:01.560 --> 0:04:04.640
it always has the full back dress.

54
0:04:04.640 --> 0:04:13.440
The reason is that it has the.eh frame section which is being used for that or debug frame.

55
0:04:13.440 --> 0:04:17.680
So most of the time either of the tool chains have this section.

56
0:04:17.680 --> 0:04:25.600
And the other idea is that you can also unwind the tables by synthesizing it from the object

57
0:04:25.600 --> 0:04:26.600
code.

58
0:04:26.600 --> 0:04:31.800
This is like the approach which is being used in org, one of the kernel second minder which

59
0:04:31.800 --> 0:04:35.960
was added I guess six years ago.

60
0:04:35.960 --> 0:04:40.200
So we'll talk about in detail about.eh frame in a minute.

61
0:04:40.200 --> 0:04:45.000
But before that, let's see who else is using.eh frame.

62
0:04:45.000 --> 0:04:48.440
So of course we are not the first one who are going to use it.

63
0:04:48.440 --> 0:04:51.640
Perf does that.

64
0:04:51.640 --> 0:04:59.920
Since I think since when the Perf event, open Cisco was introduced in 3.4, so it collects

65
0:04:59.920 --> 0:05:04.720
the registers for the profile processes as well as a copy of the stack for every sample.

66
0:05:04.720 --> 0:05:09.640
While this approach has been proven to work, there are a bunch of drawbacks which we wanted

67
0:05:09.640 --> 0:05:10.640
to avoid.

68
0:05:10.640 --> 0:05:14.200
One of the things is that kernel copies the user stake for every sample and this can be

69
0:05:14.200 --> 0:05:19.080
quite a bit of data, especially for the CPU intensive applications.

70
0:05:19.080 --> 0:05:23.320
Another thing is that when we are copying the data in the user space, the implications

71
0:05:23.320 --> 0:05:30.200
of one process having another process's data can also be complicated.

72
0:05:30.200 --> 0:05:33.560
So those are a bunch of things we wanted to avoid.

73
0:05:33.560 --> 0:05:39.800
And stack walking using BPF makes sense to us because we don't have to copy the whole

74
0:05:39.800 --> 0:05:40.800
stack.

75
0:05:40.800 --> 0:05:46.080
Instead, a lot of the information still stays in the kernel, especially in the case of stack

76
0:05:46.080 --> 0:05:47.880
walking mechanism.

77
0:05:47.880 --> 0:05:53.240
Once it has been implemented, we can leverage the Perf subsystem to get the samples on CPU

78
0:05:53.240 --> 0:05:57.440
cycles, instructions, access, et cetera.

79
0:05:57.440 --> 0:06:02.920
It can also help us to develop other tools like allocation tracers, runtime-specific

80
0:06:02.920 --> 0:06:07.880
profilers like for the JVM or Ruby, et cetera.

81
0:06:07.880 --> 0:06:13.480
Now some of you might be wondering that why do we want to implement something new when

82
0:06:13.480 --> 0:06:17.280
we already have BPF get stack ID?

83
0:06:17.280 --> 0:06:25.080
So the reason is that it also uses frame pointers to unwind it.

84
0:06:25.080 --> 0:06:29.720
Having a fully featured dwarf unwinder in kernel is unlikely to happen.

85
0:06:29.720 --> 0:06:31.680
There is a mailing list discussion.

86
0:06:31.680 --> 0:06:34.400
You can go and check it out.

87
0:06:34.400 --> 0:06:36.400
Why?

88
0:06:36.400 --> 0:06:41.560
So now before we dive into the design of our stack walker, I wanted to give some information

89
0:06:41.560 --> 0:06:45.840
on what ES frame has and how we can use it.

90
0:06:45.840 --> 0:06:54.640
So ES frame section contains one or more call frame information records.

91
0:06:54.640 --> 0:06:59.320
The main goal of the call frame information records is to provide answers on how to restore

92
0:06:59.320 --> 0:07:03.840
the register for the previous frame and the locations such as whether they have been pushed

93
0:07:03.840 --> 0:07:05.760
to the stack or not.

94
0:07:05.760 --> 0:07:09.540
And all of this would actually generate the huge unwind tables.

95
0:07:09.540 --> 0:07:12.600
And for these reasons, the format attempts to be compact.

96
0:07:12.600 --> 0:07:17.000
And it only contains the information that is being needed.

97
0:07:17.000 --> 0:07:23.440
The unwind tables encoded in the CFI format are in the form of opcodes.

98
0:07:23.440 --> 0:07:26.240
And we basically have to evaluate it.

99
0:07:26.240 --> 0:07:30.800
And then in the case of stack walking, once it has been evaluated, we generate the table

100
0:07:30.800 --> 0:07:36.680
that contains for each instruction boundary how to restore the value of the previous register.

101
0:07:36.680 --> 0:07:38.800
It has sort of two layers to it.

102
0:07:38.800 --> 0:07:44.560
One is this sort of helps with the repetitive patterns for compressing.

103
0:07:44.560 --> 0:07:48.400
And it allows for a more compact representation of some data.

104
0:07:48.400 --> 0:07:53.680
As in some cases, a specialized opcodes that consumes one or two or four bytes.

105
0:07:53.680 --> 0:07:58.400
So it doesn't have to be four bytes all the time.

106
0:07:58.400 --> 0:08:03.640
And the second layer is like a special opcode that contains under the set of opcodes which

107
0:08:03.640 --> 0:08:05.920
is like arbitrary expressions.

108
0:08:05.920 --> 0:08:09.960
And they need to be actually evaluated for that.

109
0:08:09.960 --> 0:08:17.120
And this would mean that we will actually have to implement the full below VM in the

110
0:08:17.120 --> 0:08:24.160
EBPF to evaluate any expression, which is not practical.

111
0:08:24.160 --> 0:08:31.200
So we are going to also mention what we are doing to actually come over those challenges.

112
0:08:31.200 --> 0:08:36.960
For those who are not aware of what is the general flow of the EBPF applications, generally

113
0:08:36.960 --> 0:08:38.520
this is how it would look like.

114
0:08:38.520 --> 0:08:40.400
Very high level overview.

115
0:08:40.400 --> 0:08:47.640
So in the user space, we are using the driver program, which is written in Go.

116
0:08:47.640 --> 0:08:49.720
We use the BPF Go.

117
0:08:49.720 --> 0:08:57.320
It creates the map, attaches the map, attaches the BPF program to the CPU cycles of perf

118
0:08:57.320 --> 0:09:03.480
content, and then reads, passes, and evaluates the EBPF section of the process.

119
0:09:03.480 --> 0:09:08.320
And in the BPF program, we fetched the table from the current PID and then have an unwinding

120
0:09:08.320 --> 0:09:12.840
algorithm which processes the reward for information.

121
0:09:12.840 --> 0:09:15.120
So we'll go in depth for each component.

122
0:09:15.120 --> 0:09:20.360
But let's see what the algorithm looks like.

123
0:09:20.360 --> 0:09:24.120
So first what we are doing is we are just reading three registers.

124
0:09:24.120 --> 0:09:30.560
First one is RIP, the second one is stack pointer, RSP, and the third one is RBP, which

125
0:09:30.560 --> 0:09:36.080
is commonly used as frame pointer when they are enabled.

126
0:09:36.080 --> 0:09:40.480
Next we are going for the unwind frame count, which is less than, and when it's less than

127
0:09:40.480 --> 0:09:44.000
maximum depth.

128
0:09:44.000 --> 0:09:46.760
We find the unwind table row for the program counter.

129
0:09:46.760 --> 0:09:51.160
Then we go for adding the instruction pointer to the stack, calculate the previous frames,

130
0:09:51.160 --> 0:09:56.880
stack pointer, update the registers, and continue with the next frame.

131
0:09:56.880 --> 0:10:00.160
So this is like a very simple binary search.

132
0:10:00.160 --> 0:10:06.440
But when it has to scale, we need to also think about storing the unwind information

133
0:10:06.440 --> 0:10:11.280
and how can it work with all the run times, et cetera.

134
0:10:11.280 --> 0:10:14.280
So Javier will now talk about that.

135
0:10:14.280 --> 0:10:15.280
Cool.

136
0:10:15.280 --> 0:10:20.080
So as Vaisal said, we need somewhere where to store the unwind information.

137
0:10:20.080 --> 0:10:23.920
We're going to look later at how this table looks like.

138
0:10:23.920 --> 0:10:26.720
But first let's see what are the possibilities here.

139
0:10:26.720 --> 0:10:31.000
So one possibility, for example, will be to store the unwind information in process.

140
0:10:31.000 --> 0:10:35.400
We could do this using a combination of P trace, mmap, and mlock.

141
0:10:35.400 --> 0:10:40.800
And this will require us to basically hijack the processes execution, introduce a new memory

142
0:10:40.800 --> 0:10:45.040
mapping inside of them, and then we have to lock the memory because in BPF and in our

143
0:10:45.040 --> 0:10:48.960
type of BPF programs, page faults are not allowed.

144
0:10:48.960 --> 0:10:54.040
The problem with these approaches, of course, will be altering the execution flow of applications,

145
0:10:54.040 --> 0:10:56.640
which is something that we never want to do.

146
0:10:56.640 --> 0:10:58.360
This complicates things a lot.

147
0:10:58.360 --> 0:11:01.760
For example, one of the biggest problems is the life cycle.

148
0:11:01.760 --> 0:11:07.040
So for example, if our profiler dies before we finish cleaning up, who is going to clean

149
0:11:07.040 --> 0:11:08.360
up that memory segment?

150
0:11:08.360 --> 0:11:12.960
Or how is this going to be perceived by developers if they see that the memory of their application

151
0:11:12.960 --> 0:11:18.120
has increased behind their backs just because some observability tool is doing something

152
0:11:18.120 --> 0:11:20.400
that is not great?

153
0:11:20.400 --> 0:11:23.560
But also there's another problem that is sharing memory is harder.

154
0:11:23.560 --> 0:11:27.280
There is same page optimization from the kernel.

155
0:11:27.280 --> 0:11:32.240
But if you don't think about that, it's a problem to have the same information generated

156
0:11:32.240 --> 0:11:37.040
over and over, for example, for libc for every single process in your machine.

157
0:11:37.040 --> 0:11:41.760
So that's why we ended up using another solution, which is pretty typical in the BPF space,

158
0:11:41.760 --> 0:11:43.920
which is using BPF maps.

159
0:11:43.920 --> 0:11:48.720
In case you're not familiar, BPF maps are data structures that can be written or read

160
0:11:48.720 --> 0:11:51.000
from both user and kernel space.

161
0:11:51.000 --> 0:11:55.480
We're using hash tables everywhere, which in the case of BPF, they're basically a mapping

162
0:11:55.480 --> 0:12:00.600
of bytes to bytes that store arbitrary information.

163
0:12:00.600 --> 0:12:06.120
So some BPF maps, some BPF programs as well allow to lazily allocate memory for their

164
0:12:06.120 --> 0:12:07.120
data structures.

165
0:12:07.120 --> 0:12:10.280
But in the case of our tracing program, we cannot do that.

166
0:12:10.280 --> 0:12:11.400
And this has some implications.

167
0:12:11.400 --> 0:12:16.600
So we need to memlock that, well, the kernel, sorry, user space memlocks that memory, and

168
0:12:16.600 --> 0:12:19.960
otherwise our program wouldn't be able to run.

169
0:12:19.960 --> 0:12:25.160
And by using this approach, we are also able to reuse these memory mappings, which is great,

170
0:12:25.160 --> 0:12:26.160
right?

171
0:12:26.160 --> 0:12:31.080
Because that means we don't have to do the same work over and over, and we use less space.

172
0:12:31.080 --> 0:12:34.600
So let's take a look at the logical representation of the unwind tables.

173
0:12:34.600 --> 0:12:38.840
So this is not how the layout is in memory, but think about, for example, the unwind tables

174
0:12:38.840 --> 0:12:41.960
for libc, MySQL, Zlib, and systemd.

175
0:12:41.960 --> 0:12:47.040
How they will be laid out in memory if we could allocate a large chunk of memory.

176
0:12:47.040 --> 0:12:50.160
But in reality, there's limits everywhere, obviously.

177
0:12:50.160 --> 0:12:52.960
And in BPF, we have done some tests.

178
0:12:52.960 --> 0:12:57.560
I mean, the machines that we want, the kernels we want to support, we can allocate up to

179
0:12:57.560 --> 0:13:03.900
25,000 unwind entries per value of the hash map.

180
0:13:03.900 --> 0:13:08.320
So obviously, this was a problem for us, because in some cases, we have some customers that

181
0:13:08.320 --> 0:13:14.160
have applications with unwind tables with three, four million unwind rows, which is

182
0:13:14.160 --> 0:13:15.160
quite ridiculous.

183
0:13:15.160 --> 0:13:20.320
Just to give you an idea, libc, I think, has like 60K entries.

184
0:13:20.320 --> 0:13:23.480
So having a couple million is significant.

185
0:13:23.480 --> 0:13:28.320
But yeah, we came up with the same solution that you would use in any other data-intensive

186
0:13:28.320 --> 0:13:32.800
application, which is to partition or shard the data.

187
0:13:32.800 --> 0:13:37.800
So the way we're doing this is we have multiple entries that are allocated when our profilers

188
0:13:37.800 --> 0:13:39.200
start running.

189
0:13:39.200 --> 0:13:42.760
We allocate a different number, depending on the available memory on the system and

190
0:13:42.760 --> 0:13:45.880
the overhead that you're willing to pay.

191
0:13:45.880 --> 0:13:53.160
And yeah, depending on how many shards you have, you have a different CPU to memory trade-off,

192
0:13:53.160 --> 0:13:55.400
because the more memory you use, it has to be locked in memory.

193
0:13:55.400 --> 0:14:00.400
It can never be swapped out, which is, in some applications, no ideal.

194
0:14:00.400 --> 0:14:03.080
But at the same time, that means that you don't have to regenerate the tables if they

195
0:14:03.080 --> 0:14:08.880
are full, and then you want to give other processes a fair chance to be profiled.

196
0:14:08.880 --> 0:14:13.760
So the way this will work, for example, for a process like systemd is that will be the

197
0:14:13.760 --> 0:14:17.360
representation of the size of its unwind tables.

198
0:14:17.360 --> 0:14:22.880
Because it's bigger than the size of a shard, it will have to be chunked.

199
0:14:22.880 --> 0:14:25.400
So here, we can see how this is chunked in two.

200
0:14:25.400 --> 0:14:32.360
The first chunk will go in the shard zero, and a bunch of the unwind entries from the

201
0:14:32.360 --> 0:14:35.200
tail will go to the shard one.

202
0:14:35.200 --> 0:14:42.040
And of course, because we have this new layer of indirection, we need to somehow keep track

203
0:14:42.040 --> 0:14:46.200
of all this bookkeeping and know what is the state of the world.

204
0:14:46.200 --> 0:14:49.680
We're doing this, of course, with more BVF maps.

205
0:14:49.680 --> 0:14:52.400
So our process has multiple mappings.

206
0:14:52.400 --> 0:14:55.860
Each mapping has one or more chunks.

207
0:14:55.860 --> 0:15:01.520
And then each chunk maps to exactly one shard, and in particular the region within one shard,

208
0:15:01.520 --> 0:15:08.280
because you can have from one unwind entry up to 2050k.

209
0:15:08.280 --> 0:15:10.760
Of course, this has the benefit that I was mentioning before.

210
0:15:10.760 --> 0:15:15.280
That is, because we were sharing the unwind tables, that means that we spend actually

211
0:15:15.280 --> 0:15:18.680
not that many CPU cycles doing all the work that Charlie was mentioning before.

212
0:15:18.680 --> 0:15:23.440
We need to find the elf section where the door of CFI information is, but also we need

213
0:15:23.440 --> 0:15:24.640
to parse, evaluate it.

214
0:15:24.640 --> 0:15:29.760
We have two levels of VM that have to run, which is not something that is very CPU consuming

215
0:15:29.760 --> 0:15:30.760
but still has to happen.

216
0:15:30.760 --> 0:15:35.480
It has to process a bunch of information and generate these unwind tables in our custom

217
0:15:35.480 --> 0:15:36.480
formats.

218
0:15:36.480 --> 0:15:43.000
So by sharing this, for example, LIPST will be shared for all the processes, so that means

219
0:15:43.000 --> 0:15:48.200
that we only need to add the bookkeeping data structures, which are really cheap to generate.

220
0:15:48.200 --> 0:15:52.560
In some of the tests that we've been running, we use less than 0.9% CPU within the total

221
0:15:52.560 --> 0:15:57.840
CPU cycles that our profiler uses to generate the unwind tables.

222
0:15:57.840 --> 0:16:01.360
Of course, there's a lot of things that we need to take into account, like, for example,

223
0:16:01.360 --> 0:16:03.800
what happens if we run out of space, right?

224
0:16:03.800 --> 0:16:08.480
So what we do is we adaptively decide what to do in the moment.

225
0:16:08.480 --> 0:16:14.760
We might wait a little bit until resetting the state, or we might decide to give chance

226
0:16:14.760 --> 0:16:18.240
to other processes to be profiled, so we wipe the whole thing and start again.

227
0:16:18.240 --> 0:16:21.000
As you can see, this is very similar to a Bump allocator.

228
0:16:21.000 --> 0:16:25.920
This is basically a Bump allocator that has been chunked up.

229
0:16:25.920 --> 0:16:33.360
So the process of unwinding this is we start with a PID, we check if it has unwind information,

230
0:16:33.360 --> 0:16:39.720
then we need to find the mapping, and for each mapping, we know the minimum and the

231
0:16:39.720 --> 0:16:45.040
maximum program counter, so we need to do a linear search to find it.

232
0:16:45.040 --> 0:16:46.040
Then we find the chunk.

233
0:16:46.040 --> 0:16:51.600
With the chunk, we already have the shared information, so once we have the shared information,

234
0:16:51.600 --> 0:16:54.520
we have to traverse up to 250,000 items.

235
0:16:54.520 --> 0:16:59.720
We do this with a simple binary search.

236
0:16:59.720 --> 0:17:04.600
This takes between seven or eight iterations.

237
0:17:04.600 --> 0:17:09.520
Once we have the unwind action that tells us how to restore the previous frame registers,

238
0:17:09.520 --> 0:17:14.520
we do those operations and we are ready to go to the next frame.

239
0:17:14.520 --> 0:17:17.640
We are pretty much done for that frame.

240
0:17:17.640 --> 0:17:24.120
If the stack trace is correct, we know this because basically a stack, when you have frame

241
0:17:24.120 --> 0:17:29.200
pointers, the bottom of the stack is defined in applications with frame pointers.

242
0:17:29.200 --> 0:17:32.400
When you reach RBP equals zero, this is defined by the API.

243
0:17:32.400 --> 0:17:37.440
When you have unwind tables, it's defined by not having that program counter cover by

244
0:17:37.440 --> 0:17:40.360
any unwind table and having RBP zero.

245
0:17:40.360 --> 0:17:45.080
This is a requirement by the API, so if some application doesn't respect it, it's completely

246
0:17:45.080 --> 0:17:46.080
broken.

247
0:17:46.080 --> 0:17:50.720
So once we verify that the stack is correct, that we have reached the bottom of the stack,

248
0:17:50.720 --> 0:17:56.200
we hash the addresses and we add the hash to a map and we bump a counter.

249
0:17:56.200 --> 0:18:02.440
We do this, I think it is 19 times a second for every single CPU in your box.

250
0:18:02.440 --> 0:18:06.560
Every couple seconds we collect all this information, we generate a profile, we send it to some

251
0:18:06.560 --> 0:18:10.280
server for inspection.

252
0:18:10.280 --> 0:18:14.000
Of course, BPF is an interesting environment to work with.

253
0:18:14.000 --> 0:18:16.360
It is amazing and we really, really like it.

254
0:18:16.360 --> 0:18:18.140
But we need to be aware of some stuff.

255
0:18:18.140 --> 0:18:24.160
First of all, because we cannot page in or page out pages that contain the unwind tables,

256
0:18:24.160 --> 0:18:25.600
that has to be locked in memory.

257
0:18:25.600 --> 0:18:30.600
We need to be very careful with how we organize our data and the layout of that data to make

258
0:18:30.600 --> 0:18:31.760
it as small as possible.

259
0:18:31.760 --> 0:18:35.680
So we basically pack every single thing that can be packed.

260
0:18:35.680 --> 0:18:39.320
Then there's some interesting BPF things that for most people that have written BPF programs,

261
0:18:39.320 --> 0:18:42.720
this is well known, but I just want to talk a little bit about how we are dealing with

262
0:18:42.720 --> 0:18:44.960
some of the BPF challenges.

263
0:18:44.960 --> 0:18:48.560
So one of these is a stack size which is easy to work around.

264
0:18:48.560 --> 0:18:52.440
If I am not mistaken, we have 512 bytes which is not a lot.

265
0:18:52.440 --> 0:18:56.680
So we use another BPF map that we use sort of a global data structure.

266
0:18:56.680 --> 0:19:00.560
And that stores basically like kind of your heap, if you will.

267
0:19:00.560 --> 0:19:05.440
And then for the program size, this is a limitation that comes in two ways.

268
0:19:05.440 --> 0:19:10.480
First, there's probably some limitation in the amount of how many upcodes you can load

269
0:19:10.480 --> 0:19:14.840
in the kernel, but also the BPF verifier that ensures that the BPF code is safe to load,

270
0:19:14.840 --> 0:19:15.840
for example.

271
0:19:15.840 --> 0:19:22.320
You don't do any dereference that could go wrong or that your program terminates.

272
0:19:22.320 --> 0:19:23.480
It has some limits.

273
0:19:23.480 --> 0:19:28.280
It could theoretically run forever trying to verify your program, but it has some limits.

274
0:19:28.280 --> 0:19:30.520
And we hit this limit everywhere in our code.

275
0:19:30.520 --> 0:19:33.560
For example, we hit it when running our binary search.

276
0:19:33.560 --> 0:19:37.840
It complains, saying that it is too complex for each to analyze.

277
0:19:37.840 --> 0:19:42.520
So what we do here is that not only we have sharded our data, we have sharded our code,

278
0:19:42.520 --> 0:19:44.080
our code and data, the same thing, right?

279
0:19:44.080 --> 0:19:49.440
So we basically have our program split into many subprograms, and we keep the states, and

280
0:19:49.440 --> 0:19:54.000
we basically execute one program after each other and continue the state.

281
0:19:54.000 --> 0:19:57.720
So one of the techniques we use is BPF tail calls.

282
0:19:57.720 --> 0:20:02.440
Two other things that are way more modern and they're amazing too are bounded loops

283
0:20:02.440 --> 0:20:05.220
and BPF loop, which is a wonderful helper.

284
0:20:05.220 --> 0:20:10.600
The problem is that while we use bounded loops right now, we don't use BPF loop because it's

285
0:20:10.600 --> 0:20:12.520
only supported in modern kernels.

286
0:20:12.520 --> 0:20:13.520
But it's great.

287
0:20:13.520 --> 0:20:16.520
If you can use it, you should.

288
0:20:16.520 --> 0:20:20.320
Now because we're a profiler and we want to minimize the impact we have on the machines

289
0:20:20.320 --> 0:20:24.100
we run, I want to talk a little bit about performance in user space.

290
0:20:24.100 --> 0:20:27.360
So many Go applications, well, our profiler is written in Go, and many Go applications

291
0:20:27.360 --> 0:20:29.760
and APIs are in design with performance in mind.

292
0:20:29.760 --> 0:20:34.880
And this is something that can be seen in the dwarf and elf library that Go ships with

293
0:20:34.880 --> 0:20:39.000
in the Sandal library, as well as binary read and binary write that we use everywhere because

294
0:20:39.000 --> 0:20:43.220
we're dealing with raw bytes and we read them and write them all the time to the BPF maps.

295
0:20:43.220 --> 0:20:50.920
So it is interesting to know that both these binary read and binary write low level APIs

296
0:20:50.920 --> 0:20:54.280
actually allocate in the fast path, which can be problematic.

297
0:20:54.280 --> 0:20:57.920
So there's a lot of things that in the future we're going to be reinventing to make faster.

298
0:20:57.920 --> 0:21:00.260
And then we profiler, profiler, alerting, production.

299
0:21:00.260 --> 0:21:04.440
We have found a lot of opportunities and there's a lot more work to do because there's not

300
0:21:04.440 --> 0:21:05.440
much time.

301
0:21:05.440 --> 0:21:09.200
We're going to quickly skip through testing, but the idea here is that we try to be pragmatic

302
0:21:09.200 --> 0:21:12.800
and we have a lot of unit tests for the core functions and then we use snapshot testing

303
0:21:12.800 --> 0:21:14.760
for the unwind tables.

304
0:21:14.760 --> 0:21:19.000
We have a GitHub repository where we have a visual representation of the unwind tables

305
0:21:19.000 --> 0:21:23.920
and then we generate them every time on CI and locally and we verify that there are no

306
0:21:23.920 --> 0:21:28.560
changes compared to last time.

307
0:21:28.560 --> 0:21:31.000
I think there's only like two or three minutes left.

308
0:21:31.000 --> 0:21:33.840
So let me talk about the different environments and some of the interesting stuff that we

309
0:21:33.840 --> 0:21:34.840
have found.

310
0:21:34.840 --> 0:21:39.200
While we were profiling our profiler in production, we realized that we were spending a ridiculous

311
0:21:39.200 --> 0:21:41.840
amount of CPU cycles reading files from disk.

312
0:21:41.840 --> 0:21:46.120
I think the total is just a bunch, a part of the flame graph, but I think in total it

313
0:21:46.120 --> 0:21:47.840
was like 20% of the CPU cycles.

314
0:21:47.840 --> 0:21:52.680
So it turns out this was because our cloud provider has very slow disks that are orders

315
0:21:52.680 --> 0:21:57.720
of magnitude slower than our fast NVMe's in the team.

316
0:21:57.720 --> 0:22:02.600
Another thing that is very interesting that is not a new fact and everybody knows about

317
0:22:02.600 --> 0:22:07.280
is that different configuration is the biggest source of trouble.

318
0:22:07.280 --> 0:22:11.320
And we could see this the other day and if you're interested you can check the pull request.

319
0:22:11.320 --> 0:22:17.520
The whole project is open source which is the interaction between signals and BPF.

320
0:22:17.520 --> 0:22:22.720
What happened basically, Go has an embedded profiler and we use it all in production for

321
0:22:22.720 --> 0:22:28.240
reasons, but it triggers SIGPROF a couple of times a second.

322
0:22:28.240 --> 0:22:32.440
That was interrupting the process execution and at that time our process was booting up

323
0:22:32.440 --> 0:22:36.360
and it was loading our BPF program because it's quite long and complex.

324
0:22:36.360 --> 0:22:39.840
The verifier takes a couple milliseconds to load it, but it was getting interrupted all

325
0:22:39.840 --> 0:22:40.840
the time.

326
0:22:40.840 --> 0:22:46.480
So the BPF whenever it detects that the verifier has been interrupted it retries the process,

327
0:22:46.480 --> 0:22:49.720
basically wasting all the previous CPU cycles because it starts from scratch.

328
0:22:49.720 --> 0:22:53.200
But then it retries up to five times and then it says I couldn't do it and of course when

329
0:22:53.200 --> 0:22:59.000
we can't load a BPF program we are completely useless so we just crash.

330
0:22:59.000 --> 0:23:02.440
There is many other considerations such as what do you do with short leaf processes because

331
0:23:02.440 --> 0:23:03.720
you have to generate metadata.

332
0:23:03.720 --> 0:23:08.840
But even though we have an optimizer for this, we are not that bad and we can profile processes

333
0:23:08.840 --> 0:23:12.160
that run even for one second in your box.

334
0:23:12.160 --> 0:23:15.680
And then the important thing here is that this is our format for our custom at one table

335
0:23:15.680 --> 0:23:16.680
but it doesn't matter.

336
0:23:16.680 --> 0:23:22.120
The important bit here is that it mostly fits in L2 cache so we basically incur on two L2

337
0:23:22.120 --> 0:23:25.720
misses and it is pretty fast.

338
0:23:25.720 --> 0:23:30.840
On a machine with a bunch of processes with up to 90 frames we can do the full and one

339
0:23:30.840 --> 0:23:34.080
processing 0.5 milliseconds.

340
0:23:34.080 --> 0:23:37.600
On a CPU that is five years old.

341
0:23:37.600 --> 0:23:38.600
Cool.

342
0:23:38.600 --> 0:23:40.920
So we are going to do mixing and wind modes.

343
0:23:40.920 --> 0:23:45.640
So being able to unwind JIT sections we are applying RM64 support by the end of the year

344
0:23:45.640 --> 0:23:48.880
and this feature is going to be enabled by default in a few weeks because right now it

345
0:23:48.880 --> 0:23:50.640
is under a feature flag.

346
0:23:50.640 --> 0:23:54.640
We have many other things we want to support including high level languages and we are

347
0:23:54.640 --> 0:23:55.640
open source.

348
0:23:55.640 --> 0:24:01.160
So if you want to contribute or you have anything you want to discuss we meet biweekly on Mondays

349
0:24:01.160 --> 0:24:02.400
as part of the program project.

350
0:24:02.400 --> 0:24:06.040
So there is a bunch of links that we are going to upload to the presentation in the Folsom

351
0:24:06.040 --> 0:24:19.520
website and thank you.

