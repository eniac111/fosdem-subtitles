1
0:00:00.000 --> 0:00:09.000
This is a file system update and its future.

2
0:00:09.000 --> 0:00:16.000
Hello everyone, thanks for listening to my topic.

3
0:00:16.000 --> 0:00:20.000
Here are the file system updates and its future.

4
0:00:20.000 --> 0:00:28.000
Due to my visa application issue, I didn't find a proper way to go to Brussels on site.

5
0:00:28.000 --> 0:00:33.000
Therefore, I have to upload a video for online presentation.

6
0:00:33.000 --> 0:00:39.000
My name is Xiang Gao and I've been working on Lynx kernel staffs for most 7 years,

7
0:00:39.000 --> 0:00:43.000
mainly focusing on the local file system staff.

8
0:00:43.000 --> 0:00:48.000
I guess E-RFS is still a familiar staff for some people.

9
0:00:48.000 --> 0:00:54.000
And here I try to give more useful information of E-RFS.

10
0:00:54.000 --> 0:00:59.000
Hopefully, it is helpful to everyone.

11
0:00:59.000 --> 0:01:04.000
So E-RFS stands for Enhanced Read-Only File System.

12
0:01:04.000 --> 0:01:14.000
It was originally started in late 2017 and available since 4.19.

13
0:01:14.000 --> 0:01:19.000
It is designed to be a generic high-performance read-only file system

14
0:01:19.000 --> 0:01:24.000
with a very simple but effective core-on-disk format design.

15
0:01:24.000 --> 0:01:33.000
As a result, it almost has a better performance among the current kernel read-only file system.

16
0:01:33.000 --> 0:01:46.000
E-RFS is kernel multiple as a SQL or cover format replacement of traditional CPIO and TAR.

17
0:01:46.000 --> 0:01:51.000
And it is currently contributed by community libraries,

18
0:01:51.000 --> 0:02:00.000
IWAC, Dance, Copad, Google, Huawei, OPPO, and more.

19
0:02:00.000 --> 0:02:15.000
So as an option, E-RFS supports profile of the four and LVM a transparent data compression.

20
0:02:15.000 --> 0:02:24.000
However, E-RFS can live without compression as well.

21
0:02:24.000 --> 0:02:30.000
It is targeted for various high-performance read-only solutions

22
0:02:30.000 --> 0:02:35.000
such as system partitions and APX for Android, Smartphone,

23
0:02:35.000 --> 0:02:39.000
and other embedded systems,

24
0:02:39.000 --> 0:02:48.000
Blue CDs and Container Images, as well as AR datasets.

25
0:02:48.000 --> 0:02:53.000
There are many useful features which are actively under development,

26
0:02:53.000 --> 0:03:05.000
so if you have any suggestions or contributions, always welcome.

27
0:03:05.000 --> 0:03:10.000
There are several main use cases for E-RFS.

28
0:03:10.000 --> 0:03:15.000
The first main use case is Android system partitions.

29
0:03:15.000 --> 0:03:22.000
So Android has several read-only partitions which behave as system firewall,

30
0:03:22.000 --> 0:03:29.000
which means Android core can only be changed by way of an update.

31
0:03:29.000 --> 0:03:36.000
So in this way, it has many benefits such as it is easy for vendors to shift, distribute,

32
0:03:36.000 --> 0:03:41.000
keep original signing, golden images to each instance,

33
0:03:41.000 --> 0:03:52.000
and it is easy to go back to the original ship's date or do incremental updates.

34
0:03:52.000 --> 0:04:02.000
And it is easy to check data corruption or do data recovery even in a very low level,

35
0:04:02.000 --> 0:04:05.000
such as hardware.

36
0:04:05.000 --> 0:04:14.000
Also, it is easy for real storage devices to do hardware write protection and even more.

37
0:04:14.000 --> 0:04:17.000
So why are we introducing E-RFS?

38
0:04:17.000 --> 0:04:24.000
Basically, because it is read-only compression solutions,

39
0:04:24.000 --> 0:04:31.000
the internal cannot meet our performance requirements.

40
0:04:31.000 --> 0:04:39.000
But we need to do compression for our low-ended Android devices, smartphones at that time.

41
0:04:39.000 --> 0:04:50.000
That is why we designed E-RFS and sorted out from the beginning.

42
0:04:50.000 --> 0:04:59.000
We handled many basic common issues of generating read-only use cases

43
0:04:59.000 --> 0:05:05.000
to get high performance read-only file system.

44
0:05:05.000 --> 0:05:13.000
In addition, it is good to switch E-PX to E-RFS on disk format as well.

45
0:05:13.000 --> 0:05:19.000
Also, currently, E-PK is also another cover format.

46
0:05:19.000 --> 0:05:32.760
If it becomes E-RFS mumble, that may leverage the latest on-demand

47
0:05:32.760 --> 0:05:37.760
devices running on Android Cloudfish emulator.

48
0:06:32.760 --> 0:06:55.760
Our second main language use case for E-RFS is container image with a user space program called Netas.

49
0:06:55.760 --> 0:07:04.760
Tellingfly Netas is a user space example which uses E-RFS to leverage its functionality

50
0:07:04.760 --> 0:07:13.760
to do faster container image distribution like lazy polling and data de-tuberation across layers and images.

51
0:07:13.760 --> 0:07:21.760
Currently, Netas can do lazy polling for Netas, E-RFS images, as well as StarGZ images

52
0:07:21.760 --> 0:07:30.760
and original OCI images with an extra minimal index, which is much similar to another project

53
0:07:30.760 --> 0:07:33.760
which is called OCI.

54
0:07:33.760 --> 0:07:44.760
For more details of Netas itself, you could also refer to another topic which is called Netas image service

55
0:07:44.760 --> 0:07:50.760
for confidential containers at confidential computing dev room.

56
0:07:50.760 --> 0:07:54.760
On the left-hand side, it is Netas architecture.

57
0:07:54.760 --> 0:08:06.760
You could see that image format could be built with advanced features such as lazy polling, data de-tuberation,

58
0:08:06.760 --> 0:08:12.760
and native OCI compatible molds.

59
0:08:12.760 --> 0:08:20.760
And the read-only file system for containers such as 1C, KATA, KATA-CC, E-R Molds,

60
0:08:20.760 --> 0:08:33.760
and software package can be run by Netas D with Linux, Mac OS, FUSE, Wotawa, FFS, and E-RFS,

61
0:08:33.760 --> 0:08:37.760
over FFS cache with pitch cache sharing.

62
0:08:37.760 --> 0:08:48.760
So on the right-hand side, it is some partners which are Netas and Dragonfly solutions.

63
0:08:48.760 --> 0:09:17.760
The second demo, E-RFS is running with Netas for container images.

64
0:09:19.760 --> 0:09:47.760
So first, we need to run Netas container.

65
0:09:47.760 --> 0:10:15.760
And it finished in 16 seconds.

66
0:10:15.760 --> 0:10:27.760
Then it runs OCI container.

67
0:10:27.760 --> 0:10:33.760
You can see that it finished in 27 seconds.

68
0:10:33.760 --> 0:10:40.760
So that it induces time due to lazy polling.

69
0:10:40.760 --> 0:10:54.760
So this is the third demo. In this demo, E-RFS is running with original OCI and Netas LIM indexes for lazy polling.

70
0:10:54.760 --> 0:11:21.760
Note that this use case is still under development so that we could optimize it even further.

71
0:11:21.760 --> 0:11:32.760
So first, we start or read in OCI container.

72
0:11:32.760 --> 0:11:41.760
And it costs 26 seconds.

73
0:11:41.760 --> 0:11:57.760
And we view Netas LIM indexes for OCI images.

74
0:11:57.760 --> 0:12:05.760
So next, we start a zero OCI container.

75
0:12:05.760 --> 0:12:17.760
And you can see it costs 21 seconds. And that is the file system.

76
0:12:17.760 --> 0:12:25.760
You could see that the Netas LIM indexes is very small.

77
0:12:25.760 --> 0:12:35.760
So next, I will go into take some minutes to give a brief introduction of E-RFS core internals.

78
0:12:35.760 --> 0:12:41.760
So as an effective read-only internal solutions, core E-RFS on disk format is quite simple.

79
0:12:41.760 --> 0:12:48.760
Almost all E-RFS on disk structures are well aligned and lead within your single block,

80
0:12:48.760 --> 0:12:53.760
which means they are never across two blocks for performance.

81
0:12:53.760 --> 0:12:59.760
So on the left-hand side, it is on disk superblock format,

82
0:12:59.760 --> 0:13:05.760
which contains the overall five system statistics and the root I-node NID.

83
0:13:05.760 --> 0:13:16.760
Each E-RFS I-node is aligned in I-node slot so that the basic I-node information can be in the same block.

84
0:13:16.760 --> 0:13:20.760
And they can be read and advanced.

85
0:13:20.760 --> 0:13:29.760
On the right-hand side, there are E-RFS on this I-node format.

86
0:13:29.760 --> 0:13:36.760
Short, extended attributes can be kept just next to the core on disk I-node,

87
0:13:36.760 --> 0:13:43.760
as well as chunk, compress indexes, and the in-line data.

88
0:13:43.760 --> 0:13:48.760
Here is E-RFS on disk directory format.

89
0:13:48.760 --> 0:13:53.760
E-RFS directories consist of several directory blocks.

90
0:13:53.760 --> 0:14:02.760
Each block contains two parts called the direct part and the name part,

91
0:14:02.760 --> 0:14:11.760
so that with such on-disk design, E-RFS can do a name lookup with binary search,

92
0:14:11.760 --> 0:14:20.760
which makes E-RFS more effective than others exist in kernel read-only FL systems

93
0:14:20.760 --> 0:14:29.760
and keep it in a simple implementation.

94
0:14:29.760 --> 0:14:34.760
So here is an overview of the net-as-use case.

95
0:14:34.760 --> 0:14:41.760
You can see that it has almost two parts.

96
0:14:41.760 --> 0:14:57.760
One part is called bootstrap, also called primary device, which has a meta-block and a data block.

97
0:14:57.760 --> 0:15:05.760
So the meta-block could have super-block, I-nodes, and some in-line data.

98
0:15:05.760 --> 0:15:19.760
And the other data blocks could have directory blocks or some blocks for regular files.

99
0:15:19.760 --> 0:15:32.760
And the other part is called the blocks, which could have external data,

100
0:15:32.760 --> 0:15:44.760
which is separated with chunks, so that in such designs,

101
0:15:44.760 --> 0:15:54.760
the blocks can be referred with the metadata.

102
0:15:54.760 --> 0:15:59.760
And the details of compressed data is somewhat not quite trivial,

103
0:15:59.760 --> 0:16:12.760
but it could be referred from the following links as well, if you have more interesting.

104
0:16:12.760 --> 0:16:16.760
Here is the E-RFS recent updates.

105
0:16:16.760 --> 0:16:22.760
The first two features called chunk-based file,

106
0:16:22.760 --> 0:16:32.760
which could implement sparse files and data-deduplicated plain files.

107
0:16:32.760 --> 0:16:37.760
The next feature is called multiple devices and blocks.

108
0:16:37.760 --> 0:16:43.760
So E-RFS images can refer to other external data as well.

109
0:16:43.760 --> 0:16:53.760
Since 5.19, E-RFS over IFS cache has been already landed,

110
0:16:53.760 --> 0:17:03.760
which is already mentioned by some materials available online as the following links.

111
0:17:03.760 --> 0:17:13.760
Since 6.1, E-RFS has been introduced a special I-node called piped I-node for tail data,

112
0:17:13.760 --> 0:17:21.760
so that tail data or the whole files can be deduced or compressed together.

113
0:17:21.760 --> 0:17:35.760
Also, since 6.1, E-RFS supported global compressed data deduplication by using ruling cache.

114
0:17:35.760 --> 0:17:45.760
E-RFS over IFS cache sharing is still working in progress.

115
0:17:45.760 --> 0:17:51.760
So here is E-RFS compressed data deduplication test result.

116
0:17:51.760 --> 0:18:10.760
You can see that compared with scratchFS, E-RFS is more space-saving by using this new optimization.

117
0:18:10.760 --> 0:18:14.760
So in the next year, we've already planted some new features.

118
0:18:14.760 --> 0:18:19.760
Many of them are already working in progress, such as verification solutions

119
0:18:19.760 --> 0:18:23.760
and data deduplicated encryption solutions.

120
0:18:23.760 --> 0:18:30.760
We also have IFS cache improvements together with bad-dense forks,

121
0:18:30.760 --> 0:18:36.760
such as failover, multiple dimmers and directories, as well as dimmers.

122
0:18:36.760 --> 0:18:43.760
And more features can be referred to with the following links.

123
0:18:43.760 --> 0:18:48.760
So that's all of my topic. Thanks for listening again.

124
0:18:48.760 --> 0:18:59.760
If you have more interest in E-RFS, please kindly contact and join us. Thank you.

125
0:18:59.760 --> 0:19:03.760
We actually have time for five minutes of questions.

126
0:19:03.760 --> 0:19:11.760
We don't know how bad the lag actually is, but we can type the question into the chat if you have one.

127
0:19:11.760 --> 0:19:32.760
Or you can just ask it.

128
0:19:32.760 --> 0:19:37.760
Thanks for the talk. There was mention of a self-contained verification solution.

129
0:19:37.760 --> 0:19:52.760
Did you compare it with the m-variety and what advantages do you see in the verification solution you are working on?

130
0:19:52.760 --> 0:19:57.760
I mean, you can also write it, yeah.

131
0:19:57.760 --> 0:20:07.760
We have no idea what the lag is.

132
0:20:07.760 --> 0:20:09.760
Can you put some of your out?

133
0:20:09.760 --> 0:20:12.760
Sure. Do you have the app installed, like the FOSM app?

134
0:20:12.760 --> 0:20:17.760
If you go into the schedule, then you just need to click a link.

135
0:20:17.760 --> 0:20:46.760
I'm sorry.

136
0:20:46.760 --> 0:21:15.760
Okay.

137
0:21:15.760 --> 0:21:44.760
Okay.

138
0:21:44.760 --> 0:22:13.760
This is a text-only development room, by the way, as you can see.

139
0:22:13.760 --> 0:22:41.760
Okay.

140
0:22:41.760 --> 0:22:54.760
Okay.

