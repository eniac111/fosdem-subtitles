WEBVTT

00:00.000 --> 00:14.560
All right.

00:14.560 --> 00:16.200
So let's get started again.

00:16.200 --> 00:21.520
The next talk is by Anton about optimizing BPF hash map in France.

00:21.520 --> 00:22.520
Hello.

00:22.520 --> 00:29.040
I'm Anton, working at DataPod team at Isovalent.

00:29.040 --> 00:35.920
And yeah, this is talk about how simple it is to optimize BPF hash map.

00:35.920 --> 00:44.720
So about a year ago, like a little less, Andrej Nachryk proposed, suggested to try new hash

00:44.720 --> 00:51.920
functions for BPF hash map and BPF text trace map.

00:51.920 --> 00:57.920
And in Xilu, we use, and in TatraGone, we use hashes extensively.

00:57.920 --> 01:04.120
So for us, it's a big deal, and I decided to give it a try.

01:04.120 --> 01:12.720
So I will briefly provide some benchmarking how-to 101, and then we'll take a look at

01:12.720 --> 01:15.760
different hash functions.

01:15.760 --> 01:24.360
And then we'll see benchmarks for hash maps, which utilize old hash functions and new hash

01:24.360 --> 01:25.360
functions.

01:25.360 --> 01:34.200
So first thing to do when you benchmark is to try to reduce noise, because modern CPUs

01:34.200 --> 01:36.440
do everything to ruin your benchmarking.

01:36.440 --> 01:40.160
They will run on different frequency.

01:40.160 --> 01:47.040
Hyperthreading will get in, and in the best case, you benchmark inside kernel, because

01:47.040 --> 01:50.180
you can disable preemption and interrupts.

01:50.180 --> 01:58.500
So benchmark, we take, if we want to benchmark some function, we first measure some kind

01:58.500 --> 02:03.620
of time source or clock source, and then we execute our function, maybe in a loop, and

02:03.620 --> 02:11.920
then we measure time once again, and then we divide the number of observations by the

02:11.920 --> 02:15.200
number of loops and get our result.

02:15.200 --> 02:19.280
So in some cases, we can't execute our function in a loop.

02:19.280 --> 02:24.960
For example, if it's not like an abstract hash function, if it's just part of kernel,

02:24.960 --> 02:30.640
then n is obviously equal to one, and we need to take into account the time it takes to

02:30.640 --> 02:33.200
get time.

02:33.200 --> 02:42.240
And one obvious way to do this is to benchmark an empty loop, and this gives us roughly how

02:42.240 --> 02:45.900
long the get time function works.

02:45.900 --> 02:52.760
And typically, people benchmark with something like RDSC instruction, and if you don't reduce

02:52.760 --> 02:56.400
noise, then RDSC instruction is pretty unstable.

02:56.400 --> 03:03.880
So here, my CPU obviously runs on two different frequencies, and if my function, which I want

03:03.880 --> 03:18.200
to benchmark, runs 30, 40 cycles, then deviation in this case is bigger than the function itself,

03:18.200 --> 03:19.200
so I control-line it.

03:19.200 --> 03:26.480
So if we disable, like, reduce noise, as I said, then results become way more stable.

03:26.480 --> 03:32.420
So here, it's like, maybe it looks scary, but it's actually like 37 plus minus one cycle.

03:32.420 --> 03:38.560
So even for very small functions, this makes benchmarking more reliable.

03:38.560 --> 03:44.200
But in this form, RDSC doesn't work because it's not a serializing instruction, which

03:44.200 --> 03:51.720
means that if you insert your code here, then it can be executed in the middle of code and

03:51.720 --> 03:59.120
even after your code, so, and it can differ from execution to execution, and so we need

03:59.120 --> 04:02.880
some way to serialize it.

04:02.880 --> 04:07.400
Luckily, there are known ways to do this, so we just serialize it.

04:07.400 --> 04:14.240
And there is a white paper written maybe 10 years ago about Itanium, and it's still valid

04:14.240 --> 04:19.840
with some changes from architecture to architecture, but yeah, we can do this.

04:19.840 --> 04:27.440
In this case, benchmarking, like the offset, takes a little bit more, like it's not 37

04:27.440 --> 04:32.720
cycles anymore, it's 70 cycles, but again, it's pretty stable, and we can use this number

04:32.720 --> 04:35.720
to offset our measurements.

04:35.720 --> 04:40.120
And in fact, I did such benchmarking when I don't run in a loop.

04:40.120 --> 04:46.920
It's then switched to, like, more dumb benchmarks when you do loops over BPF maps because it's

04:46.920 --> 04:54.200
harder to port things, and if you want someone else to try these benchmarks, they will have

04:54.200 --> 04:58.000
to patch their kernel, and this is not simple.

04:58.000 --> 05:02.880
So let's take a look at several hash functions of interest.

05:02.880 --> 05:10.240
So JHash is currently used hash function in the BPF.

05:10.240 --> 05:16.160
It's Bob Jenkins hash, and it probably was developed some 30 years ago.

05:16.160 --> 05:22.280
So SpookyHash is another, it's a newer version, it's not a newer version, it's a newer hash

05:22.280 --> 05:32.760
from Bob Jenkins, and then there goes XXH hashes, XXH32.964 is the previous generation

05:32.760 --> 05:33.760
of these functions.

05:33.760 --> 05:42.360
They are available in kernel, and we can try them as well, and XXH3 is like the state of

05:42.360 --> 05:45.160
art hash function.

05:45.160 --> 05:53.480
So if we just take a look at this plot, the orange line which goes there is JHash, which

05:53.480 --> 05:55.400
is currently used.

05:55.400 --> 06:04.120
The green line, which looks to be green in here, is like the previous generation XXH64.

06:04.120 --> 06:13.040
The blue line, this is the newest generation XXH3, and while it looks here that it doesn't

06:13.040 --> 06:23.280
perform as well as XXH64, for small keys it does outperform it, and for BPF hash maps we

06:23.280 --> 06:27.720
are primarily interested in using it for small keys.

06:27.720 --> 06:39.680
I don't know, actually I never use it for huge keys, and in any case this XXH3 works

06:39.680 --> 06:45.760
faster than Jenkins hash, and later I will show that it can actually run even more faster.

06:45.760 --> 06:51.960
But one interesting thing is that this spooky hash, it actually performs pretty bad for

06:51.960 --> 06:57.900
small keys, because it has a lot of setup which it does in any case, but later it starts

06:57.900 --> 07:03.360
outperform every hash function, I was interested when it does it, so it does it at about key

07:03.360 --> 07:07.280
size of 9000.

07:07.280 --> 07:13.440
It's cool but it's not the key size of interest for us.

07:13.440 --> 07:23.120
If we take a look at XXH3 and JHash, we can see that the blue line XXH3, it actually outperforms

07:23.120 --> 07:33.680
like JHash for all key sizes, and there is also this green line, it's JHash2, it's an

07:33.680 --> 07:40.400
optimized version of JHash which can be used if your key size is multiple of four, and

07:40.400 --> 07:45.360
it's actually used in Blom filters but for some reason not in hash maps.

07:45.360 --> 07:55.920
So if we take a closer look at small key sizes, we see that yes, XXH3 outperforms JHash, so

07:55.920 --> 08:01.480
for me it's enough reason to try to benchmark maps with it.

08:01.480 --> 08:12.080
And let's take a look now at BPF maps which use hash functions, so first one is tech trace

08:12.080 --> 08:16.000
map and then hash map in Blom filters.

08:16.000 --> 08:25.280
So stick trace was actually the main reason for Andre to propose XXH3, because what it

08:25.280 --> 08:33.520
does is it takes a stack trace and then it hashes it and creates a map of IDs which refers

08:33.520 --> 08:42.080
to the traces, and if there are hash collisions then old stack traces are lost and we get

08:42.080 --> 08:45.080
incorrect picture about the system.

08:45.080 --> 08:54.760
And stack traces is not too random, so if your hash function is not very good, if it

08:54.760 --> 09:01.540
doesn't have very good avalanche properties, then it will create more collisions for less

09:01.540 --> 09:03.200
random data.

09:03.200 --> 09:14.040
And XXH3 behaves way better than JHash for avalanche properties and this is one of reasons

09:14.040 --> 09:17.840
and the main reason to use it for stack trace.

09:17.840 --> 09:23.200
The other reason as a benchmark, it's also like for stack trace, it also runs about twice

09:23.200 --> 09:30.640
faster for typical key sizes because typical key size is like 8 bytes per stack depth and

09:30.640 --> 09:35.240
this is typically like 60, 80, 100 bytes.

09:35.240 --> 09:44.860
So XXH3 is a very good candidate here.

09:44.860 --> 09:50.680
So for hash benchmarks I was primarily focused on lookups because this is what we do the

09:50.680 --> 10:00.680
most and this is the thing which is easy to measure compared to more complex pictures.

10:00.680 --> 10:05.200
So there are some links to benchmarks I used and scripts to actually execute benchmark

10:05.200 --> 10:13.760
and plot it because for every change I had to draw some like 100 or 150 pictures for

10:13.760 --> 10:20.360
different key sizes, for different fullness of hash maps, so it's impossible to do otherwise.

10:20.360 --> 10:22.200
And I had a few pictures here.

10:22.200 --> 10:31.760
So if we just use XXH3 then it looks like the new map which is orange, it outperforms

10:31.760 --> 10:33.920
the original map which is blue.

10:33.920 --> 10:45.360
And here is lookup speed in cycles, vertical and horizontal axis is key size.

10:45.360 --> 10:53.080
So the bigger the key the better the more gain from using the new hash function.

10:53.080 --> 11:01.600
But if we take like a bigger map I see that XXH3 as it is degrades for key size 4 and

11:01.600 --> 11:07.480
this is already like for me as a blocker I can't like propose a change which degrades

11:07.480 --> 11:10.600
existing applications.

11:10.600 --> 11:19.160
So then I went to a different architecture, micro architecture and here I saw that it

11:19.160 --> 11:26.400
degrades for different key size like here it degrades for key size 12 and if we take

11:26.400 --> 11:31.360
like a bigger map it also degrades for key size 24.

11:31.360 --> 11:40.040
And then I thought how to fix this because if it works for bigger keys then maybe I can

11:40.040 --> 11:41.040
utilize this.

11:41.040 --> 11:45.440
And I did the same thing as bloom filter currently does.

11:45.440 --> 11:53.440
So bloom filter executes JHash2 for key sizes divisible by 4 and it uses JHash in other cases.

11:53.440 --> 11:56.040
So I did the same.

11:56.040 --> 12:02.720
Utilize JHash2 for key sizes of which are divisible by 4 but for small ones like it's

12:02.720 --> 12:10.960
plus key length divided by 4, key length 32 it's actually computed.

12:10.960 --> 12:16.920
It's just key length divided by 4 but it's computed during hash initialization and we

12:16.920 --> 12:20.880
can decide for which key sizes we do this.

12:20.880 --> 12:24.960
And with this hash function I finally see that it doesn't degrade anywhere.

12:24.960 --> 12:33.440
So this is like 10K, 100K and 100% full which is like the worst case.

12:33.440 --> 12:43.320
And if we take another slice this is 100K map with key size 8 and on the left side it's

12:43.320 --> 12:44.560
almost empty.

12:44.560 --> 12:47.760
On the right side it's 100% full.

12:47.760 --> 12:53.080
And the bigger key size the bigger gain for particular key size.

12:53.080 --> 13:01.400
So for key size 64 map with new hash function runs about 50% faster and for key size 128

13:01.400 --> 13:04.560
it runs almost twice faster.

13:04.560 --> 13:13.840
And bloom filters as I mentioned they use the JHash2 for keys divisible by 4.

13:13.840 --> 13:19.560
So I don't expect any gain for keys divisible by 4 at least for small keys.

13:19.560 --> 13:20.560
And it looks like this.

13:20.560 --> 13:25.960
So this is like an extreme case of bloom filter with 9 hashes.

13:25.960 --> 13:33.000
But I just did it so it reproduces the plot of hash function here.

13:33.000 --> 13:35.880
And yes for small keys it is the same.

13:35.880 --> 13:39.200
And for bigger keys we have a gain.

13:39.200 --> 13:50.040
And here is the key size 240 where XH3 function originally utilizes vector instructions and

13:50.040 --> 13:54.320
we can't use obviously vector instructions in BPF maps.

13:54.320 --> 14:00.760
And for key size 240 it is expected to start using vector instructions.

14:00.760 --> 14:05.560
But there is also scalar implementation which works faster than JHash but it degrades at

14:05.560 --> 14:07.240
this point.

14:07.240 --> 14:19.920
So and another thing to mention that old hash functions JHash XXH64 they were...

14:19.920 --> 14:25.040
They were designed and optimized with O2 option in mind.

14:25.040 --> 14:31.720
So if we switch to O3 then they will behave the same.

14:31.720 --> 14:37.240
But XXH3 actually runs like 50, 60% faster.

14:37.240 --> 14:41.240
So it actually performs way better with O3.

14:41.240 --> 14:43.760
So I just jump like here.

14:43.760 --> 14:51.480
So and I know that like O3 is no go for kernel.

14:51.480 --> 14:54.680
Like there were several attempts to introduce it.

14:54.680 --> 15:01.440
And the reason was that there are no candidates which benefit from this O3.

15:01.440 --> 15:07.120
But this one is a particular candidate like hash.

15:07.120 --> 15:18.040
If we could use O3 for hash map because not only for XXH3 because it should be inlined.

15:18.040 --> 15:27.200
Then in this case we would be able to get rid of this composite hash which mixes hashes

15:27.200 --> 15:30.240
and just use it as is.

15:30.240 --> 15:39.000
So yeah as I said for stuck trace map it definitely makes sense to use it.

15:39.000 --> 15:44.440
So there is both benefiting in speed.

15:44.440 --> 15:52.320
Small one because stuck trace map the bottleneck for speed is not the hash.

15:52.320 --> 15:55.920
But the bottleneck for hash collisions is the hash.

15:55.920 --> 16:04.680
And for hash map it's a question maybe someone would advise me on what to do with O3.

16:04.680 --> 16:14.720
And after I run like benchmarks on slightly bigger number of architectures then I think

16:14.720 --> 16:19.480
there's also like a good candidate to use in the hash map.

16:19.480 --> 16:25.320
So here are some links for benchmarks and paper which I use it for those who will be

16:25.320 --> 16:28.640
reading this and thank you.

16:28.640 --> 16:39.560
All right thanks a lot any questions you.

16:39.560 --> 16:47.240
For the O3 thing can you only compile maybe the like the hash map file with O3.

16:47.240 --> 16:54.480
Yeah yeah if it is like currently it is disabled like for every file in kernel.

16:54.480 --> 17:00.200
For custom build we can enable it but like generally we just pass O2 everywhere.

17:00.200 --> 17:05.120
If it's possible just to compile BPF maps with O3 this will solve the thing.

17:05.120 --> 17:07.120
It's not such a big change.

17:07.120 --> 17:09.160
So you don't have to compile the whole kernel with O3?

17:09.160 --> 17:10.160
Yeah yeah yeah.

17:10.160 --> 17:13.560
So only it's local code.

17:13.560 --> 17:15.520
Okay.

17:15.520 --> 17:23.560
Any other questions?

17:23.560 --> 17:25.800
No then thanks a lot.
