WEBVTT

00:00.000 --> 00:09.000
This is a file system update and its future.

00:09.000 --> 00:16.000
Hello everyone, thanks for listening to my topic.

00:16.000 --> 00:20.000
Here are the file system updates and its future.

00:20.000 --> 00:28.000
Due to my visa application issue, I didn't find a proper way to go to Brussels on site.

00:28.000 --> 00:33.000
Therefore, I have to upload a video for online presentation.

00:33.000 --> 00:39.000
My name is Xiang Gao and I've been working on Lynx kernel staffs for most 7 years,

00:39.000 --> 00:43.000
mainly focusing on the local file system staff.

00:43.000 --> 00:48.000
I guess E-RFS is still a familiar staff for some people.

00:48.000 --> 00:54.000
And here I try to give more useful information of E-RFS.

00:54.000 --> 00:59.000
Hopefully, it is helpful to everyone.

00:59.000 --> 01:04.000
So E-RFS stands for Enhanced Read-Only File System.

01:04.000 --> 01:14.000
It was originally started in late 2017 and available since 4.19.

01:14.000 --> 01:19.000
It is designed to be a generic high-performance read-only file system

01:19.000 --> 01:24.000
with a very simple but effective core-on-disk format design.

01:24.000 --> 01:33.000
As a result, it almost has a better performance among the current kernel read-only file system.

01:33.000 --> 01:46.000
E-RFS is kernel multiple as a SQL or cover format replacement of traditional CPIO and TAR.

01:46.000 --> 01:51.000
And it is currently contributed by community libraries,

01:51.000 --> 02:00.000
IWAC, Dance, Copad, Google, Huawei, OPPO, and more.

02:00.000 --> 02:15.000
So as an option, E-RFS supports profile of the four and LVM a transparent data compression.

02:15.000 --> 02:24.000
However, E-RFS can live without compression as well.

02:24.000 --> 02:30.000
It is targeted for various high-performance read-only solutions

02:30.000 --> 02:35.000
such as system partitions and APX for Android, Smartphone,

02:35.000 --> 02:39.000
and other embedded systems,

02:39.000 --> 02:48.000
Blue CDs and Container Images, as well as AR datasets.

02:48.000 --> 02:53.000
There are many useful features which are actively under development,

02:53.000 --> 03:05.000
so if you have any suggestions or contributions, always welcome.

03:05.000 --> 03:10.000
There are several main use cases for E-RFS.

03:10.000 --> 03:15.000
The first main use case is Android system partitions.

03:15.000 --> 03:22.000
So Android has several read-only partitions which behave as system firewall,

03:22.000 --> 03:29.000
which means Android core can only be changed by way of an update.

03:29.000 --> 03:36.000
So in this way, it has many benefits such as it is easy for vendors to shift, distribute,

03:36.000 --> 03:41.000
keep original signing, golden images to each instance,

03:41.000 --> 03:52.000
and it is easy to go back to the original ship's date or do incremental updates.

03:52.000 --> 04:02.000
And it is easy to check data corruption or do data recovery even in a very low level,

04:02.000 --> 04:05.000
such as hardware.

04:05.000 --> 04:14.000
Also, it is easy for real storage devices to do hardware write protection and even more.

04:14.000 --> 04:17.000
So why are we introducing E-RFS?

04:17.000 --> 04:24.000
Basically, because it is read-only compression solutions,

04:24.000 --> 04:31.000
the internal cannot meet our performance requirements.

04:31.000 --> 04:39.000
But we need to do compression for our low-ended Android devices, smartphones at that time.

04:39.000 --> 04:50.000
That is why we designed E-RFS and sorted out from the beginning.

04:50.000 --> 04:59.000
We handled many basic common issues of generating read-only use cases

04:59.000 --> 05:05.000
to get high performance read-only file system.

05:05.000 --> 05:13.000
In addition, it is good to switch E-PX to E-RFS on disk format as well.

05:13.000 --> 05:19.000
Also, currently, E-PK is also another cover format.

05:19.000 --> 05:32.760
If it becomes E-RFS mumble, that may leverage the latest on-demand

05:32.760 --> 05:37.760
devices running on Android Cloudfish emulator.

06:32.760 --> 06:55.760
Our second main language use case for E-RFS is container image with a user space program called Netas.

06:55.760 --> 07:04.760
Tellingfly Netas is a user space example which uses E-RFS to leverage its functionality

07:04.760 --> 07:13.760
to do faster container image distribution like lazy polling and data de-tuberation across layers and images.

07:13.760 --> 07:21.760
Currently, Netas can do lazy polling for Netas, E-RFS images, as well as StarGZ images

07:21.760 --> 07:30.760
and original OCI images with an extra minimal index, which is much similar to another project

07:30.760 --> 07:33.760
which is called OCI.

07:33.760 --> 07:44.760
For more details of Netas itself, you could also refer to another topic which is called Netas image service

07:44.760 --> 07:50.760
for confidential containers at confidential computing dev room.

07:50.760 --> 07:54.760
On the left-hand side, it is Netas architecture.

07:54.760 --> 08:06.760
You could see that image format could be built with advanced features such as lazy polling, data de-tuberation,

08:06.760 --> 08:12.760
and native OCI compatible molds.

08:12.760 --> 08:20.760
And the read-only file system for containers such as 1C, KATA, KATA-CC, E-R Molds,

08:20.760 --> 08:33.760
and software package can be run by Netas D with Linux, Mac OS, FUSE, Wotawa, FFS, and E-RFS,

08:33.760 --> 08:37.760
over FFS cache with pitch cache sharing.

08:37.760 --> 08:48.760
So on the right-hand side, it is some partners which are Netas and Dragonfly solutions.

08:48.760 --> 09:17.760
The second demo, E-RFS is running with Netas for container images.

09:19.760 --> 09:47.760
So first, we need to run Netas container.

09:47.760 --> 10:15.760
And it finished in 16 seconds.

10:15.760 --> 10:27.760
Then it runs OCI container.

10:27.760 --> 10:33.760
You can see that it finished in 27 seconds.

10:33.760 --> 10:40.760
So that it induces time due to lazy polling.

10:40.760 --> 10:54.760
So this is the third demo. In this demo, E-RFS is running with original OCI and Netas LIM indexes for lazy polling.

10:54.760 --> 11:21.760
Note that this use case is still under development so that we could optimize it even further.

11:21.760 --> 11:32.760
So first, we start or read in OCI container.

11:32.760 --> 11:41.760
And it costs 26 seconds.

11:41.760 --> 11:57.760
And we view Netas LIM indexes for OCI images.

11:57.760 --> 12:05.760
So next, we start a zero OCI container.

12:05.760 --> 12:17.760
And you can see it costs 21 seconds. And that is the file system.

12:17.760 --> 12:25.760
You could see that the Netas LIM indexes is very small.

12:25.760 --> 12:35.760
So next, I will go into take some minutes to give a brief introduction of E-RFS core internals.

12:35.760 --> 12:41.760
So as an effective read-only internal solutions, core E-RFS on disk format is quite simple.

12:41.760 --> 12:48.760
Almost all E-RFS on disk structures are well aligned and lead within your single block,

12:48.760 --> 12:53.760
which means they are never across two blocks for performance.

12:53.760 --> 12:59.760
So on the left-hand side, it is on disk superblock format,

12:59.760 --> 13:05.760
which contains the overall five system statistics and the root I-node NID.

13:05.760 --> 13:16.760
Each E-RFS I-node is aligned in I-node slot so that the basic I-node information can be in the same block.

13:16.760 --> 13:20.760
And they can be read and advanced.

13:20.760 --> 13:29.760
On the right-hand side, there are E-RFS on this I-node format.

13:29.760 --> 13:36.760
Short, extended attributes can be kept just next to the core on disk I-node,

13:36.760 --> 13:43.760
as well as chunk, compress indexes, and the in-line data.

13:43.760 --> 13:48.760
Here is E-RFS on disk directory format.

13:48.760 --> 13:53.760
E-RFS directories consist of several directory blocks.

13:53.760 --> 14:02.760
Each block contains two parts called the direct part and the name part,

14:02.760 --> 14:11.760
so that with such on-disk design, E-RFS can do a name lookup with binary search,

14:11.760 --> 14:20.760
which makes E-RFS more effective than others exist in kernel read-only FL systems

14:20.760 --> 14:29.760
and keep it in a simple implementation.

14:29.760 --> 14:34.760
So here is an overview of the net-as-use case.

14:34.760 --> 14:41.760
You can see that it has almost two parts.

14:41.760 --> 14:57.760
One part is called bootstrap, also called primary device, which has a meta-block and a data block.

14:57.760 --> 15:05.760
So the meta-block could have super-block, I-nodes, and some in-line data.

15:05.760 --> 15:19.760
And the other data blocks could have directory blocks or some blocks for regular files.

15:19.760 --> 15:32.760
And the other part is called the blocks, which could have external data,

15:32.760 --> 15:44.760
which is separated with chunks, so that in such designs,

15:44.760 --> 15:54.760
the blocks can be referred with the metadata.

15:54.760 --> 15:59.760
And the details of compressed data is somewhat not quite trivial,

15:59.760 --> 16:12.760
but it could be referred from the following links as well, if you have more interesting.

16:12.760 --> 16:16.760
Here is the E-RFS recent updates.

16:16.760 --> 16:22.760
The first two features called chunk-based file,

16:22.760 --> 16:32.760
which could implement sparse files and data-deduplicated plain files.

16:32.760 --> 16:37.760
The next feature is called multiple devices and blocks.

16:37.760 --> 16:43.760
So E-RFS images can refer to other external data as well.

16:43.760 --> 16:53.760
Since 5.19, E-RFS over IFS cache has been already landed,

16:53.760 --> 17:03.760
which is already mentioned by some materials available online as the following links.

17:03.760 --> 17:13.760
Since 6.1, E-RFS has been introduced a special I-node called piped I-node for tail data,

17:13.760 --> 17:21.760
so that tail data or the whole files can be deduced or compressed together.

17:21.760 --> 17:35.760
Also, since 6.1, E-RFS supported global compressed data deduplication by using ruling cache.

17:35.760 --> 17:45.760
E-RFS over IFS cache sharing is still working in progress.

17:45.760 --> 17:51.760
So here is E-RFS compressed data deduplication test result.

17:51.760 --> 18:10.760
You can see that compared with scratchFS, E-RFS is more space-saving by using this new optimization.

18:10.760 --> 18:14.760
So in the next year, we've already planted some new features.

18:14.760 --> 18:19.760
Many of them are already working in progress, such as verification solutions

18:19.760 --> 18:23.760
and data deduplicated encryption solutions.

18:23.760 --> 18:30.760
We also have IFS cache improvements together with bad-dense forks,

18:30.760 --> 18:36.760
such as failover, multiple dimmers and directories, as well as dimmers.

18:36.760 --> 18:43.760
And more features can be referred to with the following links.

18:43.760 --> 18:48.760
So that's all of my topic. Thanks for listening again.

18:48.760 --> 18:59.760
If you have more interest in E-RFS, please kindly contact and join us. Thank you.

18:59.760 --> 19:03.760
We actually have time for five minutes of questions.

19:03.760 --> 19:11.760
We don't know how bad the lag actually is, but we can type the question into the chat if you have one.

19:11.760 --> 19:32.760
Or you can just ask it.

19:32.760 --> 19:37.760
Thanks for the talk. There was mention of a self-contained verification solution.

19:37.760 --> 19:52.760
Did you compare it with the m-variety and what advantages do you see in the verification solution you are working on?

19:52.760 --> 19:57.760
I mean, you can also write it, yeah.

19:57.760 --> 20:07.760
We have no idea what the lag is.

20:07.760 --> 20:09.760
Can you put some of your out?

20:09.760 --> 20:12.760
Sure. Do you have the app installed, like the FOSM app?

20:12.760 --> 20:17.760
If you go into the schedule, then you just need to click a link.

20:17.760 --> 20:46.760
I'm sorry.

20:46.760 --> 21:15.760
Okay.

21:15.760 --> 21:44.760
Okay.

21:44.760 --> 22:13.760
This is a text-only development room, by the way, as you can see.

22:13.760 --> 22:41.760
Okay.

22:41.760 --> 22:54.760
Okay.
