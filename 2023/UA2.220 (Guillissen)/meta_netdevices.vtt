WEBVTT

00:00.000 --> 00:08.060
Okay, we're ready for our next talk.

00:08.060 --> 00:10.640
Daniel is going to talk about MetaNet devices.

00:10.640 --> 00:11.640
Thanks.

00:11.640 --> 00:12.640
All right.

00:12.640 --> 00:13.640
Thanks a lot.

00:13.640 --> 00:14.640
So, yeah.

00:14.640 --> 00:15.640
So, this talk is about MetaNet devices.

00:15.640 --> 00:19.680
This work has been done by my colleague and myself.

00:19.680 --> 00:24.600
We are at Isovalent software engineers working on the kernel and also psyllium.

00:24.600 --> 00:31.480
So, really, the goal is the question we were asking ourselves, like, how can we leverage

00:31.480 --> 00:35.840
the BPF infrastructure in the kernel and also the networking features to really achieve

00:35.840 --> 00:40.360
maximum performance for Kubernetes parts?

00:40.360 --> 00:46.720
And before I go into the kernel bits, just a really quick recap around Kubernetes and

00:46.720 --> 00:48.320
parts and what it is.

00:48.320 --> 00:51.480
So, basically, what you can see here is a host.

00:51.480 --> 00:55.520
A host can have one or many parts in Kubernetes.

00:55.520 --> 01:00.800
It's an orchestration system, essentially, and a part is usually defined as a network

01:00.800 --> 01:07.800
namespace and it is connected to typically to weave devices to get traffic in and out

01:07.800 --> 01:09.320
of them.

01:09.320 --> 01:14.240
A part can have one or many containers that are sharing this network namespace.

01:14.240 --> 01:22.760
So, yeah, and CNI is basically a networking plugin which will set up various things.

01:22.760 --> 01:28.520
When a part comes up, for example, it will set up Net devices, assign IP addresses.

01:28.520 --> 01:33.640
It has an IPAM infrastructure to manage a pool of addresses.

01:33.640 --> 01:36.480
It will install routes.

01:36.480 --> 01:42.760
In the case of psyllium, which is a CNI, it will also set up the BPF data paths to basically

01:42.760 --> 01:45.280
route traffic in and out.

01:45.280 --> 01:50.720
It has various features on top as well, such as policy enforcement, load balancing, bandwidth

01:50.720 --> 01:52.520
management, and so on and so forth.

01:52.520 --> 01:58.000
But I don't want to make this talk about all the different features about psyllium, but

01:58.000 --> 02:02.080
rather about performance.

02:02.080 --> 02:07.640
There was an interesting keynote last year at the SA Econ from Brandon Gregg where he

02:07.640 --> 02:11.880
talked about computing performance and what's on the horizon.

02:11.880 --> 02:17.080
He had a couple of predictions and one was quite interesting when he was talking about

02:17.080 --> 02:20.440
OS performance.

02:20.440 --> 02:27.280
The statement that he made is that given the kernel is becoming increasingly complex, the

02:27.280 --> 02:32.480
performance defaults are getting worse and worse.

02:32.480 --> 02:38.560
He stated that it basically takes a whole US team to make the operating system perform

02:38.560 --> 02:40.480
well.

02:40.480 --> 02:48.840
The problem is, given all these performance teams they are trying to optimize at the larger

02:48.840 --> 02:54.800
scale, nobody is actually looking at the defaults anymore and how they can be optimized.

02:54.800 --> 02:58.080
This was quite interesting.

02:58.080 --> 03:04.640
In case of defaults, we were wondering, given two Kubernetes nodes with part to part and

03:04.640 --> 03:10.440
they connected with 100 gigabit NIC, we wanted to look at the single flow, like a single

03:10.440 --> 03:14.200
TCP stream.

03:14.200 --> 03:21.000
We asked what's the default baseline you can get to, where are bottlenecks, how can they

03:21.000 --> 03:27.440
be overcome, and actually can we provide better defaults out of that, what we figure out.

03:27.440 --> 03:33.080
Why buttering with single stream performance?

03:33.080 --> 03:38.240
First of all, it's interesting for the kernel to cope with growing NIC speeds, 100, 200

03:38.240 --> 03:43.000
gigabit or more, how can this be maxed out with the single stream.

03:43.000 --> 03:49.360
There are lots of data intensive workloads from user and customer sites around machine

03:49.360 --> 03:55.320
learning, AI and so on, but generally it's also interesting to be able to free up the

03:55.320 --> 04:02.400
resources and give them to the application instead of the kernel having to block them.

04:02.400 --> 04:07.840
The assumptions for our test is usually the Kubernetes worker nodes that users run, they

04:07.840 --> 04:11.160
are quite generic, they can run any kind of workloads.

04:11.160 --> 04:15.120
What we are also seeing is a large number of users typically just stick to defaults,

04:15.120 --> 04:19.400
they don't tune specifically the kernel.

04:19.400 --> 04:27.160
There's an interesting cloud native usage report where they try to get some insights

04:27.160 --> 04:33.360
into how Kubernetes deployments usually look like.

04:33.360 --> 04:38.920
There's definitely an increasing trend to have a higher density of containers per host,

04:38.920 --> 04:50.680
so around 50 or more is expected these days and the number of pods per node is also increasing.

04:50.680 --> 04:57.000
The question now is, so a very basic compatibility setting, for example, for the kernel, for

04:57.000 --> 05:03.520
the case of psyllium, we used the, like if you deployed in a basic mode, we just used

05:03.520 --> 05:06.680
the upper stack for routing and forwarding.

05:06.680 --> 05:10.680
There are various reasons why people might want to use that.

05:10.680 --> 05:15.760
For example, in case of Kubernetes, there's a component called QProxy which uses NetFilter

05:15.760 --> 05:24.800
IP tables for service management, for service load balancing, some people stick to that.

05:24.800 --> 05:30.440
Or they have custom NetFilter rules, so maybe they require to use the upper stack for that,

05:30.440 --> 05:35.960
or just simply they for now just went with defaults and might look into more tuning at

05:35.960 --> 05:37.680
a later point in time.

05:37.680 --> 05:42.040
When we try to look at the performance for that, what you can see here is on the yellow

05:42.040 --> 05:49.520
bar is the host to host performance for a single stream, so we got to 4 gigabit per

05:49.520 --> 06:02.400
second but then if you do the pod to pod connectivity, it's really reduced dramatically.

06:02.400 --> 06:07.920
One of the reasons is, and I will get into this in a bit, is because the upper stack

06:07.920 --> 06:21.200
is giving false feedback to the TCP stack, one thing that we did a year ago or so is

06:21.200 --> 06:27.360
to introduce a feature which is called BPF host routing, where we don't use the upper

06:27.360 --> 06:30.600
stack.

06:30.600 --> 06:35.360
For BPF itself, given we attach to physical devices but also to these devices, we added

06:35.360 --> 06:42.520
a couple of new helper functions there, one is called BPF redirect peer, what this is

06:42.520 --> 06:48.320
basically doing is adding a fast switch into the network name space for the ingress traffic,

06:48.320 --> 06:55.400
so basically we're just, instead of going to the usual Xmid route to the weaves devices,

06:55.400 --> 07:04.200
we can retrieve the weaves device inside the pod and just scrub the packet to remove the

07:04.200 --> 07:09.320
necessary data that we typically remove from switching network namespaces but also to set

07:09.320 --> 07:16.040
the device to the device inside the pod and then circle around in the main receive loop

07:16.040 --> 07:22.480
without going to a per CPU backlog queue that you would normally do when you transfer data

07:22.480 --> 07:27.000
through weaves devices and we don't need to use the upper stack because there's all the

07:27.000 --> 07:35.480
information already available in BPF context and for the way out we added a helper which

07:35.480 --> 07:41.320
is called BPF redirect neighbor, so that one will basically insert the packet into the

07:41.320 --> 07:50.840
neighboring subsystem of the kernel, so usually we can do a flip lookup out of BPF, so there's

07:50.840 --> 07:57.720
a helper for this as well and then combined with this for the resolution of neighbors,

07:57.720 --> 08:03.560
it will allow that you don't need to go to the upper stack, so the nice benefit you get

08:03.560 --> 08:10.800
as well with this is that the socket context for the network packet for the SKP is retained

08:10.800 --> 08:16.440
all the way to the physical device until the packet is actually sent out and this is not

08:16.440 --> 08:23.160
the case when you normally go to the upper stack, so then the TCP stack actually thinks

08:23.160 --> 08:28.640
that like once you go to the upper stack that it already left the node but it's actually

08:28.640 --> 08:35.560
not the case and this way it can be retained and this is how the complete picture looks

08:35.560 --> 08:41.880
like and if you look at the performance it's already much better, so we were able to get

08:41.880 --> 08:49.440
almost to 40 gigabit per second under 1.5k MTU, so this was interesting, now the question

08:49.440 --> 08:55.560
is like how could we close the remaining gap?

08:55.560 --> 09:03.160
On the 8k MTU we also did some tests and one thing to note here is that we were able to

09:03.160 --> 09:09.600
like for single TCP stream to get to 98 gigabit per second for the host to host case but still

09:09.600 --> 09:14.760
like the situation looks quite the same for the WEEF with the BPF host routing, so there's

09:14.760 --> 09:21.840
still like a small gap that we want to close here as well and that's where we introduce

09:21.840 --> 09:30.320
a new device type as a WEEF replacement, so we call this meta device because it's programmable

09:30.320 --> 09:36.920
to a BPF and you can implement various, you know, like your own business logic into this,

09:36.920 --> 09:43.080
so it's flexible and this time like the main difference is that this also gets like a faster

09:43.080 --> 09:49.480
switch on the Egress side for the Egress traffic so it doesn't need to go to the per CPU backlog

09:49.480 --> 09:53.880
view for the Egress as well.

09:53.880 --> 09:59.280
So if you look at the flame graphs, so that's like the worst case scenario where we compared

09:59.280 --> 10:08.000
the WEEF and the meta device, so what you can see here on the WEEF device, like on XMID

10:08.000 --> 10:13.040
it will basically scrub the packet data, it will unqueue the packet to a per CPU backlog

10:13.040 --> 10:18.840
view and then at some point there's a network's action, it will pick up the packets from the

10:18.840 --> 10:23.920
queue again and in the worst case it can be deferred to the kernel software queue daemon

10:23.920 --> 10:31.680
and then you see this rescheduling where you have a new stack where this is processed again

10:31.680 --> 10:39.960
and then from the BPF side it will reach BPF from the TC ingress on the host WEEF where

10:39.960 --> 10:45.960
we then can only forward this to the physical device to leave the node, right?

10:45.960 --> 10:52.640
And all of this can be done in one go without rescheduling through this meta device, so

10:52.640 --> 10:55.960
it will scrub the packet, it will switch the network namespace, it will reset the device

10:55.960 --> 11:04.480
pointers and it will then directly call the BPF program and if the BPF program says that

11:04.480 --> 11:09.520
based on the FIP lookup and so on that it will forward the packet directly to the physical

11:09.520 --> 11:16.600
device then it will avoid this rescheduling scenario.

11:16.600 --> 11:22.560
So like on the right side, I mean it's really straightforward, that's how the implementation

11:22.560 --> 11:26.960
of the driver ex-mid routine looks like, so it will basically just call into BPF and then

11:26.960 --> 11:30.880
based on the verdict push it out.

11:30.880 --> 11:36.280
It's really just like 500 lines of code, so it's very simple and straightforward, I think

11:36.280 --> 11:42.520
it's just one fifth of the WEEF driver that we have right now and the other focus that

11:42.520 --> 11:48.480
we wanted to put into is compatibility as well so that like given in Sium we need to

11:48.480 --> 11:56.160
support multiple kernels, the ideal case would be like that we don't need to change much

11:56.160 --> 11:59.680
of the BPF program and can keep it as is.

11:59.680 --> 12:06.840
And in case of XTP we didn't want to implement it because like for the WEEF case it really

12:06.840 --> 12:12.440
is very complex, it even adds multi-queue support which you normally would not need

12:12.440 --> 12:18.440
on a virtual device, so we wanted to keep it as simple as possible and to have to flip

12:18.440 --> 12:22.760
the ability that this can be added as a single or paired device.

12:22.760 --> 12:25.960
Like for the WEEF replacement it would be a paired device but you could also do it as

12:25.960 --> 12:34.680
a single device and then implement whatever logic you would want in BPF for that.

12:34.680 --> 12:41.160
So looking at the performance, again like the TCP stream under 8K, so this is really

12:41.160 --> 12:50.640
able to reach through this approach the full 98 gigabit per second and in terms of latency,

12:50.640 --> 12:56.480
so we did some Netperf TCP or R measurements as well where you get the minimum, the P90,

12:56.480 --> 13:04.640
99 latency and so on, so this is really on par with the host.

13:04.640 --> 13:09.560
So now we were asking ourselves can we push this even further, I mean well so we were

13:09.560 --> 13:16.920
able to get to 98 gigabit per second but like the cost for like a megabyte to transfer can

13:16.920 --> 13:23.160
just push even more and there's a relatively recent kernel feature which is called big

13:23.160 --> 13:24.960
TCP.

13:24.960 --> 13:33.360
It landed for IPv6 only in 519 and was developed by Google and the whole idea behind big TCP

13:33.360 --> 13:42.880
is to even more aggressively aggregate for geo and gso, so normally the kernel will try

13:42.880 --> 13:47.320
it basically out of the incoming packet stream, create a super packet and then we'll push

13:47.320 --> 13:52.840
it up to the networking stack so it only needs to be traversed once and the limit up until

13:52.840 --> 13:59.920
that point was for 64K packets simply because in the IP header that's the maximum packet

13:59.920 --> 14:06.720
size that you can do and the idea for a big TCP for IPv6 was that well maybe we could

14:06.720 --> 14:15.280
create a hop by hop header in the geo layer and then add and then like the 16 bit packet

14:15.280 --> 14:21.720
length field can be overcome because there is a jumbogram extension in there which allows

14:21.720 --> 14:23.760
for 32 bits field.

14:23.760 --> 14:32.440
So you can do much more aggressive aggregation and yeah, so this is also now supported with

14:32.440 --> 14:39.120
the new release where this will be set up for all the devices and the need for automatically

14:39.120 --> 14:40.200
for IPv6.

14:40.200 --> 14:45.160
Actually like this week there was also, got also merged for IPv4 now so this will land

14:45.160 --> 14:53.400
in kernel 6.3 which is exciting and when we looked at the performance again under big

14:53.400 --> 15:00.680
TCP turns out like the using the upper stack is currently broken into kernel so that still

15:00.680 --> 15:04.800
needs to be fixed, we will look into that.

15:04.800 --> 15:11.840
So forwarding there wouldn't work then with the host routing cases so it will basically

15:11.840 --> 15:20.280
bump up the regular weave one to get this on par with the meta and also the host so

15:20.280 --> 15:25.440
it will basically hide those glitches.

15:25.440 --> 15:37.440
The latency is still better in terms of like the short packet request response type workloads

15:37.440 --> 15:41.720
for the meta so that's still on par with the host.

15:41.720 --> 15:48.360
So what is the remaining offender like when you run all these features together it's basically

15:48.360 --> 15:54.760
the copying to users so like between 60 and 70% of the cycles is really spent on copying

15:54.760 --> 15:57.640
all this data to user space.

15:57.640 --> 16:03.800
So the next question we ask ourselves actually in this experiment, so what if we combine

16:03.800 --> 16:09.360
the whole big TCP stuff with TCP zero copy so what if we could leverage the memory map

16:09.360 --> 16:15.080
TCP and it turns out that's currently not possible in the kernel, that's a limitation

16:15.080 --> 16:20.840
because in the GRO layer big TCP will create a frag list which is essentially like a list

16:20.840 --> 16:27.040
of SKBs you know as a single big one that is being pushed up the stack and TCP zero

16:27.040 --> 16:33.560
copy only works on the SKB frags so that's like an internal so where basically you have

16:33.560 --> 16:41.820
a single SKB and it has like the pages as read only attached in the nonlinear section.

16:41.820 --> 16:47.480
So that currently does not work combining those two would probably have like really

16:47.480 --> 16:54.720
big potential but what we now try to do is we just looked at the just using the TCP zero

16:54.720 --> 17:00.480
copy to see how it looks without the big TCP.

17:00.480 --> 17:05.040
Generally speaking is not as straightforward to deploy because first of all you need to

17:05.040 --> 17:12.520
rewrite your application in order to leverage memory map TCP this can be done for RX but

17:12.520 --> 17:20.080
also for TX or both and it needs driver changes and in particular driver changes to be able

17:20.080 --> 17:27.560
to split the header from the data because the data you want to memory map to user space.

17:27.560 --> 17:32.360
Some Nix might do this with the hardware and some others you would have to do some kind

17:32.360 --> 17:40.080
of pseudo header data splits over you basically just copy the header into a linear section.

17:40.080 --> 17:46.760
So this is how it would look like we tried this for 4K and 8K MTU.

17:46.760 --> 17:52.720
There's a great talk from Eric Dumasay the TCP maintainer in terms of what all the all

17:52.720 --> 17:57.880
those things you need to do in order to be able to make use of this for example you also

17:57.880 --> 18:06.560
need to align the TCP window scale to 12 segments so that you fill exactly those pages.

18:06.560 --> 18:14.200
And yeah the driver support can be very different like we had in our lab MLX 500 gigabit Nix

18:14.200 --> 18:21.000
and they did not implement the header data splits so my colleague Nikolai did the POC

18:21.000 --> 18:26.280
implementation to change the driver to be able to do that so that we could get some

18:26.280 --> 18:32.960
measurements out of this and if you want to look into an example application and how you

18:32.960 --> 18:38.180
can implement that there's one in the networking self test in the upstream tree called TCP

18:38.180 --> 18:41.440
mmap which is useful also for the benchmarking.

18:41.440 --> 18:48.640
So you really need to align various different settings like for our test implementation

18:48.640 --> 18:53.880
we used for MLX 5 the non-striding modes or the legacy modes we need to switch that off

18:53.880 --> 18:59.960
and Eve tool first because I mean for the POC it was easier to do the way like the packet

18:59.960 --> 19:09.880
layout is done there then you need to align the MTU to either one page or two pages and

19:09.880 --> 19:14.120
you need to do various other settings like in the for the course of time I will not go

19:14.120 --> 19:17.240
into all of the details.

19:17.240 --> 19:23.640
In generally I would think it would be useful addition to the kernel to have like a configuration

19:23.640 --> 19:30.160
framework for that and to be able to have more drivers supporting the header data split

19:30.160 --> 19:35.120
there's actually one in Windows kernel turns out so there's some documentation around there

19:35.120 --> 19:40.240
that we found while preparing for the talk the other thing is a caveat is like the TCP

19:40.240 --> 19:46.320
zero copy may like the benefits might be limited if you then actually go in your application

19:46.320 --> 19:49.800
and then touch the data because then they need to be pulled into the cache which they

19:49.800 --> 19:54.240
would be if like if you would have to copy things to user but they may not be like if

19:54.240 --> 20:01.520
you just memory map so the applications there is mostly like on the for example storage

20:01.520 --> 20:08.760
side where you wouldn't have to do that and looking at the data for 4k MTU we tried with

20:08.760 --> 20:19.160
the implementation we got to 81 gigabit per second so that's a bit limiting could also

20:19.160 --> 20:26.400
be that this is mostly because the implementation was you know a PLC with lots of optimization

20:26.400 --> 20:32.000
potential that can still be there but we looked at it came to you and there we were able to

20:32.000 --> 20:39.160
get to 98 gigabit per second but the interesting piece here is the cost per megabyte we could

20:39.160 --> 20:45.160
we were able to reduce from 85 down to 27 microseconds per megabyte so this really is

20:45.160 --> 20:56.160
significant and yeah because of the avoidance that you don't copy anymore so as a summary

20:56.160 --> 21:03.480
so we started with the default and then you know switch to the 8k MTU then we went to

21:03.480 --> 21:08.960
the host routing that we covered then the addition of the meta device where we can avoid

21:08.960 --> 21:14.320
where we can do this low latency you know for ingress and egress then the big TCP and

21:14.320 --> 21:21.280
that all works without changes to the application and with that you can already get like a 2x

21:21.280 --> 21:28.120
improvement and then it gets even more dramatic but it's of course dependent on your application

21:28.120 --> 21:34.000
for the zero copy some of the future directions as I mentioned earlier it would be useful

21:34.000 --> 21:39.920
to have like a generic header data split framework for NIC drivers that they can implement that

21:39.920 --> 21:46.680
and expose it this setting there's potential here as well to optimize for example like

21:46.680 --> 21:55.520
the header pages could be the head page could be packed with the headers it could get recycling

21:55.520 --> 22:03.120
which we didn't implement here and in future big TCP would be interesting to combine with

22:03.120 --> 22:09.160
the zero copy so that discovers the frag list in geo and the other thing that is at

22:09.160 --> 22:15.160
some point on the horizon is to push the big TCP actually onto the wire as well if the

22:15.160 --> 22:23.640
hardware supports that and yeah so with that I'm done with the talk and the prototype for

22:23.640 --> 22:29.040
the meta device is currently you can find on this branch we are working on pushing this

22:29.040 --> 22:34.800
upstream in the coming months and there's also the prototype public for the MLX 5 header

22:34.800 --> 22:40.280
data split and yeah so the plan is basically to get this into upstream kernel and then

22:40.280 --> 22:47.120
also to get this integrated into psyllium for the for the next release yeah thank you

22:47.120 --> 22:58.640
are there any questions thanks we already have two questions in the chat so we can start

22:58.640 --> 23:04.760
with them if there are no in the room alright so the first question we got was can we my

23:04.760 --> 23:07.760
well not really a question so much as a comment I think which is can we please come up with

23:07.760 --> 23:15.240
a better name than meta for this just call it BPF or BPF dev or something yeah the other

23:15.240 --> 23:22.760
thing was if the perf benefit comes from calling sure from inside the network namespace can

23:22.760 --> 23:31.440
we just make vf do that instead I think we could but the question is like I haven't looked

23:31.440 --> 23:38.440
into the the like how it would affect the XTP related bits so it felt easier to do something

23:38.440 --> 23:43.720
simple and start from scratch like one thing I don't really like about the weave devices

23:43.720 --> 23:50.880
to be honest how complex it got with the older XTP additions and it's not even that beneficial

23:50.880 --> 23:55.680
and it added like multi queue support and all of that which was not needed at all for

23:55.680 --> 24:06.600
a virtual device so I really wanted to have something simple and yeah and easy so any

24:06.600 --> 24:23.800
more questions from the audience here I have a question you are about big TCP and TCP zero

24:23.800 --> 24:30.720
copy it's in your lab you only did benchmarks host to host right because so but also part

24:30.720 --> 24:37.120
to part so we tried both yeah okay because do you know how it would work with the switches

24:37.120 --> 24:41.880
do you need special switch support for no no so the big TCP is only local to the node

24:41.880 --> 24:47.000
so like the packets on the wire they will still be your MTU size packets it's just that

24:47.000 --> 24:53.080
the the aggregation on like for the local stack is much bigger so it's it doesn't affect

24:53.080 --> 25:00.720
anything on the wire that's a nice thing okay and for zero copy I mean like for the zero

25:00.720 --> 25:04.840
copy you need to change definitely the MTU which also affects the rest of your network

25:04.840 --> 25:23.480
of course so that's one thing that would be required for that thank you.
