WEBVTT

00:00.000 --> 00:08.280
All right, good afternoon everyone.

00:08.280 --> 00:14.200
Before I get started, how many of you attended the session about Service Mesh this afternoon?

00:14.200 --> 00:15.200
Sorry about that.

00:15.200 --> 00:19.640
If you're fed up with me, you have to stay with me for another session.

00:19.640 --> 00:25.880
I will repeat some parts of it to introduce the topic, but otherwise, I'm sorry about

00:25.880 --> 00:26.880
that.

00:26.880 --> 00:30.080
Welcome to the session, Golden Signals with Siljem and Grafana.

00:30.080 --> 00:31.080
My name is Ray MediÃ³n.

00:31.080 --> 00:34.560
I'm field CTO for EMEA at Isovalent.

00:34.560 --> 00:38.200
Isovalent is a company which originated Siljem.

00:38.200 --> 00:43.040
And today I'm going to talk about Grafana and how Siljem enables you to get golden signals

00:43.040 --> 00:45.520
out of the box.

00:45.520 --> 00:51.200
Introduction about technology, a little bit about EBPF and Siljem, then about observability

00:51.200 --> 00:56.960
challenges and how we can tackle those challenges using our observability.

00:56.960 --> 01:01.920
A bit on monitoring, day two operations and the default dashboards we provide.

01:01.920 --> 01:06.040
And then hopefully the demo gods are with me for a small demo to actually see how we

01:06.040 --> 01:15.520
get layer seven metrics and we can see return codes and monitor application response times,

01:15.520 --> 01:18.040
etc.

01:18.040 --> 01:20.800
So to start this topic, I want to introduce Siljem and EBPF.

01:20.800 --> 01:24.160
How many of you know about EBPF?

01:24.160 --> 01:25.660
Quite a lot, awesome.

01:25.660 --> 01:31.320
How many of you know about Siljem and are using Siljem?

01:31.320 --> 01:34.280
Cool, great.

01:34.280 --> 01:40.040
So Isovalent is the company behind each Siljem and EBPF.

01:40.040 --> 01:42.280
We maintain it with the community.

01:42.280 --> 01:48.120
And to start with, EBPF is explaining what EBPF is and how it works.

01:48.120 --> 01:52.960
So we like to say what JavaScript is to the browser, EBPF is to the kernel.

01:52.960 --> 01:56.440
What we mean by that is that we make it extensible in a dynamic way.

01:56.440 --> 02:02.320
So that means we're not changing the kernel, but at kernel speeds we can run programs based

02:02.320 --> 02:04.240
on kernel events.

02:04.240 --> 02:10.360
And for the context of this session, what's important is that considering metrics, considering

02:10.360 --> 02:15.680
getting useful information from your applications, what we're doing here is whenever a process

02:15.680 --> 02:22.000
opens a socket or a packet gets sent on the network device on the node, we can expose

02:22.000 --> 02:27.880
metrics or we can export those metrics using EBPF and we can use tools like Raffana to

02:27.880 --> 02:36.880
display it in a good way so you can do data operations or you can see how your application

02:36.880 --> 02:39.760
or cluster is performing.

02:39.760 --> 02:41.440
Siljem runs on EBPF.

02:41.440 --> 02:46.280
The good thing about Siljem is you don't need to be an EBPF engineer to run and operate

02:46.280 --> 02:47.280
Siljem.

02:47.280 --> 02:53.680
You just set certain configuration options and EBPF programs will be mounted on the nodes

02:53.680 --> 02:56.160
and will run when they need to run.

02:56.160 --> 03:02.520
And Siljem and the high level provides a number of capabilities, networking capabilities,

03:02.520 --> 03:08.920
observability capabilities, also surface mesh and using Tetracon solution we can also use

03:08.920 --> 03:17.520
runtime feasibility and observability security based on processes such as opening files,

03:17.520 --> 03:19.560
process execution, etc.

03:19.560 --> 03:22.960
Today we'll focus about on the observability part.

03:22.960 --> 03:30.640
So besides networking we can get rich visibility of metrics on your cluster.

03:30.640 --> 03:35.360
As you may know, Google uses on data plane V2 actually under the hood Siljem.

03:35.360 --> 03:36.840
Microsoft has moved.

03:36.840 --> 03:43.160
Azure default AKS clusters to Siljem so all the cloud providers see that Siljem is a powerful

03:43.160 --> 03:48.720
technology to run clusters at scale and to get useful metrics running them.

03:48.720 --> 03:51.520
So let's start with common challenges we see.

03:51.520 --> 03:55.880
One of the main challenges if we talk about performance of your application or performance

03:55.880 --> 04:01.440
of your cluster set scale is that you run into this issue of the finger pointing problem.

04:01.440 --> 04:06.680
What I mean by that is that network connectivity is layered and when you run into issues you

04:06.680 --> 04:13.440
need to, especially at scale, you need to be easily aware where a possible problem could

04:13.440 --> 04:14.440
exist.

04:14.440 --> 04:17.640
And it can be in multiple layers and if you look at the OZ layer obviously it can be at

04:17.640 --> 04:22.340
the data link layer, the network layer, the transport layer or in the application layer.

04:22.340 --> 04:28.300
So the goal of Siljem with Kefan is to give you the tools to quickly inspect what's going

04:28.300 --> 04:36.120
on and to be more efficient in troubleshooting your cluster and or applications.

04:36.120 --> 04:40.800
Another common issue, especially at scale, is obviously the signal to noise problem.

04:40.800 --> 04:46.600
You may run in the clouds, you get data from your nodes, you see IP addresses communicating

04:46.600 --> 04:47.900
with each other.

04:47.900 --> 04:51.240
But IP addresses by itself mean nothing in Kubernetes clusters.

04:51.240 --> 04:57.300
They come, they go and at scale it's impossible to track and trace what's going on with service

04:57.300 --> 05:02.280
to service communication in your applications.

05:02.280 --> 05:05.280
Also where are existing mechanisms falling short?

05:05.280 --> 05:11.200
So first of all network devices, think about centralized monitoring or firewall solutions,

05:11.200 --> 05:12.640
think about Splunk.

05:12.640 --> 05:19.520
They are great to get our alerts, to get dashboards, but again at scale they can be very costly

05:19.520 --> 05:21.940
or they can be a bottleneck.

05:21.940 --> 05:27.400
Also these devices don't have awareness of the identities of your applications running

05:27.400 --> 05:30.240
on your Kubernetes clusters.

05:30.240 --> 05:34.920
Cloud provider network logs are nice for node to node communication but don't provide identity

05:34.920 --> 05:37.000
as well.

05:37.000 --> 05:44.120
You can monitor the host, you can see Linux host statistics but that gives you only visibility

05:44.120 --> 05:50.440
on the node level and again a Linux node doesn't have any awareness of the identities of the

05:50.440 --> 05:53.580
services running in your Kubernetes cluster.

05:53.580 --> 05:57.800
You may want to try to modify application code and this applies a bit to the service

05:57.800 --> 05:58.800
mesh session.

05:58.800 --> 06:03.360
You may want to install libraries which gives you visibility of the application but then

06:03.360 --> 06:07.800
you don't have visibility on the networking layer.

06:07.800 --> 06:13.040
Service mesh, main goal of service mesh is obviously visibility of the network and trend

06:13.040 --> 06:18.000
communication to get metrics out of that but that with the sidecar based implementation

06:18.000 --> 06:24.640
comes with a footprint and with induced latency plus you have operational complexity maintaining

06:24.640 --> 06:29.400
your service mesh solution on top of your Kubernetes clusters.

06:29.400 --> 06:37.320
So this is where Cilium comes around the corner instead we provide identity aware based observability

06:37.320 --> 06:38.960
and security.

06:38.960 --> 06:46.440
What that means is based on the labels you set on your workloads we create unique identities

06:46.440 --> 06:52.680
and we're able to attach that identity in the data plane using eBPF and using that identity

06:52.680 --> 06:54.260
we can do things with that.

06:54.260 --> 06:59.640
So we can secure the connectivity in this example a front end to a back end is allowed

06:59.640 --> 07:05.360
to communicate based on the network policies we set and identities we are aware of and

07:05.360 --> 07:10.720
this identity is a cluster wide property but in terms of observability this also means

07:10.720 --> 07:17.080
that we can use this identity to get rich metrics and data for that identity and you

07:17.080 --> 07:19.520
can monitor it effectively.

07:19.520 --> 07:24.400
This means that you're not looking anymore at IP addresses you're looking at identities

07:24.400 --> 07:29.800
so the whole set of workloads the surface to surface communication for a front end to

07:29.800 --> 07:33.840
a back end.

07:33.840 --> 07:39.280
Hubble is our observability solution built on top of Cilium how it works is that Cilium

07:39.280 --> 07:46.480
runs as a democet on your cluster nodes as an agent and Hubble can retrieve data from

07:46.480 --> 07:53.640
those agents through a CLI or UI and we can export metrics based on your workloads.

07:53.640 --> 07:58.680
So there are three parts first of all the Hubble UI gives a service dependency map so

07:58.680 --> 08:05.080
on a namespace level you can see what's deployed what is communicating with each other what

08:05.080 --> 08:11.040
kind of protocols they're using what's coming between namespaces so you would see for example

08:11.040 --> 08:16.800
if there's inter namespace communication you can identify the source namespace if you use

08:16.800 --> 08:23.040
cluster mesh you can even identify the source cluster you can also identify egress traffic

08:23.040 --> 08:28.900
and ingress traffic on a namespace level the Hubble CLI is more a power users tool to give

08:28.900 --> 08:35.200
you detailed flow you can export it to JSON you can do a lot of filtering based on labels

08:35.200 --> 08:40.640
Hubble metrics is the part where it's mostly the topic for today is where you export metrics

08:40.640 --> 08:46.480
and you use Grafana for example to observe the performance of your cluster and application

08:46.480 --> 08:51.600
this is all fueled through eBPF so again think about a network device sending a packet that's

08:51.600 --> 08:59.840
a kernel event eBPF program gets attached to it gets the metrics and it's done this

08:59.840 --> 09:06.760
is a small screenshot of the CLI so this gives you flow visibility using Hubble observe

09:06.760 --> 09:11.880
commands you can follow for example based on the label in this case xwing so we're following

09:11.880 --> 09:18.840
all the workloads labels with xwing so again no IP addresses just labels in purple is highlighted

09:18.840 --> 09:24.680
these IDs we use so again each unique set of labels gets a unique cluster wide ID and

09:24.680 --> 09:31.160
based on those IDs we can track based on labels what communication is going on and there's

09:31.160 --> 09:37.240
a lot of metadata you can filter on things like headers things like ports things like

09:37.240 --> 09:44.920
protocols obviously labels in QNAD spot names surfaces worker nodes DNS we also have cilium

09:44.920 --> 09:52.200
network policies which allow you to filter and observe two FQDN rules meaning we can inspect

09:53.000 --> 09:59.560
queries to external domains and we can filter based on that and obviously cilium related

09:59.560 --> 10:05.720
identities such as world ingress egress host and that kind of stuff policy verdict matches

10:06.440 --> 10:13.960
things like dropped allowed and stuff this is the Hubble UI service map like I said before

10:13.960 --> 10:19.240
this gives you a namespace level view in this case we have a jobs app and I'm using this app

10:19.240 --> 10:25.160
as well in the demo I'm showing a bit later so here you're looking at a namespace level view

10:25.160 --> 10:29.640
where you can see all the service to service communication of your application running in that

10:29.640 --> 10:36.360
namespace in this case it's only intran namespace communication and you can see for example that

10:36.360 --> 10:42.120
the recruiter is talking to core API the core API is talking to elastic search we have a zookeeper

10:42.120 --> 10:49.240
component we identify Kafka also identifying Kafka protocols so there are a number of protocols we

10:49.240 --> 10:56.760
can inspect and see and we also see layer 7 information so in this case HTTP calls to a

10:56.760 --> 11:02.280
specific URI or URL with specific method and return calls and this is triggered through

11:03.000 --> 11:09.880
just simple construct as a cilium network policy if you just allow let's say intran namespace

11:09.880 --> 11:20.040
traffic and you are accepting HTTP that already triggers this visibility for you to see now using

11:20.040 --> 11:28.440
this data we can also export metrics to Grafana so we have into we are working with Grafana a lot

11:28.440 --> 11:35.480
more lately that means that we are building a lot of more useful dashboards and also integrating

11:35.480 --> 11:42.040
with things like tempo for getting transparent tracing in Grafana all powered through cilium and

11:42.040 --> 11:50.040
eBPF this allows us to not only see on the network level metrics on performance on the node but also

11:50.040 --> 11:56.040
for service to service communication to provide golden signals things like HTTP request rate

11:56.040 --> 12:02.520
latency response codes and error codes which would as an application engineer would allow you to

12:02.520 --> 12:09.000
quickly see which components of your application is not responding as it should

12:11.880 --> 12:16.680
but also detecting transduced network layers so this will be more network related we may see

12:16.680 --> 12:22.440
retransmissions we can see byte sent and received and we can indicate things like

12:22.440 --> 12:27.880
round-trip time to indicate a network layer problem so maybe in a data center you have a

12:27.880 --> 12:33.000
specific reg switch or top of reg switch not performing as it should so nodes connected to

12:33.000 --> 12:40.120
that switch will have improved will have reduced performance and you would see latency increasing

12:43.320 --> 12:50.040
now with the latest dashboards we also able to see programmatic API requests using transparent

12:50.040 --> 12:57.480
tracing this goes to the integration with Grafana so at the moment your application

12:57.480 --> 13:03.240
need to be able to support it so you need to be able to inject traces but we're working on

13:03.880 --> 13:09.640
out of the box getting also this support and be able to help by help support HTTP

13:10.840 --> 13:15.960
traces as such and then you get this exemplar so i'm pointing at a small exemplar here

13:16.600 --> 13:22.360
after which you can inspect this with with tempo and you can see a span of specific request

13:22.360 --> 13:30.920
and see where the problem may reside a bit more on monitoring so this is more day to ops

13:30.920 --> 13:36.440
i want to highlight that if you run cilium and you are also using Grafana make sure that you

13:36.440 --> 13:42.040
install the agent hubble and operator metrics plugins these are out of the box plugins we

13:42.040 --> 13:47.640
provide through the Grafana marketplace you can download and this gives you visibility

13:47.640 --> 13:52.760
in the performance of your cluster so first of all agent metrics everything on the node level how

13:52.760 --> 13:59.560
the node is performing how many throughputs they are processing how many memory the bpf is using

13:59.560 --> 14:06.280
all that related stuff hubble metrics gives you the visibility across your cluster in terms of

14:06.280 --> 14:14.440
application layer seven return calls policy verdicts so allows versus drops so you can

14:14.440 --> 14:21.400
monitor on the cluster level the performance of your applications and in some cases you

14:21.400 --> 14:27.000
run an operator so you may want to track the number of identities how the cluster in general

14:27.000 --> 14:34.440
is behaving api responses and such and finally what we released just a few days ago thanks to

14:34.440 --> 14:40.440
rafael who is also here is the cilium policy verdict metrics dashboard which gives you the

14:40.440 --> 14:50.520
capability to get meaningful graphs if you have workloads actually hitting network policies you

14:50.520 --> 14:56.360
set what i mean by that is that when we work with customers with cilium is they want to go to this

14:56.360 --> 15:02.680
micro segmentation zero trust model and you can use obviously hubble to monitor service-to-service

15:02.680 --> 15:10.600
communication and to see if traffic is allowed and denied but this dashboard also is a very useful

15:10.600 --> 15:16.120
tool to confirm if you have either ingress or egress policies which are matching with your traffic

15:16.680 --> 15:23.160
so in this case we see a green graphs which means that on ingress and egress we have matching

15:23.160 --> 15:30.520
traffic the purple represents dns matching traffic but if there's some yellow traffic that's either

15:30.520 --> 15:36.840
allow all match traffic which is too broadly which should trigger you to get even better

15:36.840 --> 15:42.760
network policies to make sure that kind of flows are actually related to a network policy to ensure

15:43.320 --> 15:48.760
that both ingress and egress traffic is secured as such if you do so all the grounds will turn

15:48.760 --> 15:53.640
green and you know and you can confirm for that given namespace that you have securities

15:53.640 --> 16:04.600
all right i've prepared a little demo this runs this tenon jobs application i mentioned before

16:06.440 --> 16:12.360
i'm running this on kind so just a simple kind cluster on my laptop i'm showing here the

16:12.360 --> 16:18.600
components of my application so this you know a number of of workloads i've shown before on a

16:18.600 --> 16:25.400
grid screenshot to help me through this demo i've created a little script and what this does it only

16:25.400 --> 16:32.520
updates a helm chart for this application so it makes my workflow a lot easier i don't have to

16:32.520 --> 16:39.800
enter commands but we should see some things changing in our confana dashboard before i

16:39.800 --> 16:50.600
start this let me highlight the metrics so i need to log in so i've installed graphana i've

16:50.600 --> 16:54.920
installed tempo i've installed prometheus and configured cilium to export those metrics

16:55.560 --> 17:00.680
so this is currently the performance of my application running on my kind cluster on my laptop

17:01.480 --> 17:04.840
so as you can see we have 100 success rate we have

17:04.840 --> 17:13.880
uh incoming 100 and we also have good graphs of information for the performance of the application

17:13.880 --> 17:19.800
okay so let me start with starting this script

17:22.200 --> 17:29.400
yes so i mentioned before that the hubble metrics are available as soon as you configure some kind

17:29.400 --> 17:34.200
of layer seven cilium network policy because that triggers the collection of those metrics

17:34.200 --> 17:42.600
for layer seven and i'm showing this but i will show this a bit better in in in a different window

17:43.800 --> 17:49.640
so what i'm going to do now is i want to increase the request volume so i'm configuring the crawler

17:49.640 --> 17:54.600
component to get more requests in my application as you can see it's redeploying the crawler

17:54.600 --> 18:02.600
component so this would is something we should see in the graphana dashboard while this is redeploying

18:02.600 --> 18:08.520
i can show the helm chart i've used i'm using you need to be a bit patient with me because it takes

18:08.520 --> 18:14.360
one minute before the graphics the the graphana dashboards are updated you can actually see

18:15.080 --> 18:20.760
the impact of this new version of the application so typically you configure cilium through a helm

18:20.760 --> 18:27.000
values file so in this case on the operator component i've enabled metrics and prometheus

18:27.000 --> 18:33.800
on the hubble side i've configured hubble relay together the metrics and also prometheus and metrics

18:34.920 --> 18:39.640
so and this part is very interesting because if you want to have layer seven visibility you need

18:39.640 --> 18:46.120
to have specific metrics being enabled this will be documented in the cilium i.o website

18:46.120 --> 18:51.880
once we have the new release ready so as you can see we are matching HTTP v2 we have enabled

18:51.880 --> 18:59.000
exemplars we are looking for labels in terms of context source ip source namespace etc so these

18:59.000 --> 19:06.040
are important sets of labels you need to set and on the prometheus side we've enabled it to get at

19:06.040 --> 19:13.560
the graphs the cilium network policy i mentioned before this is just a simple example we allow

19:13.560 --> 19:23.160
everything within the namespace we have enabled dns visibility so we're inspecting all dns traffic

19:23.160 --> 19:31.880
to cube dns that allows us to get visibility of dns queries we've enabled ingress and egress

19:31.880 --> 19:37.800
for the purpose of the demo so we can also see that traffic and what's important is that we have

19:37.800 --> 19:44.680
an empty or open rule HTTP which allows us to see all traffic it allows all traffic

19:45.400 --> 19:53.880
but that triggers the collection of metrics all right so on the demo site so it has deployed a

19:53.880 --> 20:04.120
new version of my application looking at my metrics i can see incoming request volume increasing

20:04.120 --> 20:11.640
so you see already an increase of volume we also see requests by source response codes increasing

20:11.640 --> 20:17.160
so still 200 always fine always good just a number just an increase of requests per seconds

20:18.120 --> 20:22.280
and also on the incoming site okay all good

20:22.280 --> 20:33.640
okay let's now deploy a new configuration of our app and this app is has an error

20:35.320 --> 20:42.040
so let's see what we can see there again redeploying the core api components

20:43.560 --> 20:48.120
and now we should be able to see the error rate increase as a result of core api configuration

20:48.120 --> 20:53.160
changing so this will take one minute

20:55.560 --> 21:00.680
here i can select the destination workload so i can switch between core api or the loader

21:00.680 --> 21:05.320
component to see how the traffic for that destination is matching and how it's performing

21:06.280 --> 21:12.840
let's give it a few seconds to actually show what i'm looking for is the incoming request success

21:12.840 --> 21:19.880
rate apparently this application has version as an error so the success rate will be lower than before

21:27.000 --> 21:29.160
okay it's running okay

21:29.160 --> 21:40.120
okay yeah it's always a bit takes a bit longer than i wanted

21:49.080 --> 21:54.600
bear with me you know what in the meantime i'll already start the next step so i can

21:54.600 --> 21:58.600
i don't have you waiting

22:03.720 --> 22:10.520
ah there you go so here you see that the success rate is decreasing because of this faulty version

22:10.520 --> 22:15.960
of my application so i can really see there's something wrong with my application and as

22:15.960 --> 22:22.200
application developer or owning this namespace i should now be able to investigate what's going on

22:22.200 --> 22:28.600
this also means that here on the incoming request by source and response code i would see

22:29.160 --> 22:36.840
the resumes components showing 500 and 503 error return codes which triggers me to check that

22:36.840 --> 22:45.000
component and communication between those components also on the destination site all right

22:45.000 --> 22:58.840
so now i've introduced a new version and what this does is changing the request duration so

22:58.840 --> 23:03.240
again a new version of the application and let's see how we can monitor this performance of the

23:03.240 --> 23:09.000
application in grafana so let's check the request duration increase as a result of core api

23:09.000 --> 23:20.760
configuration changing okay so let's use here so i'm monitoring http request duration by source

23:20.760 --> 23:25.880
and destination so if the demo works well we see an increase there

23:25.880 --> 23:31.080
so

23:35.080 --> 23:35.400
okay

23:41.000 --> 23:45.640
takes a bit too long uncomfortable with but should be there any minutes

23:45.640 --> 23:56.680
with me

24:04.040 --> 24:05.240
should appear any moment

24:09.560 --> 24:10.200
let me just

24:10.200 --> 24:18.440
post in the meantime i will deploy a new version of the application which also introduces

24:18.440 --> 24:25.080
tracing so again for tracing to be supported you at this moment your application needs support that

24:26.280 --> 24:30.360
so in this case i deploy a new version of this application to support tracing

24:32.840 --> 24:37.880
and this is using open telemetry so let's deploy that in the meantime

24:37.880 --> 24:46.120
that's deploying in the meantime i can check how the request duration is doing

24:50.920 --> 24:56.680
okay this part is not working yet but we should see a request duration increase

24:56.680 --> 25:06.040
oh god yeah thanks that doesn't help i clicked on something ah yes thank you so much

25:07.880 --> 25:14.520
yeah here you can see the request duration increasing and i just deployed a new version

25:15.720 --> 25:20.840
of my application which supports tracing using open telemetry and then you already can see that

25:20.840 --> 25:29.080
i have this exemplars appearing so i now only can not only inspect htp request duration

25:29.080 --> 25:34.360
but i can also inspect specific traces and exemplars so let's if you click on this little

25:35.000 --> 25:41.320
box you get this window you can get valuable information about this trace point and then you

25:41.320 --> 25:50.840
can query it with track tempo go yep let's leave this side so here you can see a specific trace id

25:50.840 --> 25:57.160
and you can see a node graph so this is also nice you can see between nodes what's what's going on

25:57.160 --> 26:05.000
and you see highlighted in red here what's has a high latency as such and here we can see that in

26:05.000 --> 26:17.880
this specific uh api call there is an error so post scroll and it has some events exception

26:18.600 --> 26:22.200
random error so something is wrong with my application so this enables me

26:22.200 --> 26:28.680
as an application owner to troubleshoot my application effectively so this concludes the

26:28.680 --> 26:38.440
demo um let me quickly move to here all right so if you want to know more how to configure cilium

26:38.440 --> 26:45.480
to enable metrics how to configure cilium to uh with the right values for layer 7 monitoring

26:45.480 --> 26:51.160
i recommend to read the documentation on cilium.io um if you're using cilium or planning to use cilium

26:51.160 --> 26:56.200
you have questions go to our slack channel we're happy to help you there the community is out there

26:56.200 --> 27:02.920
and very helpful answering questions if you want to know more about eBPF go to eBPF.io we also have

27:03.560 --> 27:09.160
released or close to release a lab with this kind of dashboards as well so feel free to check them

27:09.160 --> 27:13.960
out at isovalent.com forward slash labs and if you want to know more about isovalent as a company

27:13.960 --> 27:20.280
or you may want to contribute we also have open uh positions for engineering as such so if you

27:20.280 --> 27:30.360
want to know more please check us out i'm happy to take questions any questions

27:37.320 --> 27:43.080
hello um thank you for a talk is it possible in the service graph of the hubble um

27:43.080 --> 27:51.720
um ui to show um transitive dependencies of services with the tracing enabled

27:54.440 --> 28:00.520
so with hubble ui the open source version you will see uh service to service community connectivity

28:00.520 --> 28:08.760
only so that related information is not integrated in hubble as such so you would switch between

28:08.760 --> 28:15.240
hubble and grafana to get that information on the enterprise we have been built in dashboards for

28:17.240 --> 28:18.600
getting that specific

28:21.720 --> 28:27.480
areas of monitoring so let's say application or node performance or cluster-wide performance we

28:27.480 --> 28:33.880
have some dashboards which should quickly highlight performance issues there okay thank you okay

28:33.880 --> 28:42.600
um other question hello did you measure the impact of a matrix my correct of matrix on

28:43.400 --> 28:48.920
network performance yeah we do have some performance related reports on cilium.io so

28:49.560 --> 28:55.560
yes it comes with a price using ebpf we keep it as low as possible it's a very hard question

28:55.560 --> 29:02.520
to answer because it will depend on which flags you configure right so if you have full layer

29:02.520 --> 29:07.880
seven visibility across all workloads in your cluster of course it will have some performance

29:07.880 --> 29:14.520
impact for sure yes using ebpf we keep it as low as possible um but yeah it's a multi-dimensional

29:14.520 --> 29:19.240
question depends on the amount of traffic the amount of applications how big your cluster is

29:19.240 --> 29:27.640
etc so we have some performance reports you can check so that's 500 nodes thousand network

29:27.640 --> 29:34.360
policies helpful enables and you get some feel of how memory consumption and processing is

29:34.360 --> 29:39.160
with cilium so feel free to check them out on the cilium.io website but in practice it's

29:39.720 --> 29:48.920
it's a multi-dimensional story welcome any other question in the back yeah hi thanks for the talk

29:48.920 --> 30:01.000
a couple questions about the integration of cilium on aks and gke is there anything specific

30:01.000 --> 30:10.120
regarding those implementations or all the tools work natively on on these kind of clusters

30:10.120 --> 30:18.520
and second questions regarding uh abo UI it's possible to see uh inter namespace

30:20.600 --> 30:26.440
traffic flows or is it limited to intran namespace okay good question so to answer the second

30:26.440 --> 30:33.160
question first yes you can see that so if there is communication between from a different namespace

30:33.160 --> 30:37.720
ingress to your namespace and monitoring you will see that you will see those labels and you will

30:37.720 --> 30:42.920
see the workloads even across clusters if you enable cluster mesh so yes that works out of the

30:42.920 --> 30:50.920
box on the cloud provider side um so if you run aks with azure cni powered by cilium you have a

30:50.920 --> 30:57.640
limited set of metrics which is are enabled and that's obviously from support reasons for microsoft

30:57.640 --> 31:04.680
to support that solution out of the box however you can also choose to bring your own cni with aks

31:04.680 --> 31:11.800
and that also applies to gke and eks to manage cilium yourself right so then you have the freedom

31:11.800 --> 31:17.720
to configure the flags i just shown and to configure configure cilium as such but keep

31:17.720 --> 31:22.600
in mind that you're responsible obviously of monitoring managing cilium um and the cloud

31:22.600 --> 31:30.680
provider will manage the cluster any other question cool we have to oh yeah sorry okay

31:30.680 --> 31:40.040
thank you very much thank you so much
