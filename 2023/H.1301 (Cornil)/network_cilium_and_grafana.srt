1
0:00:00.000 --> 0:00:08.280
All right, good afternoon everyone.

2
0:00:08.280 --> 0:00:14.200
Before I get started, how many of you attended the session about Service Mesh this afternoon?

3
0:00:14.200 --> 0:00:15.200
Sorry about that.

4
0:00:15.200 --> 0:00:19.640
If you're fed up with me, you have to stay with me for another session.

5
0:00:19.640 --> 0:00:25.880
I will repeat some parts of it to introduce the topic, but otherwise, I'm sorry about

6
0:00:25.880 --> 0:00:26.880
that.

7
0:00:26.880 --> 0:00:30.080
Welcome to the session, Golden Signals with Siljem and Grafana.

8
0:00:30.080 --> 0:00:31.080
My name is Ray MediÃ³n.

9
0:00:31.080 --> 0:00:34.560
I'm field CTO for EMEA at Isovalent.

10
0:00:34.560 --> 0:00:38.200
Isovalent is a company which originated Siljem.

11
0:00:38.200 --> 0:00:43.040
And today I'm going to talk about Grafana and how Siljem enables you to get golden signals

12
0:00:43.040 --> 0:00:45.520
out of the box.

13
0:00:45.520 --> 0:00:51.200
Introduction about technology, a little bit about EBPF and Siljem, then about observability

14
0:00:51.200 --> 0:00:56.960
challenges and how we can tackle those challenges using our observability.

15
0:00:56.960 --> 0:01:01.920
A bit on monitoring, day two operations and the default dashboards we provide.

16
0:01:01.920 --> 0:01:06.040
And then hopefully the demo gods are with me for a small demo to actually see how we

17
0:01:06.040 --> 0:01:15.520
get layer seven metrics and we can see return codes and monitor application response times,

18
0:01:15.520 --> 0:01:18.040
etc.

19
0:01:18.040 --> 0:01:20.800
So to start this topic, I want to introduce Siljem and EBPF.

20
0:01:20.800 --> 0:01:24.160
How many of you know about EBPF?

21
0:01:24.160 --> 0:01:25.660
Quite a lot, awesome.

22
0:01:25.660 --> 0:01:31.320
How many of you know about Siljem and are using Siljem?

23
0:01:31.320 --> 0:01:34.280
Cool, great.

24
0:01:34.280 --> 0:01:40.040
So Isovalent is the company behind each Siljem and EBPF.

25
0:01:40.040 --> 0:01:42.280
We maintain it with the community.

26
0:01:42.280 --> 0:01:48.120
And to start with, EBPF is explaining what EBPF is and how it works.

27
0:01:48.120 --> 0:01:52.960
So we like to say what JavaScript is to the browser, EBPF is to the kernel.

28
0:01:52.960 --> 0:01:56.440
What we mean by that is that we make it extensible in a dynamic way.

29
0:01:56.440 --> 0:02:02.320
So that means we're not changing the kernel, but at kernel speeds we can run programs based

30
0:02:02.320 --> 0:02:04.240
on kernel events.

31
0:02:04.240 --> 0:02:10.360
And for the context of this session, what's important is that considering metrics, considering

32
0:02:10.360 --> 0:02:15.680
getting useful information from your applications, what we're doing here is whenever a process

33
0:02:15.680 --> 0:02:22.000
opens a socket or a packet gets sent on the network device on the node, we can expose

34
0:02:22.000 --> 0:02:27.880
metrics or we can export those metrics using EBPF and we can use tools like Raffana to

35
0:02:27.880 --> 0:02:36.880
display it in a good way so you can do data operations or you can see how your application

36
0:02:36.880 --> 0:02:39.760
or cluster is performing.

37
0:02:39.760 --> 0:02:41.440
Siljem runs on EBPF.

38
0:02:41.440 --> 0:02:46.280
The good thing about Siljem is you don't need to be an EBPF engineer to run and operate

39
0:02:46.280 --> 0:02:47.280
Siljem.

40
0:02:47.280 --> 0:02:53.680
You just set certain configuration options and EBPF programs will be mounted on the nodes

41
0:02:53.680 --> 0:02:56.160
and will run when they need to run.

42
0:02:56.160 --> 0:03:02.520
And Siljem and the high level provides a number of capabilities, networking capabilities,

43
0:03:02.520 --> 0:03:08.920
observability capabilities, also surface mesh and using Tetracon solution we can also use

44
0:03:08.920 --> 0:03:17.520
runtime feasibility and observability security based on processes such as opening files,

45
0:03:17.520 --> 0:03:19.560
process execution, etc.

46
0:03:19.560 --> 0:03:22.960
Today we'll focus about on the observability part.

47
0:03:22.960 --> 0:03:30.640
So besides networking we can get rich visibility of metrics on your cluster.

48
0:03:30.640 --> 0:03:35.360
As you may know, Google uses on data plane V2 actually under the hood Siljem.

49
0:03:35.360 --> 0:03:36.840
Microsoft has moved.

50
0:03:36.840 --> 0:03:43.160
Azure default AKS clusters to Siljem so all the cloud providers see that Siljem is a powerful

51
0:03:43.160 --> 0:03:48.720
technology to run clusters at scale and to get useful metrics running them.

52
0:03:48.720 --> 0:03:51.520
So let's start with common challenges we see.

53
0:03:51.520 --> 0:03:55.880
One of the main challenges if we talk about performance of your application or performance

54
0:03:55.880 --> 0:04:01.440
of your cluster set scale is that you run into this issue of the finger pointing problem.

55
0:04:01.440 --> 0:04:06.680
What I mean by that is that network connectivity is layered and when you run into issues you

56
0:04:06.680 --> 0:04:13.440
need to, especially at scale, you need to be easily aware where a possible problem could

57
0:04:13.440 --> 0:04:14.440
exist.

58
0:04:14.440 --> 0:04:17.640
And it can be in multiple layers and if you look at the OZ layer obviously it can be at

59
0:04:17.640 --> 0:04:22.340
the data link layer, the network layer, the transport layer or in the application layer.

60
0:04:22.340 --> 0:04:28.300
So the goal of Siljem with Kefan is to give you the tools to quickly inspect what's going

61
0:04:28.300 --> 0:04:36.120
on and to be more efficient in troubleshooting your cluster and or applications.

62
0:04:36.120 --> 0:04:40.800
Another common issue, especially at scale, is obviously the signal to noise problem.

63
0:04:40.800 --> 0:04:46.600
You may run in the clouds, you get data from your nodes, you see IP addresses communicating

64
0:04:46.600 --> 0:04:47.900
with each other.

65
0:04:47.900 --> 0:04:51.240
But IP addresses by itself mean nothing in Kubernetes clusters.

66
0:04:51.240 --> 0:04:57.300
They come, they go and at scale it's impossible to track and trace what's going on with service

67
0:04:57.300 --> 0:05:02.280
to service communication in your applications.

68
0:05:02.280 --> 0:05:05.280
Also where are existing mechanisms falling short?

69
0:05:05.280 --> 0:05:11.200
So first of all network devices, think about centralized monitoring or firewall solutions,

70
0:05:11.200 --> 0:05:12.640
think about Splunk.

71
0:05:12.640 --> 0:05:19.520
They are great to get our alerts, to get dashboards, but again at scale they can be very costly

72
0:05:19.520 --> 0:05:21.940
or they can be a bottleneck.

73
0:05:21.940 --> 0:05:27.400
Also these devices don't have awareness of the identities of your applications running

74
0:05:27.400 --> 0:05:30.240
on your Kubernetes clusters.

75
0:05:30.240 --> 0:05:34.920
Cloud provider network logs are nice for node to node communication but don't provide identity

76
0:05:34.920 --> 0:05:37.000
as well.

77
0:05:37.000 --> 0:05:44.120
You can monitor the host, you can see Linux host statistics but that gives you only visibility

78
0:05:44.120 --> 0:05:50.440
on the node level and again a Linux node doesn't have any awareness of the identities of the

79
0:05:50.440 --> 0:05:53.580
services running in your Kubernetes cluster.

80
0:05:53.580 --> 0:05:57.800
You may want to try to modify application code and this applies a bit to the service

81
0:05:57.800 --> 0:05:58.800
mesh session.

82
0:05:58.800 --> 0:06:03.360
You may want to install libraries which gives you visibility of the application but then

83
0:06:03.360 --> 0:06:07.800
you don't have visibility on the networking layer.

84
0:06:07.800 --> 0:06:13.040
Service mesh, main goal of service mesh is obviously visibility of the network and trend

85
0:06:13.040 --> 0:06:18.000
communication to get metrics out of that but that with the sidecar based implementation

86
0:06:18.000 --> 0:06:24.640
comes with a footprint and with induced latency plus you have operational complexity maintaining

87
0:06:24.640 --> 0:06:29.400
your service mesh solution on top of your Kubernetes clusters.

88
0:06:29.400 --> 0:06:37.320
So this is where Cilium comes around the corner instead we provide identity aware based observability

89
0:06:37.320 --> 0:06:38.960
and security.

90
0:06:38.960 --> 0:06:46.440
What that means is based on the labels you set on your workloads we create unique identities

91
0:06:46.440 --> 0:06:52.680
and we're able to attach that identity in the data plane using eBPF and using that identity

92
0:06:52.680 --> 0:06:54.260
we can do things with that.

93
0:06:54.260 --> 0:06:59.640
So we can secure the connectivity in this example a front end to a back end is allowed

94
0:06:59.640 --> 0:07:05.360
to communicate based on the network policies we set and identities we are aware of and

95
0:07:05.360 --> 0:07:10.720
this identity is a cluster wide property but in terms of observability this also means

96
0:07:10.720 --> 0:07:17.080
that we can use this identity to get rich metrics and data for that identity and you

97
0:07:17.080 --> 0:07:19.520
can monitor it effectively.

98
0:07:19.520 --> 0:07:24.400
This means that you're not looking anymore at IP addresses you're looking at identities

99
0:07:24.400 --> 0:07:29.800
so the whole set of workloads the surface to surface communication for a front end to

100
0:07:29.800 --> 0:07:33.840
a back end.

101
0:07:33.840 --> 0:07:39.280
Hubble is our observability solution built on top of Cilium how it works is that Cilium

102
0:07:39.280 --> 0:07:46.480
runs as a democet on your cluster nodes as an agent and Hubble can retrieve data from

103
0:07:46.480 --> 0:07:53.640
those agents through a CLI or UI and we can export metrics based on your workloads.

104
0:07:53.640 --> 0:07:58.680
So there are three parts first of all the Hubble UI gives a service dependency map so

105
0:07:58.680 --> 0:08:05.080
on a namespace level you can see what's deployed what is communicating with each other what

106
0:08:05.080 --> 0:08:11.040
kind of protocols they're using what's coming between namespaces so you would see for example

107
0:08:11.040 --> 0:08:16.800
if there's inter namespace communication you can identify the source namespace if you use

108
0:08:16.800 --> 0:08:23.040
cluster mesh you can even identify the source cluster you can also identify egress traffic

109
0:08:23.040 --> 0:08:28.900
and ingress traffic on a namespace level the Hubble CLI is more a power users tool to give

110
0:08:28.900 --> 0:08:35.200
you detailed flow you can export it to JSON you can do a lot of filtering based on labels

111
0:08:35.200 --> 0:08:40.640
Hubble metrics is the part where it's mostly the topic for today is where you export metrics

112
0:08:40.640 --> 0:08:46.480
and you use Grafana for example to observe the performance of your cluster and application

113
0:08:46.480 --> 0:08:51.600
this is all fueled through eBPF so again think about a network device sending a packet that's

114
0:08:51.600 --> 0:08:59.840
a kernel event eBPF program gets attached to it gets the metrics and it's done this

115
0:08:59.840 --> 0:09:06.760
is a small screenshot of the CLI so this gives you flow visibility using Hubble observe

116
0:09:06.760 --> 0:09:11.880
commands you can follow for example based on the label in this case xwing so we're following

117
0:09:11.880 --> 0:09:18.840
all the workloads labels with xwing so again no IP addresses just labels in purple is highlighted

118
0:09:18.840 --> 0:09:24.680
these IDs we use so again each unique set of labels gets a unique cluster wide ID and

119
0:09:24.680 --> 0:09:31.160
based on those IDs we can track based on labels what communication is going on and there's

120
0:09:31.160 --> 0:09:37.240
a lot of metadata you can filter on things like headers things like ports things like

121
0:09:37.240 --> 0:09:44.920
protocols obviously labels in QNAD spot names surfaces worker nodes DNS we also have cilium

122
0:09:44.920 --> 0:09:52.200
network policies which allow you to filter and observe two FQDN rules meaning we can inspect

123
0:09:53.000 --> 0:09:59.560
queries to external domains and we can filter based on that and obviously cilium related

124
0:09:59.560 --> 0:10:05.720
identities such as world ingress egress host and that kind of stuff policy verdict matches

125
0:10:06.440 --> 0:10:13.960
things like dropped allowed and stuff this is the Hubble UI service map like I said before

126
0:10:13.960 --> 0:10:19.240
this gives you a namespace level view in this case we have a jobs app and I'm using this app

127
0:10:19.240 --> 0:10:25.160
as well in the demo I'm showing a bit later so here you're looking at a namespace level view

128
0:10:25.160 --> 0:10:29.640
where you can see all the service to service communication of your application running in that

129
0:10:29.640 --> 0:10:36.360
namespace in this case it's only intran namespace communication and you can see for example that

130
0:10:36.360 --> 0:10:42.120
the recruiter is talking to core API the core API is talking to elastic search we have a zookeeper

131
0:10:42.120 --> 0:10:49.240
component we identify Kafka also identifying Kafka protocols so there are a number of protocols we

132
0:10:49.240 --> 0:10:56.760
can inspect and see and we also see layer 7 information so in this case HTTP calls to a

133
0:10:56.760 --> 0:11:02.280
specific URI or URL with specific method and return calls and this is triggered through

134
0:11:03.000 --> 0:11:09.880
just simple construct as a cilium network policy if you just allow let's say intran namespace

135
0:11:09.880 --> 0:11:20.040
traffic and you are accepting HTTP that already triggers this visibility for you to see now using

136
0:11:20.040 --> 0:11:28.440
this data we can also export metrics to Grafana so we have into we are working with Grafana a lot

137
0:11:28.440 --> 0:11:35.480
more lately that means that we are building a lot of more useful dashboards and also integrating

138
0:11:35.480 --> 0:11:42.040
with things like tempo for getting transparent tracing in Grafana all powered through cilium and

139
0:11:42.040 --> 0:11:50.040
eBPF this allows us to not only see on the network level metrics on performance on the node but also

140
0:11:50.040 --> 0:11:56.040
for service to service communication to provide golden signals things like HTTP request rate

141
0:11:56.040 --> 0:12:02.520
latency response codes and error codes which would as an application engineer would allow you to

142
0:12:02.520 --> 0:12:09.000
quickly see which components of your application is not responding as it should

143
0:12:11.880 --> 0:12:16.680
but also detecting transduced network layers so this will be more network related we may see

144
0:12:16.680 --> 0:12:22.440
retransmissions we can see byte sent and received and we can indicate things like

145
0:12:22.440 --> 0:12:27.880
round-trip time to indicate a network layer problem so maybe in a data center you have a

146
0:12:27.880 --> 0:12:33.000
specific reg switch or top of reg switch not performing as it should so nodes connected to

147
0:12:33.000 --> 0:12:40.120
that switch will have improved will have reduced performance and you would see latency increasing

148
0:12:43.320 --> 0:12:50.040
now with the latest dashboards we also able to see programmatic API requests using transparent

149
0:12:50.040 --> 0:12:57.480
tracing this goes to the integration with Grafana so at the moment your application

150
0:12:57.480 --> 0:13:03.240
need to be able to support it so you need to be able to inject traces but we're working on

151
0:13:03.880 --> 0:13:09.640
out of the box getting also this support and be able to help by help support HTTP

152
0:13:10.840 --> 0:13:15.960
traces as such and then you get this exemplar so i'm pointing at a small exemplar here

153
0:13:16.600 --> 0:13:22.360
after which you can inspect this with with tempo and you can see a span of specific request

154
0:13:22.360 --> 0:13:30.920
and see where the problem may reside a bit more on monitoring so this is more day to ops

155
0:13:30.920 --> 0:13:36.440
i want to highlight that if you run cilium and you are also using Grafana make sure that you

156
0:13:36.440 --> 0:13:42.040
install the agent hubble and operator metrics plugins these are out of the box plugins we

157
0:13:42.040 --> 0:13:47.640
provide through the Grafana marketplace you can download and this gives you visibility

158
0:13:47.640 --> 0:13:52.760
in the performance of your cluster so first of all agent metrics everything on the node level how

159
0:13:52.760 --> 0:13:59.560
the node is performing how many throughputs they are processing how many memory the bpf is using

160
0:13:59.560 --> 0:14:06.280
all that related stuff hubble metrics gives you the visibility across your cluster in terms of

161
0:14:06.280 --> 0:14:14.440
application layer seven return calls policy verdicts so allows versus drops so you can

162
0:14:14.440 --> 0:14:21.400
monitor on the cluster level the performance of your applications and in some cases you

163
0:14:21.400 --> 0:14:27.000
run an operator so you may want to track the number of identities how the cluster in general

164
0:14:27.000 --> 0:14:34.440
is behaving api responses and such and finally what we released just a few days ago thanks to

165
0:14:34.440 --> 0:14:40.440
rafael who is also here is the cilium policy verdict metrics dashboard which gives you the

166
0:14:40.440 --> 0:14:50.520
capability to get meaningful graphs if you have workloads actually hitting network policies you

167
0:14:50.520 --> 0:14:56.360
set what i mean by that is that when we work with customers with cilium is they want to go to this

168
0:14:56.360 --> 0:15:02.680
micro segmentation zero trust model and you can use obviously hubble to monitor service-to-service

169
0:15:02.680 --> 0:15:10.600
communication and to see if traffic is allowed and denied but this dashboard also is a very useful

170
0:15:10.600 --> 0:15:16.120
tool to confirm if you have either ingress or egress policies which are matching with your traffic

171
0:15:16.680 --> 0:15:23.160
so in this case we see a green graphs which means that on ingress and egress we have matching

172
0:15:23.160 --> 0:15:30.520
traffic the purple represents dns matching traffic but if there's some yellow traffic that's either

173
0:15:30.520 --> 0:15:36.840
allow all match traffic which is too broadly which should trigger you to get even better

174
0:15:36.840 --> 0:15:42.760
network policies to make sure that kind of flows are actually related to a network policy to ensure

175
0:15:43.320 --> 0:15:48.760
that both ingress and egress traffic is secured as such if you do so all the grounds will turn

176
0:15:48.760 --> 0:15:53.640
green and you know and you can confirm for that given namespace that you have securities

177
0:15:53.640 --> 0:16:04.600
all right i've prepared a little demo this runs this tenon jobs application i mentioned before

178
0:16:06.440 --> 0:16:12.360
i'm running this on kind so just a simple kind cluster on my laptop i'm showing here the

179
0:16:12.360 --> 0:16:18.600
components of my application so this you know a number of of workloads i've shown before on a

180
0:16:18.600 --> 0:16:25.400
grid screenshot to help me through this demo i've created a little script and what this does it only

181
0:16:25.400 --> 0:16:32.520
updates a helm chart for this application so it makes my workflow a lot easier i don't have to

182
0:16:32.520 --> 0:16:39.800
enter commands but we should see some things changing in our confana dashboard before i

183
0:16:39.800 --> 0:16:50.600
start this let me highlight the metrics so i need to log in so i've installed graphana i've

184
0:16:50.600 --> 0:16:54.920
installed tempo i've installed prometheus and configured cilium to export those metrics

185
0:16:55.560 --> 0:17:00.680
so this is currently the performance of my application running on my kind cluster on my laptop

186
0:17:01.480 --> 0:17:04.840
so as you can see we have 100 success rate we have

187
0:17:04.840 --> 0:17:13.880
uh incoming 100 and we also have good graphs of information for the performance of the application

188
0:17:13.880 --> 0:17:19.800
okay so let me start with starting this script

189
0:17:22.200 --> 0:17:29.400
yes so i mentioned before that the hubble metrics are available as soon as you configure some kind

190
0:17:29.400 --> 0:17:34.200
of layer seven cilium network policy because that triggers the collection of those metrics

191
0:17:34.200 --> 0:17:42.600
for layer seven and i'm showing this but i will show this a bit better in in in a different window

192
0:17:43.800 --> 0:17:49.640
so what i'm going to do now is i want to increase the request volume so i'm configuring the crawler

193
0:17:49.640 --> 0:17:54.600
component to get more requests in my application as you can see it's redeploying the crawler

194
0:17:54.600 --> 0:18:02.600
component so this would is something we should see in the graphana dashboard while this is redeploying

195
0:18:02.600 --> 0:18:08.520
i can show the helm chart i've used i'm using you need to be a bit patient with me because it takes

196
0:18:08.520 --> 0:18:14.360
one minute before the graphics the the graphana dashboards are updated you can actually see

197
0:18:15.080 --> 0:18:20.760
the impact of this new version of the application so typically you configure cilium through a helm

198
0:18:20.760 --> 0:18:27.000
values file so in this case on the operator component i've enabled metrics and prometheus

199
0:18:27.000 --> 0:18:33.800
on the hubble side i've configured hubble relay together the metrics and also prometheus and metrics

200
0:18:34.920 --> 0:18:39.640
so and this part is very interesting because if you want to have layer seven visibility you need

201
0:18:39.640 --> 0:18:46.120
to have specific metrics being enabled this will be documented in the cilium i.o website

202
0:18:46.120 --> 0:18:51.880
once we have the new release ready so as you can see we are matching HTTP v2 we have enabled

203
0:18:51.880 --> 0:18:59.000
exemplars we are looking for labels in terms of context source ip source namespace etc so these

204
0:18:59.000 --> 0:19:06.040
are important sets of labels you need to set and on the prometheus side we've enabled it to get at

205
0:19:06.040 --> 0:19:13.560
the graphs the cilium network policy i mentioned before this is just a simple example we allow

206
0:19:13.560 --> 0:19:23.160
everything within the namespace we have enabled dns visibility so we're inspecting all dns traffic

207
0:19:23.160 --> 0:19:31.880
to cube dns that allows us to get visibility of dns queries we've enabled ingress and egress

208
0:19:31.880 --> 0:19:37.800
for the purpose of the demo so we can also see that traffic and what's important is that we have

209
0:19:37.800 --> 0:19:44.680
an empty or open rule HTTP which allows us to see all traffic it allows all traffic

210
0:19:45.400 --> 0:19:53.880
but that triggers the collection of metrics all right so on the demo site so it has deployed a

211
0:19:53.880 --> 0:20:04.120
new version of my application looking at my metrics i can see incoming request volume increasing

212
0:20:04.120 --> 0:20:11.640
so you see already an increase of volume we also see requests by source response codes increasing

213
0:20:11.640 --> 0:20:17.160
so still 200 always fine always good just a number just an increase of requests per seconds

214
0:20:18.120 --> 0:20:22.280
and also on the incoming site okay all good

215
0:20:22.280 --> 0:20:33.640
okay let's now deploy a new configuration of our app and this app is has an error

216
0:20:35.320 --> 0:20:42.040
so let's see what we can see there again redeploying the core api components

217
0:20:43.560 --> 0:20:48.120
and now we should be able to see the error rate increase as a result of core api configuration

218
0:20:48.120 --> 0:20:53.160
changing so this will take one minute

219
0:20:55.560 --> 0:21:00.680
here i can select the destination workload so i can switch between core api or the loader

220
0:21:00.680 --> 0:21:05.320
component to see how the traffic for that destination is matching and how it's performing

221
0:21:06.280 --> 0:21:12.840
let's give it a few seconds to actually show what i'm looking for is the incoming request success

222
0:21:12.840 --> 0:21:19.880
rate apparently this application has version as an error so the success rate will be lower than before

223
0:21:27.000 --> 0:21:29.160
okay it's running okay

224
0:21:29.160 --> 0:21:40.120
okay yeah it's always a bit takes a bit longer than i wanted

225
0:21:49.080 --> 0:21:54.600
bear with me you know what in the meantime i'll already start the next step so i can

226
0:21:54.600 --> 0:21:58.600
i don't have you waiting

227
0:22:03.720 --> 0:22:10.520
ah there you go so here you see that the success rate is decreasing because of this faulty version

228
0:22:10.520 --> 0:22:15.960
of my application so i can really see there's something wrong with my application and as

229
0:22:15.960 --> 0:22:22.200
application developer or owning this namespace i should now be able to investigate what's going on

230
0:22:22.200 --> 0:22:28.600
this also means that here on the incoming request by source and response code i would see

231
0:22:29.160 --> 0:22:36.840
the resumes components showing 500 and 503 error return codes which triggers me to check that

232
0:22:36.840 --> 0:22:45.000
component and communication between those components also on the destination site all right

233
0:22:45.000 --> 0:22:58.840
so now i've introduced a new version and what this does is changing the request duration so

234
0:22:58.840 --> 0:23:03.240
again a new version of the application and let's see how we can monitor this performance of the

235
0:23:03.240 --> 0:23:09.000
application in grafana so let's check the request duration increase as a result of core api

236
0:23:09.000 --> 0:23:20.760
configuration changing okay so let's use here so i'm monitoring http request duration by source

237
0:23:20.760 --> 0:23:25.880
and destination so if the demo works well we see an increase there

238
0:23:25.880 --> 0:23:31.080
so

239
0:23:35.080 --> 0:23:35.400
okay

240
0:23:41.000 --> 0:23:45.640
takes a bit too long uncomfortable with but should be there any minutes

241
0:23:45.640 --> 0:23:56.680
with me

242
0:24:04.040 --> 0:24:05.240
should appear any moment

243
0:24:09.560 --> 0:24:10.200
let me just

244
0:24:10.200 --> 0:24:18.440
post in the meantime i will deploy a new version of the application which also introduces

245
0:24:18.440 --> 0:24:25.080
tracing so again for tracing to be supported you at this moment your application needs support that

246
0:24:26.280 --> 0:24:30.360
so in this case i deploy a new version of this application to support tracing

247
0:24:32.840 --> 0:24:37.880
and this is using open telemetry so let's deploy that in the meantime

248
0:24:37.880 --> 0:24:46.120
that's deploying in the meantime i can check how the request duration is doing

249
0:24:50.920 --> 0:24:56.680
okay this part is not working yet but we should see a request duration increase

250
0:24:56.680 --> 0:25:06.040
oh god yeah thanks that doesn't help i clicked on something ah yes thank you so much

251
0:25:07.880 --> 0:25:14.520
yeah here you can see the request duration increasing and i just deployed a new version

252
0:25:15.720 --> 0:25:20.840
of my application which supports tracing using open telemetry and then you already can see that

253
0:25:20.840 --> 0:25:29.080
i have this exemplars appearing so i now only can not only inspect htp request duration

254
0:25:29.080 --> 0:25:34.360
but i can also inspect specific traces and exemplars so let's if you click on this little

255
0:25:35.000 --> 0:25:41.320
box you get this window you can get valuable information about this trace point and then you

256
0:25:41.320 --> 0:25:50.840
can query it with track tempo go yep let's leave this side so here you can see a specific trace id

257
0:25:50.840 --> 0:25:57.160
and you can see a node graph so this is also nice you can see between nodes what's what's going on

258
0:25:57.160 --> 0:26:05.000
and you see highlighted in red here what's has a high latency as such and here we can see that in

259
0:26:05.000 --> 0:26:17.880
this specific uh api call there is an error so post scroll and it has some events exception

260
0:26:18.600 --> 0:26:22.200
random error so something is wrong with my application so this enables me

261
0:26:22.200 --> 0:26:28.680
as an application owner to troubleshoot my application effectively so this concludes the

262
0:26:28.680 --> 0:26:38.440
demo um let me quickly move to here all right so if you want to know more how to configure cilium

263
0:26:38.440 --> 0:26:45.480
to enable metrics how to configure cilium to uh with the right values for layer 7 monitoring

264
0:26:45.480 --> 0:26:51.160
i recommend to read the documentation on cilium.io um if you're using cilium or planning to use cilium

265
0:26:51.160 --> 0:26:56.200
you have questions go to our slack channel we're happy to help you there the community is out there

266
0:26:56.200 --> 0:27:02.920
and very helpful answering questions if you want to know more about eBPF go to eBPF.io we also have

267
0:27:03.560 --> 0:27:09.160
released or close to release a lab with this kind of dashboards as well so feel free to check them

268
0:27:09.160 --> 0:27:13.960
out at isovalent.com forward slash labs and if you want to know more about isovalent as a company

269
0:27:13.960 --> 0:27:20.280
or you may want to contribute we also have open uh positions for engineering as such so if you

270
0:27:20.280 --> 0:27:30.360
want to know more please check us out i'm happy to take questions any questions

271
0:27:37.320 --> 0:27:43.080
hello um thank you for a talk is it possible in the service graph of the hubble um

272
0:27:43.080 --> 0:27:51.720
um ui to show um transitive dependencies of services with the tracing enabled

273
0:27:54.440 --> 0:28:00.520
so with hubble ui the open source version you will see uh service to service community connectivity

274
0:28:00.520 --> 0:28:08.760
only so that related information is not integrated in hubble as such so you would switch between

275
0:28:08.760 --> 0:28:15.240
hubble and grafana to get that information on the enterprise we have been built in dashboards for

276
0:28:17.240 --> 0:28:18.600
getting that specific

277
0:28:21.720 --> 0:28:27.480
areas of monitoring so let's say application or node performance or cluster-wide performance we

278
0:28:27.480 --> 0:28:33.880
have some dashboards which should quickly highlight performance issues there okay thank you okay

279
0:28:33.880 --> 0:28:42.600
um other question hello did you measure the impact of a matrix my correct of matrix on

280
0:28:43.400 --> 0:28:48.920
network performance yeah we do have some performance related reports on cilium.io so

281
0:28:49.560 --> 0:28:55.560
yes it comes with a price using ebpf we keep it as low as possible it's a very hard question

282
0:28:55.560 --> 0:29:02.520
to answer because it will depend on which flags you configure right so if you have full layer

283
0:29:02.520 --> 0:29:07.880
seven visibility across all workloads in your cluster of course it will have some performance

284
0:29:07.880 --> 0:29:14.520
impact for sure yes using ebpf we keep it as low as possible um but yeah it's a multi-dimensional

285
0:29:14.520 --> 0:29:19.240
question depends on the amount of traffic the amount of applications how big your cluster is

286
0:29:19.240 --> 0:29:27.640
etc so we have some performance reports you can check so that's 500 nodes thousand network

287
0:29:27.640 --> 0:29:34.360
policies helpful enables and you get some feel of how memory consumption and processing is

288
0:29:34.360 --> 0:29:39.160
with cilium so feel free to check them out on the cilium.io website but in practice it's

289
0:29:39.720 --> 0:29:48.920
it's a multi-dimensional story welcome any other question in the back yeah hi thanks for the talk

290
0:29:48.920 --> 0:30:01.000
a couple questions about the integration of cilium on aks and gke is there anything specific

291
0:30:01.000 --> 0:30:10.120
regarding those implementations or all the tools work natively on on these kind of clusters

292
0:30:10.120 --> 0:30:18.520
and second questions regarding uh abo UI it's possible to see uh inter namespace

293
0:30:20.600 --> 0:30:26.440
traffic flows or is it limited to intran namespace okay good question so to answer the second

294
0:30:26.440 --> 0:30:33.160
question first yes you can see that so if there is communication between from a different namespace

295
0:30:33.160 --> 0:30:37.720
ingress to your namespace and monitoring you will see that you will see those labels and you will

296
0:30:37.720 --> 0:30:42.920
see the workloads even across clusters if you enable cluster mesh so yes that works out of the

297
0:30:42.920 --> 0:30:50.920
box on the cloud provider side um so if you run aks with azure cni powered by cilium you have a

298
0:30:50.920 --> 0:30:57.640
limited set of metrics which is are enabled and that's obviously from support reasons for microsoft

299
0:30:57.640 --> 0:31:04.680
to support that solution out of the box however you can also choose to bring your own cni with aks

300
0:31:04.680 --> 0:31:11.800
and that also applies to gke and eks to manage cilium yourself right so then you have the freedom

301
0:31:11.800 --> 0:31:17.720
to configure the flags i just shown and to configure configure cilium as such but keep

302
0:31:17.720 --> 0:31:22.600
in mind that you're responsible obviously of monitoring managing cilium um and the cloud

303
0:31:22.600 --> 0:31:30.680
provider will manage the cluster any other question cool we have to oh yeah sorry okay

304
0:31:30.680 --> 0:31:40.040
thank you very much thank you so much

