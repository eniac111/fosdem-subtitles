WEBVTT

00:00.000 --> 00:11.520
So far, your stage is yours. I'm really looking forward to hearing about on-premise data centers

00:11.520 --> 00:22.600
do not need to be a legacy. Thank you. So, hello everyone. And just to be clear, this is going to

00:22.600 --> 00:29.320
be the topic that we are going to cover a little bit of history, some lesser-learned, and then

00:29.320 --> 00:36.440
some technology bets that I think that would make sense in such conversation. So, about me, I have

00:36.440 --> 00:43.240
been a Linux user for 20-ish years. I've been working with Linux for close to 20 years now,

00:43.240 --> 00:50.520
and I currently work with Red Hat and do basically similar kind of conversation in my day-to-day job.

00:50.520 --> 00:57.400
So, let's start with a little bit of history of the cloud. Let's call it this way. So,

00:57.400 --> 01:07.160
Rackspace got founded in 1998 and I think was the first company that defined itself as cloud. In

01:07.160 --> 01:14.800
2005, Softlayer was founded. They defined themselves as bare-meta cloud. And then in 2006,

01:14.800 --> 01:26.040
we have the S3 launched by AWS, which was the first service of AWS. 2006 again, EC2. Sorry,

01:26.040 --> 01:35.720
yeah. And then Google App Engine arrived, IBM bought Softlayer, creating it, IBM Cloud, now

01:35.720 --> 01:48.640
called. And by 2021, AWS has more than 200 different services. So, what about the history

01:48.640 --> 01:54.800
of the known cloud? Because what we have seen are old cloud environments, but those are nothing

01:54.800 --> 02:04.200
new if you think about this. So, in 1964, which is probably older than anyone or most of the people

02:04.200 --> 02:16.800
in this room, IBM introduced the CP40 and this machine had time-sharing technology, which was

02:16.800 --> 02:25.760
very different from what we call today cloud, but still it was probably the initial point of the

02:25.760 --> 02:36.520
history of the cloud. And in the late 60s, IBM released Simon, which is an hypervisor. By 1974,

02:36.520 --> 02:44.640
the two kinds of hypervisor get defined as step one, the bare-meta virtualization, step two,

02:44.640 --> 02:54.000
the hosted virtualization. And by 1998, VMware got founded and in 2000s, majority of companies

02:54.000 --> 03:06.480
moved from bare-meta to VMware's VMs. 2001, ESX got released, which was type one kind of virtualization.

03:06.480 --> 03:16.080
2003, we have the first type one open source virtualization, SAN. And in still 2003, VMware

03:16.080 --> 03:23.560
introduces Vmotion, which allows you to basically move a machine from one host to the other without

03:23.560 --> 03:31.440
rebooting it. And 2008, Microsoft arrives with Hyper-V. It previously had some other kind of

03:31.440 --> 03:37.760
virtualization tool, but Hyper-V got launched in 2008. So what is the cloud? Why we are

03:37.760 --> 03:45.560
distinguished the first group and the second one? Wikipedia says that cloud computing is the

03:45.560 --> 03:50.960
on-demand availability of computing system resources, especially data storage and computing

03:50.960 --> 03:56.240
power without direct active management by the user. I don't think this is a good definition.

03:56.240 --> 04:05.240
I think that a better definition is a business model where one party runs to a second party

04:05.240 --> 04:11.160
computer system resources, especially data storage, cloud storage and computing power,

04:11.160 --> 04:18.000
with the smallest granularity possible. And my point is, cloud is not technical,

04:18.000 --> 04:27.640
it's a business model. And if you think about, we move from renting machines like VPS on a

04:27.640 --> 04:35.880
monthly basis, and then AWS introduced the concept of EC2 that was initially on an hourly

04:35.880 --> 04:41.800
basis and then minute and then second, and now you can buy lambdas or similar kind of

04:41.800 --> 04:49.640
things for milliseconds. And in a way, also, CPUs had the same shrinkage. So we move from

04:49.640 --> 05:00.680
full CPUs or sockets to VCPUs, which basically is hyper-threaded threads, to fractional VCPUs

05:00.680 --> 05:07.280
with lambdas or similar services. So my point being, the whole thing about cloud is not

05:07.280 --> 05:14.560
technical, is only about the business side of it. So what can we learn from not only

05:14.560 --> 05:22.960
the last 20 years of what we can define as cloud, but also the previous 50 of what we

05:22.960 --> 05:28.440
can define as non-cloud, and more specifically, because we have seen that the cloud model

05:28.440 --> 05:36.520
actually works. The non-cloud model was not very functional to the business, to the point

05:36.520 --> 05:45.920
that very often those data centers got outsourced or in some different ways moved to the cloud

05:45.920 --> 05:54.220
in the sense that moved to someone else. And the business started to expand constantly

05:54.220 --> 06:03.400
those machines due to the basically OPEX model instead of the CAPEX model. So there is one

06:03.400 --> 06:08.640
big aspect that we need to remember about this, which is the separation of concerns.

06:08.640 --> 06:14.840
So standardize the interface between the infrastructure and the workload. If you go in legacy data

06:14.840 --> 06:21.760
centers, very often you have 1,000 different kind of systems that the infra people have

06:21.760 --> 06:27.920
to provide to the workload people. And this is because, oh, my system is different, my

06:27.920 --> 06:35.760
software is different, whatever. In the end of the day, that is a huge load for the infrastructure

06:35.760 --> 06:44.160
part of the business. Second, the scalability needs to be at workload level. So the infrastructure

06:44.160 --> 06:52.760
also needs to be somehow reliable and within some SLAs. But if the system has to stay up,

06:52.760 --> 06:57.280
if the application has to stay up, the application will have to take care about this.

06:57.280 --> 07:04.960
And third, workload have an abstract concept of whatever is underneath it. So the physical

07:04.960 --> 07:11.560
architecture. They don't need to know which data center they are in or in which rack,

07:11.560 --> 07:21.120
what is the nearby server, and so on. The function, so we also need a functional business

07:21.120 --> 07:31.640
model for a good managed IT system. And the first part is, as before, standardize the

07:31.640 --> 07:39.720
interface between the workload and infrastructure so that it's easily countable and priceable.

07:39.720 --> 07:47.040
Second, build back the infrastructure cost to the workload owners. We have seen, at least

07:47.040 --> 07:52.680
in my definition of cloud, that we still have two parties. One that delivers a service and

07:52.680 --> 07:58.240
the other one that consumes it and pays for it. So it's very important to create this

07:58.240 --> 08:07.240
also internally in companies or organizations of any kind because this allows the infrastructure

08:07.240 --> 08:17.560
side of the business to justify their expenses over some kind of at least recognition of

08:17.560 --> 08:24.760
revenue or whatever, cost recovery, whatever. And third, keep the cost down. This is a key

08:24.760 --> 08:31.800
point. AWS, Google, those companies will do everything they can to keep the cost down

08:31.800 --> 08:38.600
because they need to be positive, cash flow positive. Obviously, if you are a department

08:38.600 --> 08:43.760
in a company, it's slightly different, but it's very important to still be cash flow

08:43.760 --> 08:50.960
positive because this will guarantee you that you will not have any issues over time with

08:50.960 --> 08:59.440
this part of the financial model. And third, maintain control. We have seen the clouds

08:59.440 --> 09:05.240
are obsessed about maintaining control and obtaining even more control on their hardware,

09:05.240 --> 09:11.040
their system, whatever. And this is very important for your own cloud if you want to be able

09:11.040 --> 09:18.920
to maintain it for 10, 20, 50 years. So the first one is, I would say do not use, but

09:18.920 --> 09:25.800
be very cautious on using third-party properties software. Those companies can go away, can

09:25.800 --> 09:31.920
change pricing model, can do whatever, be aware of this. Second, evaluate very strongly

09:31.920 --> 09:41.040
the buy versus build decision because when you buy, obviously it's here now, but you

09:41.040 --> 09:48.880
don't have the know-how about this. So probably you will want to build a lot of your systems,

09:48.880 --> 09:53.720
not the core parts, but maybe the dashboard layer or that kind of thing so that you can

09:53.720 --> 10:01.160
effectively manage it however you think better. And third, be very aware of lockings because

10:01.160 --> 10:12.640
those will bite you over the course of the years. So, okay. How do I define the locking?

10:12.640 --> 10:18.280
I define it as the product between the probability that a component will require substitution

10:18.280 --> 10:24.240
during the solution lifetime and the total cost of the substitution. So for instance,

10:24.240 --> 10:31.040
Linux, if you base all your architecture on Linux, it's going to be very expensive to

10:31.040 --> 10:37.480
move out of Linux. But in the other hand, it's very improbable that you will be needed

10:37.480 --> 10:46.640
to do it because very probably in 10, 20 years, Linux will be here. So a couple of points

10:46.640 --> 10:53.360
on technologies. The first one is keep the complexity of your system at the lowest level

10:53.360 --> 10:59.440
possible. Systems will get more complex and more absurd over time. So at least at the

10:59.440 --> 11:06.040
beginning, start with the simple thing possible. Second, prefer build time complexity over

11:06.040 --> 11:12.880
runtime complexity. It's way easier to automate a build thing than to automate something to

11:12.880 --> 11:18.920
be run. And also when something breaks, it's better if it's simple because it's easier

11:18.920 --> 11:26.760
to fix. If you have to compile yourself, compile it, but try to keep the complexity at the

11:26.760 --> 11:35.800
runtime at the minimum possible. Third, minimize the amount of services that you deliver to

11:35.800 --> 11:43.200
your business or your workloads owners so that effectively you can guarantee that those

11:43.200 --> 11:47.760
services are exactly what they require and you are able to deliver them in a sensible

11:47.760 --> 11:57.240
way. So I think that one big point is containers. Delivering a container-based solution at least,

11:57.240 --> 12:03.960
it's probably the best option I think today. And use a Kubernetes distribution, whatever

12:03.960 --> 12:12.400
you prefer and choose. That makes sense. It's fine. And we'll see later the Kubernetes

12:12.400 --> 12:20.480
APIs are now fairly well-known, fairly abstract, and fairly used so that those can be a good

12:20.480 --> 12:27.360
interface between the infra and the workload side. Also you can use a do-it-yourself community,

12:27.360 --> 12:34.040
call it whatever, a distribution. You can buy a commercial distribution of Kubernetes.

12:34.040 --> 12:40.560
If you do it, first, be sure that it's fully open source, what you're buying, so that you

12:40.560 --> 12:46.400
decrease your lock-in because you are decreasing the cost that it will take you to move from

12:46.400 --> 12:52.640
this to any other solution. Second, from a trustworthy company, hopefully that company

12:52.640 --> 12:57.720
that you buy it from will not fail tomorrow because if it does, you will have a bigger

12:57.720 --> 13:05.080
problem. And third, with a long track record of not screwing their customers because it's

13:05.080 --> 13:10.760
not good. And if they are heavily involved into the open source community, it's even

13:10.760 --> 13:19.600
better because that means that they are driving the development and they do have all the knowledge

13:19.600 --> 13:28.440
needed to eventually fix issues as soon as they arise. So around automation, use an immutable

13:28.440 --> 13:33.920
approach to your infrastructure. If you start to have different things and weird infrastructure

13:33.920 --> 13:40.840
going on, it will be a dead sentence. Second, version your infrastructure, GitOps is an

13:40.840 --> 13:47.720
option. There are many others. No matter what you do, try to have versions so that effectively

13:47.720 --> 13:52.480
you can potentially roll back or at least see what changed from a version that is known

13:52.480 --> 13:59.960
to be working to the current one and automate the whole process. If you have humans involved,

13:59.960 --> 14:06.120
you will have issues. It will cost more and it will be effectively less resilient and

14:06.120 --> 14:13.800
reliable. So putting all together what we have seen, I would suggest to first create

14:13.800 --> 14:19.560
a multi-data center architecture so that effectively you have all that redundancy and that kind

14:19.560 --> 14:27.640
of things, but hide them from your developers. Maybe they know the region concept or the

14:27.640 --> 14:35.760
exact concept, but don't show the physical layout to your users, otherwise they will

14:35.760 --> 14:42.720
start to do weird stuff. Second, use a tool to manage the clusters. Open cluster management

14:42.720 --> 14:49.400
is an open source project that does it. There are other projects that do similar things.

14:49.400 --> 14:53.600
It's very, very useful and it will help you over time because probably you will end up

14:53.600 --> 15:00.360
running many clusters. Third, I would suggest personally to standardize on the Kubernetes

15:00.360 --> 15:07.400
APIs as the only interface between the workload and the infrastructure because those are,

15:07.400 --> 15:16.360
as seen, very known. Use a bare metal container platform so don't use virtualization or other

15:16.360 --> 15:23.080
stuff into it. You will have hopefully enough workload to justify tons of physical servers

15:23.080 --> 15:28.880
that don't add complexity with virtualization in between. Automate all the infrastructure

15:28.880 --> 15:37.680
pieces and configurations, obviously, as seen. Start providing only a few interfaces to your

15:37.680 --> 15:45.920
business and then eventually extend them when needed. An example would be an OCR registry,

15:45.920 --> 15:52.240
object storage, and pods, deployments, those kind of basic things. Then if your business

15:52.240 --> 16:00.400
comes out saying, oh, we really need that, then eventually you expand. The thing is,

16:00.400 --> 16:06.040
only provide new services when you are sure that there is the requirement for it. For

16:06.040 --> 16:10.840
instance, let's say that you want to do a database as a service. You already have onboarded

16:10.840 --> 16:17.640
100 applications. 80 of those actually use MySQL. It would make sense to provide a MySQL

16:17.640 --> 16:23.160
as a service, but it does not make sense to provide 50 different databases as a service

16:23.160 --> 16:29.800
of which 48 will never be used. It's only complexity and cost for you. Then create a

16:29.800 --> 16:37.080
simple UX for your users that completely obstruct everything that is below. It could be even,

16:37.080 --> 16:43.960
you know, push your Kubernetes configuration here and we will manage it. Hopefully, then

16:43.960 --> 16:52.080
you will be ensuring that all this stuff is versioned and so on so that even when the

16:52.080 --> 16:59.640
workload fails for some reason, you can say, look, a version N minus one was working. You

16:59.640 --> 17:07.680
did something. Now it's broken. It's not the infra. This was it. Thank you. I don't

17:07.680 --> 17:34.920
know if we have a couple of minutes for questions. No, if there are. Thank you for your talk.

17:34.920 --> 17:39.640
Could you expand a bit on the I didn't get why the what was the advantages of building

17:39.640 --> 17:47.560
multiple data centers at first? Yeah, so that is usually a business requirement because

17:47.560 --> 17:52.760
they will say, oh, we want to have everything that is a or at least this service needs to

17:52.760 --> 17:59.520
be a and with one data center, it would be hard. Also, it really depends if you are a

17:59.520 --> 18:05.600
small organization, maybe two data center, three data center could be okay. If you are

18:05.600 --> 18:12.640
a big organization, maybe spread throughout five, 10 legally different regions, then you

18:12.640 --> 18:19.120
will need OZ 3050 data center. That's a completely different scale. OZ all those are very generic

18:19.120 --> 18:25.840
suggestions and then you have to apply them to your specific situation. And just a quick

18:25.840 --> 18:32.240
follow up for that. How do you hide that from the workload developer? So the line just after

18:32.240 --> 18:37.200
that one where you say they have to not know about the multiple clusters. How does that

18:37.200 --> 18:43.640
work? Yeah, so if you pick AWS, for instance, they have the concept of region and a Z. Some

18:43.640 --> 18:50.600
a Z. So it's are not that centers. Some are data centers. Other are parts of a data center,

18:50.600 --> 18:56.120
but different availability zones within the data center are others are containers in the

18:56.120 --> 19:02.800
sense of like 40 food containers full of servers. So the user does not know. They know that

19:02.800 --> 19:09.860
there is region X, a Z one, two, three. What one, two, three means no one knows and no

19:09.860 --> 19:29.200
one cares. And that's the thing. Thank you for the talk. And in your definition of lock-in,

19:29.200 --> 19:34.760
you spoke about cost of portability and multiply by probability of portability, but doesn't

19:34.760 --> 19:44.080
it like if you file to assess the probability of portability, wouldn't you fall in lock-in

19:44.080 --> 19:53.560
without being aware of it? Sorry, what do you mean? Okay, I know I will always run my

19:53.560 --> 20:02.200
cloud in Amazon Web Service. Why would I need portability? But then then I start using locked

20:02.200 --> 20:10.360
in products. So I will never be able to leave. Yes. Well, you will be able to leave. It's

20:10.360 --> 20:14.760
always possible to leave. You will simply re-write from scratch your whole application

20:14.760 --> 20:21.000
and you leave. So what is the cost of that? A billion? Okay. So now it becomes a billion

20:21.000 --> 20:28.280
of lock-in. That is my point. You can re-write tomorrow from scratch from the West up. It's

20:28.280 --> 20:34.000
possible. How much it will cost you? A billion, five billion, a trillion? Okay, that is your

20:34.000 --> 20:41.200
lock-in value. And that's the thing. Obviously, I would suggest you to keep the lock-in as

20:41.200 --> 20:46.760
low as possible. So try not to re-write. To be in a situation where you have to re-write

20:46.760 --> 20:58.240
everything. Yes, thank you. So we short-transmitted that one. Hello. One quick question. I'm going

20:58.240 --> 21:04.440
to ask you a question. So if your organization has a traditional manual approach to operations,

21:04.440 --> 21:11.560
which thing would you automate first? I would start from very simple processes just to ensure

21:11.560 --> 21:18.600
that, you know, it works in the organization. The organizations start to understand it. Processes

21:18.600 --> 21:26.680
like, you know, create VMs or create containers, whatever kind of thing you do. And then some

21:26.680 --> 21:32.360
things such as patching and so on. But if you really want to go the automation way,

21:32.360 --> 21:37.400
it's way easier to, after you have tested a little bit the thing, start to say, okay,

21:37.400 --> 21:42.920
now we have the version two of the environment that is fully automated from day zero. Otherwise,

21:42.920 --> 21:49.200
you will always be in a kind of automated but not completely automated situation. Thank

21:49.200 --> 21:57.680
you. Thank you all. Thank you.
