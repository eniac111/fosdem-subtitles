1
0:00:00.000 --> 0:00:17.000
Okay, I think maybe I can stop there.

2
0:00:17.000 --> 0:00:22.000
Yes, a few more minutes than the spectrum.

3
0:00:22.000 --> 0:00:23.000
Okay.

4
0:00:23.000 --> 0:00:24.000
All right.

5
0:00:24.000 --> 0:00:25.000
Another one.

6
0:00:25.000 --> 0:00:26.000
Yeah.

7
0:00:26.000 --> 0:00:29.000
So, let's go right into the topic.

8
0:00:29.000 --> 0:00:30.000
So, I'm Alex.

9
0:00:30.000 --> 0:00:31.000
I work for SWITCH.

10
0:00:31.000 --> 0:00:35.000
SWITCH is the national research and education network in Switzerland.

11
0:00:35.000 --> 0:00:41.000
Like, most countries have something like us, like in Belgium, it's Belnet, and in Germany,

12
0:00:41.000 --> 0:00:43.000
it's DFN, and in France, it's Renateir.

13
0:00:43.000 --> 0:00:47.000
So, we connect to Swiss universities and universities of applied sciences.

14
0:00:47.000 --> 0:00:52.000
We are the ISP of those institutions.

15
0:00:52.000 --> 0:00:58.000
So, NetFlow, I'm not sure if everyone is familiar with NetFlow, so I just recaptured the essential

16
0:00:58.000 --> 0:01:01.000
thing about what the NetFlow actually is.

17
0:01:01.000 --> 0:01:06.000
So, when you look at an IP packet, you extract the source and destination addresses, the

18
0:01:06.000 --> 0:01:11.000
IP protocol, and if the protocol is UDP or TCP, also the source and destination ports,

19
0:01:11.000 --> 0:01:15.000
and those five numbers identify a flow.

20
0:01:15.000 --> 0:01:20.000
So, every packet with the same values is said to belong to the same flow.

21
0:01:20.000 --> 0:01:26.000
And then, in the simplest possible way, you basically just aggregate the count of bytes

22
0:01:26.000 --> 0:01:31.000
and packets of all the packets that belong to the flow, and then you export this information

23
0:01:31.000 --> 0:01:37.000
to a collector, and you can then analyze the data.

24
0:01:37.000 --> 0:01:39.000
So, this is an old thing.

25
0:01:39.000 --> 0:01:43.000
Like, these days, people talk about network telemetry, and back in the day when this was

26
0:01:43.000 --> 0:01:45.000
developed, that name didn't exist yet.

27
0:01:45.000 --> 0:01:49.000
And I'm not sure when exactly Cisco came up with this, but it must have been the early

28
0:01:49.000 --> 0:01:51.000
90s or mid-90s.

29
0:01:51.000 --> 0:01:57.000
And it used to be a de facto standard for a long time, where people just figured out

30
0:01:57.000 --> 0:02:00.000
what Cisco did and then did the same thing.

31
0:02:00.000 --> 0:02:06.000
And then, finally, it got properly standardized with the IP fix ITF standard.

32
0:02:06.000 --> 0:02:10.000
And you can do this either in the sampled mode or unsampled mode.

33
0:02:10.000 --> 0:02:15.000
So, unsampled means you just look at every single packet and account for it in the flow,

34
0:02:15.000 --> 0:02:24.000
and with sampling, you just look at every nth packet, and then you have to make certain

35
0:02:24.000 --> 0:02:28.000
assumptions to then reconstruct the actual values.

36
0:02:28.000 --> 0:02:35.000
So, we at Switch, we've been using NetFlow for a very long time as the basic, as the

37
0:02:35.000 --> 0:02:44.000
most important metric or means to analyze our data, our network data, since the mid-1990s.

38
0:02:44.000 --> 0:02:49.000
And it used to be that this was provided by the routers themselves, which is reasonable,

39
0:02:49.000 --> 0:02:54.000
and the packets pass through that device, and so the device is immediately accessed

40
0:02:54.000 --> 0:02:59.000
to the packets and then can construct the flow data itself.

41
0:02:59.000 --> 0:03:04.000
So, initially, that was done in software, then it was done in hardware.

42
0:03:04.000 --> 0:03:11.000
It used to be basically always unsampled, but with the advent of more powerful networking

43
0:03:11.000 --> 0:03:17.000
gear, and especially with the arrival of the 100-gig ports, it became basically

44
0:03:17.000 --> 0:03:24.000
unfeasible to do this on the routers themselves because of typically software restrictions,

45
0:03:24.000 --> 0:03:25.000
also hardware restrictions.

46
0:03:25.000 --> 0:03:28.000
If you want to do this in software, you usually can't because the routers are not very

47
0:03:28.000 --> 0:03:32.000
powerful in terms of CPU, and in hardware, it becomes very expensive.

48
0:03:32.000 --> 0:03:37.000
So, the vendors started to basically only implement sampled NetFlow.

49
0:03:37.000 --> 0:03:44.000
So, these days, if you buy a Cisco or a Juniper box and you do NetFlow, you get sampling.

50
0:03:44.000 --> 0:03:49.000
And sampling is fine, of course, if you're only interested in aggregate data anyway.

51
0:03:49.000 --> 0:03:53.000
So, big aggregated network flows between networks, for instance.

52
0:03:53.000 --> 0:03:54.000
Sampling is perfectly fine.

53
0:03:54.000 --> 0:04:00.000
You make certain assumptions about the traffic, and then you just upscale it, and you get

54
0:04:00.000 --> 0:04:02.000
fairly reasonable numbers.

55
0:04:02.000 --> 0:04:05.000
So, why would you even want to do unsampled NetFlow?

56
0:04:05.000 --> 0:04:10.000
Well, there are some couple of use cases that are really useful.

57
0:04:10.000 --> 0:04:16.000
So, for instance, in terms of security, one thing that sampling is fine is detect DDoS,

58
0:04:16.000 --> 0:04:17.000
for instance.

59
0:04:17.000 --> 0:04:19.000
That's volumetric DDoS.

60
0:04:19.000 --> 0:04:20.000
That's very simple.

61
0:04:20.000 --> 0:04:24.000
So, you basically have a constant package rate, and if you just look at every end package,

62
0:04:24.000 --> 0:04:25.000
it's easy to scale this up.

63
0:04:25.000 --> 0:04:31.000
But if you want to detect a bot, for instance, in your network, then it's more difficult.

64
0:04:31.000 --> 0:04:37.000
So, maybe you want to do this by looking at the communication with the command and control

65
0:04:37.000 --> 0:04:38.000
channels.

66
0:04:38.000 --> 0:04:44.000
Those are short-lived flows, and if you do sampling, you're probably going to miss them.

67
0:04:44.000 --> 0:04:49.000
But with unsampled NetFlow, you see every single flow, so you can identify these things.

68
0:04:49.000 --> 0:04:54.000
And we, as a network operator, we use this fairly often to troubleshoot network problems.

69
0:04:54.000 --> 0:04:59.000
So, if a customer says, complaints, you can't reach a certain IP address in the Internet,

70
0:04:59.000 --> 0:05:04.000
we can actually go look in our flows for the outgoing TCP-syn packet and see whether there's

71
0:05:04.000 --> 0:05:06.000
a TCP-syn coming back in.

72
0:05:06.000 --> 0:05:08.000
We can do this because we see every single flow.

73
0:05:08.000 --> 0:05:11.000
So, this is extremely useful.

74
0:05:11.000 --> 0:05:18.000
But, as I said, we cannot long do that on our big new core routers.

75
0:05:18.000 --> 0:05:19.000
We can't do that.

76
0:05:19.000 --> 0:05:21.000
They only give us sampled NetFlow.

77
0:05:21.000 --> 0:05:25.000
So, we started to do this with an external box.

78
0:05:25.000 --> 0:05:33.000
And that's where this SnapFlow software implementation comes in.

79
0:05:33.000 --> 0:05:37.000
Because, I mean, there are always ways to do that, but they might be very expensive if

80
0:05:37.000 --> 0:05:40.000
you have to buy dedicated hardware, for instance.

81
0:05:40.000 --> 0:05:45.000
So, just to give an idea of what type kind of traffic we're dealing with, Switzerland

82
0:05:45.000 --> 0:05:46.000
is a small country.

83
0:05:46.000 --> 0:05:48.000
We are a small network.

84
0:05:48.000 --> 0:05:51.000
And we only do NetFlow on our borders.

85
0:05:51.000 --> 0:05:57.080
So, when we, the traffic that we exchange with neighboring networks, and the peak values

86
0:05:57.080 --> 0:06:03.760
are these days, it's roughly maybe 180 gigabits per second, something like that.

87
0:06:03.760 --> 0:06:11.800
And 20 million packets per second, and roughly 350,000 flows per second, un-sampled.

88
0:06:11.800 --> 0:06:14.160
And this can actually be even much more.

89
0:06:14.160 --> 0:06:19.320
The flow rate, because of the aggressive scanning that's going on for the past couple of years,

90
0:06:19.320 --> 0:06:27.160
started to perform very aggressive network scans, like plain TCP-SIN scans, as fast as

91
0:06:27.160 --> 0:06:28.160
they can.

92
0:06:28.160 --> 0:06:33.560
So, sometimes a single host can easily generate 100,000 flows per second.

93
0:06:33.560 --> 0:06:40.920
So, the actual IP-thick traffic that the export is then in the order of 200 to 300 megabits

94
0:06:40.920 --> 0:06:41.920
per second.

95
0:06:41.920 --> 0:06:43.120
So, the flow records themselves.

96
0:06:43.120 --> 0:06:47.200
So, this is all for the un-sampled flow.

97
0:06:47.200 --> 0:06:51.000
The average flow rate is maybe just around 200,000 per second.

98
0:06:51.000 --> 0:06:57.480
And the data generates the actual NetFlow data is like roughly 1.5 terabits per day.

99
0:06:57.480 --> 0:07:02.720
So, the actual scaling problem is more on the collector side then.

100
0:07:02.720 --> 0:07:05.960
You have 10 gig, 100 gig, and 400 gig ports.

101
0:07:05.960 --> 0:07:08.880
So, that's what our solution needs to support.

102
0:07:08.880 --> 0:07:15.240
So, we used to do this historically on the routers themselves until a couple years ago.

103
0:07:15.240 --> 0:07:21.440
Then we moved to a commercial NetFlow generator that did that in hardware, which was pretty

104
0:07:21.440 --> 0:07:22.680
expensive.

105
0:07:22.680 --> 0:07:27.400
Maybe the whole solution for one pop was 100,000 euros, something like that.

106
0:07:27.400 --> 0:07:31.120
And then we finally moved to SnapFlow and Pure Software.

107
0:07:31.120 --> 0:07:33.800
So, how do we do this?

108
0:07:33.800 --> 0:07:35.800
On the borders, these are all fiber connections.

109
0:07:35.800 --> 0:07:37.920
So, we have optical splitters.

110
0:07:37.920 --> 0:07:40.840
We create a copy of all the traffic flow.

111
0:07:40.840 --> 0:07:46.720
And then we have a second device, or the primary device that these tabs are connected to is

112
0:07:46.720 --> 0:07:48.520
what we call a packet broker.

113
0:07:48.520 --> 0:07:54.440
It's basically a switch that aggregates all the packets and sends it out on 200 gig ports

114
0:07:54.440 --> 0:07:57.680
to our actual exporter box.

115
0:07:57.680 --> 0:08:00.960
So, it uses VLAN tags to identify.

116
0:08:00.960 --> 0:08:06.440
So, in NetFlow, we also want to keep track of the router ports where the traffic was

117
0:08:06.440 --> 0:08:08.040
sent or received from.

118
0:08:08.040 --> 0:08:12.760
So, because then that information gets lost and you aggregate them, so we use VLANs to

119
0:08:12.760 --> 0:08:14.720
tag them.

120
0:08:14.720 --> 0:08:19.240
The box we use are white box switches based on the Tofino ASIC.

121
0:08:19.240 --> 0:08:22.920
The ones that Intel just decided they stopped developing, unfortunately.

122
0:08:22.920 --> 0:08:29.960
These are very nice boxes for, like there's one for 3200 gig ports for 5000 euros, and

123
0:08:29.960 --> 0:08:33.920
the other one has 32, 400 gig ports and costs about 20,000 euros.

124
0:08:33.920 --> 0:08:36.600
The thing is you have to program them yourself and you buy them.

125
0:08:36.600 --> 0:08:41.320
They're just plain hardware, and so you can use the P4 language to do this.

126
0:08:41.320 --> 0:08:48.040
I link here to another project of mine where I actually developed a P4 program to do that.

127
0:08:48.040 --> 0:08:50.960
So, that's also part of this entire architecture.

128
0:08:50.960 --> 0:08:57.560
And then the traffic gets to the NetFlow exporter box, which is currently just a one rack unit,

129
0:08:57.560 --> 0:09:01.040
a basic rack mount server.

130
0:09:01.040 --> 0:09:09.960
We use AMD Epix, mainly these days with a fairly large number of cores.

131
0:09:09.960 --> 0:09:12.080
That's the way we scale with the number of cores.

132
0:09:12.080 --> 0:09:15.840
NetFlow always scales very well with cores because you just have to make sure that you

133
0:09:15.840 --> 0:09:23.160
keep the packets to flow together.

134
0:09:23.160 --> 0:09:28.800
The exporter has a Mellanox 2 port 100 gig card that's connected to the packet broker.

135
0:09:28.800 --> 0:09:33.680
That's where it receives the packets.

136
0:09:33.680 --> 0:09:36.480
In the picture, that's what it looks like.

137
0:09:36.480 --> 0:09:39.600
On the upper left, that would be our border router.

138
0:09:39.600 --> 0:09:44.320
On the upper right, that would be the bordering router of our neighboring networks.

139
0:09:44.320 --> 0:09:50.000
In the middle, you have this optical splitter, which is completely on passive box, just as

140
0:09:50.000 --> 0:09:51.680
an optical splitter.

141
0:09:51.680 --> 0:09:57.360
And then you have this packet broker switch in between that aggregates all the packets

142
0:09:57.360 --> 0:10:02.600
and distributes them by flow on these two links currently.

143
0:10:02.600 --> 0:10:07.240
These are not 200 gig ports between the broker and the exporter.

144
0:10:07.240 --> 0:10:10.080
We can easily add more ports if that's not sufficient.

145
0:10:10.080 --> 0:10:17.120
And on the SnapFlow exporter, we can basically just add more cores to be able to scale.

146
0:10:17.120 --> 0:10:18.120
Right.

147
0:10:18.120 --> 0:10:28.760
So, now let's hear Max talk about the actual software.

148
0:10:28.760 --> 0:10:40.800
How do I look?

149
0:10:40.800 --> 0:10:41.800
Does this work?

150
0:10:41.800 --> 0:10:42.800
Good.

151
0:10:42.800 --> 0:10:43.800
All right.

152
0:10:43.800 --> 0:10:57.480
How do we know how SnapFlow is deployed?

153
0:10:57.480 --> 0:11:03.520
I want to talk about how it's built, how it scales, how you configure it, how you monitor

154
0:11:03.520 --> 0:11:08.600
your running application, et cetera.

155
0:11:08.600 --> 0:11:14.080
So SnapFlow, as the name suggests, is built using Snap.

156
0:11:14.080 --> 0:11:18.720
Snap is a toolkit for writing high performance networking applications.

157
0:11:18.720 --> 0:11:23.680
Snap is written in Lua using the amazing Lua JIT compiler.

158
0:11:23.680 --> 0:11:28.120
And it does packet IO without going through the kernel.

159
0:11:28.120 --> 0:11:34.600
Like, generally, the Linux kernel packet networking stack is slow from an ISP perspective.

160
0:11:34.600 --> 0:11:39.440
So Snap bypasses that, uses its own device drivers.

161
0:11:39.440 --> 0:11:41.960
And this is also often called kernel bypass networking.

162
0:11:41.960 --> 0:11:44.480
I think nowadays it's fairly common.

163
0:11:44.480 --> 0:11:46.280
And Snap is open source and independent.

164
0:11:46.280 --> 0:11:52.880
We're not sponsored by any vendor in particular.

165
0:11:52.880 --> 0:11:57.440
So Snap is built with these three core values in mind.

166
0:11:57.440 --> 0:12:00.880
We prefer simple designs over complex designs.

167
0:12:00.880 --> 0:12:04.360
We prefer our software to be small rather than large.

168
0:12:04.360 --> 0:12:07.200
And we are open.

169
0:12:07.200 --> 0:12:10.800
You can read the source, you can understand it, you can modify it, you can rewrite it,

170
0:12:10.800 --> 0:12:17.080
et cetera.

171
0:12:17.080 --> 0:12:23.720
So here I have a snippet of code taken directly from SnapFlow, unedited.

172
0:12:23.720 --> 0:12:28.440
So this is how the Lua code that powers the usual Snap application sort of looks like,

173
0:12:28.440 --> 0:12:32.160
just to give you an idea.

174
0:12:32.160 --> 0:12:36.360
In this particular example, we read a batch of packets from an incoming link.

175
0:12:36.360 --> 0:12:40.640
We extract some metadata that tells us which flow this packet belongs to.

176
0:12:40.640 --> 0:12:44.640
Then we look up a matching flow and a flow table that we maintain.

177
0:12:44.640 --> 0:12:47.600
If we already have a flow, we count that packet towards the flow.

178
0:12:47.600 --> 0:12:55.480
If not, we create a new entry in the flow table.

179
0:12:55.480 --> 0:12:56.480
Got one more snippet.

180
0:12:56.480 --> 0:13:01.180
This function is called every now and then to actually export the flows.

181
0:13:01.180 --> 0:13:08.680
So we walk over a section of the flow table here and add flow aggregates from that flow

182
0:13:08.680 --> 0:13:12.280
table into a next data export record.

183
0:13:12.280 --> 0:13:15.440
And if it's time to export the data record, we send it off to an IP fix collector, which

184
0:13:15.440 --> 0:13:22.280
is a separate program.

185
0:13:22.280 --> 0:13:31.280
So from bird's eye view, SnapFlow works sort of like this, we read packets from a 100 gigabit

186
0:13:31.280 --> 0:13:33.880
SNK, the garden hole, so to speak.

187
0:13:33.880 --> 0:13:39.200
We process those packets to extract flow information in a Snap process.

188
0:13:39.200 --> 0:13:45.820
And then we send off data records over a ton tap interface to the IP fix collector.

189
0:13:45.820 --> 0:13:51.720
So on the right side here, you have a device driver written, like that is part of Snap

190
0:13:51.720 --> 0:13:57.920
written in Lua that actually handles the actual traffic, the bulk of it.

191
0:13:57.920 --> 0:14:01.680
And on the left side, you have an interface to the Linux network stack.

192
0:14:01.680 --> 0:14:07.680
So since the flow export data is rather small in comparison, you can just do that over the

193
0:14:07.680 --> 0:14:11.000
regular Linux network stack, and that works.

194
0:14:11.000 --> 0:14:13.520
And on the very left side, you have the IP fix collector.

195
0:14:13.520 --> 0:14:18.240
That's a different application, like a separate program that we sent the flow data to in the

196
0:14:18.240 --> 0:14:19.240
end.

197
0:14:19.240 --> 0:14:30.840
So sadly, or I mean, I guess just obviously, single CPU core is not enough to handle 100

198
0:14:30.840 --> 0:14:33.760
gigabits of traffic.

199
0:14:33.760 --> 0:14:38.200
So instead what we do is we do receive side scaling provided by the network device.

200
0:14:38.200 --> 0:14:42.560
This way we can process n different sets of flows on n different processes running on

201
0:14:42.560 --> 0:14:43.560
n different CPU cores.

202
0:14:43.560 --> 0:14:51.960
So every circle here is a CPU core.

203
0:14:51.960 --> 0:14:55.040
And we also support to repeat basically the same trick in software.

204
0:14:55.040 --> 0:15:02.440
So we can do another one to receive side scaling after filtering the traffic by protocol.

205
0:15:02.440 --> 0:15:06.760
And this way we can process, for example, DNS traffic on different set of cores than

206
0:15:06.760 --> 0:15:13.520
IP traffic, like non-DNS IP traffic.

207
0:15:13.520 --> 0:15:18.080
And that way we can sort of like segregate the server resources into the workloads that

208
0:15:18.080 --> 0:15:20.400
we actually care about.

209
0:15:20.400 --> 0:15:25.960
We might, for example, care more about that we have an accurate general IP flow profile

210
0:15:25.960 --> 0:15:28.000
to send to the collectors.

211
0:15:28.000 --> 0:15:31.720
And maybe if we still have some time left, we will also do some DNS analysis.

212
0:15:31.720 --> 0:15:40.360
But we don't want one to slow down the other necessarily.

213
0:15:40.360 --> 0:15:44.880
So snap programs are organized into independent apps.

214
0:15:44.880 --> 0:15:49.000
So an app is like a logical packet processing component.

215
0:15:49.000 --> 0:15:54.840
Could be, for example, a device driver or an app that implements the address resolution

216
0:15:54.840 --> 0:15:56.280
protocol.

217
0:15:56.280 --> 0:16:02.000
And these apps are combined into applications like SnapFlow using links.

218
0:16:02.000 --> 0:16:05.000
Links are unidirectional.

219
0:16:05.000 --> 0:16:07.200
They're really just ring buffers.

220
0:16:07.200 --> 0:16:12.400
And any app can have any number of them to use as input or output for packet data.

221
0:16:12.400 --> 0:16:15.560
And you communicate with, like, use those links, like shown here.

222
0:16:15.560 --> 0:16:20.240
That's basically the API that you call link receive on a link to receive a packet.

223
0:16:20.240 --> 0:16:30.720
And you call link transmit on an output link to send a packet.

224
0:16:30.720 --> 0:16:34.400
So now to forward packets from one CPU core to another CPU core, we have this thing called

225
0:16:34.400 --> 0:16:35.400
libinterlink.

226
0:16:35.400 --> 0:16:40.120
These are really just like regular links, except that they span process and CPU core

227
0:16:40.120 --> 0:16:43.080
boundaries.

228
0:16:43.080 --> 0:16:45.160
And you can also use them just like any link.

229
0:16:45.160 --> 0:16:48.960
You have the same interface if you want to operate with them.

230
0:16:48.960 --> 0:16:58.240
And we use those to implement the software-based receive set scaling that I talked about earlier.

231
0:16:58.240 --> 0:17:00.520
We also have libptree.

232
0:17:00.520 --> 0:17:04.960
So libptree implements a very strict control plane, data plane segregation.

233
0:17:04.960 --> 0:17:09.720
I think for most networking folks, the concept of control plane and data plane is pretty

234
0:17:09.720 --> 0:17:11.280
common.

235
0:17:11.280 --> 0:17:16.200
But just to recap it, control plane is something that basically is fancy and elaborate.

236
0:17:16.200 --> 0:17:17.760
You expect it to be really nice.

237
0:17:17.760 --> 0:17:21.680
You want to have a nice interface to configure your application and monitor it.

238
0:17:21.680 --> 0:17:24.200
The data plane, on the other hand, you really just want it to work.

239
0:17:24.200 --> 0:17:27.080
It should, like, preferably run at line rate.

240
0:17:27.080 --> 0:17:34.520
And you don't really have any time to mess around.

241
0:17:34.520 --> 0:17:38.960
So since these, like, two different parts of the application have very different requirements,

242
0:17:38.960 --> 0:17:40.240
it has to keep them separate.

243
0:17:40.240 --> 0:17:46.320
And that's what we do.

244
0:17:46.320 --> 0:17:48.120
We also have libyang.

245
0:17:48.120 --> 0:17:53.420
So you see, both the configuration and the application state of SnapFlow are actually

246
0:17:53.420 --> 0:17:58.120
managed by described in the yang schema.

247
0:17:58.120 --> 0:18:02.840
So for example, you can tell the control plane to load a new configuration of SnapFlow, or

248
0:18:02.840 --> 0:18:06.120
you can query it for some state counters while it's running.

249
0:18:06.120 --> 0:18:10.200
And on the slide, I have some examples how you will use the Snap command line interface

250
0:18:10.200 --> 0:18:19.960
to do those things.

251
0:18:19.960 --> 0:18:24.960
So here we have a snippet of the SnapFlow yang schema.

252
0:18:24.960 --> 0:18:28.120
And yang is one of these things where at the beginning you wonder if you're really going

253
0:18:28.120 --> 0:18:29.220
to need it.

254
0:18:29.220 --> 0:18:32.940
But once you have it, you're usually really happy that you do have it.

255
0:18:32.940 --> 0:18:39.800
So what I like specifically about yang is it's very expressive.

256
0:18:39.800 --> 0:18:43.640
If a configuration passes the control plane and it doesn't reject it because it says,

257
0:18:43.640 --> 0:18:46.480
hey, this is invalid, I'm pretty confident that this configuration will do something

258
0:18:46.480 --> 0:18:50.520
useful in the data plane and it will not just, like, crash.

259
0:18:50.520 --> 0:18:53.840
For example, here we have a list of interfaces.

260
0:18:53.840 --> 0:18:57.240
And one of the fields is a device, which is a PCI address.

261
0:18:57.240 --> 0:19:01.440
And the PCI address, in this case, this type is attached to some regular expression and

262
0:19:01.440 --> 0:19:03.640
makes sure that it actually looks like a PCI address.

263
0:19:03.640 --> 0:19:08.080
And you cannot just pass any string in there and validate it somewhere way down the line.

264
0:19:08.080 --> 0:19:14.600
Like, if you don't put a thing that at least looks like a PCI address, then this won't

265
0:19:14.600 --> 0:19:20.520
even be loaded.

266
0:19:20.520 --> 0:19:23.040
So sadly, any piece of software has bugs.

267
0:19:23.040 --> 0:19:28.200
And in our case, even suboptimal performance is often considered a bug, right?

268
0:19:28.200 --> 0:19:32.280
And we deal with the second issue here, with the performance, by shipping Snap with a flight

269
0:19:32.280 --> 0:19:33.560
recorder.

270
0:19:33.560 --> 0:19:35.200
So this flight recorder has minimal overhead.

271
0:19:35.200 --> 0:19:36.200
It's always on.

272
0:19:36.200 --> 0:19:39.000
You even run into production, preferably.

273
0:19:39.000 --> 0:19:40.360
And it stores useful data.

274
0:19:40.360 --> 0:19:46.320
Part of that data is really useful to profile your application after the fact all while

275
0:19:46.320 --> 0:19:55.200
it's running.

276
0:19:55.200 --> 0:20:00.720
So to analyze the collected data, we have built a little UI that we used to do that.

277
0:20:00.720 --> 0:20:03.520
It's usually running on one of our development servers, so we test stuff.

278
0:20:03.520 --> 0:20:05.520
But you can really run it anywhere.

279
0:20:05.520 --> 0:20:07.960
Did I mention Snap to use the UOG?

280
0:20:07.960 --> 0:20:08.960
I did, right?

281
0:20:08.960 --> 0:20:12.680
So we're dealing with a JIT compiler here.

282
0:20:12.680 --> 0:20:16.840
So the UI shows you stuff that you would expect from a profiler, like basically where does

283
0:20:16.840 --> 0:20:23.560
my program spend its time, but also some JIT-related stuff, like did the compiler have issues generating

284
0:20:23.560 --> 0:20:26.440
efficient code for particular parts of my program?

285
0:20:26.440 --> 0:20:31.400
So for example, here there's like a JGC column.

286
0:20:31.400 --> 0:20:35.560
That's like when the injected code, the garbage collector, is invoked.

287
0:20:35.560 --> 0:20:41.160
And that's, for example, something to look out for.

288
0:20:41.160 --> 0:20:49.120
And another part of the flight recorder is a high resolution event log.

289
0:20:49.120 --> 0:20:55.520
It can give you accurate latency measurements of the pieces that make up your software.

290
0:20:55.520 --> 0:21:02.120
And you can see here on the slide that the OUI has, it shows latency histograms for

291
0:21:02.120 --> 0:21:03.120
individual events.

292
0:21:03.120 --> 0:21:06.640
These events are, some of these events are like already defined in Snap, but you can

293
0:21:06.640 --> 0:21:09.560
also use a define new event.

294
0:21:09.560 --> 0:21:17.440
And here, for example, I could tell that processing a batch of packets and extracting the flow

295
0:21:17.440 --> 0:21:24.320
data, so this is like the main IP fix app main loop, takes us about 35 microseconds

296
0:21:24.320 --> 0:21:28.080
per iteration per process.

297
0:21:28.080 --> 0:21:31.840
And this is really useful if you want to debug tail latencies, right?

298
0:21:31.840 --> 0:21:37.280
And tail latencies translate basically to drop packets in our world.

299
0:21:37.280 --> 0:21:43.320
So that's something that's really valuable.

300
0:21:43.320 --> 0:21:51.280
So to close things, if you were to write a new application based on Snap today, you would

301
0:21:51.280 --> 0:21:58.440
have all these things and more ready at your disposal.

302
0:21:58.440 --> 0:22:05.800
And also, it is possible to purchase consultancy services like commercial support for Snap

303
0:22:05.800 --> 0:22:14.160
and developing Snap applications from your friendly open source consultancy, Igalio,

304
0:22:14.160 --> 0:22:16.480
which is my current employer.

305
0:22:16.480 --> 0:22:19.240
So yeah, that's all for now.

306
0:22:19.240 --> 0:22:21.240
Thanks for your attention.

307
0:22:21.240 --> 0:22:24.200
On the right, there are some pointers, some contacts.

308
0:22:24.200 --> 0:22:29.000
If you have questions or inquiries about Snap or SnapFlow, you can email us there after

309
0:22:29.000 --> 0:22:30.000
the conference.

310
0:22:30.000 --> 0:22:40.360
And for now, if you have any questions, please ask them.

311
0:22:40.360 --> 0:23:08.040
Please come down.

312
0:23:08.040 --> 0:23:19.320
There are some seats available here in the middle.

313
0:23:19.320 --> 0:23:26.400
So the next speaker is Peter Manel.

314
0:23:26.400 --> 0:23:32.520
That is one of the key guys of Suricata, a very popular open source ideas.

315
0:23:32.520 --> 0:23:38.440
And today is going to talk about this open source platform.

316
0:23:38.440 --> 0:24:03.320
Have a seat here in the middle.

