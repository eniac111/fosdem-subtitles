WEBVTT

00:00.000 --> 00:10.140
So great to see you all. So many people here. That's awesome. I'm working with my talk.

00:10.140 --> 00:16.040
It's called Decentralized Search with IPFS. Maybe first of all, like a quick pulse, how

00:16.040 --> 00:22.420
many of you have used IPFS? Please raise your hand. Okay. Okay, nice. And how many of you

00:22.420 --> 00:29.340
have heard about IPFS? Okay, all of you. Okay, cool. So you know all about it already. Yeah.

00:29.340 --> 00:34.180
So the talk is called How Does It Work Under the Hood? So we will dive in pretty deep at

00:34.180 --> 00:40.120
some points of the talk. But yeah, first things first. My name is Dennis. I'm a research engineer

00:40.120 --> 00:44.800
at protocol labs. I'm working in a team called Probe Lab, and we're doing network measurements

00:44.800 --> 00:49.720
and protocol optimizations there. I'm also an industrial PhD candidate at the University

00:49.720 --> 00:54.600
of GÃ¶tting, and you can reach me on all these handles on the internet. So if you have any

00:54.600 --> 00:58.920
questions, reach out and let me know your questions, or just hear the venue after the

00:58.920 --> 01:06.360
talk. So what's in for you today? First of all, just in words and numbers, what is IPFS?

01:06.360 --> 01:11.200
Just general overview. And at that point, after we covered that, I would just assume

01:11.200 --> 01:15.640
we have installed a local IPFS node on your computer, and I will walk you through the

01:15.640 --> 01:22.120
different commands from, yeah, we're initializing some of the repository, we're publishing content

01:22.120 --> 01:25.420
to the network, and so on, and we'll explain what happens in each of these steps so that

01:25.420 --> 01:31.080
all of you hopefully get a glimpse on what's going on under the hood. So we are importing

01:31.080 --> 01:35.160
content, we connect to the network, I explain content routing, this is the very technical

01:35.160 --> 01:43.680
part, and at the end some call-offs, basically. So what is IPFS? IPFS stands for the Interplanetary

01:43.680 --> 01:48.680
File System, and generally it's a decentralized storage and delivery network which builds

01:48.680 --> 01:53.240
on peer-to-peer networking and content-based addressing. So peer-to-peer networking, if

01:53.240 --> 01:57.640
you have followed along, or if you've been here earlier today, Max gave a great talk

01:57.640 --> 02:03.480
about LIP-to-P, about browser connectivity in general in peer-to-peer networks, and IPFS

02:03.480 --> 02:09.600
is one of the main users of the LIP-to-P library and builds on top of that. And most importantly,

02:09.600 --> 02:15.120
it's very tiny at the bottom, IPFS is not a blockchain, so also a common misconception,

02:15.120 --> 02:22.600
I'd like to emphasize that. In numbers, given these numbers are from mid-last year, so probably

02:22.600 --> 02:27.000
in need of an update, but its operation has since 2015, that hasn't changed. Numbers of

02:27.000 --> 02:33.800
requests exceed a billion in a week, and hundreds of terabytes of traffic that we see, and tens

02:33.800 --> 02:38.920
of millions of active users also weekly, but a disclaimer, this is just from our vantage

02:38.920 --> 02:43.680
point, in a decentralized network, no one has a complete view of what's going on, so

02:43.680 --> 02:51.880
these numbers could be much higher or just different in general. On ecosystem.ipfs.tech,

02:51.880 --> 02:56.760
you can find some companies that built on top of this tech, and it's all in these different

02:56.760 --> 03:05.520
areas, social media, and so on and so forth, so worth looking up. What's the value proposition

03:05.520 --> 03:11.400
of IPFS? The most important thing that it does, it decouples the content from its host,

03:11.400 --> 03:18.080
and it does this through a concept that's called content addressing. And content addresses

03:18.080 --> 03:24.360
are permanent verifiable links, and this allows you to request content with this, or request

03:24.360 --> 03:28.120
data with the content address, and anyone can serve you the content, and just from the

03:28.120 --> 03:33.800
address that you ask with, you can identify and verify that the content you got served

03:33.800 --> 03:38.960
is actually the one that you requested, and you are not dependent on the authenticity

03:38.960 --> 03:44.560
of the host, as it's the case with HTTP. Because it's a decentralized network, it's also censorship

03:44.560 --> 03:48.880
resistant, and I like to put here that it alleviates backbone addiction, so what do I

03:48.880 --> 03:54.000
mean with that? Let's imagine all of you, or all of us, wanted to download a 100 megabyte

03:54.000 --> 03:58.320
YouTube video here in this room. We would put pressure, so if we were 100 people, we

03:58.320 --> 04:03.480
would put pressure of about 10 gigabytes onto the backbone to just download the video into

04:03.480 --> 04:07.760
this room. Wouldn't it be better if we could just download it once and distribute it across

04:07.760 --> 04:12.920
each other or download different parts and be a little bit more clever about that? In

04:12.920 --> 04:19.360
a similar vein, if we were working on a Google Doc here inside this room, why does it stop

04:19.360 --> 04:25.280
working if we don't have internet connection anymore? It actually should work, it's actually

04:25.280 --> 04:31.720
ridiculous. Also, for us into the same category, this partition tolerance for emerging networks

04:31.720 --> 04:40.160
could also become very important, or if you're just in Apache coffee shop Wi-Fi. All right,

04:40.160 --> 04:46.600
so how can you install IPFS? I put down three different ways here. IPFS in general is not

04:46.600 --> 04:53.120
you don't install IPFS, IPFS is more specification, and there are different implementations of

04:53.120 --> 04:58.000
this specification. The most common one is Kubo, which was formerly known as Go IPFS,

04:58.000 --> 05:02.280
so it's a Go implementation. There's a new one called Iroh, which is in Rust, and I think

05:02.280 --> 05:07.920
the newest one is in JavaScript called Helia. I think that's also the newest kit on the

05:07.920 --> 05:15.480
block. I will talk about Kubo here. The easiest thing to get started is just download IPFS

05:15.480 --> 05:20.120
desktop, which is an electron app that bundles an IPFS node, gives you a nice UI, and you

05:20.120 --> 05:25.440
can already interact and request CIDs from the network and so on. Then there's the IPFS

05:25.440 --> 05:30.000
companion, which is a browser extension that you can install to Firefox or your browser

05:30.000 --> 05:35.880
of choice. Or you directly use Brave or Opera, which comes in with a bundled IPFS node already.

05:35.880 --> 05:41.680
If you enter an IPFS colon slash slash and a CID, it will resolve the content through

05:41.680 --> 05:46.160
the IPFS network. As I said in the beginning, in this talk we will focus on the command

05:46.160 --> 05:51.240
line because we're in a developer conference. I will also assume that we run Kubo, which

05:51.240 --> 05:59.040
is the reference implementation basically. Now we have downloaded Kubo from github.com

05:59.040 --> 06:04.320
slash ipfs slash Kubo, and we want to import some content. We just want to get started.

06:04.320 --> 06:09.280
We downloaded it, and now we have this ipfs command on our machine. The first thing that

06:09.280 --> 06:16.480
we do is run ipfs init. What this does is it generates a public provide key pair per

06:16.480 --> 06:23.640
default in ed25519. It spits out this random string of characters, which is basically your

06:23.640 --> 06:31.400
public key. Formerly it was just the hash of your public key, but now it's just encoded

06:31.400 --> 06:36.840
your public key in here. This is your peer identity, which will become important later

06:36.840 --> 06:42.800
on. Then it also initializes your ipfs repository. It's a default in your home directory under

06:42.800 --> 06:47.520
.ipfs. This is the location where it stores all the files. If you interact with the IPFS

06:47.520 --> 06:52.920
network and request files, it stores it in this directory in a specific format similar

06:52.920 --> 06:59.260
to Git, how Git objects store basically. Importantly, I will point this out a couple

06:59.260 --> 07:02.680
of times. This is just a local operation. We haven't interacted with the network at

07:02.680 --> 07:08.720
all yet. Now we are ready to go. I have a file I want

07:08.720 --> 07:16.680
to add. What I do is I run ipfs add and then my file name. In this case, ipfs gives you

07:16.680 --> 07:21.360
a progress bar, or Kubo gives you a progress bar, and spits out again a random string of

07:21.360 --> 07:26.800
characters, which is the content identifier, the CID, which is the most fundamental ingredient

07:26.800 --> 07:34.680
here. This is the content from its host. As a mental model, you can think about the CID

07:34.680 --> 07:41.160
as a hash with some metadata. It's self-describing, so the metadata is this description part.

07:41.160 --> 07:47.000
You can see the ingredients at the bottom. It's just an encoded version of some information,

07:47.000 --> 07:53.080
like a CID version. We have version 0 and 1 and some other information that I won't

07:53.080 --> 07:58.440
go into right now. Then it's self-certifying. This is the point where if you request some

07:58.440 --> 08:05.680
data from the network, you certify the data that you could serve with the CID itself and

08:05.680 --> 08:14.800
not with the host that served you the content, just reiterating this. It's an immutable identifier.

08:14.800 --> 08:18.600
All this structure, like the CID structure at the bottom and so on, is governed by a

08:18.600 --> 08:26.800
project that's called Multiformets and is also one of protocol lab's projects here.

08:26.800 --> 08:32.040
The talk is called What Happens Under the Hood? What actually happened here? IPFS saw

08:32.040 --> 08:37.880
the file, which is just this white box here, a stream of bytes, and IPFS chunked it up.

08:37.880 --> 08:43.640
It's in different pieces, which is a common technique in networking, actually. This gives

08:43.640 --> 08:49.680
us some nice properties. It allows us to do piecewise transfers, so we can request blocks

08:49.680 --> 08:56.440
from different hosts, actually. It allows for deduplication. If we have two blocks that

08:56.440 --> 09:04.680
are basically the same bytes, we can deduplicate that and save some storage space underneath.

09:04.680 --> 09:09.760
Also if the file was a video file, we also allow for random access so we could start

09:09.760 --> 09:18.160
in the middle of a video and don't need to stream all the previous bytes at all.

09:18.160 --> 09:23.400
After we have chunked that up, what we do now, or what IPFS does now, is we need to

09:23.400 --> 09:29.800
put it together again. What we do here is we hash each individual chunk. Each chunk

09:29.800 --> 09:37.440
gets its own CID, its own content identifier. Then the combination of each CID again gets

09:37.440 --> 09:44.240
another CID, and we do this for both pairs at the bottom. Then the resulting common CIDs

09:44.240 --> 09:50.760
again will be put together yet again to generate the root CID. That's how we call it. This

09:50.760 --> 09:56.120
is actually the CID that you see in the command line up there. We took the chunks, put the

09:56.120 --> 10:02.080
identifiers together to arrive at the final CID at the top. This data structure is actually

10:02.080 --> 10:08.000
called a Merkle tree, but in IPFS land, it's actually a Merkle DAG because in Merkle trees,

10:08.000 --> 10:13.160
your nodes are not allowed to have common parents. A DAG means here a directed acyclic

10:13.160 --> 10:21.320
graph. Let's imagine you didn't add a file, but a directory. How do you encode the directory

10:21.320 --> 10:28.680
structure and not only the bytes and so on? All these formatting and serialization, deserialization

10:28.680 --> 10:33.440
things are governed by yet another project. It's called IPLD, which stands for Interplanetary

10:33.440 --> 10:40.280
Linked Data. IPLD does also a lot of more things, but for now, this is specified in

10:40.280 --> 10:49.120
the scope of this project. Now we have imported the content. We have chunked it up. We've

10:49.120 --> 10:54.960
got the CID, but again, we haven't interacted with the network yet. People think if you

10:54.960 --> 10:58.840
add something to IPFS, you upload it somewhere and someone else takes care of hosting it

10:58.840 --> 11:04.440
for you for free, which is not the case. We added it to our local node. Now it ended up

11:04.440 --> 11:12.040
in this IPFS repository somewhere on our local machine. Only now we connect to the network

11:12.040 --> 11:17.800
and interact with it. For that, we run IPFS daemon, which is a long-running process that

11:17.800 --> 11:23.320
connects to nodes in the network. We see some versioning information with which Go version

11:23.320 --> 11:28.960
was compiled, which Kubo version we actually use. We see the addresses that the Kubo node

11:28.960 --> 11:34.320
listens on and also which ones are announced to the network under which network addresses

11:34.320 --> 11:41.680
we are reachable. It tells us that it started an API server, a web UI, and a gateway. The

11:41.680 --> 11:47.720
API server is just an RPC API that is used by the command line to control the IPFS node.

11:47.720 --> 11:52.880
The web UI is the thing that you saw previously when you saw the screenshot of the IPFS desktop.

11:52.880 --> 12:00.080
Your local Kubo node also serves this web UI. Then the gateway. The gateway is quite

12:00.080 --> 12:07.620
interesting. This bridges the HTTP world with the IPFS world. You can ask under this endpoint

12:07.620 --> 12:13.480
that you can see down there. If you put slash IPFS slash your CID inside the browser or

12:13.480 --> 12:19.320
in your SAD URL, the Kubo node will go ahead and resolve the CID in the network and serve

12:19.320 --> 12:25.800
it to you over HTTP. This is a bridge between both worlds. ProCollapse and Cloudflare and

12:25.800 --> 12:30.880
so on are actually running these gateways on the internet right now, which you can use

12:30.880 --> 12:38.280
just with a low barrier entry to the whole thing. Then the daemon is ready. In this process,

12:38.280 --> 12:42.160
it has also connected to bootstrap nodes, which are hard coded to actually get to know

12:42.160 --> 12:49.080
other peers in the network. You can also override it with your own bootstrap nodes.

12:49.080 --> 12:54.600
Now we are connected to the network. We've added our file to our own machine. Now the

12:54.600 --> 13:01.160
interesting or the problem or the challenge, how do we actually find content hosts for

13:01.160 --> 13:07.200
a given CID? I give my friend a CID. How does the node know that it needs to connect to

13:07.200 --> 13:12.280
me to request the content actually? I put here the solution is simple. We keep a mapping

13:12.280 --> 13:17.000
table. We just have the CID mapped to the actual peer. Every node has this on their

13:17.000 --> 13:23.960
machine. Everyone knows everything basically. As I said, the mapping table gets humongous,

13:23.960 --> 13:29.640
especially if we've split up those files into different chunks. I think the default chunking

13:29.640 --> 13:35.320
size is 256 kilobytes. We have just a lot of entries in this table, so this doesn't

13:35.320 --> 13:40.080
scale. The solution would be to split this table and each participating peer in this

13:40.080 --> 13:46.960
decentralized network holds a separate part of the table. Then we are back to square one.

13:46.960 --> 13:52.400
How do we know which peer holds which piece of this distributed hash table data? The solution

13:52.400 --> 14:01.520
here would be to use a deterministic distribution based on the Cardemna DHT. Cardemna is an

14:01.520 --> 14:10.160
implementation or a specific protocol for a distributed hash table. At this point, many

14:10.160 --> 14:15.240
talks on the internet about IPFS gloss over the DHT and how it works. When I got into

14:15.240 --> 14:22.280
this whole thing, I was lacking something. My experiment would be to just dive even a

14:22.280 --> 14:30.000
little deeper into this. I will cover a bit of Cardemna here. This is very technical,

14:30.000 --> 14:34.560
but at the end I will try to summarize everything so that every one of you guys gets a little

14:34.560 --> 14:40.480
bit out of this. This whole process is called content routing, so this resolution of a CID

14:40.480 --> 14:49.240
to the content host. IPFS uses an adaptation of the Cardemna DHT by using a 256-bit key

14:49.240 --> 14:57.120
space. We are hashing the CID and the peer ID yet again with SHA-256 to arrive in a common

14:57.120 --> 15:02.700
key space. The distributed hash table in IPFS is just a distributed system that maps these

15:02.700 --> 15:09.320
keys to values. The most important records here are provider records which map a CID

15:09.320 --> 15:14.960
to a peer ID. Remember the peer ID is what was generated when we initialize our node.

15:14.960 --> 15:21.840
And peer records which then map the peer ID to actually network addresses like IP addresses

15:21.840 --> 15:28.080
and ports. Looking up a host for a CID is actually a two-step process. First, we need

15:28.080 --> 15:32.800
to resolve the CID to a peer ID and then the peer ID to their network addresses and then

15:32.800 --> 15:39.280
we can connect to each other. The distributed hash table here has two key features. First,

15:39.280 --> 15:46.440
an X or distance metric. That means we have some notion of closeness. What this XR thing

15:46.440 --> 15:51.800
does, if I X or two numbers together, the resulting number or this operation satisfies

15:51.800 --> 15:58.400
the condition and the requirements for a metric. This means I can say a certain peer ID is

15:58.400 --> 16:05.320
closer to a CID than some other peer ID. In this case, peer ID X could be closer to CID

16:05.320 --> 16:17.280
1 than peer ID Y. This allows us to basically sort CIDs with peer IDs together. Then this

16:17.280 --> 16:21.240
tree-based routing mechanism here. In this bottom right diagram, I got this from the

16:21.240 --> 16:27.240
original paper, we have the black node. With this tree-based routing, this is super clever

16:27.240 --> 16:32.320
as in each bubble, so all the peers in the network can actually be considered as in a

16:32.320 --> 16:40.120
big try, a prefix try. If we know only one peer in each of these bubbles, we can guarantee

16:40.120 --> 16:46.640
that we can reach any other peer in the network with O log N lookups by asking for even closer

16:46.640 --> 16:55.080
peers based on this XR routing mechanism here. This was just abstractly what the distributed

16:55.080 --> 16:59.600
hash chain IPFS does. How does it work concretely for IPFS? We started the daemon process. What

16:59.600 --> 17:04.760
happened under the hood was we calculated the chart with 56 of our peer ID, which just

17:04.760 --> 17:10.080
gives us a long string of bits and bytes, or just bits basically in our case. We initialized

17:10.080 --> 17:15.620
the routing table at the bottom. This routing table consists of different buckets. Each

17:15.620 --> 17:24.240
bucket is filled with peers that have a common prefix to our peer ID, the hash from our peer

17:24.240 --> 17:31.360
ID at the top. When our node started up, we asked the bootstrap peers, hey, do you know

17:31.360 --> 17:38.880
anyone who is chart with 56 from peer ID starts with a one? This means we have no common prefix

17:38.880 --> 17:45.240
and we put them those peers in bucket zero. Then we do the same for a prefix of 00 and

17:45.240 --> 17:53.520
011. We go through all the list until 255 and we fill up these buckets. These buckets

17:53.520 --> 17:59.640
are these little blobs in these little circuits that you saw on the previous slide. Why did

17:59.640 --> 18:04.680
we do that? Because when we now want to retrieve contents, as I said, I handed the CID to my

18:04.680 --> 18:12.880
friend and my friend enters the CID in the command line with this IPFS get command. Their

18:12.880 --> 18:17.080
node also calculates the chart with 56 of the CID and then looks in its own routing

18:17.080 --> 18:23.960
table. Cs, okay, I have a prefix of two. I take one peer out of this bucket two and ask,

18:23.960 --> 18:29.040
yeah, locate the appropriate bucket, get a list of all peers. Then I asked all of these

18:29.040 --> 18:33.880
peers in the bucket, hey, do you know anyone? First of all, do you know the provider record

18:33.880 --> 18:40.720
already? Do you know the CID and the peer ID to that CID? If yes, we are done. If not,

18:40.720 --> 18:44.960
we are asking, do you know anyone closer based on this XOR metric? Then this peer yet again

18:44.960 --> 18:50.280
looks in its own routing table. We get closer and closer and closer with this log n property

18:50.280 --> 18:58.320
that I showed you previously. For publishing contents, basically the same. We calculate

18:58.320 --> 19:04.600
the chart with 56 of the CID, locate the appropriate bucket, get a list of all the peers from that,

19:04.600 --> 19:11.280
and then we start parallel queries. Instead of asking for the provider record, we ask

19:11.280 --> 19:19.560
for even closer peers. We terminate when the closest known peers in the query actually

19:19.560 --> 19:28.080
haven't replied with any peer that's closer. It hasn't replied with anyone closer to the

19:28.080 --> 19:33.920
CID than we already know. Then we start the provider record with the 20 closest peers

19:33.920 --> 19:41.040
to that CID and we do it with 20 because there's peer churn. This is a permissionless network.

19:41.040 --> 19:47.280
This means peers can come and go as they wish. If we only stored it with one peer, we would

19:47.280 --> 19:54.000
risk that the provider record is not reachable when the node comes down and in turn all content

19:54.000 --> 20:00.040
is not reachable. This is the very technical part of that.

20:00.040 --> 20:06.240
Let me summarize this. This is probably the easier way to understand all of this. First

20:06.240 --> 20:12.160
of all, we added the content to our node and so this is the file, enters the provider.

20:12.160 --> 20:16.960
The provider looks in its routing table, gets redirected to a peer that is closer to the

20:16.960 --> 20:24.840
CID and gets redirected until it finds the closest peer in this XR key space metric to

20:24.840 --> 20:29.960
the CID and then stores the provider record with that. Then off-band, the CID gets handed

20:29.960 --> 20:37.520
to the requester, to my friend. What I didn't say or told you yet, it's also IPFS maintains

20:37.520 --> 20:44.840
a long list or I don't know how many it is right now, probably 100 or so, constant connections

20:44.840 --> 20:50.320
to other peers and opportunistically just ask them, hey, do you know this CID or the

20:50.320 --> 20:57.280
provider record to the CID? If this resolves, all good, we are done. It's very unlikely

20:57.280 --> 21:03.480
for people to actually know a random CID. Let's assume this didn't work. This requester

21:03.480 --> 21:08.200
also looks in its own routing table, gets redirected, gets redirected even closer to

21:08.200 --> 21:19.440
the peer ID of that CID and then finds the peer that stores the provider record, fetches

21:19.440 --> 21:25.440
the provider record, then does again the same hops to find out the mapping from the peer

21:25.440 --> 21:31.520
ID to the network addresses and then we can actually connect with each other and transfer

21:31.520 --> 21:42.200
the content and we're done. This is the content life cycle and this is already it. Well, already

21:42.200 --> 21:52.640
it is quite involved actually. With that, it's already time for some callouts. Get involved.

21:52.640 --> 21:58.560
IPFS is an open source project. If you're into measurements and so on, we have some

21:58.560 --> 22:05.640
grants open at radius.space. If you want to get involved with some network measurements,

22:05.640 --> 22:11.440
get your applications in, all action is in public. You can follow along our work, especially

22:11.440 --> 22:19.200
my work of our team at this GitHub repository. We have plenty of requests for measurements

22:19.200 --> 22:25.000
that you can dive into and extra ideas are always welcome. In general, IPFS is I think

22:25.000 --> 22:33.640
a very welcoming community, at least for me. That's it. Thank you very much.

22:33.640 --> 22:55.600
Any questions? Is the way you describe using the DHD how all nodes in the network share

22:55.600 --> 23:03.280
files with each other? It's one content routing mechanism. There are multiple ones. It's a

23:03.280 --> 23:08.200
bit, this first thing that I said here, this opportunistic request to your immediate nodes

23:08.200 --> 23:13.040
is also some kind of content routing, so you're resolving the location of content. Then there

23:13.040 --> 23:17.640
are some new efforts for building network indexes, which are just huge nodes that store

23:17.640 --> 23:28.880
the mapping, centralized nodes, which federated centralized nodes, so not as bad. I think

23:28.880 --> 23:36.320
these are the important ones, basically. There are more ways to resolve. Also, MDNS could

23:36.320 --> 23:42.320
also be one part. If you're on the same network, you're broadcasting. No, that's just for... Sorry?

23:42.320 --> 23:45.920
We're still to the DHD and the DNS. We're using the DNS to find the location.

23:45.920 --> 23:52.160
For the local, yeah. Okay, true. Luckily, we have a core maintainer of IPFS here. It's

23:52.160 --> 24:00.600
actually not a joke, but yeah. Sorry. I see that the provider records get replicated,

24:00.600 --> 24:08.400
but does the content actually get replicated across the network too? No. Only if someone

24:08.400 --> 24:14.920
else chooses to. You're publishing the provider record, so it's public somewhere, and anyone

24:14.920 --> 24:21.640
could look that up and also store the record themselves. This is the idea. If content is

24:21.640 --> 24:29.560
popular and you care about the content staying alive in the network, it's called pin, the

24:29.560 --> 24:35.120
CID, and this means you're fetching the content from this other provider and store it yourself

24:35.120 --> 24:40.360
and become a provider yourself. Because of the CID mechanism, which is self-certifying

24:40.360 --> 24:45.840
and so on, other peers that request the content from you don't even need to trust you because

24:45.840 --> 24:53.120
the CID already encodes the trust chain here. But there's nothing that's not happening automatically

24:53.120 --> 24:58.360
here. But you can have multiple providers for the same company. Yeah, definitely. That's

24:58.360 --> 25:05.560
part of it. Another question is how does the project fit in the concept of identity and

25:05.560 --> 25:11.560
trust and personas into IPFS? I'm thinking metadata, reifications about the content and

25:11.560 --> 25:18.320
stuff like that. What do you mean exactly? For instance, just a history of the content

25:18.320 --> 25:26.400
and can you trust that this content is from a certain person or from a certain... I won't

25:26.400 --> 25:32.360
argue this, but this would probably be some mechanism on top of this content and identification.

25:32.360 --> 25:38.120
This is more for IPLD then or for... Perhaps. I would say, if you want to say some content

25:38.120 --> 25:43.920
is from some specific person, then you would work with signatures, so signing the data

25:43.920 --> 25:49.000
and so on, which is something you would bolt on top of IPFS, but nothing I think IPLD has

25:49.000 --> 26:04.240
encoded there right now. It's partly the same question about how it is entered that there

26:04.240 --> 26:11.720
are no collisions in the content ID. No collisions? Yes, because if you publish some other content

26:11.720 --> 26:18.520
with the same content ID, he said it's happening locally, the content ID generation. You could

26:18.520 --> 26:26.880
fake contents. Yeah, but then all these cryptographic hash functions would be broken then, which

26:26.880 --> 26:32.600
would be very bad. If you have a hash collision, then it actually means you have the same content.

26:32.600 --> 26:40.040
That is the assumption right now. We just use a Shadow 56 by default, and you can use

26:40.040 --> 26:44.600
also one like Blake 3, Blake 2, but if you find a collision in Shadow 56, you have bigger

26:44.600 --> 26:56.480
problems and IPFS not working. Exactly this. Follow on on this. How resilient is this against

26:56.480 --> 27:03.640
malicious actors that want to prevent me from reaching the content? It's a big question,

27:03.640 --> 27:12.280
but maybe something. Yes, on peer-to-peer networks, often these kind of civil attacks are an attack

27:12.280 --> 27:18.680
vector that is considered, which means you generate a lot of identities to populate just

27:18.680 --> 27:23.560
some part of the key space to block some requests from reaching the final destination and so

27:23.560 --> 27:36.120
on. From my experience, this is quite hard, and I haven't seen this happening. I cannot

27:36.120 --> 27:44.560
say that it's impossible, probably hard to tell. Max, do you want?

27:44.560 --> 27:57.560
True. This civil thing is just one attack vector, but this is the common one that is

27:57.560 --> 28:02.720
considered. There are many points in the code base where you need to think about what happens

28:02.720 --> 28:10.560
if this attack is going on. One thing that Cardimna does is to prefer long-running nodes,

28:10.560 --> 28:15.760
and the routing table. If someone immediately generates a lot of identities that they don't

28:15.760 --> 28:24.160
end up in your routing table and pollutes your content routing here or interferes with

28:24.160 --> 28:39.000
that. I'm not sure if I want to ask it, but removing content, deleting. We got GDPR, so

28:39.000 --> 28:46.480
is there any solution that can be done? It's hard. That's part of the thing. If you could,

28:46.480 --> 28:54.560
then it's not censorship resistant anymore. One solution, one alleviation maybe, is to

28:54.560 --> 29:04.960
have blacklists of CID that you may publish or may not to say, okay, don't replicate this

29:04.960 --> 29:10.240
CID and so on. If you have such a list, then it's very easy to just look it up and see

29:10.240 --> 29:20.320
what's inside. Deleting content is very tricky. However, I said it's permanent links. The

29:20.320 --> 29:25.760
links are permanent, but actually, content still churns in the IPFS network. These provider

29:25.760 --> 29:34.800
records that you publish into the network expire after 24 hours. If no one actually

29:34.800 --> 29:41.520
reprovides the content or keeps the content, the content is gone as well. A delete operation

29:41.520 --> 29:46.640
doesn't exist, so we just need to hope that no one will be provided anymore, which you

29:46.640 --> 30:01.080
could do with these denialists, for example. Who is able to write into that blacklist?

30:01.080 --> 30:07.600
This is just one, I don't know, to be completely honest, but this is just one, maybe Geropo

30:07.600 --> 30:14.040
knows. There is no blacklist in the network right now. It's some people, it's a few people

30:14.040 --> 30:20.240
that want that. But we have, so, sorry, earlier you said that we have gateways, and gateways

30:20.240 --> 30:26.680
is just a node that publicly is reachable. Those gateway, because many people say that

30:26.680 --> 30:32.720
they find some content illegal on IPFS, and instead of reporting to the actual node, storing

30:32.720 --> 30:36.480
the content on IPFS, they just report it on the gateway, because they know HTTP and they

30:36.480 --> 30:42.440
don't know IPFS. Our gateway has some blacklist that is somewhere, but it's not shared by

30:42.440 --> 30:47.160
the complete network. It's just for our gateway IPFS.io.

30:47.160 --> 30:52.720
CloudFare, for example, and Percolabso, are already writing these gateways, or more, anyone

30:52.720 --> 30:59.800
could operate a gateway. You could file a request for this, don't replicate the CID,

30:59.800 --> 31:04.480
it's a phishing website, for example. Then these CIDs are not served through the gateways,

31:04.480 --> 31:06.680
which is a common way to interact with the network right now.

31:06.680 --> 31:08.960
Just the gateway that follows the list. It's not automatic.

31:08.960 --> 31:12.120
Yeah, of course. Of course, yeah.

31:12.120 --> 31:22.640
Okay, we're running out of time, unless there is one more. I have a question regarding searching

31:22.640 --> 31:32.880
through the stored content. Is there any mechanism on how to go through or index the files that

31:32.880 --> 31:37.040
are there to have some sort of a search engine for that?

31:37.040 --> 31:45.760
There's a project called IPFS Search, and this makes use, among other things, of this

31:45.760 --> 31:52.160
immediate request for CIDs. It's just sitting there connecting to a lot of nodes. As I said,

31:52.160 --> 31:58.040
if someone requests content, you immediately ask your connected peers, and you're connected

31:58.040 --> 32:01.320
to a lot of peers. These IPFS Search nodes are just sitting there listening to these

32:01.320 --> 32:07.240
requests. They see, okay, someone wants the CID, so I go ahead and request that CID as

32:07.240 --> 32:13.680
well, and then index that content on myself. You can then search on this IPFS Search website

32:13.680 --> 32:20.000
for something with Google, and then you see CIDs popping in, and then you can request

32:20.000 --> 32:26.480
those CIDs from the IPFS network. This is one approach to index content.

32:26.480 --> 32:28.840
Okay, thank you.

32:28.840 --> 32:31.840
Thank you.
