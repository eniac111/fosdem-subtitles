1
0:00:00.000 --> 0:00:11.520
So far, your stage is yours. I'm really looking forward to hearing about on-premise data centers

2
0:00:11.520 --> 0:00:22.600
do not need to be a legacy. Thank you. So, hello everyone. And just to be clear, this is going to

3
0:00:22.600 --> 0:00:29.320
be the topic that we are going to cover a little bit of history, some lesser-learned, and then

4
0:00:29.320 --> 0:00:36.440
some technology bets that I think that would make sense in such conversation. So, about me, I have

5
0:00:36.440 --> 0:00:43.240
been a Linux user for 20-ish years. I've been working with Linux for close to 20 years now,

6
0:00:43.240 --> 0:00:50.520
and I currently work with Red Hat and do basically similar kind of conversation in my day-to-day job.

7
0:00:50.520 --> 0:00:57.400
So, let's start with a little bit of history of the cloud. Let's call it this way. So,

8
0:00:57.400 --> 0:01:07.160
Rackspace got founded in 1998 and I think was the first company that defined itself as cloud. In

9
0:01:07.160 --> 0:01:14.800
2005, Softlayer was founded. They defined themselves as bare-meta cloud. And then in 2006,

10
0:01:14.800 --> 0:01:26.040
we have the S3 launched by AWS, which was the first service of AWS. 2006 again, EC2. Sorry,

11
0:01:26.040 --> 0:01:35.720
yeah. And then Google App Engine arrived, IBM bought Softlayer, creating it, IBM Cloud, now

12
0:01:35.720 --> 0:01:48.640
called. And by 2021, AWS has more than 200 different services. So, what about the history

13
0:01:48.640 --> 0:01:54.800
of the known cloud? Because what we have seen are old cloud environments, but those are nothing

14
0:01:54.800 --> 0:02:04.200
new if you think about this. So, in 1964, which is probably older than anyone or most of the people

15
0:02:04.200 --> 0:02:16.800
in this room, IBM introduced the CP40 and this machine had time-sharing technology, which was

16
0:02:16.800 --> 0:02:25.760
very different from what we call today cloud, but still it was probably the initial point of the

17
0:02:25.760 --> 0:02:36.520
history of the cloud. And in the late 60s, IBM released Simon, which is an hypervisor. By 1974,

18
0:02:36.520 --> 0:02:44.640
the two kinds of hypervisor get defined as step one, the bare-meta virtualization, step two,

19
0:02:44.640 --> 0:02:54.000
the hosted virtualization. And by 1998, VMware got founded and in 2000s, majority of companies

20
0:02:54.000 --> 0:03:06.480
moved from bare-meta to VMware's VMs. 2001, ESX got released, which was type one kind of virtualization.

21
0:03:06.480 --> 0:03:16.080
2003, we have the first type one open source virtualization, SAN. And in still 2003, VMware

22
0:03:16.080 --> 0:03:23.560
introduces Vmotion, which allows you to basically move a machine from one host to the other without

23
0:03:23.560 --> 0:03:31.440
rebooting it. And 2008, Microsoft arrives with Hyper-V. It previously had some other kind of

24
0:03:31.440 --> 0:03:37.760
virtualization tool, but Hyper-V got launched in 2008. So what is the cloud? Why we are

25
0:03:37.760 --> 0:03:45.560
distinguished the first group and the second one? Wikipedia says that cloud computing is the

26
0:03:45.560 --> 0:03:50.960
on-demand availability of computing system resources, especially data storage and computing

27
0:03:50.960 --> 0:03:56.240
power without direct active management by the user. I don't think this is a good definition.

28
0:03:56.240 --> 0:04:05.240
I think that a better definition is a business model where one party runs to a second party

29
0:04:05.240 --> 0:04:11.160
computer system resources, especially data storage, cloud storage and computing power,

30
0:04:11.160 --> 0:04:18.000
with the smallest granularity possible. And my point is, cloud is not technical,

31
0:04:18.000 --> 0:04:27.640
it's a business model. And if you think about, we move from renting machines like VPS on a

32
0:04:27.640 --> 0:04:35.880
monthly basis, and then AWS introduced the concept of EC2 that was initially on an hourly

33
0:04:35.880 --> 0:04:41.800
basis and then minute and then second, and now you can buy lambdas or similar kind of

34
0:04:41.800 --> 0:04:49.640
things for milliseconds. And in a way, also, CPUs had the same shrinkage. So we move from

35
0:04:49.640 --> 0:05:00.680
full CPUs or sockets to VCPUs, which basically is hyper-threaded threads, to fractional VCPUs

36
0:05:00.680 --> 0:05:07.280
with lambdas or similar services. So my point being, the whole thing about cloud is not

37
0:05:07.280 --> 0:05:14.560
technical, is only about the business side of it. So what can we learn from not only

38
0:05:14.560 --> 0:05:22.960
the last 20 years of what we can define as cloud, but also the previous 50 of what we

39
0:05:22.960 --> 0:05:28.440
can define as non-cloud, and more specifically, because we have seen that the cloud model

40
0:05:28.440 --> 0:05:36.520
actually works. The non-cloud model was not very functional to the business, to the point

41
0:05:36.520 --> 0:05:45.920
that very often those data centers got outsourced or in some different ways moved to the cloud

42
0:05:45.920 --> 0:05:54.220
in the sense that moved to someone else. And the business started to expand constantly

43
0:05:54.220 --> 0:06:03.400
those machines due to the basically OPEX model instead of the CAPEX model. So there is one

44
0:06:03.400 --> 0:06:08.640
big aspect that we need to remember about this, which is the separation of concerns.

45
0:06:08.640 --> 0:06:14.840
So standardize the interface between the infrastructure and the workload. If you go in legacy data

46
0:06:14.840 --> 0:06:21.760
centers, very often you have 1,000 different kind of systems that the infra people have

47
0:06:21.760 --> 0:06:27.920
to provide to the workload people. And this is because, oh, my system is different, my

48
0:06:27.920 --> 0:06:35.760
software is different, whatever. In the end of the day, that is a huge load for the infrastructure

49
0:06:35.760 --> 0:06:44.160
part of the business. Second, the scalability needs to be at workload level. So the infrastructure

50
0:06:44.160 --> 0:06:52.760
also needs to be somehow reliable and within some SLAs. But if the system has to stay up,

51
0:06:52.760 --> 0:06:57.280
if the application has to stay up, the application will have to take care about this.

52
0:06:57.280 --> 0:07:04.960
And third, workload have an abstract concept of whatever is underneath it. So the physical

53
0:07:04.960 --> 0:07:11.560
architecture. They don't need to know which data center they are in or in which rack,

54
0:07:11.560 --> 0:07:21.120
what is the nearby server, and so on. The function, so we also need a functional business

55
0:07:21.120 --> 0:07:31.640
model for a good managed IT system. And the first part is, as before, standardize the

56
0:07:31.640 --> 0:07:39.720
interface between the workload and infrastructure so that it's easily countable and priceable.

57
0:07:39.720 --> 0:07:47.040
Second, build back the infrastructure cost to the workload owners. We have seen, at least

58
0:07:47.040 --> 0:07:52.680
in my definition of cloud, that we still have two parties. One that delivers a service and

59
0:07:52.680 --> 0:07:58.240
the other one that consumes it and pays for it. So it's very important to create this

60
0:07:58.240 --> 0:08:07.240
also internally in companies or organizations of any kind because this allows the infrastructure

61
0:08:07.240 --> 0:08:17.560
side of the business to justify their expenses over some kind of at least recognition of

62
0:08:17.560 --> 0:08:24.760
revenue or whatever, cost recovery, whatever. And third, keep the cost down. This is a key

63
0:08:24.760 --> 0:08:31.800
point. AWS, Google, those companies will do everything they can to keep the cost down

64
0:08:31.800 --> 0:08:38.600
because they need to be positive, cash flow positive. Obviously, if you are a department

65
0:08:38.600 --> 0:08:43.760
in a company, it's slightly different, but it's very important to still be cash flow

66
0:08:43.760 --> 0:08:50.960
positive because this will guarantee you that you will not have any issues over time with

67
0:08:50.960 --> 0:08:59.440
this part of the financial model. And third, maintain control. We have seen the clouds

68
0:08:59.440 --> 0:09:05.240
are obsessed about maintaining control and obtaining even more control on their hardware,

69
0:09:05.240 --> 0:09:11.040
their system, whatever. And this is very important for your own cloud if you want to be able

70
0:09:11.040 --> 0:09:18.920
to maintain it for 10, 20, 50 years. So the first one is, I would say do not use, but

71
0:09:18.920 --> 0:09:25.800
be very cautious on using third-party properties software. Those companies can go away, can

72
0:09:25.800 --> 0:09:31.920
change pricing model, can do whatever, be aware of this. Second, evaluate very strongly

73
0:09:31.920 --> 0:09:41.040
the buy versus build decision because when you buy, obviously it's here now, but you

74
0:09:41.040 --> 0:09:48.880
don't have the know-how about this. So probably you will want to build a lot of your systems,

75
0:09:48.880 --> 0:09:53.720
not the core parts, but maybe the dashboard layer or that kind of thing so that you can

76
0:09:53.720 --> 0:10:01.160
effectively manage it however you think better. And third, be very aware of lockings because

77
0:10:01.160 --> 0:10:12.640
those will bite you over the course of the years. So, okay. How do I define the locking?

78
0:10:12.640 --> 0:10:18.280
I define it as the product between the probability that a component will require substitution

79
0:10:18.280 --> 0:10:24.240
during the solution lifetime and the total cost of the substitution. So for instance,

80
0:10:24.240 --> 0:10:31.040
Linux, if you base all your architecture on Linux, it's going to be very expensive to

81
0:10:31.040 --> 0:10:37.480
move out of Linux. But in the other hand, it's very improbable that you will be needed

82
0:10:37.480 --> 0:10:46.640
to do it because very probably in 10, 20 years, Linux will be here. So a couple of points

83
0:10:46.640 --> 0:10:53.360
on technologies. The first one is keep the complexity of your system at the lowest level

84
0:10:53.360 --> 0:10:59.440
possible. Systems will get more complex and more absurd over time. So at least at the

85
0:10:59.440 --> 0:11:06.040
beginning, start with the simple thing possible. Second, prefer build time complexity over

86
0:11:06.040 --> 0:11:12.880
runtime complexity. It's way easier to automate a build thing than to automate something to

87
0:11:12.880 --> 0:11:18.920
be run. And also when something breaks, it's better if it's simple because it's easier

88
0:11:18.920 --> 0:11:26.760
to fix. If you have to compile yourself, compile it, but try to keep the complexity at the

89
0:11:26.760 --> 0:11:35.800
runtime at the minimum possible. Third, minimize the amount of services that you deliver to

90
0:11:35.800 --> 0:11:43.200
your business or your workloads owners so that effectively you can guarantee that those

91
0:11:43.200 --> 0:11:47.760
services are exactly what they require and you are able to deliver them in a sensible

92
0:11:47.760 --> 0:11:57.240
way. So I think that one big point is containers. Delivering a container-based solution at least,

93
0:11:57.240 --> 0:12:03.960
it's probably the best option I think today. And use a Kubernetes distribution, whatever

94
0:12:03.960 --> 0:12:12.400
you prefer and choose. That makes sense. It's fine. And we'll see later the Kubernetes

95
0:12:12.400 --> 0:12:20.480
APIs are now fairly well-known, fairly abstract, and fairly used so that those can be a good

96
0:12:20.480 --> 0:12:27.360
interface between the infra and the workload side. Also you can use a do-it-yourself community,

97
0:12:27.360 --> 0:12:34.040
call it whatever, a distribution. You can buy a commercial distribution of Kubernetes.

98
0:12:34.040 --> 0:12:40.560
If you do it, first, be sure that it's fully open source, what you're buying, so that you

99
0:12:40.560 --> 0:12:46.400
decrease your lock-in because you are decreasing the cost that it will take you to move from

100
0:12:46.400 --> 0:12:52.640
this to any other solution. Second, from a trustworthy company, hopefully that company

101
0:12:52.640 --> 0:12:57.720
that you buy it from will not fail tomorrow because if it does, you will have a bigger

102
0:12:57.720 --> 0:13:05.080
problem. And third, with a long track record of not screwing their customers because it's

103
0:13:05.080 --> 0:13:10.760
not good. And if they are heavily involved into the open source community, it's even

104
0:13:10.760 --> 0:13:19.600
better because that means that they are driving the development and they do have all the knowledge

105
0:13:19.600 --> 0:13:28.440
needed to eventually fix issues as soon as they arise. So around automation, use an immutable

106
0:13:28.440 --> 0:13:33.920
approach to your infrastructure. If you start to have different things and weird infrastructure

107
0:13:33.920 --> 0:13:40.840
going on, it will be a dead sentence. Second, version your infrastructure, GitOps is an

108
0:13:40.840 --> 0:13:47.720
option. There are many others. No matter what you do, try to have versions so that effectively

109
0:13:47.720 --> 0:13:52.480
you can potentially roll back or at least see what changed from a version that is known

110
0:13:52.480 --> 0:13:59.960
to be working to the current one and automate the whole process. If you have humans involved,

111
0:13:59.960 --> 0:14:06.120
you will have issues. It will cost more and it will be effectively less resilient and

112
0:14:06.120 --> 0:14:13.800
reliable. So putting all together what we have seen, I would suggest to first create

113
0:14:13.800 --> 0:14:19.560
a multi-data center architecture so that effectively you have all that redundancy and that kind

114
0:14:19.560 --> 0:14:27.640
of things, but hide them from your developers. Maybe they know the region concept or the

115
0:14:27.640 --> 0:14:35.760
exact concept, but don't show the physical layout to your users, otherwise they will

116
0:14:35.760 --> 0:14:42.720
start to do weird stuff. Second, use a tool to manage the clusters. Open cluster management

117
0:14:42.720 --> 0:14:49.400
is an open source project that does it. There are other projects that do similar things.

118
0:14:49.400 --> 0:14:53.600
It's very, very useful and it will help you over time because probably you will end up

119
0:14:53.600 --> 0:15:00.360
running many clusters. Third, I would suggest personally to standardize on the Kubernetes

120
0:15:00.360 --> 0:15:07.400
APIs as the only interface between the workload and the infrastructure because those are,

121
0:15:07.400 --> 0:15:16.360
as seen, very known. Use a bare metal container platform so don't use virtualization or other

122
0:15:16.360 --> 0:15:23.080
stuff into it. You will have hopefully enough workload to justify tons of physical servers

123
0:15:23.080 --> 0:15:28.880
that don't add complexity with virtualization in between. Automate all the infrastructure

124
0:15:28.880 --> 0:15:37.680
pieces and configurations, obviously, as seen. Start providing only a few interfaces to your

125
0:15:37.680 --> 0:15:45.920
business and then eventually extend them when needed. An example would be an OCR registry,

126
0:15:45.920 --> 0:15:52.240
object storage, and pods, deployments, those kind of basic things. Then if your business

127
0:15:52.240 --> 0:16:00.400
comes out saying, oh, we really need that, then eventually you expand. The thing is,

128
0:16:00.400 --> 0:16:06.040
only provide new services when you are sure that there is the requirement for it. For

129
0:16:06.040 --> 0:16:10.840
instance, let's say that you want to do a database as a service. You already have onboarded

130
0:16:10.840 --> 0:16:17.640
100 applications. 80 of those actually use MySQL. It would make sense to provide a MySQL

131
0:16:17.640 --> 0:16:23.160
as a service, but it does not make sense to provide 50 different databases as a service

132
0:16:23.160 --> 0:16:29.800
of which 48 will never be used. It's only complexity and cost for you. Then create a

133
0:16:29.800 --> 0:16:37.080
simple UX for your users that completely obstruct everything that is below. It could be even,

134
0:16:37.080 --> 0:16:43.960
you know, push your Kubernetes configuration here and we will manage it. Hopefully, then

135
0:16:43.960 --> 0:16:52.080
you will be ensuring that all this stuff is versioned and so on so that even when the

136
0:16:52.080 --> 0:16:59.640
workload fails for some reason, you can say, look, a version N minus one was working. You

137
0:16:59.640 --> 0:17:07.680
did something. Now it's broken. It's not the infra. This was it. Thank you. I don't

138
0:17:07.680 --> 0:17:34.920
know if we have a couple of minutes for questions. No, if there are. Thank you for your talk.

139
0:17:34.920 --> 0:17:39.640
Could you expand a bit on the I didn't get why the what was the advantages of building

140
0:17:39.640 --> 0:17:47.560
multiple data centers at first? Yeah, so that is usually a business requirement because

141
0:17:47.560 --> 0:17:52.760
they will say, oh, we want to have everything that is a or at least this service needs to

142
0:17:52.760 --> 0:17:59.520
be a and with one data center, it would be hard. Also, it really depends if you are a

143
0:17:59.520 --> 0:18:05.600
small organization, maybe two data center, three data center could be okay. If you are

144
0:18:05.600 --> 0:18:12.640
a big organization, maybe spread throughout five, 10 legally different regions, then you

145
0:18:12.640 --> 0:18:19.120
will need OZ 3050 data center. That's a completely different scale. OZ all those are very generic

146
0:18:19.120 --> 0:18:25.840
suggestions and then you have to apply them to your specific situation. And just a quick

147
0:18:25.840 --> 0:18:32.240
follow up for that. How do you hide that from the workload developer? So the line just after

148
0:18:32.240 --> 0:18:37.200
that one where you say they have to not know about the multiple clusters. How does that

149
0:18:37.200 --> 0:18:43.640
work? Yeah, so if you pick AWS, for instance, they have the concept of region and a Z. Some

150
0:18:43.640 --> 0:18:50.600
a Z. So it's are not that centers. Some are data centers. Other are parts of a data center,

151
0:18:50.600 --> 0:18:56.120
but different availability zones within the data center are others are containers in the

152
0:18:56.120 --> 0:19:02.800
sense of like 40 food containers full of servers. So the user does not know. They know that

153
0:19:02.800 --> 0:19:09.860
there is region X, a Z one, two, three. What one, two, three means no one knows and no

154
0:19:09.860 --> 0:19:29.200
one cares. And that's the thing. Thank you for the talk. And in your definition of lock-in,

155
0:19:29.200 --> 0:19:34.760
you spoke about cost of portability and multiply by probability of portability, but doesn't

156
0:19:34.760 --> 0:19:44.080
it like if you file to assess the probability of portability, wouldn't you fall in lock-in

157
0:19:44.080 --> 0:19:53.560
without being aware of it? Sorry, what do you mean? Okay, I know I will always run my

158
0:19:53.560 --> 0:20:02.200
cloud in Amazon Web Service. Why would I need portability? But then then I start using locked

159
0:20:02.200 --> 0:20:10.360
in products. So I will never be able to leave. Yes. Well, you will be able to leave. It's

160
0:20:10.360 --> 0:20:14.760
always possible to leave. You will simply re-write from scratch your whole application

161
0:20:14.760 --> 0:20:21.000
and you leave. So what is the cost of that? A billion? Okay. So now it becomes a billion

162
0:20:21.000 --> 0:20:28.280
of lock-in. That is my point. You can re-write tomorrow from scratch from the West up. It's

163
0:20:28.280 --> 0:20:34.000
possible. How much it will cost you? A billion, five billion, a trillion? Okay, that is your

164
0:20:34.000 --> 0:20:41.200
lock-in value. And that's the thing. Obviously, I would suggest you to keep the lock-in as

165
0:20:41.200 --> 0:20:46.760
low as possible. So try not to re-write. To be in a situation where you have to re-write

166
0:20:46.760 --> 0:20:58.240
everything. Yes, thank you. So we short-transmitted that one. Hello. One quick question. I'm going

167
0:20:58.240 --> 0:21:04.440
to ask you a question. So if your organization has a traditional manual approach to operations,

168
0:21:04.440 --> 0:21:11.560
which thing would you automate first? I would start from very simple processes just to ensure

169
0:21:11.560 --> 0:21:18.600
that, you know, it works in the organization. The organizations start to understand it. Processes

170
0:21:18.600 --> 0:21:26.680
like, you know, create VMs or create containers, whatever kind of thing you do. And then some

171
0:21:26.680 --> 0:21:32.360
things such as patching and so on. But if you really want to go the automation way,

172
0:21:32.360 --> 0:21:37.400
it's way easier to, after you have tested a little bit the thing, start to say, okay,

173
0:21:37.400 --> 0:21:42.920
now we have the version two of the environment that is fully automated from day zero. Otherwise,

174
0:21:42.920 --> 0:21:49.200
you will always be in a kind of automated but not completely automated situation. Thank

175
0:21:49.200 --> 0:21:57.680
you. Thank you all. Thank you.

