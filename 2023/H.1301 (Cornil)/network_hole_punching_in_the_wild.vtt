WEBVTT

00:00.000 --> 00:07.060
Hello, everyone.

00:07.060 --> 00:09.000
Thanks for joining today.

00:09.000 --> 00:13.480
Welcome to our talk on hole punching in the wild.

00:13.480 --> 00:19.020
Sometimes I would say we're going to talk about the biggest hack of the Internet, which

00:19.020 --> 00:21.200
I would refer to as hole punching.

00:21.200 --> 00:29.360
We want to talk a bit about learnings from doing hole punching on larger networks.

00:29.360 --> 00:34.060
Some might remember me from last year in Fosstem where I introduced our way of doing hole

00:34.060 --> 00:42.160
punching and today we're coming here with a bunch of data.

00:42.160 --> 00:43.160
So who are we?

00:43.160 --> 00:45.120
Dennis, do you want to introduce yourself?

00:45.120 --> 00:46.120
Yeah, okay.

00:46.120 --> 00:47.120
My name is Dennis.

00:47.120 --> 00:52.280
I'm working at ProcoLab as a research engineer at a team called ProbLab and I'm mainly focusing

00:52.280 --> 00:59.120
on network measurements and proco-optimizations that come out of these measurements and, yeah,

00:59.120 --> 01:03.360
I was working with Max on this hole punching campaign.

01:03.360 --> 01:04.360
Very cool.

01:04.360 --> 01:08.160
And Max, again, software engineer.

01:08.160 --> 01:11.840
You can find us anywhere there online if you want.

01:11.840 --> 01:16.920
Yeah, happy to communicate online further after the talk and we're also around at the

01:16.920 --> 01:17.920
venue.

01:17.920 --> 01:18.920
Wonderful.

01:18.920 --> 01:25.080
Okay, what were we doing today?

01:25.080 --> 01:31.360
I want to do a very quick intro to LIP2P, a peer-to-peer networking library, but then

01:31.360 --> 01:38.360
dive right into the problem of why firewalls and nats are rather hard for peer-to-peer

01:38.360 --> 01:39.920
networking.

01:39.920 --> 01:46.520
The solution, which in some cases is hole punching, then how LIP2P does all of that

01:46.520 --> 01:53.240
and then we have been running a large measurement campaign on the internet in the wild collecting

01:53.240 --> 01:59.160
data how well hole punching works out there and we're going to present those findings

01:59.160 --> 02:05.680
and then have takeaways of what we learned from there and where we're going from there.

02:05.680 --> 02:11.520
All right, LIP2P, just a quick introduction.

02:11.520 --> 02:13.600
It's a peer-to-peer networking library.

02:13.600 --> 02:16.160
It's an open source project.

02:16.160 --> 02:21.720
There's one specification and then there are many implementations of that specification,

02:21.720 --> 02:30.080
among other languages in Go, JS, Rust, NIM, C++, Java, many, many out there.

02:30.080 --> 02:33.520
It provides, I would say, two levels.

02:33.520 --> 02:38.480
On the low level, it provides all kind of different connectivity options.

02:38.480 --> 02:45.240
It takes care of the encryption and authentication, here being mutual authentication.

02:45.240 --> 02:48.400
And then things like hole punching, for example.

02:48.400 --> 02:52.520
Once you have these low level features of being able to connect to anyone out there

02:52.520 --> 02:57.200
in an encrypted and authenticated way, you can then build higher level protocols on top

02:57.200 --> 03:03.280
of that, which LIP2P also provides, like a DHT distributed hash table or gossiping protocols

03:03.280 --> 03:05.480
and things like that.

03:05.480 --> 03:13.080
My big statement always about LIP2P is it's all you need to build your peer-to-peer application.

03:13.080 --> 03:16.440
All right.

03:16.440 --> 03:19.280
To zoom out a little bit, that's LIP2P.

03:19.280 --> 03:23.440
All the things that we're talking about today are implemented in LIP2P, but that doesn't

03:23.440 --> 03:29.440
mean you can't implement it in any other networking library if you want to.

03:29.440 --> 03:35.000
Our great motivation for LIP2P and in general for peer-to-peer networking is that we have

03:35.000 --> 03:42.360
full connectivity among all the nodes within the network to the best of their capabilities,

03:42.360 --> 03:43.860
obviously.

03:43.860 --> 03:49.440
In this talk, we're going to focus on the problem of NATs and firewalls for peer-to-peer

03:49.440 --> 03:50.440
networking.

03:50.440 --> 03:55.640
Now, before all of you yell, I'm not saying let's get rid of firewalls.

03:55.640 --> 03:57.160
Please let's not do that.

03:57.160 --> 04:03.520
They have a very important purpose, but in some cases, we want to get around them.

04:03.520 --> 04:05.000
Okay.

04:05.000 --> 04:06.400
Cool.

04:06.400 --> 04:07.400
Yeah.

04:07.400 --> 04:08.880
I'm here in the network dev room.

04:08.880 --> 04:12.240
I'm not going to explain what NATs and firewalls are.

04:12.240 --> 04:16.640
I forget, let's say, it is firewalls.

04:16.640 --> 04:20.120
But we will go a little bit into what that means for hole punching.

04:20.120 --> 04:27.480
In general, for hole punching, NATs and firewalls are our big ones that we can have to get around.

04:27.480 --> 04:29.240
Okay.

04:29.240 --> 04:34.960
What is the problem in some fancy pictures?

04:34.960 --> 04:40.720
A wants to send a packet to B, whether that's a TCP SIN or anything.

04:40.720 --> 04:44.680
And A and B are both behind their home routers.

04:44.680 --> 04:48.040
Just imagine two laptops in two different houses.

04:48.040 --> 04:51.280
And they want to communicate directly with each other.

04:51.280 --> 04:57.360
So A sends a packet to B. It crosses A's router.

04:57.360 --> 05:02.560
A's router sets a five tuple in its routing table for that packet.

05:02.560 --> 05:09.040
And the packet makes it to B. And obviously, a very good thing is that B drops that packet

05:09.040 --> 05:13.040
because it's a packet that has no clue where it's coming from.

05:13.040 --> 05:15.000
Probably some wider internet.

05:15.000 --> 05:16.040
And it might be an attack.

05:16.040 --> 05:17.360
So it's dropping it.

05:17.360 --> 05:21.760
It doesn't have any five tuple in its routing table.

05:21.760 --> 05:22.760
Right?

05:22.760 --> 05:23.760
Okay.

05:23.760 --> 05:25.200
So that is the problem.

05:25.200 --> 05:30.640
And we somehow want to make A and B communicate with each other.

05:30.640 --> 05:35.400
So the solution here, in some cases, it's hole punching.

05:35.400 --> 05:40.680
We want A and B to connect to each other.

05:40.680 --> 05:45.920
Instead of only having A send a packet to B, we have both of them send a packet at the

05:45.920 --> 05:46.920
same time.

05:46.920 --> 05:50.520
I'm talking in a little bit about what at the same time means.

05:50.520 --> 05:55.720
But let's just for now say we have some magic synchronization mechanism.

05:55.720 --> 06:02.840
So A sends a packet to B. B sends a packet to A. The packet from A punches a hole in

06:02.840 --> 06:04.820
its routing table.

06:04.820 --> 06:07.120
So adding a five tuple for it.

06:07.120 --> 06:14.000
The packet from B punches a hole in its routing table on its side.

06:14.000 --> 06:16.160
The packets cross somewhere in the internet.

06:16.160 --> 06:17.200
Obviously, they don't.

06:17.200 --> 06:21.600
But it's a nice metaphor.

06:21.600 --> 06:27.000
And at some point, packet B arrives at router A. Router A checks its routing table.

06:27.000 --> 06:30.400
A little bit simplified here.

06:30.400 --> 06:39.160
It lets packet B pass same on router B. And this way, we actually exchange packets.

06:39.160 --> 06:42.480
Cool.

06:42.480 --> 06:50.440
So now the big problem is how does A and B know when to send those packets?

06:50.440 --> 06:53.840
It has to happen at the same time, at least for TCP.

06:53.840 --> 06:56.860
We might go a little bit into what that means for UDP.

06:56.860 --> 07:02.220
But at least for TCP, this needs to happen at the same time for TCP simultaneous open

07:02.220 --> 07:04.640
to happen in the end.

07:04.640 --> 07:06.560
So how do we do that?

07:06.560 --> 07:08.320
This is LIPIDAP specific.

07:08.320 --> 07:09.960
It doesn't need to be LIPIDAP.

07:09.960 --> 07:14.080
You can use any signaling protocol on top.

07:14.080 --> 07:16.320
Let's say A and B want to connect.

07:16.320 --> 07:18.720
And they need to hole punch at the same time.

07:18.720 --> 07:22.260
They need to send those two packets from both sides at the same time.

07:22.260 --> 07:28.600
So one can go through the hole of the other to the other side.

07:28.600 --> 07:29.600
What do we do?

07:29.600 --> 07:31.440
We need some kind of coordination mechanism.

07:31.440 --> 07:37.080
So some kind of public server out there that is not behind a firewall on that.

07:37.080 --> 07:40.760
B connects to the relay.

07:40.760 --> 07:44.000
A learns B's address through the relay.

07:44.000 --> 07:45.840
A connects through the relay.

07:45.840 --> 07:52.280
So now the two, A and B, have a communication channel over the relay.

07:52.280 --> 08:01.840
B sends a message to A. You can just think of it as a time synchronization protocol.

08:01.840 --> 08:10.920
And at the same time, while sending that message, it measures the time it takes for A to send

08:10.920 --> 08:12.120
a message back.

08:12.120 --> 08:15.000
So at this time, we know the round trip time.

08:15.000 --> 08:23.360
And then once we know the round trip time, B sends another message to A and waits exactly

08:23.360 --> 08:25.480
half the round trip time.

08:25.480 --> 08:31.440
And once A receives that sun down there, you can do the math.

08:31.440 --> 08:37.160
If now both of them start, so A when it receives the packet, and B after half the round trip

08:37.160 --> 08:42.020
time, they actually do the hole punch.

08:42.020 --> 08:45.960
They exchange the packets, they cross somewhere in the internet.

08:45.960 --> 08:49.600
Both of them punch the hole into their routers.

08:49.600 --> 08:51.300
And ta-da.

08:51.300 --> 08:52.300
We succeeded.

08:52.300 --> 08:53.300
We have a hole punch.

08:53.300 --> 08:56.400
We have a connection established.

08:56.400 --> 08:57.880
Cool.

08:57.880 --> 08:58.880
Okay.

08:58.880 --> 09:03.280
A little bit in terms of timeline on all of this.

09:03.280 --> 09:04.600
Hole punching is nothing new.

09:04.600 --> 09:07.560
It's definitely nothing that Lippy to P invented.

09:07.560 --> 09:11.160
Not at all.

09:11.160 --> 09:16.920
The most obvious mention I know is an RFC 5128.

09:16.920 --> 09:20.120
But again, it predates that for sure.

09:20.120 --> 09:24.040
But I think it's a nice introduction to hole punching in general in case you like reading

09:24.040 --> 09:27.280
ITF documents.

09:27.280 --> 09:33.800
Since then, we have been implementing it around 2021, 2022, basing on a lot of past knowledge

09:33.800 --> 09:34.960
around that.

09:34.960 --> 09:43.360
I've been presenting this work at Fostem 2022 last year remotely.

09:43.360 --> 09:49.840
And since then, we have rolled it out on a larger network, which is the IPFS network,

09:49.840 --> 09:56.560
in a two phase way where all public nodes act as relay nodes, very limited relays.

09:56.560 --> 10:02.280
And then in a second phase, all the clients gained the hole punching capabilities.

10:02.280 --> 10:08.600
And now on this large peer-to-peer network, we actually have on the public nodes relaying

10:08.600 --> 10:10.200
for the signaling.

10:10.200 --> 10:14.760
And then the clients actually being able to do the hole punching work.

10:14.760 --> 10:18.160
And so we have this deployed now in this large network.

10:18.160 --> 10:23.080
But it's very hard to know how it's working, especially across the internet, across all

10:23.080 --> 10:28.280
the networks, across all the different endpoints, across all the routing hardware and so on.

10:28.280 --> 10:32.720
So that's why we launched the hole punching month, which is kind of like a measurement

10:32.720 --> 10:36.120
campaign, which Dennis now is going to introduce.

10:36.120 --> 10:37.120
Sorry.

10:37.120 --> 10:39.720
Can you hear me?

10:39.720 --> 10:40.720
Yes.

10:40.720 --> 10:41.720
All right.

10:41.720 --> 10:42.720
Thanks, Max.

10:42.720 --> 10:50.000
Yeah, as Max said, the LIPDP folks conceived this new DCUTR protocol and then deployed

10:50.000 --> 10:51.000
it to the network.

10:51.000 --> 10:53.800
And now we want to know how well does it actually work.

10:53.800 --> 10:57.400
And for this, we launched, as Max said, a measurement campaign during December.

10:57.400 --> 10:59.680
I will get to this in a second.

10:59.680 --> 11:03.920
But how actually do we measure these hole punching success rates?

11:03.920 --> 11:11.600
And the challenge here is that we actually don't know the clients that are DCUTR capable.

11:11.600 --> 11:14.240
So where are the clients that we want to hole punch?

11:14.240 --> 11:15.640
Because they are behind nets.

11:15.640 --> 11:16.840
We cannot enumerate them.

11:16.840 --> 11:22.800
They don't register themselves in a central registry or so.

11:22.800 --> 11:25.880
So we conceived this three-component architecture.

11:25.880 --> 11:29.880
And the crucial thing here probably is this honeypot component, which is just a DHT server

11:29.880 --> 11:34.960
node that interacts with, as Max said, the IPFS network.

11:34.960 --> 11:36.400
And it's a very stable node.

11:36.400 --> 11:41.200
And this means that it gets added to routing tables of different peers in the network.

11:41.200 --> 11:47.000
And this increases chances if peers behind nets interact with this IPFS network come

11:47.000 --> 11:49.160
across this honeypot.

11:49.160 --> 11:54.960
So peers behind nets is in this diagram, the top right corner, some DCUTR-capable peer.

11:54.960 --> 11:59.000
This one, by chance, by interacting with the network, comes across the honeypot.

11:59.000 --> 12:04.040
And the honeypot then keeps track of those peers and writes it into a database.

12:04.040 --> 12:09.560
And then this database is interfaced by a server component that serves those identified

12:09.560 --> 12:14.000
and detected peers to a fleet of clients.

12:14.000 --> 12:19.880
And the hole punch measurement campaign consisted of a deployment of those clients to a wide

12:19.880 --> 12:28.440
variety of different laptops or users that agreed to run these kinds of clients.

12:28.440 --> 12:33.440
And this client then queries the server for a peer-to-hole punch.

12:33.440 --> 12:39.160
As Max said, it connects to the other peer-to-relay node and then exchanges those couple of packages,

12:39.160 --> 12:41.400
tries to establish a direct connection.

12:41.400 --> 12:45.960
And then at the end, it reports back if it worked, if it didn't work, what went wrong,

12:45.960 --> 12:46.960
and so on.

12:46.960 --> 12:55.800
And so we can probe the whole network, or like many, many clients and many network configurations.

12:55.800 --> 12:58.080
So we did this measurement campaign.

12:58.080 --> 13:04.680
We made some fuss about it during November internally at ProCollapse and also reached

13:04.680 --> 13:06.240
out to the community.

13:06.240 --> 13:11.760
And starting from the beginning of December, we said, OK, please download these clients.

13:11.760 --> 13:15.240
Run it on your machines.

13:15.240 --> 13:21.000
And let's try to gather as much data as possible during that time.

13:21.000 --> 13:25.440
And as you can see here, so we collected around 6.25 million hole punch results.

13:25.440 --> 13:31.720
So this is quite a lot of data from 154 clients that participated.

13:31.720 --> 13:37.160
And we punched around 47,000 unique peers in this network.

13:37.160 --> 13:41.520
And on the right-hand side, you can see the deployment of our clients, of our controlled

13:41.520 --> 13:42.520
clients.

13:42.520 --> 13:45.840
The color here is the number of contributed results.

13:45.840 --> 13:47.760
So the US was dominant here.

13:47.760 --> 13:52.880
But we have many other nodes deployed in Europe, but also Australia and New Zealand, and also

13:52.880 --> 13:57.760
South America, and also one client from the continent of Africa.

13:57.760 --> 14:05.560
And these clients interacted with these other peers that are basically all around the world.

14:05.560 --> 14:11.360
So we could measure hole punch success rates all across the globe.

14:11.360 --> 14:16.960
And I think we have a very comprehensive data set here.

14:16.960 --> 14:21.520
And so we now gathered the data.

14:21.520 --> 14:26.040
And at the beginning of January, I started.

14:26.040 --> 14:28.160
So I said, OK, the hole punch link month is over.

14:28.160 --> 14:31.440
And I started to analyze the data a little bit.

14:31.440 --> 14:37.240
And what you can see here on the x-axis is the, so each bar is a unique client.

14:37.240 --> 14:40.760
And on the y-axis, we can see these different outcomes.

14:40.760 --> 14:45.600
So each hole punch result, as I said, can have, so the clients report back these results,

14:45.600 --> 14:47.720
and each result can have a different outcome.

14:47.720 --> 14:51.120
These outcomes are at the top, so it can be successful.

14:51.120 --> 14:55.640
So we actually were able to establish a direct connection through hole punching.

14:55.640 --> 14:57.200
Then connection reversed.

14:57.200 --> 15:03.320
This means I'm trying to hole punch, so I'm connecting to the other peer through the relay.

15:03.320 --> 15:07.560
And the first thing before we do the hole punching dance is for the peer to directly

15:07.560 --> 15:12.160
connect to us, because if we are directly reachable because we have port mapping in

15:12.160 --> 15:16.860
place in the router, we don't actually need to do the hole punching exchange.

15:16.860 --> 15:18.240
This is a connection reversed.

15:18.240 --> 15:21.760
And as we can see here, it's a little hard to see.

15:21.760 --> 15:25.160
But some clients actually have a lot of these results.

15:25.160 --> 15:29.320
So this means they have a unique router configuration in place.

15:29.320 --> 15:31.640
Then failed is the obvious thing.

15:31.640 --> 15:34.640
So we tried, we exchanged these messages.

15:34.640 --> 15:37.840
But in the end, we weren't able to establish a connection.

15:37.840 --> 15:45.640
No stream is some internal error that's unique to our setup, so probably nothing to worry

15:45.640 --> 15:46.640
about here.

15:46.640 --> 15:50.800
And no connection means we try to connect to the other peer through a relay, but the

15:50.800 --> 15:52.080
other peer was already gone.

15:52.080 --> 15:55.840
It's a permissionless peer to peer network, so it could be from the time that the honeypot

15:55.840 --> 16:01.280
detected the peer to the client trying to establish a connection to the peer that the

16:01.280 --> 16:04.400
client has already churned and left the network.

16:04.400 --> 16:10.120
But actually looking at these clients is a distorted view on the data because we allowed

16:10.120 --> 16:15.880
everyone who participated in the campaign to freely move around.

16:15.880 --> 16:19.680
So I was running this client on my laptop, and I was moving from a coffee shop, Wi-Fi

16:19.680 --> 16:23.440
network, to a home network, to a university network, and so on.

16:23.440 --> 16:28.000
And hole punching is actually dependent on those network configurations instead of just

16:28.000 --> 16:29.880
me running the client.

16:29.880 --> 16:35.640
So the challenge here with the data analysis was, so I'm also not done with that yet, and

16:35.640 --> 16:40.520
happy to open for suggestions, to detect these individual networks that the clients operated

16:40.520 --> 16:41.520
in.

16:41.520 --> 16:47.600
With each hole punch result, the client reported their listening IP addresses and so on, and

16:47.600 --> 16:53.040
I grouped them together to actually find out, to identify unique networks that those clients

16:53.040 --> 16:54.720
operated in.

16:54.720 --> 17:01.160
And at the end, I arrived at 342 unique client networks, and then the graph looks like this,

17:01.160 --> 17:03.680
probably not much different than before.

17:03.680 --> 17:10.800
But also there are some interesting unique network outcomes here that I will also get

17:10.800 --> 17:14.040
to in a bit.

17:14.040 --> 17:19.600
The most interesting graph is probably this one, so what's the success rate of this protocol?

17:19.600 --> 17:26.800
And on the x-axis we have the success rate, binned by, yeah, just 5% binning, and on the

17:26.800 --> 17:33.040
y-axis the number of networks that had the success rate by probing the whole other network.

17:33.040 --> 17:37.440
And the majority of networks actually had a success rate of 70%, so I think this is

17:37.440 --> 17:44.200
already, actually I think it's amazing, because from not being able to connect at all to having

17:44.200 --> 17:49.080
a 70% chance to establish a direct connection without an intermediary, it's actually pretty

17:49.080 --> 17:50.720
great.

17:50.720 --> 17:54.760
But then also there are some networks that have very low success rate, and these are

17:54.760 --> 18:00.320
the ones that are probably the most interesting ones.

18:00.320 --> 18:08.280
Then also, the IP and transport dependence is also quite interesting, as an angle to

18:08.280 --> 18:11.400
look at the data.

18:11.400 --> 18:17.680
Here we can see the top row we used IPv4 and TCP to hole punch, so when these clients exchange

18:17.680 --> 18:25.320
these connect messages, they actually exchange the publicly reachable IP addresses of those

18:25.320 --> 18:27.400
two peers that want to hole punch.

18:27.400 --> 18:32.440
And in our measurement campaign we restricted this to actually only IPv4 and TCP, and with

18:32.440 --> 18:37.520
some other hole punches only to IPv6 and QUIC, which is on the bottom right.

18:37.520 --> 18:42.520
And so we can take a look at which combination is more successful than the other.

18:42.520 --> 18:49.440
And here we can see that IPv4 and TCP and QUIC is actually, if you average the numbers,

18:49.440 --> 18:55.080
has a similar success rate, but on IPv6 it's basically not working at all.

18:55.080 --> 18:59.720
And these unexpected things are actually the interesting ones for us.

18:59.720 --> 19:04.320
Either it's a measurement error, or there's some inherent property to the networking setup

19:04.320 --> 19:11.520
that prevents IPv6 from being hole punchable, basically.

19:11.520 --> 19:20.240
So if we actually allow both transports, so in the previous graph we showed we were only

19:20.240 --> 19:26.000
using TCP and QUIC, but if we allow both transports to simultaneously try to hole punch, we can

19:26.000 --> 19:31.720
see that with 81% we end up with a QUIC connection, and this is just because QUIC connection establishment

19:31.720 --> 19:35.320
is way faster than TCP connection.

19:35.320 --> 19:41.480
So this is like an expected result here, just to verify some of the data here.

19:41.480 --> 19:45.440
And now two takeaways for us for protocol improvements.

19:45.440 --> 19:51.000
So if we took at private VPNs, so if clients are running in VPNs, we can see that the success

19:51.000 --> 19:56.200
rate actually drops significantly from around 70% to less than 40%.

19:56.200 --> 20:00.240
And my hypothesis here is that the router time that Max showed previously is measured

20:00.240 --> 20:05.520
between A and B, but what we actually need is the router time between the router A and

20:05.520 --> 20:12.320
router B, and if your router basically is your exit node or your gateway that you're

20:12.320 --> 20:17.960
connected to from your VPN, this can differ by dozens of milliseconds actually, and so

20:17.960 --> 20:21.720
the router time doesn't add up and the whole synchronization is a little off.

20:21.720 --> 20:25.960
So this is potentially a protocol improvement here.

20:25.960 --> 20:31.880
And then also interesting, so Max said they are exchanging these messages during the hole

20:31.880 --> 20:35.880
punch, but actually we tried this three times.

20:35.880 --> 20:39.480
So if it doesn't work the first time, we try it again, and if it doesn't work the second

20:39.480 --> 20:41.480
time, we try it yet again.

20:41.480 --> 20:47.160
But when we look at the data, if we end up with the successful hole punch connection,

20:47.160 --> 20:53.320
it was actually successful with the first attempt in 97% or 98% of the cases.

20:53.320 --> 21:00.600
So this is also something for the next steps for us.

21:00.600 --> 21:05.120
We should consider changing our strategy on the second and third try to increase the odds.

21:05.120 --> 21:10.440
So if we stick with the three retries, we shouldn't do the same thing over again, because

21:10.440 --> 21:14.580
as we saw from the data, it doesn't make a difference.

21:14.580 --> 21:17.080
So we should change our strategy here.

21:17.080 --> 21:29.200
And so one thing would be to reverse the client server roles in this quick hole punching exchange.

21:29.200 --> 21:32.360
This would be something.

21:32.360 --> 21:37.600
And also the other protocol improvement for us, as I said, would be to change the measurement

21:37.600 --> 21:39.800
of the round trip time.

21:39.800 --> 21:45.360
And for the future, the data analysis, right now what I showed here is basically aggregates

21:45.360 --> 21:46.880
across all the data.

21:46.880 --> 21:53.880
And the interesting part is basically, so why is a specific client or a specific network,

21:53.880 --> 21:56.960
why has it less or a worse success rate than others?

21:56.960 --> 22:00.360
So these are these individual things to look into to increase.

22:00.360 --> 22:04.640
Maybe there's a common pattern that you can address with a protocol to increase the success

22:04.640 --> 22:08.160
rate and then identify those causes.

22:08.160 --> 22:12.580
And also, at the end of all of this, we want to craft a follow-up publication to something

22:12.580 --> 22:20.480
that Max and some fellow friends, I would say, have published just last year.

22:20.480 --> 22:27.120
And we want to make the data set public and so on and so forth for others to benefit from

22:27.120 --> 22:31.240
the data and can do their own analysis.

22:31.240 --> 22:32.960
And with that, get involved.

22:32.960 --> 22:34.920
Talk to us here at the venue about all of this.

22:34.920 --> 22:37.280
LIP-ITV is a great project.

22:37.280 --> 22:39.400
Have a look at all these links.

22:39.400 --> 22:41.360
Get in touch and contribute.

22:41.360 --> 22:42.880
Join our community calls.

22:42.880 --> 22:44.960
And I think that's it.

22:44.960 --> 22:45.960
Thank you very much.

22:45.960 --> 22:59.160
This is what you implemented there.

22:59.160 --> 23:07.560
Is it exactly Ice turn stuff or how different it is from this?

23:07.560 --> 23:11.440
So we differ in some cases.

23:11.440 --> 23:14.400
It's definitely very much motivated by Ice in turn.

23:14.400 --> 23:16.400
So a couple of things.

23:16.400 --> 23:17.920
We don't do turn itself.

23:17.920 --> 23:20.260
We have our own relay protocol.

23:20.260 --> 23:26.480
Because nodes in the network act as public for the public as relay nodes.

23:26.480 --> 23:30.240
And the problem is you don't want to relay any traffic for anyone, but you want to make

23:30.240 --> 23:34.040
this really restricted in terms of how much traffic, how long.

23:34.040 --> 23:40.640
If you run a public node, you don't want to be the next relay node for everyone out there.

23:40.640 --> 23:48.200
And then what we built here is very much TCP specific, but it also works well with UDP.

23:48.200 --> 23:50.040
We need the synchronization.

23:50.040 --> 23:54.120
And as far as I know, at least the WebRTC stack is very focused on UDP where timing

23:54.120 --> 23:56.080
doesn't matter as much.

23:56.080 --> 23:59.600
So you saw the timing protocol.

23:59.600 --> 24:04.960
And that is very TCP specific where we want a TCP simultaneous connect which allows two

24:04.960 --> 24:17.240
sends to actually result in a single TCP connection.

24:17.240 --> 24:18.600
This is for your analysis.

24:18.600 --> 24:23.280
I guess a lot of this depends on the default configurations of the firewall.

24:23.280 --> 24:31.200
Did you kind of find out what are the branch type of firewalls or configurations that stops

24:31.200 --> 24:32.840
hole punching in your research?

24:32.840 --> 24:40.820
So yeah, so not in its entirety, but what we did is people that signed up for the measurement

24:40.820 --> 24:43.440
campaign gave us information about the networks.

24:43.440 --> 24:49.120
And so if we find something fishy in the data, we could also reach out to them and ask what's

24:49.120 --> 24:53.320
the with the firewall setup in your specific network.

24:53.320 --> 24:57.240
We also gather data about port mappings that are in place.

24:57.240 --> 25:02.720
So what the host tries to do is establish a port mapping inside your router.

25:02.720 --> 25:06.560
And this is also reported back.

25:06.560 --> 25:15.840
And what we also did is try to query the login page from these routers and get some information

25:15.840 --> 25:24.600
about what kind of firewall or router actually was preventing you from connecting to someone

25:24.600 --> 25:25.560
else.

25:25.560 --> 25:31.520
So these are the data points that we have to get some conclusions around this.

25:31.520 --> 25:34.600
But more than this, we don't have.

25:34.600 --> 25:43.440
But I think this is already pretty conclusive to a wide variety of analysis.

25:43.440 --> 25:45.560
What I was just wondering about is, do you have any data?

25:45.560 --> 25:49.800
How many clients actually were behind the net?

25:49.800 --> 25:57.260
So all these clients that the honeypot detected were clients that are behind the net.

25:57.260 --> 25:58.560
So these are all LTPP hosts.

25:58.560 --> 26:03.920
And with the default configuration of LTPP hosts, if they only announce relay addresses,

26:03.920 --> 26:10.320
this means that they must be not publicly reachable, which is for us equivalent with

26:10.320 --> 26:11.320
being behind the net.

26:11.320 --> 26:13.320
So it should be.

26:13.320 --> 26:16.640
There's probably some error there.

26:16.640 --> 26:20.960
So then all of the IPv6 kind of hosts you were trying to connect to also were behind

26:20.960 --> 26:21.960
the net.

26:21.960 --> 26:22.960
Kind of IPv6.

26:22.960 --> 26:23.960
Yes, yes.

26:23.960 --> 26:25.040
And this is the interesting thing.

26:25.040 --> 26:26.800
So I cannot explain this yet.

26:26.800 --> 26:30.120
Maybe it's a measurement error from us.

26:30.120 --> 26:34.240
Maybe it's some, as I said, inherent property to something.

26:34.240 --> 26:35.240
Maybe it's a protocol error.

26:35.240 --> 26:36.240
I don't know.

26:36.240 --> 26:38.840
And this is the interesting stuff in these kinds of things.

26:38.840 --> 26:39.840
Thanks.

26:39.840 --> 26:46.040
I'm very curious.

26:46.040 --> 26:49.320
I was wondering, does it also work with multiple nets?

26:49.320 --> 27:00.120
Can you hold punch through two nets?

27:00.120 --> 27:04.840
So if another friend of mine who I convinced to run these clients actually was running

27:04.840 --> 27:09.880
behind two nets and it was working.

27:09.880 --> 27:13.080
But I'm not sure how many people actually ran behind two nets.

27:13.080 --> 27:16.640
But in theory, maybe Max you can explain.

27:16.640 --> 27:19.520
Right now we don't have really a lot of data about two nets.

27:19.520 --> 27:24.000
And also we don't have the data, which I think was it called needle.

27:24.000 --> 27:27.480
I don't quite know where you're within the same network.

27:27.480 --> 27:29.800
But you don't know that you're next to each other.

27:29.800 --> 27:33.560
And you actually want to hold punch through your own net, even though you can't connect

27:33.560 --> 27:34.560
to each other.

27:34.560 --> 27:35.560
So there's some challenges.

27:35.560 --> 27:36.560
Do we still have time for another question?

27:36.560 --> 27:37.560
Yeah, yeah.

27:37.560 --> 27:38.560
Sorry.

27:38.560 --> 27:55.320
So you said that for UDP should work similarly.

27:55.320 --> 27:57.240
Did you do any experiments with that?

27:57.240 --> 28:01.360
Because in the past we had a custom UDP hold punching thing and the routers were pretty

28:01.360 --> 28:02.360
brain-dead.

28:02.360 --> 28:07.960
They forgot the mapping within 20 seconds or something.

28:07.960 --> 28:11.160
So we run this measurement campaign on TCP and quick.

28:11.160 --> 28:13.440
And quick in the end is just UDP.

28:13.440 --> 28:19.200
And what we do is something similar to stun in the ICE suit where we continuously try

28:19.200 --> 28:20.980
to keep our mapping up.

28:20.980 --> 28:27.360
And then on nets that do endpoint independent mappings, that actually helps.

28:27.360 --> 28:31.920
So as long as we keep that up for like, I don't know, every 10 seconds or so, then our

28:31.920 --> 28:34.640
mapping survives even on UDP.

28:34.640 --> 28:37.880
Okay, cool.

28:37.880 --> 28:38.400
Thank you very much.
