WEBVTT

00:00.000 --> 00:10.600
Hello everyone, welcome to this session about psyllium surface mesh.

00:10.600 --> 00:18.220
My name is Raymond Dion, I'm field CTO for Isofalent, the originators from psyllium.

00:18.220 --> 00:23.320
Today I'm going to talk a bit about eBPF and psyllium as an introduction, after which I'm

00:23.320 --> 00:28.480
going to talk about how the surface mesh is evolving, after which we'll talk about the

00:28.480 --> 00:33.640
psyllium surface mesh features, what we can do today and what we're planning to support

00:33.640 --> 00:36.080
in the future.

00:36.080 --> 00:40.480
Quick highlight of some upcoming features and some current features and if we have time

00:40.480 --> 00:44.760
I have a little demo to show how it works.

00:44.760 --> 00:50.160
Can I see some hands from you if you know eBPF?

00:50.160 --> 00:51.480
Quite a lot, good.

00:51.480 --> 00:53.520
How many of you know psyllium?

00:53.520 --> 00:54.520
Cool.

00:54.520 --> 00:58.520
How many of you use psyllium actually?

00:58.520 --> 00:59.520
Not as much.

00:59.520 --> 01:01.200
Okay, cool.

01:01.200 --> 01:06.800
For the ones who don't know what eBPF is, I'm going to do an introduction here, eBPF

01:06.800 --> 01:11.080
is standing for Extended Berkeley Packet Filter.

01:11.080 --> 01:14.560
By itself that doesn't mean a lot.

01:14.560 --> 01:20.720
What we like to compare it with is what JavaScript is to the browser, eBPF is to the kernel.

01:20.720 --> 01:28.320
What that means is that using eBPF we can attach programs to kernel events and for the

01:28.320 --> 01:34.320
purpose of this session is that we can attach eBPF programs to kernel events related to

01:34.320 --> 01:35.320
networking.

01:35.320 --> 01:42.320
So that's either a socket being opened network packet being sent on a network device, that

01:42.320 --> 01:45.760
means that's a kernel event and that means that we can attach a program to it and we

01:45.760 --> 01:54.480
can get a matrix from that packet for example or we can do load balancing and such.

01:54.480 --> 02:04.640
So psyllium is built on eBPF, you don't need to be an eBPF developer to actually work with

02:04.640 --> 02:05.640
psyllium.

02:05.640 --> 02:09.120
Psyllium abstracts this complexity and technology under the hood.

02:09.120 --> 02:15.240
So based on the configuration you set, psyllium will mount the required eBPF programs for

02:15.240 --> 02:21.480
you to run and psyllium in short provides networking and load balancing capabilities,

02:21.480 --> 02:28.080
security capabilities and also a lot of observability capabilities using eBPF.

02:28.080 --> 02:31.360
So this is the 30,000 feet view where we are today.

02:31.360 --> 02:39.180
We started with plain networking, IPv6, IPv4 years ago and now we expanded all the networking

02:39.180 --> 02:45.960
capabilities with BGP implementations, net4.6, 6.4, extended load balancing out of the box

02:45.960 --> 02:51.720
we're working on having go BGP control plane fully supported with psyllium.

02:51.720 --> 02:57.680
On top of that we have an observability layer with our Hubble technology which is an observability

02:57.680 --> 03:04.040
tool which provides service to service communication for your name spaces so you can see what's

03:04.040 --> 03:08.060
components, what services are talking to which services.

03:08.060 --> 03:11.480
After which you can make informed decisions for example what kind of network policies

03:11.480 --> 03:13.320
you want to apply.

03:13.320 --> 03:19.200
Also exporting metrics to tools like Raphana and surface mesh on top of that to provide

03:19.200 --> 03:23.840
authentication layer seven path based routing and such.

03:23.840 --> 03:26.820
On the right hand side we also have Tetragon that's not something we'll talk about today

03:26.820 --> 03:32.040
but that's runtime security using eBPF which is also very interesting and we'd run across

03:32.040 --> 03:38.000
clouds doesn't matter if it's on-prem or hybrid or multi-cluster so it's agnostic of the

03:38.000 --> 03:42.080
platform and supported by multiple cloud vendors.

03:42.080 --> 03:46.720
So as you may know Google Enforce data plane V2 on the node is actually psyllium.

03:46.720 --> 03:53.120
Microsoft has recently adopted psyllium as the default CNI for AKS clusters and all their

03:53.120 --> 03:58.240
clusters will be migrated to psyllium and AWS EKS anywhere by default is psyllium so

03:58.240 --> 04:04.880
we see huge adoption in the field of psyllium.

04:04.880 --> 04:07.340
So let's talk about surface mesh.

04:07.340 --> 04:11.960
So obviously if we talk about surface mesh we talk about observing traffic being able

04:11.960 --> 04:19.160
to secure traffic from application to application across clusters doing traffic management building

04:19.160 --> 04:24.120
resilience across applications across clouds.

04:24.120 --> 04:29.520
Surface mesh originally if you needed that capabilities originally you would program

04:29.520 --> 04:34.340
your application either in python or go to get that observability.

04:34.340 --> 04:38.800
That wasn't really useful because you have to maintain all those libraries to get the

04:38.800 --> 04:40.660
information you need.

04:40.660 --> 04:45.320
That's where the sidecars came in right so that they abstract that complexity from the

04:45.320 --> 04:52.800
application to have a standard sidecar implementation to monitor traffic to be able to route traffic

04:52.800 --> 04:56.960
and to be able to extract metrics from the traffic.

04:56.960 --> 05:01.840
However now with psyllium our goal is to move as close to the kernel as we already

05:01.840 --> 05:04.320
run in kernel with eBPF.

05:04.320 --> 05:10.460
So we're moving from a sidecar model to the kernel and where we can we will support it

05:10.460 --> 05:13.640
using eBPF.

05:13.640 --> 05:17.040
The only part which is not yet there is layer 7.

05:17.040 --> 05:24.240
So all the low balancing capabilities routing capabilities in terms of IP to IP metrics

05:24.240 --> 05:28.480
are already available with psyllium using eBPF.

05:28.480 --> 05:37.760
Layer 7 routing is not yet in eBPF for multiple reasons of which one is that eBPF has constraints

05:37.760 --> 05:40.820
in terms of how big a program can be.

05:40.820 --> 05:45.480
Obviously it runs in kernel space so it has constraints for a good reason but in the future

05:45.480 --> 05:52.880
maybe we can even transport complex layer 7 routing in eBPF.

05:52.880 --> 05:58.840
However we already provide layer 7 visibility and ops of ability in using psyllium and eBPF.

05:58.840 --> 06:03.360
We already have the capabilities to inspect traffic using eBPF.

06:03.360 --> 06:08.160
We can already do the low balancing with equip-prosper replacement.

06:08.160 --> 06:14.760
The only part is the layer 7 but the visibility of traffic so HTTP traffic and such is already

06:14.760 --> 06:15.760
there.

06:15.760 --> 06:22.160
So surface mesh capabilities are extending those capabilities moving forward.

06:22.160 --> 06:24.260
So how does it work?

06:24.260 --> 06:29.760
Some of you may know that psyllium runs as an agent as a daemon set on the nodes.

06:29.760 --> 06:35.840
It programs the nodes to be mounting the eBPF programs for the capabilities you need.

06:35.840 --> 06:40.480
We have an embedded envoy running inside the psyllium agent.

06:40.480 --> 06:47.080
This is a narrow down envoy proxy in the agent for networking capabilities.

06:47.080 --> 06:53.460
We leverage this envoy proxy on the node level to do surface mesh capabilities.

06:53.460 --> 06:57.920
So all the things like HTTP path routing and such.

06:57.920 --> 07:02.200
So for each namespace you would create and where you create for example an ingress resource

07:02.200 --> 07:08.760
or a gateway resource that means that a listener will be created through the envoy for that

07:08.760 --> 07:12.520
specific namespace, for that specific work.

07:12.520 --> 07:18.320
And we leverage C groups and stuff to have separation as well for the security reasons

07:18.320 --> 07:23.780
to not be able to have traffic across namespaces as such.

07:23.780 --> 07:29.980
So what is different with psyllium service mesh compared to other service mesh implementations?

07:29.980 --> 07:34.880
First of all our goal is to reduce operational complexity by removing sidecars.

07:34.880 --> 07:40.440
Resource usage reduced better performance and avoids sidecar startup shut down race

07:40.440 --> 07:42.540
conditions.

07:42.540 --> 07:46.560
So obviously if you're not running sidecars at scale this makes a huge difference.

07:46.560 --> 07:51.080
You don't have all the sidecar pods running alongside your normal pods.

07:51.080 --> 07:57.920
It will save memory, it will save CPU, it will save connection tracking etc.

07:57.920 --> 08:00.320
So a lot more efficient.

08:00.320 --> 08:04.680
And also in terms of latency running a sidecar has a cost.

08:04.680 --> 08:09.960
So in this diagram you see that an application wants to send traffic to another application.

08:09.960 --> 08:14.720
What that means technically is that it goes through the TCP IP stack three times with

08:14.720 --> 08:15.960
the sidecar.

08:15.960 --> 08:20.560
First from the app then inbound in the sidecar where the sidecar does its processing and

08:20.560 --> 08:25.600
then external from the sidecar to the physical network device to hit the network to reach

08:25.600 --> 08:28.920
another node.

08:28.920 --> 08:37.840
With eBPF we are able to shortcut that connection and improve the latency because we can detect

08:37.840 --> 08:42.960
the traffic and we can see if it's destined for the physical network or it should be routed

08:42.960 --> 08:46.000
through the proxy.

08:46.000 --> 08:52.200
So once this app opens the socket using eBPF we can shortcut that connection to the physical

08:52.200 --> 08:56.480
network device to be routed on the physical network.

08:56.480 --> 09:02.520
If we need layer seven processing that means that using eBPF we can shortcut the connection

09:02.520 --> 09:06.520
on the socket layer directly to the Envoy proxy where the Envoy proxy on the node does

09:06.520 --> 09:11.480
its HTTP routing and then forwards the traffic again on the physical network.

09:11.480 --> 09:15.040
So a lot less hops there.

09:15.040 --> 09:20.160
And it means that latency is much, much improved because we're not going through this TCP IP

09:20.160 --> 09:22.800
stack multiple times.

09:22.800 --> 09:28.240
In terms of throughput there's also a small difference because we can push more packets.

09:28.240 --> 09:33.800
And in terms of pod ready performance this is also a consideration at scale because when

09:33.800 --> 09:37.800
you're scaling out your applications you always with traditional sidecars you need to wait

09:37.800 --> 09:43.440
for the sidecar to be spun up as well and to be ready to serve connections for that

09:43.440 --> 09:44.900
application.

09:44.900 --> 09:49.040
So without the sidecars with Psyllium Surface Mesh it's already there.

09:49.040 --> 09:53.880
It's running on the node, it's embedded in the proxy so once you scale out your application

09:53.880 --> 10:00.080
the proxy immediately on that node can serve connections.

10:00.080 --> 10:04.120
So in short Psyllium Surface Mesh provides traffic management, observability, security

10:04.120 --> 10:05.520
and resilience.

10:05.520 --> 10:07.440
The goal is to bring your own control plane.

10:07.440 --> 10:11.760
We are not developing a control plane on our own.

10:11.760 --> 10:17.120
What it means is that you can already use Ingress resources with Psyllium 1.13, we'll

10:17.120 --> 10:19.280
support Gateway API.

10:19.280 --> 10:26.360
We are working on Spiffy integration so with the 1.13 release actually the groundwork for

10:26.360 --> 10:30.400
MTLS and Spiffy integration is already there.

10:30.400 --> 10:36.800
You're not really able to use it yet but the goal is to support both MTLS and Spiffy using

10:36.800 --> 10:42.600
Psyllium network policies so you can reference for example a Spiffy ID as a source and destination

10:42.600 --> 10:47.960
using Psyllium network policies and then under the hood the Psyllium agent part will connect

10:47.960 --> 10:55.080
to a Spivey server where that identity is tracked and confirm if that's allowed.

10:55.080 --> 11:00.480
In terms of observability you can leverage the already available observability with Grafana

11:00.480 --> 11:01.560
or Hubble.

11:01.560 --> 11:07.160
If you need to export events you can use scene platforms such as Splunk and OpenTelemetry

11:07.160 --> 11:11.440
is also supported.

11:11.440 --> 11:15.200
If you're new, if you're running new classes you have an option, you can run Psyllium and

11:15.200 --> 11:19.880
you can already use Psyllium Surface Mesh out of the box.

11:19.880 --> 11:27.480
This is obviously the preferred method but if you're running already an Istio based implementation

11:27.480 --> 11:34.160
there's still a lot of benefit to run Psyllium under the hood there as well because for example

11:34.160 --> 11:40.520
we already encrypt the connectivity between the sidecar from an Istio based implementation

11:40.520 --> 11:43.520
towards the destination pod.

11:43.520 --> 11:50.880
What I mean by that is that when you run sidecars, when you run MTLS between applications, that

11:50.880 --> 11:56.720
connectivity may be secure but the connection between the sidecar and the actual destination

11:56.720 --> 11:58.520
is unencrypted on the node.

11:58.520 --> 12:04.320
Anyone with specific privileges on a node could potentially listen on that virtual interface

12:04.320 --> 12:08.680
and eavesdrop traffic and that's obviously not secure.

12:08.680 --> 12:13.520
Running Psyllium under the hood already gives you the benefit because we can encrypt on

12:13.520 --> 12:21.520
layer 4 directly on a socket layer to the destination pod obviously.

12:21.520 --> 12:28.400
With 1.12, so currently we have 1.12 available since I think seven months, we already have

12:28.400 --> 12:32.720
a production ready Psyllium Surface Mesh, a conformant ingress controller which you

12:32.720 --> 12:38.680
can use for HDPath routing, canary releases and such.

12:38.680 --> 12:42.040
You can use Kubernetes as your Surface Mesh control plane.

12:42.040 --> 12:45.800
Prometheus Metrix OpenTelemetry is supported.

12:45.800 --> 12:51.520
For power users we have Psyllium Envoy config and Psyllium Cluster-wide Envoy config CRDs

12:51.520 --> 12:52.520
available.

12:52.520 --> 12:57.360
These are temporarily I would say because the goal is to replace all that capabilities

12:57.360 --> 13:00.700
with Gateway API.

13:00.700 --> 13:04.680
And we're releasing more and more extended Grafana dashboards for layer 7 visibility

13:04.680 --> 13:11.280
so you can actually see between surface to surface what kind of metrics there are and

13:11.280 --> 13:18.720
what the latencies are and what return codes are, so golden signals.

13:18.720 --> 13:24.960
The roadmap for 1.13 and we're very close for releasing 1.13 expected somewhere this

13:24.960 --> 13:27.600
month hopefully.

13:27.600 --> 13:34.640
You can already try a release candidate for Psyllium 1.13 which includes a Gateway API

13:34.640 --> 13:41.880
support for HTTP routing, TLS termination, HTTP traffic splitting and waiting.

13:41.880 --> 13:48.160
So this allows you to do percentage based routing or canary releases as such without

13:48.160 --> 13:51.240
configuring Psyllium Envoy config resources.

13:51.240 --> 13:55.280
And also the capability to have multiple ingresses per load balancer.

13:55.280 --> 14:00.760
What that means is that currently when you create a Psyllium ingress we rely on the load

14:00.760 --> 14:06.280
balancer to attract traffic and forward that to the proxy.

14:06.280 --> 14:10.640
Obviously at scale having a load balancer for each ingress especially in clouds is

14:10.640 --> 14:16.360
expensive so this with an annotation we allow multiple ingresses per load balancer so you

14:16.360 --> 14:20.640
can save cost there.

14:20.640 --> 14:23.560
So how am I doing in time?

14:23.560 --> 14:34.000
So today ingress 1.12 also with services we are having support for annotations so imagine

14:34.000 --> 14:40.800
you have received traffic from your ingress you forward it to a service that means we

14:40.800 --> 14:47.040
support annotations on a simple cluster IP to forward traffic for example to a specific

14:47.040 --> 14:48.500
endpoint.

14:48.500 --> 14:52.640
If you know what Psyllium cluster mesh is we can connect Psyllium across clusters.

14:52.640 --> 14:58.040
With simple annotations you can have even high availability of services across clusters.

14:58.040 --> 15:02.760
Gateway API which I will show a bit later and the Envoy config.

15:02.760 --> 15:06.640
So this is a simple example of ingress and this is also something I will show in the

15:06.640 --> 15:08.360
demo.

15:08.360 --> 15:13.280
You have an ingress and from a specific path you want to forward traffic to a specific

15:13.280 --> 15:14.820
service.

15:14.820 --> 15:21.200
We also support GRPC so you can also have specific GRPC URLs to be forwarded to specific

15:21.200 --> 15:22.800
services.

15:22.800 --> 15:29.160
TLS termination to terminate TLS using secrets using ingress.

15:29.160 --> 15:34.680
A question I get a lot is what about SSL pass through that's on the road map so keep that

15:34.680 --> 15:37.200
in mind.

15:37.200 --> 15:42.080
And obviously new in 1.13 is Gateway API and how it looks like is you will configure a

15:42.080 --> 15:49.180
gateway resource you specify the gateway class name for Psyllium to make sure that the gateway

15:49.180 --> 15:53.080
is created and maintained through Psyllium and then create listeners so in this case

15:53.080 --> 15:56.000
an HTTP listener on port 80.

15:56.000 --> 16:03.260
Then additionally you create multiple HTTP routes one or more and this specify for example

16:03.260 --> 16:09.400
a path prefix for values forward slash details to be forwarded to a backend reference service

16:09.400 --> 16:12.680
called details.

16:12.680 --> 16:15.520
In terms of TLS termination same constructs.

16:15.520 --> 16:21.880
You can also have for example a host name in there to only accept traffic for this given

16:21.880 --> 16:28.640
host name and you reference a secret in the gateway resource and then in the HTTP routes

16:28.640 --> 16:35.640
you will specify the host name, you will reference the gateway you want to use and then again

16:35.640 --> 16:41.260
a path prefix for example to forward to a specific service.

16:41.260 --> 16:48.180
And then traffic splitting very simple also using HTTP routes again referencing your gateway

16:48.180 --> 16:55.460
a path prefix and then you have in this case an echo 1 and echo 2 service where you want

16:55.460 --> 17:02.640
to introduce slowly echo 2 in this case 25% of that traffic will be forwarded to the echo

17:02.640 --> 17:05.360
2 service.

17:05.360 --> 17:10.240
And this is the example what I talked about earlier using simple annotations you can extend

17:10.240 --> 17:16.660
service miss capabilities by annotating services so in this case this service will receive

17:16.660 --> 17:23.960
traffic for gRPC and we can attach load balancing modes for in this case weighted loss request

17:23.960 --> 17:27.480
to be forwarded to backend endpoints.

17:27.480 --> 17:32.720
And using multi cluster capabilities you can extend these capabilities across two or more

17:32.720 --> 17:36.360
clusters depending on your cluster mesh configuration.

17:36.360 --> 17:42.040
And can we roll out so you can even introduce new clusters have the new version of the application

17:42.040 --> 17:46.080
running on the new cluster so we're absolutely sure that you have no resource contention

17:46.080 --> 17:51.920
on your original cluster and then on the service annotate traffic to forward slowly to a remote

17:51.920 --> 17:57.280
cluster before you do the flip over.

17:57.280 --> 18:03.200
So this concludes the presentation part so for example when you want to know more about

18:03.200 --> 18:08.120
cilium go to the cilium community I encourage you to join our slack channel if you have

18:08.120 --> 18:13.840
any questions our team is there as well to answer questions for in slack any issues you

18:13.840 --> 18:17.460
may have or any roadmap or feature requests you may have we're very interested to hear

18:17.460 --> 18:19.120
from you.

18:19.120 --> 18:24.320
You can also contribute so obviously if you want to develop on cilium join the cilium

18:24.320 --> 18:30.720
GitHub and contribute if you want to know more about eBPF go to eBPF.io and if you want

18:30.720 --> 18:35.560
to know more about Isovalent the company who originated cilium and want to for example

18:35.560 --> 18:40.880
work there have a look there we are looking for engineers as well so feel free to have

18:40.880 --> 18:45.920
a look and if you want to know more ask me after the session as well.

18:45.920 --> 18:55.360
All right let me do see how I'm doing with time so in order to run ingress and gator

18:55.360 --> 19:01.760
API you need to set a certain amount of flex on your for example your mvalue style so this

19:01.760 --> 19:10.040
is an example I've run a small demo on GKE so this is a GKE cluster with cilium installed

19:10.040 --> 19:15.520
what you need to do is you need to enable the ingress controller and in this case I'm

19:15.520 --> 19:21.160
also enabling metrics just because it's interesting to see what's going on for gateway API there's

19:21.160 --> 19:28.080
also a value so gateway API enabled through this will trigger gateway API to be enabled

19:28.080 --> 19:35.480
for surface mesh it's important to configure the cube proxy replacement to strict or probe

19:35.480 --> 19:40.000
strict is recommended because you have the full cube proxy replacement capabilities in

19:40.000 --> 19:45.400
your cluster this is also required for surface mesh and that's that's basically it to get

19:45.400 --> 19:53.000
started so for the simple demo this I've created a simple gateway the gateway class name name

19:53.000 --> 20:00.440
cilium so this is running cilium 1.13 release candidate 5 which has the gateway API support

20:00.440 --> 20:06.960
and then a simple HTTP route for the book info example application which has matches

20:06.960 --> 20:14.440
for the details and the default path prefixes so when I go into my environment I want to

20:14.440 --> 20:21.760
show quickly the following so if I do a kubectl get surface you can see I already for sake

20:21.760 --> 20:26.880
for the sake of time created this gateways what I wanted to show you is that obviously

20:26.880 --> 20:34.520
a low balance is required so GKE proficient me a low balancer and low bands IP I can use

20:34.520 --> 20:42.280
to attract traffic in this case I'm demoing a default HTTP gateway and a default HTTPS

20:42.280 --> 20:51.520
gateway so I have two low balancers with each an external IP addresses assigned so this

20:51.520 --> 21:01.360
configuration is applied so if I do a kubectl get a gateway good point you also obviously

21:01.360 --> 21:07.520
your cluster you also need to install the CRDs for gateway API support here you can

21:07.520 --> 21:19.600
see I have my gateway and a TLS gateway and if I do a kubectl kubectl get HTTP routes

21:19.600 --> 21:27.800
I can see I have my book info HTTP route installed and this relates to this part obviously so

21:27.800 --> 21:36.160
with that I can should be able to connect to the details so this is running the bookstore

21:36.160 --> 21:42.080
example so I'm using that public IP as you can see it works and if I go to details I

21:42.080 --> 21:48.800
should be forwarded using the gateway API HTTP routes to that specific detail surface

21:48.800 --> 21:58.560
and that works as well for HTTPS again a simple example I've created that gateway TLS gateway

21:58.560 --> 22:04.520
I've created two listeners so a listener for book info dot psyllium dot rocks and a listener

22:04.520 --> 22:08.680
for hipster shop dot psyllium dot rocks I don't have installed I didn't have installed

22:08.680 --> 22:16.080
the hipster shop but for demo purposes I'm also referencing two secrets so I've using

22:16.080 --> 22:21.080
I've used make cert to create a simple self-signed certificate installed it in my certificate

22:21.080 --> 22:28.400
store and created a secret which I reference using this listener then again HTTP routes

22:28.400 --> 22:36.560
for the TLS gateway for book info dot psyllium dot rocks matches to only the details path

22:36.560 --> 22:45.760
prefix on port 9080 and again a part for the hipster shop so that's what I'm going to show

22:45.760 --> 22:52.280
here so if I do the default URL that doesn't work there's no list there's no HTTP route

22:52.280 --> 22:58.640
configured but for details I can see I can connect it securely and this certificate is

22:58.640 --> 23:08.840
being run from the gateway resource as well obviously this is a self-signed certificate

23:08.840 --> 23:15.520
but obviously you can create signed certificates as well with that that concludes my presentation

23:15.520 --> 23:34.160
and the demo I'm open for questions any questions hi thank you very much for your presentation

23:34.160 --> 23:45.080
okay when you talk about a no layer 7 support in BPF yeah you said maybe in the future and

23:45.080 --> 23:56.200
what you spec is it going to come or not I'm not sure about that HTTP routing requires

23:56.200 --> 24:02.640
quite a lot of memory right so obviously memory is limited to the BPF programs for good little

24:02.640 --> 24:08.800
reasons so it will depend on the EPF foundation and the roadmap there what we can support

24:08.800 --> 24:13.240
technically there's no reason why we shouldn't be able to do that but yeah in terms of memory

24:13.240 --> 24:19.320
we have constraints so if those are being raised we potentially can have parts of even

24:19.320 --> 24:36.280
all parts using eBPF any other yeah hi does it provide or can you provide end-to-end encryption

24:36.280 --> 24:41.720
between especially between the node automatically or not yes so our vision there is that you

24:41.720 --> 24:48.600
should configure for example IPSec or wire guard for node to node encryption in transit

24:48.600 --> 24:54.280
and if you want authentication and authorization on top of that to configure SPF or NTLS between

24:54.280 --> 24:59.340
your applications so it's a multi-layered approach so we're not doing the encryption

24:59.340 --> 25:06.520
on the NTLS part but on the node level as that makes if that makes sense so NTLS again

25:06.520 --> 25:16.800
a SPF is on roadmap hopefully for 1.13 any other no okay thank you thank you
