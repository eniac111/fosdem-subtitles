WEBVTT

00:00.000 --> 00:17.000
Okay, I think maybe I can stop there.

00:17.000 --> 00:22.000
Yes, a few more minutes than the spectrum.

00:22.000 --> 00:23.000
Okay.

00:23.000 --> 00:24.000
All right.

00:24.000 --> 00:25.000
Another one.

00:25.000 --> 00:26.000
Yeah.

00:26.000 --> 00:29.000
So, let's go right into the topic.

00:29.000 --> 00:30.000
So, I'm Alex.

00:30.000 --> 00:31.000
I work for SWITCH.

00:31.000 --> 00:35.000
SWITCH is the national research and education network in Switzerland.

00:35.000 --> 00:41.000
Like, most countries have something like us, like in Belgium, it's Belnet, and in Germany,

00:41.000 --> 00:43.000
it's DFN, and in France, it's Renateir.

00:43.000 --> 00:47.000
So, we connect to Swiss universities and universities of applied sciences.

00:47.000 --> 00:52.000
We are the ISP of those institutions.

00:52.000 --> 00:58.000
So, NetFlow, I'm not sure if everyone is familiar with NetFlow, so I just recaptured the essential

00:58.000 --> 01:01.000
thing about what the NetFlow actually is.

01:01.000 --> 01:06.000
So, when you look at an IP packet, you extract the source and destination addresses, the

01:06.000 --> 01:11.000
IP protocol, and if the protocol is UDP or TCP, also the source and destination ports,

01:11.000 --> 01:15.000
and those five numbers identify a flow.

01:15.000 --> 01:20.000
So, every packet with the same values is said to belong to the same flow.

01:20.000 --> 01:26.000
And then, in the simplest possible way, you basically just aggregate the count of bytes

01:26.000 --> 01:31.000
and packets of all the packets that belong to the flow, and then you export this information

01:31.000 --> 01:37.000
to a collector, and you can then analyze the data.

01:37.000 --> 01:39.000
So, this is an old thing.

01:39.000 --> 01:43.000
Like, these days, people talk about network telemetry, and back in the day when this was

01:43.000 --> 01:45.000
developed, that name didn't exist yet.

01:45.000 --> 01:49.000
And I'm not sure when exactly Cisco came up with this, but it must have been the early

01:49.000 --> 01:51.000
90s or mid-90s.

01:51.000 --> 01:57.000
And it used to be a de facto standard for a long time, where people just figured out

01:57.000 --> 02:00.000
what Cisco did and then did the same thing.

02:00.000 --> 02:06.000
And then, finally, it got properly standardized with the IP fix ITF standard.

02:06.000 --> 02:10.000
And you can do this either in the sampled mode or unsampled mode.

02:10.000 --> 02:15.000
So, unsampled means you just look at every single packet and account for it in the flow,

02:15.000 --> 02:24.000
and with sampling, you just look at every nth packet, and then you have to make certain

02:24.000 --> 02:28.000
assumptions to then reconstruct the actual values.

02:28.000 --> 02:35.000
So, we at Switch, we've been using NetFlow for a very long time as the basic, as the

02:35.000 --> 02:44.000
most important metric or means to analyze our data, our network data, since the mid-1990s.

02:44.000 --> 02:49.000
And it used to be that this was provided by the routers themselves, which is reasonable,

02:49.000 --> 02:54.000
and the packets pass through that device, and so the device is immediately accessed

02:54.000 --> 02:59.000
to the packets and then can construct the flow data itself.

02:59.000 --> 03:04.000
So, initially, that was done in software, then it was done in hardware.

03:04.000 --> 03:11.000
It used to be basically always unsampled, but with the advent of more powerful networking

03:11.000 --> 03:17.000
gear, and especially with the arrival of the 100-gig ports, it became basically

03:17.000 --> 03:24.000
unfeasible to do this on the routers themselves because of typically software restrictions,

03:24.000 --> 03:25.000
also hardware restrictions.

03:25.000 --> 03:28.000
If you want to do this in software, you usually can't because the routers are not very

03:28.000 --> 03:32.000
powerful in terms of CPU, and in hardware, it becomes very expensive.

03:32.000 --> 03:37.000
So, the vendors started to basically only implement sampled NetFlow.

03:37.000 --> 03:44.000
So, these days, if you buy a Cisco or a Juniper box and you do NetFlow, you get sampling.

03:44.000 --> 03:49.000
And sampling is fine, of course, if you're only interested in aggregate data anyway.

03:49.000 --> 03:53.000
So, big aggregated network flows between networks, for instance.

03:53.000 --> 03:54.000
Sampling is perfectly fine.

03:54.000 --> 04:00.000
You make certain assumptions about the traffic, and then you just upscale it, and you get

04:00.000 --> 04:02.000
fairly reasonable numbers.

04:02.000 --> 04:05.000
So, why would you even want to do unsampled NetFlow?

04:05.000 --> 04:10.000
Well, there are some couple of use cases that are really useful.

04:10.000 --> 04:16.000
So, for instance, in terms of security, one thing that sampling is fine is detect DDoS,

04:16.000 --> 04:17.000
for instance.

04:17.000 --> 04:19.000
That's volumetric DDoS.

04:19.000 --> 04:20.000
That's very simple.

04:20.000 --> 04:24.000
So, you basically have a constant package rate, and if you just look at every end package,

04:24.000 --> 04:25.000
it's easy to scale this up.

04:25.000 --> 04:31.000
But if you want to detect a bot, for instance, in your network, then it's more difficult.

04:31.000 --> 04:37.000
So, maybe you want to do this by looking at the communication with the command and control

04:37.000 --> 04:38.000
channels.

04:38.000 --> 04:44.000
Those are short-lived flows, and if you do sampling, you're probably going to miss them.

04:44.000 --> 04:49.000
But with unsampled NetFlow, you see every single flow, so you can identify these things.

04:49.000 --> 04:54.000
And we, as a network operator, we use this fairly often to troubleshoot network problems.

04:54.000 --> 04:59.000
So, if a customer says, complaints, you can't reach a certain IP address in the Internet,

04:59.000 --> 05:04.000
we can actually go look in our flows for the outgoing TCP-syn packet and see whether there's

05:04.000 --> 05:06.000
a TCP-syn coming back in.

05:06.000 --> 05:08.000
We can do this because we see every single flow.

05:08.000 --> 05:11.000
So, this is extremely useful.

05:11.000 --> 05:18.000
But, as I said, we cannot long do that on our big new core routers.

05:18.000 --> 05:19.000
We can't do that.

05:19.000 --> 05:21.000
They only give us sampled NetFlow.

05:21.000 --> 05:25.000
So, we started to do this with an external box.

05:25.000 --> 05:33.000
And that's where this SnapFlow software implementation comes in.

05:33.000 --> 05:37.000
Because, I mean, there are always ways to do that, but they might be very expensive if

05:37.000 --> 05:40.000
you have to buy dedicated hardware, for instance.

05:40.000 --> 05:45.000
So, just to give an idea of what type kind of traffic we're dealing with, Switzerland

05:45.000 --> 05:46.000
is a small country.

05:46.000 --> 05:48.000
We are a small network.

05:48.000 --> 05:51.000
And we only do NetFlow on our borders.

05:51.000 --> 05:57.080
So, when we, the traffic that we exchange with neighboring networks, and the peak values

05:57.080 --> 06:03.760
are these days, it's roughly maybe 180 gigabits per second, something like that.

06:03.760 --> 06:11.800
And 20 million packets per second, and roughly 350,000 flows per second, un-sampled.

06:11.800 --> 06:14.160
And this can actually be even much more.

06:14.160 --> 06:19.320
The flow rate, because of the aggressive scanning that's going on for the past couple of years,

06:19.320 --> 06:27.160
started to perform very aggressive network scans, like plain TCP-SIN scans, as fast as

06:27.160 --> 06:28.160
they can.

06:28.160 --> 06:33.560
So, sometimes a single host can easily generate 100,000 flows per second.

06:33.560 --> 06:40.920
So, the actual IP-thick traffic that the export is then in the order of 200 to 300 megabits

06:40.920 --> 06:41.920
per second.

06:41.920 --> 06:43.120
So, the flow records themselves.

06:43.120 --> 06:47.200
So, this is all for the un-sampled flow.

06:47.200 --> 06:51.000
The average flow rate is maybe just around 200,000 per second.

06:51.000 --> 06:57.480
And the data generates the actual NetFlow data is like roughly 1.5 terabits per day.

06:57.480 --> 07:02.720
So, the actual scaling problem is more on the collector side then.

07:02.720 --> 07:05.960
You have 10 gig, 100 gig, and 400 gig ports.

07:05.960 --> 07:08.880
So, that's what our solution needs to support.

07:08.880 --> 07:15.240
So, we used to do this historically on the routers themselves until a couple years ago.

07:15.240 --> 07:21.440
Then we moved to a commercial NetFlow generator that did that in hardware, which was pretty

07:21.440 --> 07:22.680
expensive.

07:22.680 --> 07:27.400
Maybe the whole solution for one pop was 100,000 euros, something like that.

07:27.400 --> 07:31.120
And then we finally moved to SnapFlow and Pure Software.

07:31.120 --> 07:33.800
So, how do we do this?

07:33.800 --> 07:35.800
On the borders, these are all fiber connections.

07:35.800 --> 07:37.920
So, we have optical splitters.

07:37.920 --> 07:40.840
We create a copy of all the traffic flow.

07:40.840 --> 07:46.720
And then we have a second device, or the primary device that these tabs are connected to is

07:46.720 --> 07:48.520
what we call a packet broker.

07:48.520 --> 07:54.440
It's basically a switch that aggregates all the packets and sends it out on 200 gig ports

07:54.440 --> 07:57.680
to our actual exporter box.

07:57.680 --> 08:00.960
So, it uses VLAN tags to identify.

08:00.960 --> 08:06.440
So, in NetFlow, we also want to keep track of the router ports where the traffic was

08:06.440 --> 08:08.040
sent or received from.

08:08.040 --> 08:12.760
So, because then that information gets lost and you aggregate them, so we use VLANs to

08:12.760 --> 08:14.720
tag them.

08:14.720 --> 08:19.240
The box we use are white box switches based on the Tofino ASIC.

08:19.240 --> 08:22.920
The ones that Intel just decided they stopped developing, unfortunately.

08:22.920 --> 08:29.960
These are very nice boxes for, like there's one for 3200 gig ports for 5000 euros, and

08:29.960 --> 08:33.920
the other one has 32, 400 gig ports and costs about 20,000 euros.

08:33.920 --> 08:36.600
The thing is you have to program them yourself and you buy them.

08:36.600 --> 08:41.320
They're just plain hardware, and so you can use the P4 language to do this.

08:41.320 --> 08:48.040
I link here to another project of mine where I actually developed a P4 program to do that.

08:48.040 --> 08:50.960
So, that's also part of this entire architecture.

08:50.960 --> 08:57.560
And then the traffic gets to the NetFlow exporter box, which is currently just a one rack unit,

08:57.560 --> 09:01.040
a basic rack mount server.

09:01.040 --> 09:09.960
We use AMD Epix, mainly these days with a fairly large number of cores.

09:09.960 --> 09:12.080
That's the way we scale with the number of cores.

09:12.080 --> 09:15.840
NetFlow always scales very well with cores because you just have to make sure that you

09:15.840 --> 09:23.160
keep the packets to flow together.

09:23.160 --> 09:28.800
The exporter has a Mellanox 2 port 100 gig card that's connected to the packet broker.

09:28.800 --> 09:33.680
That's where it receives the packets.

09:33.680 --> 09:36.480
In the picture, that's what it looks like.

09:36.480 --> 09:39.600
On the upper left, that would be our border router.

09:39.600 --> 09:44.320
On the upper right, that would be the bordering router of our neighboring networks.

09:44.320 --> 09:50.000
In the middle, you have this optical splitter, which is completely on passive box, just as

09:50.000 --> 09:51.680
an optical splitter.

09:51.680 --> 09:57.360
And then you have this packet broker switch in between that aggregates all the packets

09:57.360 --> 10:02.600
and distributes them by flow on these two links currently.

10:02.600 --> 10:07.240
These are not 200 gig ports between the broker and the exporter.

10:07.240 --> 10:10.080
We can easily add more ports if that's not sufficient.

10:10.080 --> 10:17.120
And on the SnapFlow exporter, we can basically just add more cores to be able to scale.

10:17.120 --> 10:18.120
Right.

10:18.120 --> 10:28.760
So, now let's hear Max talk about the actual software.

10:28.760 --> 10:40.800
How do I look?

10:40.800 --> 10:41.800
Does this work?

10:41.800 --> 10:42.800
Good.

10:42.800 --> 10:43.800
All right.

10:43.800 --> 10:57.480
How do we know how SnapFlow is deployed?

10:57.480 --> 11:03.520
I want to talk about how it's built, how it scales, how you configure it, how you monitor

11:03.520 --> 11:08.600
your running application, et cetera.

11:08.600 --> 11:14.080
So SnapFlow, as the name suggests, is built using Snap.

11:14.080 --> 11:18.720
Snap is a toolkit for writing high performance networking applications.

11:18.720 --> 11:23.680
Snap is written in Lua using the amazing Lua JIT compiler.

11:23.680 --> 11:28.120
And it does packet IO without going through the kernel.

11:28.120 --> 11:34.600
Like, generally, the Linux kernel packet networking stack is slow from an ISP perspective.

11:34.600 --> 11:39.440
So Snap bypasses that, uses its own device drivers.

11:39.440 --> 11:41.960
And this is also often called kernel bypass networking.

11:41.960 --> 11:44.480
I think nowadays it's fairly common.

11:44.480 --> 11:46.280
And Snap is open source and independent.

11:46.280 --> 11:52.880
We're not sponsored by any vendor in particular.

11:52.880 --> 11:57.440
So Snap is built with these three core values in mind.

11:57.440 --> 12:00.880
We prefer simple designs over complex designs.

12:00.880 --> 12:04.360
We prefer our software to be small rather than large.

12:04.360 --> 12:07.200
And we are open.

12:07.200 --> 12:10.800
You can read the source, you can understand it, you can modify it, you can rewrite it,

12:10.800 --> 12:17.080
et cetera.

12:17.080 --> 12:23.720
So here I have a snippet of code taken directly from SnapFlow, unedited.

12:23.720 --> 12:28.440
So this is how the Lua code that powers the usual Snap application sort of looks like,

12:28.440 --> 12:32.160
just to give you an idea.

12:32.160 --> 12:36.360
In this particular example, we read a batch of packets from an incoming link.

12:36.360 --> 12:40.640
We extract some metadata that tells us which flow this packet belongs to.

12:40.640 --> 12:44.640
Then we look up a matching flow and a flow table that we maintain.

12:44.640 --> 12:47.600
If we already have a flow, we count that packet towards the flow.

12:47.600 --> 12:55.480
If not, we create a new entry in the flow table.

12:55.480 --> 12:56.480
Got one more snippet.

12:56.480 --> 13:01.180
This function is called every now and then to actually export the flows.

13:01.180 --> 13:08.680
So we walk over a section of the flow table here and add flow aggregates from that flow

13:08.680 --> 13:12.280
table into a next data export record.

13:12.280 --> 13:15.440
And if it's time to export the data record, we send it off to an IP fix collector, which

13:15.440 --> 13:22.280
is a separate program.

13:22.280 --> 13:31.280
So from bird's eye view, SnapFlow works sort of like this, we read packets from a 100 gigabit

13:31.280 --> 13:33.880
SNK, the garden hole, so to speak.

13:33.880 --> 13:39.200
We process those packets to extract flow information in a Snap process.

13:39.200 --> 13:45.820
And then we send off data records over a ton tap interface to the IP fix collector.

13:45.820 --> 13:51.720
So on the right side here, you have a device driver written, like that is part of Snap

13:51.720 --> 13:57.920
written in Lua that actually handles the actual traffic, the bulk of it.

13:57.920 --> 14:01.680
And on the left side, you have an interface to the Linux network stack.

14:01.680 --> 14:07.680
So since the flow export data is rather small in comparison, you can just do that over the

14:07.680 --> 14:11.000
regular Linux network stack, and that works.

14:11.000 --> 14:13.520
And on the very left side, you have the IP fix collector.

14:13.520 --> 14:18.240
That's a different application, like a separate program that we sent the flow data to in the

14:18.240 --> 14:19.240
end.

14:19.240 --> 14:30.840
So sadly, or I mean, I guess just obviously, single CPU core is not enough to handle 100

14:30.840 --> 14:33.760
gigabits of traffic.

14:33.760 --> 14:38.200
So instead what we do is we do receive side scaling provided by the network device.

14:38.200 --> 14:42.560
This way we can process n different sets of flows on n different processes running on

14:42.560 --> 14:43.560
n different CPU cores.

14:43.560 --> 14:51.960
So every circle here is a CPU core.

14:51.960 --> 14:55.040
And we also support to repeat basically the same trick in software.

14:55.040 --> 15:02.440
So we can do another one to receive side scaling after filtering the traffic by protocol.

15:02.440 --> 15:06.760
And this way we can process, for example, DNS traffic on different set of cores than

15:06.760 --> 15:13.520
IP traffic, like non-DNS IP traffic.

15:13.520 --> 15:18.080
And that way we can sort of like segregate the server resources into the workloads that

15:18.080 --> 15:20.400
we actually care about.

15:20.400 --> 15:25.960
We might, for example, care more about that we have an accurate general IP flow profile

15:25.960 --> 15:28.000
to send to the collectors.

15:28.000 --> 15:31.720
And maybe if we still have some time left, we will also do some DNS analysis.

15:31.720 --> 15:40.360
But we don't want one to slow down the other necessarily.

15:40.360 --> 15:44.880
So snap programs are organized into independent apps.

15:44.880 --> 15:49.000
So an app is like a logical packet processing component.

15:49.000 --> 15:54.840
Could be, for example, a device driver or an app that implements the address resolution

15:54.840 --> 15:56.280
protocol.

15:56.280 --> 16:02.000
And these apps are combined into applications like SnapFlow using links.

16:02.000 --> 16:05.000
Links are unidirectional.

16:05.000 --> 16:07.200
They're really just ring buffers.

16:07.200 --> 16:12.400
And any app can have any number of them to use as input or output for packet data.

16:12.400 --> 16:15.560
And you communicate with, like, use those links, like shown here.

16:15.560 --> 16:20.240
That's basically the API that you call link receive on a link to receive a packet.

16:20.240 --> 16:30.720
And you call link transmit on an output link to send a packet.

16:30.720 --> 16:34.400
So now to forward packets from one CPU core to another CPU core, we have this thing called

16:34.400 --> 16:35.400
libinterlink.

16:35.400 --> 16:40.120
These are really just like regular links, except that they span process and CPU core

16:40.120 --> 16:43.080
boundaries.

16:43.080 --> 16:45.160
And you can also use them just like any link.

16:45.160 --> 16:48.960
You have the same interface if you want to operate with them.

16:48.960 --> 16:58.240
And we use those to implement the software-based receive set scaling that I talked about earlier.

16:58.240 --> 17:00.520
We also have libptree.

17:00.520 --> 17:04.960
So libptree implements a very strict control plane, data plane segregation.

17:04.960 --> 17:09.720
I think for most networking folks, the concept of control plane and data plane is pretty

17:09.720 --> 17:11.280
common.

17:11.280 --> 17:16.200
But just to recap it, control plane is something that basically is fancy and elaborate.

17:16.200 --> 17:17.760
You expect it to be really nice.

17:17.760 --> 17:21.680
You want to have a nice interface to configure your application and monitor it.

17:21.680 --> 17:24.200
The data plane, on the other hand, you really just want it to work.

17:24.200 --> 17:27.080
It should, like, preferably run at line rate.

17:27.080 --> 17:34.520
And you don't really have any time to mess around.

17:34.520 --> 17:38.960
So since these, like, two different parts of the application have very different requirements,

17:38.960 --> 17:40.240
it has to keep them separate.

17:40.240 --> 17:46.320
And that's what we do.

17:46.320 --> 17:48.120
We also have libyang.

17:48.120 --> 17:53.420
So you see, both the configuration and the application state of SnapFlow are actually

17:53.420 --> 17:58.120
managed by described in the yang schema.

17:58.120 --> 18:02.840
So for example, you can tell the control plane to load a new configuration of SnapFlow, or

18:02.840 --> 18:06.120
you can query it for some state counters while it's running.

18:06.120 --> 18:10.200
And on the slide, I have some examples how you will use the Snap command line interface

18:10.200 --> 18:19.960
to do those things.

18:19.960 --> 18:24.960
So here we have a snippet of the SnapFlow yang schema.

18:24.960 --> 18:28.120
And yang is one of these things where at the beginning you wonder if you're really going

18:28.120 --> 18:29.220
to need it.

18:29.220 --> 18:32.940
But once you have it, you're usually really happy that you do have it.

18:32.940 --> 18:39.800
So what I like specifically about yang is it's very expressive.

18:39.800 --> 18:43.640
If a configuration passes the control plane and it doesn't reject it because it says,

18:43.640 --> 18:46.480
hey, this is invalid, I'm pretty confident that this configuration will do something

18:46.480 --> 18:50.520
useful in the data plane and it will not just, like, crash.

18:50.520 --> 18:53.840
For example, here we have a list of interfaces.

18:53.840 --> 18:57.240
And one of the fields is a device, which is a PCI address.

18:57.240 --> 19:01.440
And the PCI address, in this case, this type is attached to some regular expression and

19:01.440 --> 19:03.640
makes sure that it actually looks like a PCI address.

19:03.640 --> 19:08.080
And you cannot just pass any string in there and validate it somewhere way down the line.

19:08.080 --> 19:14.600
Like, if you don't put a thing that at least looks like a PCI address, then this won't

19:14.600 --> 19:20.520
even be loaded.

19:20.520 --> 19:23.040
So sadly, any piece of software has bugs.

19:23.040 --> 19:28.200
And in our case, even suboptimal performance is often considered a bug, right?

19:28.200 --> 19:32.280
And we deal with the second issue here, with the performance, by shipping Snap with a flight

19:32.280 --> 19:33.560
recorder.

19:33.560 --> 19:35.200
So this flight recorder has minimal overhead.

19:35.200 --> 19:36.200
It's always on.

19:36.200 --> 19:39.000
You even run into production, preferably.

19:39.000 --> 19:40.360
And it stores useful data.

19:40.360 --> 19:46.320
Part of that data is really useful to profile your application after the fact all while

19:46.320 --> 19:55.200
it's running.

19:55.200 --> 20:00.720
So to analyze the collected data, we have built a little UI that we used to do that.

20:00.720 --> 20:03.520
It's usually running on one of our development servers, so we test stuff.

20:03.520 --> 20:05.520
But you can really run it anywhere.

20:05.520 --> 20:07.960
Did I mention Snap to use the UOG?

20:07.960 --> 20:08.960
I did, right?

20:08.960 --> 20:12.680
So we're dealing with a JIT compiler here.

20:12.680 --> 20:16.840
So the UI shows you stuff that you would expect from a profiler, like basically where does

20:16.840 --> 20:23.560
my program spend its time, but also some JIT-related stuff, like did the compiler have issues generating

20:23.560 --> 20:26.440
efficient code for particular parts of my program?

20:26.440 --> 20:31.400
So for example, here there's like a JGC column.

20:31.400 --> 20:35.560
That's like when the injected code, the garbage collector, is invoked.

20:35.560 --> 20:41.160
And that's, for example, something to look out for.

20:41.160 --> 20:49.120
And another part of the flight recorder is a high resolution event log.

20:49.120 --> 20:55.520
It can give you accurate latency measurements of the pieces that make up your software.

20:55.520 --> 21:02.120
And you can see here on the slide that the OUI has, it shows latency histograms for

21:02.120 --> 21:03.120
individual events.

21:03.120 --> 21:06.640
These events are, some of these events are like already defined in Snap, but you can

21:06.640 --> 21:09.560
also use a define new event.

21:09.560 --> 21:17.440
And here, for example, I could tell that processing a batch of packets and extracting the flow

21:17.440 --> 21:24.320
data, so this is like the main IP fix app main loop, takes us about 35 microseconds

21:24.320 --> 21:28.080
per iteration per process.

21:28.080 --> 21:31.840
And this is really useful if you want to debug tail latencies, right?

21:31.840 --> 21:37.280
And tail latencies translate basically to drop packets in our world.

21:37.280 --> 21:43.320
So that's something that's really valuable.

21:43.320 --> 21:51.280
So to close things, if you were to write a new application based on Snap today, you would

21:51.280 --> 21:58.440
have all these things and more ready at your disposal.

21:58.440 --> 22:05.800
And also, it is possible to purchase consultancy services like commercial support for Snap

22:05.800 --> 22:14.160
and developing Snap applications from your friendly open source consultancy, Igalio,

22:14.160 --> 22:16.480
which is my current employer.

22:16.480 --> 22:19.240
So yeah, that's all for now.

22:19.240 --> 22:21.240
Thanks for your attention.

22:21.240 --> 22:24.200
On the right, there are some pointers, some contacts.

22:24.200 --> 22:29.000
If you have questions or inquiries about Snap or SnapFlow, you can email us there after

22:29.000 --> 22:30.000
the conference.

22:30.000 --> 22:40.360
And for now, if you have any questions, please ask them.

22:40.360 --> 23:08.040
Please come down.

23:08.040 --> 23:19.320
There are some seats available here in the middle.

23:19.320 --> 23:26.400
So the next speaker is Peter Manel.

23:26.400 --> 23:32.520
That is one of the key guys of Suricata, a very popular open source ideas.

23:32.520 --> 23:38.440
And today is going to talk about this open source platform.

23:38.440 --> 24:03.320
Have a seat here in the middle.
