1
0:00:00.000 --> 0:00:13.480
So, last session for today and we'll make sure it's going to be a really, really long

2
0:00:13.480 --> 0:00:18.400
one so that you have to starve and don't get to the drinks.

3
0:00:18.400 --> 0:00:24.680
And I'm happy to welcome Tom from Red Hat, have fun and enlighten us.

4
0:00:24.680 --> 0:00:25.760
Thank you.

5
0:00:25.760 --> 0:00:31.680
Should I have fun or them?

6
0:00:31.680 --> 0:00:33.360
Should I have fun or them?

7
0:00:33.360 --> 0:00:35.200
Both, but...

8
0:00:35.200 --> 0:00:36.200
Okay.

9
0:00:36.200 --> 0:00:38.200
Hello, everyone.

10
0:00:38.200 --> 0:00:40.200
My name is Tom Sofa.

11
0:00:40.200 --> 0:00:47.560
I work at Red Hat and I think the last talk was a great segue into my talk.

12
0:00:47.560 --> 0:00:52.440
So if you were here for the previous presentation, who was here?

13
0:00:52.440 --> 0:00:53.440
Good.

14
0:00:53.440 --> 0:00:54.440
Good.

15
0:00:54.440 --> 0:00:57.520
So we were talking about...

16
0:00:57.520 --> 0:01:04.320
The talk was about standardization, call for a unified platform, call for a sharing exchange

17
0:01:04.320 --> 0:01:14.040
of ideas, knowledge, of findings and how to get to some kind of an open, unified sovereign

18
0:01:14.040 --> 0:01:15.040
cloud.

19
0:01:15.040 --> 0:01:24.800
Well, we've been working on, I think, like that for past two years or so.

20
0:01:24.800 --> 0:01:33.480
In an initiative called Operate First, building open hybrid cloud platform ready for everybody

21
0:01:33.480 --> 0:01:40.200
to consume, to use, to look into operations, to look into metrics, look into whatever telemetry

22
0:01:40.200 --> 0:01:44.720
you have to actually do the operations yourself if you want to.

23
0:01:44.720 --> 0:01:52.000
So this talk is going to be focused precisely on that, on sharing a story, sharing a lesson

24
0:01:52.000 --> 0:02:02.560
that we learned during the time and maybe hopefully take it as an opportunity to not

25
0:02:02.560 --> 0:02:08.480
just share that with you, but to also encourage you to learn those lessons for yourself and

26
0:02:08.480 --> 0:02:14.680
experience our pains and our challenges yourself.

27
0:02:14.680 --> 0:02:17.760
So let's dig in.

28
0:02:17.760 --> 0:02:24.920
So the talk is called Effective Management of Kubernetes Resources, the GitOps way,

29
0:02:24.920 --> 0:02:26.360
GitOps for cluster admins.

30
0:02:26.360 --> 0:02:31.160
So first we're going to talk a bit about what is a cluster lifecycle and what's the role

31
0:02:31.160 --> 0:02:33.920
of cluster operations in that.

32
0:02:33.920 --> 0:02:38.640
Then we're going to experience the chaos that is out there in the world.

33
0:02:38.640 --> 0:02:40.840
And then we're going to talk YAML.

34
0:02:40.840 --> 0:02:45.760
If you went on the YAML lighting talk, this is going to be a very slight variation of

35
0:02:45.760 --> 0:02:49.720
that, but more Kubernetes focused.

36
0:02:49.720 --> 0:02:53.680
And then we're going to bring some order to that chaos.

37
0:02:53.680 --> 0:02:59.200
So we have these three graces of cloud management, right?

38
0:02:59.200 --> 0:03:02.160
We usually provision some resources.

39
0:03:02.160 --> 0:03:04.040
We manage those clusters.

40
0:03:04.040 --> 0:03:09.100
Once they are deployed, once they are provisioned, we then deploy applications on top of them.

41
0:03:09.100 --> 0:03:16.080
If you are talking about Kubernetes based cloud systems, this is the usual three pillars,

42
0:03:16.080 --> 0:03:20.000
three graces of what we are experiencing.

43
0:03:20.000 --> 0:03:25.280
We have tools available for both hands of the spectrum.

44
0:03:25.280 --> 0:03:33.020
So for resource provisioning, we have great tools like Ansible, Terraform, Hive, or Cluster

45
0:03:33.020 --> 0:03:35.860
API in Kubernetes.

46
0:03:35.860 --> 0:03:42.200
This is an established pattern, established workflow that is widely used across hyperscalers,

47
0:03:42.200 --> 0:03:49.200
across people who are deploying Kubernetes by themselves, and so on.

48
0:03:49.200 --> 0:03:50.200
Good.

49
0:03:50.200 --> 0:03:51.240
This is a soft issue.

50
0:03:51.240 --> 0:03:54.440
This is a non-issue.

51
0:03:54.440 --> 0:03:58.720
Then there's the application maintenance, application deployment, application lifecycle.

52
0:03:58.720 --> 0:04:10.280
Again, very well thought through aspect, very studied place.

53
0:04:10.280 --> 0:04:12.080
We have tools like Customize and Helm.

54
0:04:12.080 --> 0:04:21.240
We have Argo CD or Flux CD to do continuous deployment of your workloads and to provide

55
0:04:21.240 --> 0:04:28.020
you with all the goodies like rollbacks to previously non-broken deployment and taking

56
0:04:28.020 --> 0:04:36.880
it even further with other projects like, now I forgot the name.

57
0:04:36.880 --> 0:04:40.800
What are we talking about?

58
0:04:40.800 --> 0:04:42.800
The SRE driven deployment?

59
0:04:42.800 --> 0:04:45.200
No, you don't.

60
0:04:45.200 --> 0:04:46.200
Okay.

61
0:04:46.200 --> 0:04:47.200
Let's move on.

62
0:04:47.200 --> 0:04:49.000
So what about the middle part?

63
0:04:49.000 --> 0:04:52.920
The cluster management itself.

64
0:04:52.920 --> 0:04:57.560
If we are managing Kubernetes resources, what are we talking about?

65
0:04:57.560 --> 0:05:05.360
If we are managing nodes, if we are managing tenancy, if we are managing networks, what

66
0:05:05.360 --> 0:05:11.360
are we actually talking about and how we can manage that?

67
0:05:11.360 --> 0:05:15.940
We have these four main problems that we want to solve somehow.

68
0:05:15.940 --> 0:05:21.440
We found out that basically nowadays it wasn't the case two years ago, but nowadays it's

69
0:05:21.440 --> 0:05:30.000
the case that we can solve all of them through Kubernetes native resources, through YAMLs,

70
0:05:30.000 --> 0:05:35.120
through deploying YAMLs, applying YAMLs to our clusters.

71
0:05:35.120 --> 0:05:38.600
It's done by a few different means.

72
0:05:38.600 --> 0:05:47.400
So we have main areas within Kubernetes API that we can explore to solve these needs.

73
0:05:47.400 --> 0:05:53.000
We have multi-nancy, so we can solve that by just simple namespaces, cluster roles and

74
0:05:53.000 --> 0:05:55.000
whatnot.

75
0:05:55.000 --> 0:06:00.880
Cluster upgrades, again, we can apply install operators, talk to those operators and get

76
0:06:00.880 --> 0:06:03.020
those clusters upgraded.

77
0:06:03.020 --> 0:06:09.500
For storage management, we can use operators, we can use storage classes and storage providers

78
0:06:09.500 --> 0:06:14.920
and custom resources if we wanted to deploy our own storage on, for example, bare metal

79
0:06:14.920 --> 0:06:15.920
clusters.

80
0:06:15.920 --> 0:06:22.960
And for network management, we can do that also through operators, so things like cert

81
0:06:22.960 --> 0:06:30.680
manager, things like state, all of that can now happen through Kubernetes API natively.

82
0:06:30.680 --> 0:06:31.680
That's great.

83
0:06:31.680 --> 0:06:35.160
What did it tell us about the cluster management?

84
0:06:35.160 --> 0:06:38.560
It can be all managed as a Kubernetes application.

85
0:06:38.560 --> 0:06:40.760
It's in YAML.

86
0:06:40.760 --> 0:06:44.840
Well, YAML is a mess.

87
0:06:44.840 --> 0:06:47.120
We know that.

88
0:06:47.120 --> 0:06:51.200
And we know that thanks to multiple aspects.

89
0:06:51.200 --> 0:06:57.840
YAML can be defined and stored in files no matter how you structure it.

90
0:06:57.840 --> 0:07:00.880
It can be a single file with many different resources.

91
0:07:00.880 --> 0:07:12.000
It can be many different files, each of them holding a separate resource and only asterisks

92
0:07:12.000 --> 0:07:16.160
in bash is the limit for your kubectl apply.

93
0:07:16.160 --> 0:07:20.000
You can do whatever you like on the client side.

94
0:07:20.000 --> 0:07:25.120
On the other hand, on the server side, the manifest that you apply to the cluster is

95
0:07:25.120 --> 0:07:27.840
not the same that you get from the cluster back.

96
0:07:27.840 --> 0:07:29.460
It's modified, it's mutated.

97
0:07:29.460 --> 0:07:32.200
So you have things like status.

98
0:07:32.200 --> 0:07:35.760
Some operators, some controllers modify also specs.

99
0:07:35.760 --> 0:07:39.120
Some modify also annotations, labels and whatnot.

100
0:07:39.120 --> 0:07:43.960
And you don't have the full control over the definition.

101
0:07:43.960 --> 0:07:55.120
You need to know what subset of the keys and values you can actually define as a declarative

102
0:07:55.120 --> 0:07:57.440
manifest for a resource.

103
0:07:57.440 --> 0:08:02.400
It's not the same as the manifest applied on the cluster.

104
0:08:02.400 --> 0:08:05.920
So how people store manifests online.

105
0:08:05.920 --> 0:08:12.120
If we pull random project on GitHub that is deployed to Kubernetes, you will find many

106
0:08:12.120 --> 0:08:15.480
of these patterns.

107
0:08:15.480 --> 0:08:22.720
Kubectl doesn't have ordering, so people solve it creatively through numbering their

108
0:08:22.720 --> 0:08:24.180
manifests.

109
0:08:24.180 --> 0:08:28.600
Some are aware that their application may run in different environments.

110
0:08:28.600 --> 0:08:34.640
So they create different files with double-a-sit content, with the same deployment, with just

111
0:08:34.640 --> 0:08:38.440
few lines changed here and there.

112
0:08:38.440 --> 0:08:40.560
Some combine those approaches.

113
0:08:40.560 --> 0:08:46.280
And in some projects, and we find that even in some controllers for their dev setup, they

114
0:08:46.280 --> 0:08:52.520
have a single file with all those resources in line there.

115
0:08:52.520 --> 0:08:53.520
This is not a standard.

116
0:08:53.520 --> 0:08:55.880
This is not a good practice.

117
0:08:55.880 --> 0:09:04.560
And if we want to manage an environment which is live, which should be out-disabled, which

118
0:09:04.560 --> 0:09:10.760
should be approachable to people, this is not the way we should do it.

119
0:09:10.760 --> 0:09:15.040
So in application space, we have basically two choices.

120
0:09:15.040 --> 0:09:17.860
How to organize how to structure our manifest.

121
0:09:17.860 --> 0:09:24.680
One is through Helm, which is great if you're deploying applications and you want some templating

122
0:09:24.680 --> 0:09:30.600
involved if you want to quickly change many different places of the same manifest or of

123
0:09:30.600 --> 0:09:32.040
different manifests.

124
0:09:32.040 --> 0:09:39.260
So you can basically create this template, applying these values, and you get the full

125
0:09:39.260 --> 0:09:41.880
YAML that we saw earlier.

126
0:09:41.880 --> 0:09:43.720
Great.

127
0:09:43.720 --> 0:09:45.560
But is it readable?

128
0:09:45.560 --> 0:09:50.480
Is it understandable from the YAML itself without rendering?

129
0:09:50.480 --> 0:09:51.880
We don't think so.

130
0:09:51.880 --> 0:09:59.520
And we want our cloud manifests to be out-achable, to be approachable, to be reviewable.

131
0:09:59.520 --> 0:10:06.920
So if we want to be able to explore what those changes do on a PR review without actually

132
0:10:06.920 --> 0:10:13.880
spinning up a cluster and applying that PR and maybe do some static validation on it,

133
0:10:13.880 --> 0:10:15.640
you can do that with this.

134
0:10:15.640 --> 0:10:16.960
You would need to render it.

135
0:10:16.960 --> 0:10:19.200
You would need to understand it.

136
0:10:19.200 --> 0:10:24.120
And if you change the template and you have different values for different environments,

137
0:10:24.120 --> 0:10:27.120
how would it affect the template itself?

138
0:10:27.120 --> 0:10:29.080
So you need to explore all the possibilities.

139
0:10:29.080 --> 0:10:33.480
And this is one of the biggest challenge in the Helm space that we are currently facing

140
0:10:33.480 --> 0:10:37.840
in application development.

141
0:10:37.840 --> 0:10:43.800
Then we have the other way, the Kubernetes native configuration management customized,

142
0:10:43.800 --> 0:10:46.960
which is a bit nicer.

143
0:10:46.960 --> 0:10:55.800
All of those manifests are fairly easily organized and referenced through customization.

144
0:10:55.800 --> 0:11:00.840
And all those customizations are organized into bases and overlays.

145
0:11:00.840 --> 0:11:08.640
So it's a composition type of configuration that we have a base which defines the basics

146
0:11:08.640 --> 0:11:15.980
and then we have the overlay which can patch and mix different resources.

147
0:11:15.980 --> 0:11:18.880
These resources in the base are already complete.

148
0:11:18.880 --> 0:11:24.920
This is a complete definition, complete declaration of my resource.

149
0:11:24.920 --> 0:11:26.840
This is reviewable.

150
0:11:26.840 --> 0:11:33.840
So we kind of thought that this might be a way, but before that we defined a couple of

151
0:11:33.840 --> 0:11:40.400
rules, couple of directives that we wanted to achieve with this.

152
0:11:40.400 --> 0:11:47.600
So if we wanted to organize our manifest, we don't want to build our own solution.

153
0:11:47.600 --> 0:11:52.820
We don't want to build our own CI CD that would understand our manifest structure.

154
0:11:52.820 --> 0:11:58.040
We want to use something that is readily available with great community.

155
0:11:58.040 --> 0:12:02.840
We want the configuration to be stable.

156
0:12:02.840 --> 0:12:07.040
So if I change one manifest, it doesn't break five different clusters.

157
0:12:07.040 --> 0:12:10.400
And those things that never happen usually happen.

158
0:12:10.400 --> 0:12:18.640
So if something like that happens, I can roll back the faulty cluster, just that individual

159
0:12:18.640 --> 0:12:19.640
cluster.

160
0:12:19.640 --> 0:12:25.120
I don't need to roll back all the clusters that are working with that particular configuration.

161
0:12:25.120 --> 0:12:26.880
And it's unit testable.

162
0:12:26.880 --> 0:12:29.680
So that's also an important thing.

163
0:12:29.680 --> 0:12:34.600
File mapping, also very interesting topic because YAML allows you to inline multiple

164
0:12:34.600 --> 0:12:37.040
resources into a single file.

165
0:12:37.040 --> 0:12:39.560
But we don't want that.

166
0:12:39.560 --> 0:12:45.200
We want the file and its name to fully represent the resource.

167
0:12:45.200 --> 0:12:50.640
So before I even open the YAML, I already know what to expect inside.

168
0:12:50.640 --> 0:13:00.080
I don't have to guess from a dev.yaml or from namespace.yaml which also contains like

169
0:13:00.080 --> 0:13:04.720
OpenShoot project or whatnot.

170
0:13:04.720 --> 0:13:07.960
Each file is readable without processing.

171
0:13:07.960 --> 0:13:10.120
That's so explanatory.

172
0:13:10.120 --> 0:13:19.920
I want to be able to open the code tab on my GitHub repository and understand the manifest.

173
0:13:19.920 --> 0:13:24.400
If I'm defining the same resource on multiple clusters, if I'm applying the same resource

174
0:13:24.400 --> 0:13:31.400
on multiple clusters, let's say I have the same user group on two different clusters,

175
0:13:31.400 --> 0:13:34.720
I want to apply similar or the same RBAC.

176
0:13:34.720 --> 0:13:42.440
I want to apply the same cluster roles, project namespace, permissions and whatnot.

177
0:13:42.440 --> 0:13:48.800
I don't want this definition to be duplicit, to be defined differently, maybe differently,

178
0:13:48.800 --> 0:13:51.560
maybe slightly differently, maybe the same in two different places.

179
0:13:51.560 --> 0:13:54.560
I want to share the same definition.

180
0:13:54.560 --> 0:14:03.080
As a practice that we use in programming for ages, this is not a well-established pattern

181
0:14:03.080 --> 0:14:05.880
in Kubernetes manifests.

182
0:14:05.880 --> 0:14:09.360
We want to reuse stuff.

183
0:14:09.360 --> 0:14:17.200
And as I told before, the file name already describes what's inside.

184
0:14:17.200 --> 0:14:22.680
So we came up with this pattern and this pattern has been vetted through a couple of organizations

185
0:14:22.680 --> 0:14:26.800
that I'll show you later on.

186
0:14:26.800 --> 0:14:29.260
And this is a pattern that we come up to.

187
0:14:29.260 --> 0:14:39.080
We have a base for customize, which references every single object that we deploy to any

188
0:14:39.080 --> 0:14:44.360
our cluster that requires elevated permissions.

189
0:14:44.360 --> 0:14:52.400
If those resources are standard namespace scoped, things like deployment, config map,

190
0:14:52.400 --> 0:14:56.280
secret, whatnot, this is the developer responsibility.

191
0:14:56.280 --> 0:15:01.600
They live in their own self-contained namespace and they can do whatever they want in there.

192
0:15:01.600 --> 0:15:08.520
But if we are talking about creating namespaces or creating cluster roles, we don't want developers

193
0:15:08.520 --> 0:15:18.280
to create namespaces on their own or create limit ranges or create resource quotas on

194
0:15:18.280 --> 0:15:19.400
their own.

195
0:15:19.400 --> 0:15:29.880
We want to do this, set those things for them because we don't want them to basically expand

196
0:15:29.880 --> 0:15:36.580
and take over the cluster if we don't want them to.

197
0:15:36.580 --> 0:15:43.600
So this pattern of API group, kind and name is actually kind of working because already

198
0:15:43.600 --> 0:15:51.720
from the path, base core namespace, or in cloud, or base, force network, talks, and

199
0:15:51.720 --> 0:15:58.960
I talk, I already know what the resource is about without actually looking into the file.

200
0:15:58.960 --> 0:16:05.240
Then I have overlays, which each overlay represents a single cluster.

201
0:16:05.240 --> 0:16:12.520
Sometimes they have customization, which basically mixes and matches whatever resources I want

202
0:16:12.520 --> 0:16:14.040
to pull from base.

203
0:16:14.040 --> 0:16:19.280
And if I want to change something from the base, I basically just patch it because customize

204
0:16:19.280 --> 0:16:25.360
allows us to patch resources and applies either a strategic merge patch or a JSON patch so

205
0:16:25.360 --> 0:16:28.320
I can do various things with that.

206
0:16:28.320 --> 0:16:35.000
This is very helpful if I have, for example, cluster admins group and I want different

207
0:16:35.000 --> 0:16:42.880
cluster admins on different cluster, but the group itself is already defined in base.

208
0:16:42.880 --> 0:16:46.440
This is nice, but it doesn't work in all cases.

209
0:16:46.440 --> 0:16:48.880
It doesn't solve all the issues.

210
0:16:48.880 --> 0:16:53.440
So we had to introduce two additional concepts.

211
0:16:53.440 --> 0:17:01.600
One is components, which is also an alpha extension to customize, which allows you to

212
0:17:01.600 --> 0:17:05.840
reuse the same manifest multiple times.

213
0:17:05.840 --> 0:17:12.360
This is important in cases like RBAC if we have role bindings that we want to apply to

214
0:17:12.360 --> 0:17:20.460
multiple namespaces, like granting this user a group admin access to a certain namespace,

215
0:17:20.460 --> 0:17:26.360
because if customized by itself wouldn't allow us to use that resource multiple times.

216
0:17:26.360 --> 0:17:30.480
So this is a limitation of customize in this particular case that can be overcome through

217
0:17:30.480 --> 0:17:32.280
components.

218
0:17:32.280 --> 0:17:41.880
And then we came up with bundles, which is an addition that basically selects related

219
0:17:41.880 --> 0:17:45.760
resources from the base, which are always applied together.

220
0:17:45.760 --> 0:17:51.520
So imagine you want to install a cert manager.

221
0:17:51.520 --> 0:17:52.880
It's always a namespace.

222
0:17:52.880 --> 0:17:56.840
It's always a service account with cross-terile.

223
0:17:56.840 --> 0:18:04.520
It's always a subscription or whatever, or a cluster issuer for certificates.

224
0:18:04.520 --> 0:18:11.080
So all of these things come together, and their reference is bundles, so we don't clutter

225
0:18:11.080 --> 0:18:12.720
the overlays too much.

226
0:18:12.720 --> 0:18:18.520
And we also introduced common overlays, which are region specific, which are shared across

227
0:18:18.520 --> 0:18:25.460
regions because for some regions, we have a shared config.

228
0:18:25.460 --> 0:18:32.040
So how such single cluster overlay customization looks like.

229
0:18:32.040 --> 0:18:33.120
We reference the common.

230
0:18:33.120 --> 0:18:41.920
We take all from common, which also references some things from the base and whatnot.

231
0:18:41.920 --> 0:18:48.280
Then we can, for example, this way, deploy our customer resource definition for Prow.

232
0:18:48.280 --> 0:18:57.520
We can create an namespace for Prow, and we can apply some RBAC to Node-Labra.

233
0:18:57.520 --> 0:19:05.200
We can install a whole bundle for cert manager as is, and this ensures cert manager is deployed

234
0:19:05.200 --> 0:19:08.760
and configured properly for this cluster.

235
0:19:08.760 --> 0:19:18.480
We also can specify a specific version for the particle open-shift cluster to upgrade

236
0:19:18.480 --> 0:19:23.640
it to maintenance on the old CPU version.

237
0:19:23.640 --> 0:19:31.360
And if we want to, we can patch certain resources, as I mentioned, the cluster admin.

238
0:19:31.360 --> 0:19:38.600
So fairly simple pattern, but there's been a two-year journey to get into a state where

239
0:19:38.600 --> 0:19:43.560
it's actually working across regions, where it's actually working across multiple clusters.

240
0:19:43.560 --> 0:19:51.320
And when it's efficient in managing multiple clusters through PRs, through GitOps, through

241
0:19:51.320 --> 0:19:58.040
single file YAML based changes so it doesn't break all the clusters.

242
0:19:58.040 --> 0:20:03.400
What I didn't mention on this slide, each of the cluster has their own separate ARGOS

243
0:20:03.400 --> 0:20:04.880
ID application.

244
0:20:04.880 --> 0:20:10.320
So they act independently in the CD process.

245
0:20:10.320 --> 0:20:19.520
They reference the same code base, but they are independent, so the rollback is possible.

246
0:20:19.520 --> 0:20:27.120
So in conclusion, to evaluate what we did here, we have no duplicity.

247
0:20:27.120 --> 0:20:28.400
Manifest are readable.

248
0:20:28.400 --> 0:20:31.600
Manifest are not confusing.

249
0:20:31.600 --> 0:20:33.840
The set of rules is fairly simple.

250
0:20:33.840 --> 0:20:37.760
It's nothing very complex or bulky.

251
0:20:37.760 --> 0:20:42.360
The CICD is very easy, and we can do static validation.

252
0:20:42.360 --> 0:20:43.680
We can do unit tests.

253
0:20:43.680 --> 0:20:45.600
We can do integration tests.

254
0:20:45.600 --> 0:20:49.560
All of that can be done fairly nicely.

255
0:20:49.560 --> 0:20:51.220
What are the downsides?

256
0:20:51.220 --> 0:20:57.960
We have boilerplate in the form of customizations, in the form of components, in the form of

257
0:20:57.960 --> 0:21:05.240
very nested path structures, directory structures and whatnot.

258
0:21:05.240 --> 0:21:14.000
Not always very straightforward, so you need to learn the tools before you can use it.

259
0:21:14.000 --> 0:21:19.120
What also limits our static scheme validation is that manifests in base can be partials

260
0:21:19.120 --> 0:21:26.760
because they are not always complete because we expect to patch them in those overlays

261
0:21:26.760 --> 0:21:32.040
to set a specific channel for our operator subscription and whatnot.

262
0:21:32.040 --> 0:21:34.780
So that's that.

263
0:21:34.780 --> 0:21:39.160
We have four organizations currently adopting this scheme and running this scheme.

264
0:21:39.160 --> 0:21:45.680
We have Operate First Community Cloud, New England Research Cloud, Massachusetts Open

265
0:21:45.680 --> 0:21:53.760
Cloud and Open Source Climate Alliance all running on this pattern.

266
0:21:53.760 --> 0:21:59.800
So this is a lesson that we learned through collaboration in Cloud Operations, and I hope

267
0:21:59.800 --> 0:22:08.400
we may be able to learn more such lessons in the future by exploring Cloud together.

268
0:22:08.400 --> 0:22:13.580
So if you want to know more, you can join us in Operate First of the Cloud.

269
0:22:13.580 --> 0:22:19.080
You can see our ADRs and how we got to those outcomes.

270
0:22:19.080 --> 0:22:24.960
And on the last link over here, you can actually see the code base that we are running against

271
0:22:24.960 --> 0:22:27.280
all of those clusters.

272
0:22:27.280 --> 0:22:28.280
Thank you very much.

273
0:22:28.280 --> 0:22:29.280
There we go.

274
0:22:29.280 --> 0:22:46.840
Hello and thank you for the talk.

275
0:22:46.840 --> 0:22:53.840
We use the same, more or less the same pattern, but the one of the manifests in completion,

276
0:22:53.840 --> 0:22:55.840
we fixed it.

277
0:22:55.840 --> 0:23:01.280
We adopted an approach that we defined those attributes that are required with customization

278
0:23:01.280 --> 0:23:02.280
overlay.

279
0:23:02.280 --> 0:23:05.720
So like add only value and then you have completion.

280
0:23:05.720 --> 0:23:10.160
And then you know that that particular value, you can, it's a valid YAML because it matches

281
0:23:10.160 --> 0:23:12.240
the spec fully.

282
0:23:12.240 --> 0:23:18.080
But then you know visually that that particular field will be patching overlay.

283
0:23:18.080 --> 0:23:24.360
So we use that as a solution for the manifesting completion and the static validation.

284
0:23:24.360 --> 0:23:32.080
We always use customization over overlay, and then we know that we are going to do that.

285
0:23:32.080 --> 0:23:36.600
That's just a solution that we, I don't know if there is a better way or a better word

286
0:23:36.600 --> 0:23:39.680
to use for that, but that's our approach.

287
0:23:39.680 --> 0:23:43.480
We use the same, but it doesn't work in every case.

288
0:23:43.480 --> 0:23:47.760
We found like in some cases the scheme is very detailed.

289
0:23:47.760 --> 0:23:51.720
It requires this complex nested structure.

290
0:23:51.720 --> 0:23:55.360
Like for example, cert manager requires solvers.

291
0:23:55.360 --> 0:24:07.240
And if you define a solver, you can't remove it in a patch because it's a mapping.

292
0:24:07.240 --> 0:24:10.840
So strategic merges don't work that way in customized.

293
0:24:10.840 --> 0:24:13.800
You would need a JSON patch and you would need a long JSON patch.

294
0:24:13.800 --> 0:24:20.040
And you know, it's becoming less and less clear in this regard.

295
0:24:20.040 --> 0:24:25.440
I think another thing that we do is we have for example a common base like you, and then

296
0:24:25.440 --> 0:24:31.240
have non-production base, production base, and then for example for the admin groups.

297
0:24:31.240 --> 0:24:33.800
So we have a group of admins for the non-productions.

298
0:24:33.800 --> 0:24:38.040
And then we don't have the full group of admins in the base.

299
0:24:38.040 --> 0:24:39.040
We have what?

300
0:24:39.040 --> 0:24:43.760
And then we need it from the non-production or the production in case we need one group

301
0:24:43.760 --> 0:24:44.760
or another.

302
0:24:44.760 --> 0:24:49.680
That's another approach that we found out.

303
0:24:49.680 --> 0:24:50.680
Thank you.

304
0:24:50.680 --> 0:24:58.360
I think it forgot.

305
0:24:58.360 --> 0:25:01.800
And then the last one.

306
0:25:01.800 --> 0:25:07.080
In this case, when you have like a couple of bundles, maybe it's easy, but you have

307
0:25:07.080 --> 0:25:11.720
a cluster with 12 or 15 bundles.

308
0:25:11.720 --> 0:25:13.640
It can be a little bloated.

309
0:25:13.640 --> 0:25:18.400
Having a single I go CD app, managing all the applications of a single cluster.

310
0:25:18.400 --> 0:25:23.240
And we use this approach of we have one for the cluster deployment with Hive.

311
0:25:23.240 --> 0:25:30.100
And then we have for each operator, we have his own tree.

312
0:25:30.100 --> 0:25:34.760
So we have independent applications and when, for example, an operator breaks, it doesn't

313
0:25:34.760 --> 0:25:36.880
break the entire I go CD application of the cluster.

314
0:25:36.880 --> 0:25:42.840
It only breaks the I go CD application or when we need to upgrade or we think it's safer

315
0:25:42.840 --> 0:25:46.960
because we are like really, really scoped and you can not break the entire cluster,

316
0:25:46.960 --> 0:25:47.960
just a single application.

317
0:25:47.960 --> 0:25:48.960
That's also...

318
0:25:48.960 --> 0:25:54.440
Yeah, we do the same for operators which have like specific deployments and whatnot.

319
0:25:54.440 --> 0:26:01.840
If we can deploy operators through subscription to the OpenShift operator catalog, operator

320
0:26:01.840 --> 0:26:06.520
hub, we can do that through a single resource and then it's not bloated that way.

321
0:26:06.520 --> 0:26:16.000
So yes, same lesson that we face the same issue and we were solving it very similarly.

322
0:26:16.000 --> 0:26:20.160
We both got red fat, but we need different reports.

323
0:26:20.160 --> 0:26:21.160
In different...

324
0:26:21.160 --> 0:26:22.160
Yeah.

325
0:26:22.160 --> 0:26:23.160
Good.

326
0:26:23.160 --> 0:26:24.160
Good.

327
0:26:24.160 --> 0:26:27.440
We should talk after.

328
0:26:27.440 --> 0:26:28.440
Yeah.

329
0:26:28.440 --> 0:26:29.440
Hi.

330
0:26:29.440 --> 0:26:30.440
Really nice talk.

331
0:26:30.440 --> 0:26:31.440
Thank you.

332
0:26:31.440 --> 0:26:32.440
Thank you.

333
0:26:32.440 --> 0:26:38.280
We build a lot of internal developer platforms and we face the same issue where we lose track

334
0:26:38.280 --> 0:26:39.720
of the code bases.

335
0:26:39.720 --> 0:26:44.480
Do you implement any repo scanning or file structure scanning that makes sure that this

336
0:26:44.480 --> 0:26:49.880
is enforced among your customized charts and a two-parter?

337
0:26:49.880 --> 0:26:54.520
Do you just block all use of Helm charts because everything has a Helm chart nowadays and it

338
0:26:54.520 --> 0:26:59.440
would be limiting to have to rewrite something in this format if there's an existing Helm

339
0:26:59.440 --> 0:27:05.280
chart or existing customized or is this only for org internal YAML?

340
0:27:05.280 --> 0:27:07.480
Thank you.

341
0:27:07.480 --> 0:27:13.240
We enforce this only for resources that require elevated permissions.

342
0:27:13.240 --> 0:27:19.400
If you have a Helm chart that is deploying custom resource definition, then we tell you

343
0:27:19.400 --> 0:27:21.160
this is not a good thing.

344
0:27:21.160 --> 0:27:23.120
You shouldn't do that.

345
0:27:23.120 --> 0:27:25.880
The API wouldn't allow you to do that.

346
0:27:25.880 --> 0:27:28.800
Like our RBAC settings.

347
0:27:28.800 --> 0:27:38.040
So we basically tell those people you need to get that CRD into our repository, check

348
0:27:38.040 --> 0:27:44.720
it in our base for resources which require cluster admin or elevated permissions because

349
0:27:44.720 --> 0:27:49.440
if we would reference it from somebody else from some other repository, they can change

350
0:27:49.440 --> 0:27:50.440
it in their repository.

351
0:27:50.440 --> 0:27:53.320
We don't want to do that.

352
0:27:53.320 --> 0:27:59.000
We don't want them to be applying CRDs because those are shared on the cluster.

353
0:27:59.000 --> 0:28:03.720
If two people on the same cluster are deploying the same Helm chart in different versions

354
0:28:03.720 --> 0:28:08.480
with different CRD schema, it can fight and we don't want that.

355
0:28:08.480 --> 0:28:14.200
That's why we want a single source of truth for all the resources that are cluster scoped

356
0:28:14.200 --> 0:28:17.520
or requiring elevated permissions.

357
0:28:17.520 --> 0:28:24.160
Helm charts are allowed for developer and application workloads in their own namespace,

358
0:28:24.160 --> 0:28:31.760
self-contained or across all of their namespaces if they have more, but not under our watch

359
0:28:31.760 --> 0:28:35.360
on the elevated permissions.

360
0:28:35.360 --> 0:28:42.120
Thank you.

361
0:28:42.120 --> 0:28:52.080
You said that when you have several clusters, you can limit what the developers or the user

362
0:28:52.080 --> 0:28:54.400
of the cluster can deploy.

363
0:28:54.400 --> 0:28:55.560
But how do you manage that?

364
0:28:55.560 --> 0:28:58.160
For example, we use a pro.

365
0:28:58.160 --> 0:29:00.840
So we have a set of interface and we have ownership.

366
0:29:00.840 --> 0:29:06.840
So each environment has a set of owners, but we cannot limit.

367
0:29:06.840 --> 0:29:12.480
So a developer can create a customization that adds a new namespace and statically we

368
0:29:12.480 --> 0:29:18.280
cannot limit what kind of yamls, what kind of resources it's going to be created by the

369
0:29:18.280 --> 0:29:22.120
developers inside his cluster tree.

370
0:29:22.120 --> 0:29:24.520
How do you handle this?

371
0:29:24.520 --> 0:29:29.880
If it's deployed from our overlays, we would know that.

372
0:29:29.880 --> 0:29:37.760
And if it's deployed from his own customization repository or whatever, he wouldn't have the

373
0:29:37.760 --> 0:29:41.160
permissions.

374
0:29:41.160 --> 0:29:42.960
To create a specific resources?

375
0:29:42.960 --> 0:29:43.960
Yes.

376
0:29:43.960 --> 0:29:47.840
How do you manage that limitation?

377
0:29:47.840 --> 0:29:56.440
So if he's, maybe I don't understand the question, but if I have a developer who has access to

378
0:29:56.440 --> 0:30:00.840
set of namespaces, they can deploy only to that set of namespaces.

379
0:30:00.840 --> 0:30:07.880
And if they onboard our RGO city to manage their application through our RGO city, they

380
0:30:07.880 --> 0:30:12.840
have their specific RGO city project, which also restricts the RBAC.

381
0:30:12.840 --> 0:30:18.960
So they won't be able to deploy to any cluster, just to that cluster that they have access

382
0:30:18.960 --> 0:30:21.440
to and just to those namespaces they have access to.

383
0:30:21.440 --> 0:30:22.440
Okay.

384
0:30:22.440 --> 0:30:25.720
So the cluster resources are only managed by the operations team.

385
0:30:25.720 --> 0:30:28.760
And then developers, in your case, you have a mix it.

386
0:30:28.760 --> 0:30:36.120
So the developers can create patches and edit part of the tree of the cluster.

387
0:30:36.120 --> 0:30:41.600
So we don't know how to handle, like they only can create a specific set of resources.

388
0:30:41.600 --> 0:30:43.160
And we do that through validation.

389
0:30:43.160 --> 0:30:48.520
So they can review, but we need to approve and manually review that they are not creating

390
0:30:48.520 --> 0:30:54.760
like namespaces or operators or cluster roles or something like that.

391
0:30:54.760 --> 0:31:00.480
We limit that through a single code basically for a single repository.

392
0:31:00.480 --> 0:31:05.040
But we also like do this pro with chat apps and whatnot ownership.

393
0:31:05.040 --> 0:31:07.760
That's great addition.

394
0:31:07.760 --> 0:31:15.960
Any more questions?

395
0:31:15.960 --> 0:31:17.960
Okay then we call it a day.

396
0:31:17.960 --> 0:31:18.960
Thank you so much.

397
0:31:18.960 --> 0:31:25.180
Thank you.

398
0:31:25.180 --> 0:31:29.520
Thank you.

399
0:31:29.520 --> 0:31:35.280
That was great.

