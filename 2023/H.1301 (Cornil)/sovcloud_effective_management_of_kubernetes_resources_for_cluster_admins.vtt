WEBVTT

00:00.000 --> 00:13.480
So, last session for today and we'll make sure it's going to be a really, really long

00:13.480 --> 00:18.400
one so that you have to starve and don't get to the drinks.

00:18.400 --> 00:24.680
And I'm happy to welcome Tom from Red Hat, have fun and enlighten us.

00:24.680 --> 00:25.760
Thank you.

00:25.760 --> 00:31.680
Should I have fun or them?

00:31.680 --> 00:33.360
Should I have fun or them?

00:33.360 --> 00:35.200
Both, but...

00:35.200 --> 00:36.200
Okay.

00:36.200 --> 00:38.200
Hello, everyone.

00:38.200 --> 00:40.200
My name is Tom Sofa.

00:40.200 --> 00:47.560
I work at Red Hat and I think the last talk was a great segue into my talk.

00:47.560 --> 00:52.440
So if you were here for the previous presentation, who was here?

00:52.440 --> 00:53.440
Good.

00:53.440 --> 00:54.440
Good.

00:54.440 --> 00:57.520
So we were talking about...

00:57.520 --> 01:04.320
The talk was about standardization, call for a unified platform, call for a sharing exchange

01:04.320 --> 01:14.040
of ideas, knowledge, of findings and how to get to some kind of an open, unified sovereign

01:14.040 --> 01:15.040
cloud.

01:15.040 --> 01:24.800
Well, we've been working on, I think, like that for past two years or so.

01:24.800 --> 01:33.480
In an initiative called Operate First, building open hybrid cloud platform ready for everybody

01:33.480 --> 01:40.200
to consume, to use, to look into operations, to look into metrics, look into whatever telemetry

01:40.200 --> 01:44.720
you have to actually do the operations yourself if you want to.

01:44.720 --> 01:52.000
So this talk is going to be focused precisely on that, on sharing a story, sharing a lesson

01:52.000 --> 02:02.560
that we learned during the time and maybe hopefully take it as an opportunity to not

02:02.560 --> 02:08.480
just share that with you, but to also encourage you to learn those lessons for yourself and

02:08.480 --> 02:14.680
experience our pains and our challenges yourself.

02:14.680 --> 02:17.760
So let's dig in.

02:17.760 --> 02:24.920
So the talk is called Effective Management of Kubernetes Resources, the GitOps way,

02:24.920 --> 02:26.360
GitOps for cluster admins.

02:26.360 --> 02:31.160
So first we're going to talk a bit about what is a cluster lifecycle and what's the role

02:31.160 --> 02:33.920
of cluster operations in that.

02:33.920 --> 02:38.640
Then we're going to experience the chaos that is out there in the world.

02:38.640 --> 02:40.840
And then we're going to talk YAML.

02:40.840 --> 02:45.760
If you went on the YAML lighting talk, this is going to be a very slight variation of

02:45.760 --> 02:49.720
that, but more Kubernetes focused.

02:49.720 --> 02:53.680
And then we're going to bring some order to that chaos.

02:53.680 --> 02:59.200
So we have these three graces of cloud management, right?

02:59.200 --> 03:02.160
We usually provision some resources.

03:02.160 --> 03:04.040
We manage those clusters.

03:04.040 --> 03:09.100
Once they are deployed, once they are provisioned, we then deploy applications on top of them.

03:09.100 --> 03:16.080
If you are talking about Kubernetes based cloud systems, this is the usual three pillars,

03:16.080 --> 03:20.000
three graces of what we are experiencing.

03:20.000 --> 03:25.280
We have tools available for both hands of the spectrum.

03:25.280 --> 03:33.020
So for resource provisioning, we have great tools like Ansible, Terraform, Hive, or Cluster

03:33.020 --> 03:35.860
API in Kubernetes.

03:35.860 --> 03:42.200
This is an established pattern, established workflow that is widely used across hyperscalers,

03:42.200 --> 03:49.200
across people who are deploying Kubernetes by themselves, and so on.

03:49.200 --> 03:50.200
Good.

03:50.200 --> 03:51.240
This is a soft issue.

03:51.240 --> 03:54.440
This is a non-issue.

03:54.440 --> 03:58.720
Then there's the application maintenance, application deployment, application lifecycle.

03:58.720 --> 04:10.280
Again, very well thought through aspect, very studied place.

04:10.280 --> 04:12.080
We have tools like Customize and Helm.

04:12.080 --> 04:21.240
We have Argo CD or Flux CD to do continuous deployment of your workloads and to provide

04:21.240 --> 04:28.020
you with all the goodies like rollbacks to previously non-broken deployment and taking

04:28.020 --> 04:36.880
it even further with other projects like, now I forgot the name.

04:36.880 --> 04:40.800
What are we talking about?

04:40.800 --> 04:42.800
The SRE driven deployment?

04:42.800 --> 04:45.200
No, you don't.

04:45.200 --> 04:46.200
Okay.

04:46.200 --> 04:47.200
Let's move on.

04:47.200 --> 04:49.000
So what about the middle part?

04:49.000 --> 04:52.920
The cluster management itself.

04:52.920 --> 04:57.560
If we are managing Kubernetes resources, what are we talking about?

04:57.560 --> 05:05.360
If we are managing nodes, if we are managing tenancy, if we are managing networks, what

05:05.360 --> 05:11.360
are we actually talking about and how we can manage that?

05:11.360 --> 05:15.940
We have these four main problems that we want to solve somehow.

05:15.940 --> 05:21.440
We found out that basically nowadays it wasn't the case two years ago, but nowadays it's

05:21.440 --> 05:30.000
the case that we can solve all of them through Kubernetes native resources, through YAMLs,

05:30.000 --> 05:35.120
through deploying YAMLs, applying YAMLs to our clusters.

05:35.120 --> 05:38.600
It's done by a few different means.

05:38.600 --> 05:47.400
So we have main areas within Kubernetes API that we can explore to solve these needs.

05:47.400 --> 05:53.000
We have multi-nancy, so we can solve that by just simple namespaces, cluster roles and

05:53.000 --> 05:55.000
whatnot.

05:55.000 --> 06:00.880
Cluster upgrades, again, we can apply install operators, talk to those operators and get

06:00.880 --> 06:03.020
those clusters upgraded.

06:03.020 --> 06:09.500
For storage management, we can use operators, we can use storage classes and storage providers

06:09.500 --> 06:14.920
and custom resources if we wanted to deploy our own storage on, for example, bare metal

06:14.920 --> 06:15.920
clusters.

06:15.920 --> 06:22.960
And for network management, we can do that also through operators, so things like cert

06:22.960 --> 06:30.680
manager, things like state, all of that can now happen through Kubernetes API natively.

06:30.680 --> 06:31.680
That's great.

06:31.680 --> 06:35.160
What did it tell us about the cluster management?

06:35.160 --> 06:38.560
It can be all managed as a Kubernetes application.

06:38.560 --> 06:40.760
It's in YAML.

06:40.760 --> 06:44.840
Well, YAML is a mess.

06:44.840 --> 06:47.120
We know that.

06:47.120 --> 06:51.200
And we know that thanks to multiple aspects.

06:51.200 --> 06:57.840
YAML can be defined and stored in files no matter how you structure it.

06:57.840 --> 07:00.880
It can be a single file with many different resources.

07:00.880 --> 07:12.000
It can be many different files, each of them holding a separate resource and only asterisks

07:12.000 --> 07:16.160
in bash is the limit for your kubectl apply.

07:16.160 --> 07:20.000
You can do whatever you like on the client side.

07:20.000 --> 07:25.120
On the other hand, on the server side, the manifest that you apply to the cluster is

07:25.120 --> 07:27.840
not the same that you get from the cluster back.

07:27.840 --> 07:29.460
It's modified, it's mutated.

07:29.460 --> 07:32.200
So you have things like status.

07:32.200 --> 07:35.760
Some operators, some controllers modify also specs.

07:35.760 --> 07:39.120
Some modify also annotations, labels and whatnot.

07:39.120 --> 07:43.960
And you don't have the full control over the definition.

07:43.960 --> 07:55.120
You need to know what subset of the keys and values you can actually define as a declarative

07:55.120 --> 07:57.440
manifest for a resource.

07:57.440 --> 08:02.400
It's not the same as the manifest applied on the cluster.

08:02.400 --> 08:05.920
So how people store manifests online.

08:05.920 --> 08:12.120
If we pull random project on GitHub that is deployed to Kubernetes, you will find many

08:12.120 --> 08:15.480
of these patterns.

08:15.480 --> 08:22.720
Kubectl doesn't have ordering, so people solve it creatively through numbering their

08:22.720 --> 08:24.180
manifests.

08:24.180 --> 08:28.600
Some are aware that their application may run in different environments.

08:28.600 --> 08:34.640
So they create different files with double-a-sit content, with the same deployment, with just

08:34.640 --> 08:38.440
few lines changed here and there.

08:38.440 --> 08:40.560
Some combine those approaches.

08:40.560 --> 08:46.280
And in some projects, and we find that even in some controllers for their dev setup, they

08:46.280 --> 08:52.520
have a single file with all those resources in line there.

08:52.520 --> 08:53.520
This is not a standard.

08:53.520 --> 08:55.880
This is not a good practice.

08:55.880 --> 09:04.560
And if we want to manage an environment which is live, which should be out-disabled, which

09:04.560 --> 09:10.760
should be approachable to people, this is not the way we should do it.

09:10.760 --> 09:15.040
So in application space, we have basically two choices.

09:15.040 --> 09:17.860
How to organize how to structure our manifest.

09:17.860 --> 09:24.680
One is through Helm, which is great if you're deploying applications and you want some templating

09:24.680 --> 09:30.600
involved if you want to quickly change many different places of the same manifest or of

09:30.600 --> 09:32.040
different manifests.

09:32.040 --> 09:39.260
So you can basically create this template, applying these values, and you get the full

09:39.260 --> 09:41.880
YAML that we saw earlier.

09:41.880 --> 09:43.720
Great.

09:43.720 --> 09:45.560
But is it readable?

09:45.560 --> 09:50.480
Is it understandable from the YAML itself without rendering?

09:50.480 --> 09:51.880
We don't think so.

09:51.880 --> 09:59.520
And we want our cloud manifests to be out-achable, to be approachable, to be reviewable.

09:59.520 --> 10:06.920
So if we want to be able to explore what those changes do on a PR review without actually

10:06.920 --> 10:13.880
spinning up a cluster and applying that PR and maybe do some static validation on it,

10:13.880 --> 10:15.640
you can do that with this.

10:15.640 --> 10:16.960
You would need to render it.

10:16.960 --> 10:19.200
You would need to understand it.

10:19.200 --> 10:24.120
And if you change the template and you have different values for different environments,

10:24.120 --> 10:27.120
how would it affect the template itself?

10:27.120 --> 10:29.080
So you need to explore all the possibilities.

10:29.080 --> 10:33.480
And this is one of the biggest challenge in the Helm space that we are currently facing

10:33.480 --> 10:37.840
in application development.

10:37.840 --> 10:43.800
Then we have the other way, the Kubernetes native configuration management customized,

10:43.800 --> 10:46.960
which is a bit nicer.

10:46.960 --> 10:55.800
All of those manifests are fairly easily organized and referenced through customization.

10:55.800 --> 11:00.840
And all those customizations are organized into bases and overlays.

11:00.840 --> 11:08.640
So it's a composition type of configuration that we have a base which defines the basics

11:08.640 --> 11:15.980
and then we have the overlay which can patch and mix different resources.

11:15.980 --> 11:18.880
These resources in the base are already complete.

11:18.880 --> 11:24.920
This is a complete definition, complete declaration of my resource.

11:24.920 --> 11:26.840
This is reviewable.

11:26.840 --> 11:33.840
So we kind of thought that this might be a way, but before that we defined a couple of

11:33.840 --> 11:40.400
rules, couple of directives that we wanted to achieve with this.

11:40.400 --> 11:47.600
So if we wanted to organize our manifest, we don't want to build our own solution.

11:47.600 --> 11:52.820
We don't want to build our own CI CD that would understand our manifest structure.

11:52.820 --> 11:58.040
We want to use something that is readily available with great community.

11:58.040 --> 12:02.840
We want the configuration to be stable.

12:02.840 --> 12:07.040
So if I change one manifest, it doesn't break five different clusters.

12:07.040 --> 12:10.400
And those things that never happen usually happen.

12:10.400 --> 12:18.640
So if something like that happens, I can roll back the faulty cluster, just that individual

12:18.640 --> 12:19.640
cluster.

12:19.640 --> 12:25.120
I don't need to roll back all the clusters that are working with that particular configuration.

12:25.120 --> 12:26.880
And it's unit testable.

12:26.880 --> 12:29.680
So that's also an important thing.

12:29.680 --> 12:34.600
File mapping, also very interesting topic because YAML allows you to inline multiple

12:34.600 --> 12:37.040
resources into a single file.

12:37.040 --> 12:39.560
But we don't want that.

12:39.560 --> 12:45.200
We want the file and its name to fully represent the resource.

12:45.200 --> 12:50.640
So before I even open the YAML, I already know what to expect inside.

12:50.640 --> 13:00.080
I don't have to guess from a dev.yaml or from namespace.yaml which also contains like

13:00.080 --> 13:04.720
OpenShoot project or whatnot.

13:04.720 --> 13:07.960
Each file is readable without processing.

13:07.960 --> 13:10.120
That's so explanatory.

13:10.120 --> 13:19.920
I want to be able to open the code tab on my GitHub repository and understand the manifest.

13:19.920 --> 13:24.400
If I'm defining the same resource on multiple clusters, if I'm applying the same resource

13:24.400 --> 13:31.400
on multiple clusters, let's say I have the same user group on two different clusters,

13:31.400 --> 13:34.720
I want to apply similar or the same RBAC.

13:34.720 --> 13:42.440
I want to apply the same cluster roles, project namespace, permissions and whatnot.

13:42.440 --> 13:48.800
I don't want this definition to be duplicit, to be defined differently, maybe differently,

13:48.800 --> 13:51.560
maybe slightly differently, maybe the same in two different places.

13:51.560 --> 13:54.560
I want to share the same definition.

13:54.560 --> 14:03.080
As a practice that we use in programming for ages, this is not a well-established pattern

14:03.080 --> 14:05.880
in Kubernetes manifests.

14:05.880 --> 14:09.360
We want to reuse stuff.

14:09.360 --> 14:17.200
And as I told before, the file name already describes what's inside.

14:17.200 --> 14:22.680
So we came up with this pattern and this pattern has been vetted through a couple of organizations

14:22.680 --> 14:26.800
that I'll show you later on.

14:26.800 --> 14:29.260
And this is a pattern that we come up to.

14:29.260 --> 14:39.080
We have a base for customize, which references every single object that we deploy to any

14:39.080 --> 14:44.360
our cluster that requires elevated permissions.

14:44.360 --> 14:52.400
If those resources are standard namespace scoped, things like deployment, config map,

14:52.400 --> 14:56.280
secret, whatnot, this is the developer responsibility.

14:56.280 --> 15:01.600
They live in their own self-contained namespace and they can do whatever they want in there.

15:01.600 --> 15:08.520
But if we are talking about creating namespaces or creating cluster roles, we don't want developers

15:08.520 --> 15:18.280
to create namespaces on their own or create limit ranges or create resource quotas on

15:18.280 --> 15:19.400
their own.

15:19.400 --> 15:29.880
We want to do this, set those things for them because we don't want them to basically expand

15:29.880 --> 15:36.580
and take over the cluster if we don't want them to.

15:36.580 --> 15:43.600
So this pattern of API group, kind and name is actually kind of working because already

15:43.600 --> 15:51.720
from the path, base core namespace, or in cloud, or base, force network, talks, and

15:51.720 --> 15:58.960
I talk, I already know what the resource is about without actually looking into the file.

15:58.960 --> 16:05.240
Then I have overlays, which each overlay represents a single cluster.

16:05.240 --> 16:12.520
Sometimes they have customization, which basically mixes and matches whatever resources I want

16:12.520 --> 16:14.040
to pull from base.

16:14.040 --> 16:19.280
And if I want to change something from the base, I basically just patch it because customize

16:19.280 --> 16:25.360
allows us to patch resources and applies either a strategic merge patch or a JSON patch so

16:25.360 --> 16:28.320
I can do various things with that.

16:28.320 --> 16:35.000
This is very helpful if I have, for example, cluster admins group and I want different

16:35.000 --> 16:42.880
cluster admins on different cluster, but the group itself is already defined in base.

16:42.880 --> 16:46.440
This is nice, but it doesn't work in all cases.

16:46.440 --> 16:48.880
It doesn't solve all the issues.

16:48.880 --> 16:53.440
So we had to introduce two additional concepts.

16:53.440 --> 17:01.600
One is components, which is also an alpha extension to customize, which allows you to

17:01.600 --> 17:05.840
reuse the same manifest multiple times.

17:05.840 --> 17:12.360
This is important in cases like RBAC if we have role bindings that we want to apply to

17:12.360 --> 17:20.460
multiple namespaces, like granting this user a group admin access to a certain namespace,

17:20.460 --> 17:26.360
because if customized by itself wouldn't allow us to use that resource multiple times.

17:26.360 --> 17:30.480
So this is a limitation of customize in this particular case that can be overcome through

17:30.480 --> 17:32.280
components.

17:32.280 --> 17:41.880
And then we came up with bundles, which is an addition that basically selects related

17:41.880 --> 17:45.760
resources from the base, which are always applied together.

17:45.760 --> 17:51.520
So imagine you want to install a cert manager.

17:51.520 --> 17:52.880
It's always a namespace.

17:52.880 --> 17:56.840
It's always a service account with cross-terile.

17:56.840 --> 18:04.520
It's always a subscription or whatever, or a cluster issuer for certificates.

18:04.520 --> 18:11.080
So all of these things come together, and their reference is bundles, so we don't clutter

18:11.080 --> 18:12.720
the overlays too much.

18:12.720 --> 18:18.520
And we also introduced common overlays, which are region specific, which are shared across

18:18.520 --> 18:25.460
regions because for some regions, we have a shared config.

18:25.460 --> 18:32.040
So how such single cluster overlay customization looks like.

18:32.040 --> 18:33.120
We reference the common.

18:33.120 --> 18:41.920
We take all from common, which also references some things from the base and whatnot.

18:41.920 --> 18:48.280
Then we can, for example, this way, deploy our customer resource definition for Prow.

18:48.280 --> 18:57.520
We can create an namespace for Prow, and we can apply some RBAC to Node-Labra.

18:57.520 --> 19:05.200
We can install a whole bundle for cert manager as is, and this ensures cert manager is deployed

19:05.200 --> 19:08.760
and configured properly for this cluster.

19:08.760 --> 19:18.480
We also can specify a specific version for the particle open-shift cluster to upgrade

19:18.480 --> 19:23.640
it to maintenance on the old CPU version.

19:23.640 --> 19:31.360
And if we want to, we can patch certain resources, as I mentioned, the cluster admin.

19:31.360 --> 19:38.600
So fairly simple pattern, but there's been a two-year journey to get into a state where

19:38.600 --> 19:43.560
it's actually working across regions, where it's actually working across multiple clusters.

19:43.560 --> 19:51.320
And when it's efficient in managing multiple clusters through PRs, through GitOps, through

19:51.320 --> 19:58.040
single file YAML based changes so it doesn't break all the clusters.

19:58.040 --> 20:03.400
What I didn't mention on this slide, each of the cluster has their own separate ARGOS

20:03.400 --> 20:04.880
ID application.

20:04.880 --> 20:10.320
So they act independently in the CD process.

20:10.320 --> 20:19.520
They reference the same code base, but they are independent, so the rollback is possible.

20:19.520 --> 20:27.120
So in conclusion, to evaluate what we did here, we have no duplicity.

20:27.120 --> 20:28.400
Manifest are readable.

20:28.400 --> 20:31.600
Manifest are not confusing.

20:31.600 --> 20:33.840
The set of rules is fairly simple.

20:33.840 --> 20:37.760
It's nothing very complex or bulky.

20:37.760 --> 20:42.360
The CICD is very easy, and we can do static validation.

20:42.360 --> 20:43.680
We can do unit tests.

20:43.680 --> 20:45.600
We can do integration tests.

20:45.600 --> 20:49.560
All of that can be done fairly nicely.

20:49.560 --> 20:51.220
What are the downsides?

20:51.220 --> 20:57.960
We have boilerplate in the form of customizations, in the form of components, in the form of

20:57.960 --> 21:05.240
very nested path structures, directory structures and whatnot.

21:05.240 --> 21:14.000
Not always very straightforward, so you need to learn the tools before you can use it.

21:14.000 --> 21:19.120
What also limits our static scheme validation is that manifests in base can be partials

21:19.120 --> 21:26.760
because they are not always complete because we expect to patch them in those overlays

21:26.760 --> 21:32.040
to set a specific channel for our operator subscription and whatnot.

21:32.040 --> 21:34.780
So that's that.

21:34.780 --> 21:39.160
We have four organizations currently adopting this scheme and running this scheme.

21:39.160 --> 21:45.680
We have Operate First Community Cloud, New England Research Cloud, Massachusetts Open

21:45.680 --> 21:53.760
Cloud and Open Source Climate Alliance all running on this pattern.

21:53.760 --> 21:59.800
So this is a lesson that we learned through collaboration in Cloud Operations, and I hope

21:59.800 --> 22:08.400
we may be able to learn more such lessons in the future by exploring Cloud together.

22:08.400 --> 22:13.580
So if you want to know more, you can join us in Operate First of the Cloud.

22:13.580 --> 22:19.080
You can see our ADRs and how we got to those outcomes.

22:19.080 --> 22:24.960
And on the last link over here, you can actually see the code base that we are running against

22:24.960 --> 22:27.280
all of those clusters.

22:27.280 --> 22:28.280
Thank you very much.

22:28.280 --> 22:29.280
There we go.

22:29.280 --> 22:46.840
Hello and thank you for the talk.

22:46.840 --> 22:53.840
We use the same, more or less the same pattern, but the one of the manifests in completion,

22:53.840 --> 22:55.840
we fixed it.

22:55.840 --> 23:01.280
We adopted an approach that we defined those attributes that are required with customization

23:01.280 --> 23:02.280
overlay.

23:02.280 --> 23:05.720
So like add only value and then you have completion.

23:05.720 --> 23:10.160
And then you know that that particular value, you can, it's a valid YAML because it matches

23:10.160 --> 23:12.240
the spec fully.

23:12.240 --> 23:18.080
But then you know visually that that particular field will be patching overlay.

23:18.080 --> 23:24.360
So we use that as a solution for the manifesting completion and the static validation.

23:24.360 --> 23:32.080
We always use customization over overlay, and then we know that we are going to do that.

23:32.080 --> 23:36.600
That's just a solution that we, I don't know if there is a better way or a better word

23:36.600 --> 23:39.680
to use for that, but that's our approach.

23:39.680 --> 23:43.480
We use the same, but it doesn't work in every case.

23:43.480 --> 23:47.760
We found like in some cases the scheme is very detailed.

23:47.760 --> 23:51.720
It requires this complex nested structure.

23:51.720 --> 23:55.360
Like for example, cert manager requires solvers.

23:55.360 --> 24:07.240
And if you define a solver, you can't remove it in a patch because it's a mapping.

24:07.240 --> 24:10.840
So strategic merges don't work that way in customized.

24:10.840 --> 24:13.800
You would need a JSON patch and you would need a long JSON patch.

24:13.800 --> 24:20.040
And you know, it's becoming less and less clear in this regard.

24:20.040 --> 24:25.440
I think another thing that we do is we have for example a common base like you, and then

24:25.440 --> 24:31.240
have non-production base, production base, and then for example for the admin groups.

24:31.240 --> 24:33.800
So we have a group of admins for the non-productions.

24:33.800 --> 24:38.040
And then we don't have the full group of admins in the base.

24:38.040 --> 24:39.040
We have what?

24:39.040 --> 24:43.760
And then we need it from the non-production or the production in case we need one group

24:43.760 --> 24:44.760
or another.

24:44.760 --> 24:49.680
That's another approach that we found out.

24:49.680 --> 24:50.680
Thank you.

24:50.680 --> 24:58.360
I think it forgot.

24:58.360 --> 25:01.800
And then the last one.

25:01.800 --> 25:07.080
In this case, when you have like a couple of bundles, maybe it's easy, but you have

25:07.080 --> 25:11.720
a cluster with 12 or 15 bundles.

25:11.720 --> 25:13.640
It can be a little bloated.

25:13.640 --> 25:18.400
Having a single I go CD app, managing all the applications of a single cluster.

25:18.400 --> 25:23.240
And we use this approach of we have one for the cluster deployment with Hive.

25:23.240 --> 25:30.100
And then we have for each operator, we have his own tree.

25:30.100 --> 25:34.760
So we have independent applications and when, for example, an operator breaks, it doesn't

25:34.760 --> 25:36.880
break the entire I go CD application of the cluster.

25:36.880 --> 25:42.840
It only breaks the I go CD application or when we need to upgrade or we think it's safer

25:42.840 --> 25:46.960
because we are like really, really scoped and you can not break the entire cluster,

25:46.960 --> 25:47.960
just a single application.

25:47.960 --> 25:48.960
That's also...

25:48.960 --> 25:54.440
Yeah, we do the same for operators which have like specific deployments and whatnot.

25:54.440 --> 26:01.840
If we can deploy operators through subscription to the OpenShift operator catalog, operator

26:01.840 --> 26:06.520
hub, we can do that through a single resource and then it's not bloated that way.

26:06.520 --> 26:16.000
So yes, same lesson that we face the same issue and we were solving it very similarly.

26:16.000 --> 26:20.160
We both got red fat, but we need different reports.

26:20.160 --> 26:21.160
In different...

26:21.160 --> 26:22.160
Yeah.

26:22.160 --> 26:23.160
Good.

26:23.160 --> 26:24.160
Good.

26:24.160 --> 26:27.440
We should talk after.

26:27.440 --> 26:28.440
Yeah.

26:28.440 --> 26:29.440
Hi.

26:29.440 --> 26:30.440
Really nice talk.

26:30.440 --> 26:31.440
Thank you.

26:31.440 --> 26:32.440
Thank you.

26:32.440 --> 26:38.280
We build a lot of internal developer platforms and we face the same issue where we lose track

26:38.280 --> 26:39.720
of the code bases.

26:39.720 --> 26:44.480
Do you implement any repo scanning or file structure scanning that makes sure that this

26:44.480 --> 26:49.880
is enforced among your customized charts and a two-parter?

26:49.880 --> 26:54.520
Do you just block all use of Helm charts because everything has a Helm chart nowadays and it

26:54.520 --> 26:59.440
would be limiting to have to rewrite something in this format if there's an existing Helm

26:59.440 --> 27:05.280
chart or existing customized or is this only for org internal YAML?

27:05.280 --> 27:07.480
Thank you.

27:07.480 --> 27:13.240
We enforce this only for resources that require elevated permissions.

27:13.240 --> 27:19.400
If you have a Helm chart that is deploying custom resource definition, then we tell you

27:19.400 --> 27:21.160
this is not a good thing.

27:21.160 --> 27:23.120
You shouldn't do that.

27:23.120 --> 27:25.880
The API wouldn't allow you to do that.

27:25.880 --> 27:28.800
Like our RBAC settings.

27:28.800 --> 27:38.040
So we basically tell those people you need to get that CRD into our repository, check

27:38.040 --> 27:44.720
it in our base for resources which require cluster admin or elevated permissions because

27:44.720 --> 27:49.440
if we would reference it from somebody else from some other repository, they can change

27:49.440 --> 27:50.440
it in their repository.

27:50.440 --> 27:53.320
We don't want to do that.

27:53.320 --> 27:59.000
We don't want them to be applying CRDs because those are shared on the cluster.

27:59.000 --> 28:03.720
If two people on the same cluster are deploying the same Helm chart in different versions

28:03.720 --> 28:08.480
with different CRD schema, it can fight and we don't want that.

28:08.480 --> 28:14.200
That's why we want a single source of truth for all the resources that are cluster scoped

28:14.200 --> 28:17.520
or requiring elevated permissions.

28:17.520 --> 28:24.160
Helm charts are allowed for developer and application workloads in their own namespace,

28:24.160 --> 28:31.760
self-contained or across all of their namespaces if they have more, but not under our watch

28:31.760 --> 28:35.360
on the elevated permissions.

28:35.360 --> 28:42.120
Thank you.

28:42.120 --> 28:52.080
You said that when you have several clusters, you can limit what the developers or the user

28:52.080 --> 28:54.400
of the cluster can deploy.

28:54.400 --> 28:55.560
But how do you manage that?

28:55.560 --> 28:58.160
For example, we use a pro.

28:58.160 --> 29:00.840
So we have a set of interface and we have ownership.

29:00.840 --> 29:06.840
So each environment has a set of owners, but we cannot limit.

29:06.840 --> 29:12.480
So a developer can create a customization that adds a new namespace and statically we

29:12.480 --> 29:18.280
cannot limit what kind of yamls, what kind of resources it's going to be created by the

29:18.280 --> 29:22.120
developers inside his cluster tree.

29:22.120 --> 29:24.520
How do you handle this?

29:24.520 --> 29:29.880
If it's deployed from our overlays, we would know that.

29:29.880 --> 29:37.760
And if it's deployed from his own customization repository or whatever, he wouldn't have the

29:37.760 --> 29:41.160
permissions.

29:41.160 --> 29:42.960
To create a specific resources?

29:42.960 --> 29:43.960
Yes.

29:43.960 --> 29:47.840
How do you manage that limitation?

29:47.840 --> 29:56.440
So if he's, maybe I don't understand the question, but if I have a developer who has access to

29:56.440 --> 30:00.840
set of namespaces, they can deploy only to that set of namespaces.

30:00.840 --> 30:07.880
And if they onboard our RGO city to manage their application through our RGO city, they

30:07.880 --> 30:12.840
have their specific RGO city project, which also restricts the RBAC.

30:12.840 --> 30:18.960
So they won't be able to deploy to any cluster, just to that cluster that they have access

30:18.960 --> 30:21.440
to and just to those namespaces they have access to.

30:21.440 --> 30:22.440
Okay.

30:22.440 --> 30:25.720
So the cluster resources are only managed by the operations team.

30:25.720 --> 30:28.760
And then developers, in your case, you have a mix it.

30:28.760 --> 30:36.120
So the developers can create patches and edit part of the tree of the cluster.

30:36.120 --> 30:41.600
So we don't know how to handle, like they only can create a specific set of resources.

30:41.600 --> 30:43.160
And we do that through validation.

30:43.160 --> 30:48.520
So they can review, but we need to approve and manually review that they are not creating

30:48.520 --> 30:54.760
like namespaces or operators or cluster roles or something like that.

30:54.760 --> 31:00.480
We limit that through a single code basically for a single repository.

31:00.480 --> 31:05.040
But we also like do this pro with chat apps and whatnot ownership.

31:05.040 --> 31:07.760
That's great addition.

31:07.760 --> 31:15.960
Any more questions?

31:15.960 --> 31:17.960
Okay then we call it a day.

31:17.960 --> 31:18.960
Thank you so much.

31:18.960 --> 31:25.180
Thank you.

31:25.180 --> 31:29.520
Thank you.

31:29.520 --> 31:35.280
That was great.
