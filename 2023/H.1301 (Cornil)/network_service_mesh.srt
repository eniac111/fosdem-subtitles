1
0:00:00.000 --> 0:00:10.600
Hello everyone, welcome to this session about psyllium surface mesh.

2
0:00:10.600 --> 0:00:18.220
My name is Raymond Dion, I'm field CTO for Isofalent, the originators from psyllium.

3
0:00:18.220 --> 0:00:23.320
Today I'm going to talk a bit about eBPF and psyllium as an introduction, after which I'm

4
0:00:23.320 --> 0:00:28.480
going to talk about how the surface mesh is evolving, after which we'll talk about the

5
0:00:28.480 --> 0:00:33.640
psyllium surface mesh features, what we can do today and what we're planning to support

6
0:00:33.640 --> 0:00:36.080
in the future.

7
0:00:36.080 --> 0:00:40.480
Quick highlight of some upcoming features and some current features and if we have time

8
0:00:40.480 --> 0:00:44.760
I have a little demo to show how it works.

9
0:00:44.760 --> 0:00:50.160
Can I see some hands from you if you know eBPF?

10
0:00:50.160 --> 0:00:51.480
Quite a lot, good.

11
0:00:51.480 --> 0:00:53.520
How many of you know psyllium?

12
0:00:53.520 --> 0:00:54.520
Cool.

13
0:00:54.520 --> 0:00:58.520
How many of you use psyllium actually?

14
0:00:58.520 --> 0:00:59.520
Not as much.

15
0:00:59.520 --> 0:01:01.200
Okay, cool.

16
0:01:01.200 --> 0:01:06.800
For the ones who don't know what eBPF is, I'm going to do an introduction here, eBPF

17
0:01:06.800 --> 0:01:11.080
is standing for Extended Berkeley Packet Filter.

18
0:01:11.080 --> 0:01:14.560
By itself that doesn't mean a lot.

19
0:01:14.560 --> 0:01:20.720
What we like to compare it with is what JavaScript is to the browser, eBPF is to the kernel.

20
0:01:20.720 --> 0:01:28.320
What that means is that using eBPF we can attach programs to kernel events and for the

21
0:01:28.320 --> 0:01:34.320
purpose of this session is that we can attach eBPF programs to kernel events related to

22
0:01:34.320 --> 0:01:35.320
networking.

23
0:01:35.320 --> 0:01:42.320
So that's either a socket being opened network packet being sent on a network device, that

24
0:01:42.320 --> 0:01:45.760
means that's a kernel event and that means that we can attach a program to it and we

25
0:01:45.760 --> 0:01:54.480
can get a matrix from that packet for example or we can do load balancing and such.

26
0:01:54.480 --> 0:02:04.640
So psyllium is built on eBPF, you don't need to be an eBPF developer to actually work with

27
0:02:04.640 --> 0:02:05.640
psyllium.

28
0:02:05.640 --> 0:02:09.120
Psyllium abstracts this complexity and technology under the hood.

29
0:02:09.120 --> 0:02:15.240
So based on the configuration you set, psyllium will mount the required eBPF programs for

30
0:02:15.240 --> 0:02:21.480
you to run and psyllium in short provides networking and load balancing capabilities,

31
0:02:21.480 --> 0:02:28.080
security capabilities and also a lot of observability capabilities using eBPF.

32
0:02:28.080 --> 0:02:31.360
So this is the 30,000 feet view where we are today.

33
0:02:31.360 --> 0:02:39.180
We started with plain networking, IPv6, IPv4 years ago and now we expanded all the networking

34
0:02:39.180 --> 0:02:45.960
capabilities with BGP implementations, net4.6, 6.4, extended load balancing out of the box

35
0:02:45.960 --> 0:02:51.720
we're working on having go BGP control plane fully supported with psyllium.

36
0:02:51.720 --> 0:02:57.680
On top of that we have an observability layer with our Hubble technology which is an observability

37
0:02:57.680 --> 0:03:04.040
tool which provides service to service communication for your name spaces so you can see what's

38
0:03:04.040 --> 0:03:08.060
components, what services are talking to which services.

39
0:03:08.060 --> 0:03:11.480
After which you can make informed decisions for example what kind of network policies

40
0:03:11.480 --> 0:03:13.320
you want to apply.

41
0:03:13.320 --> 0:03:19.200
Also exporting metrics to tools like Raphana and surface mesh on top of that to provide

42
0:03:19.200 --> 0:03:23.840
authentication layer seven path based routing and such.

43
0:03:23.840 --> 0:03:26.820
On the right hand side we also have Tetragon that's not something we'll talk about today

44
0:03:26.820 --> 0:03:32.040
but that's runtime security using eBPF which is also very interesting and we'd run across

45
0:03:32.040 --> 0:03:38.000
clouds doesn't matter if it's on-prem or hybrid or multi-cluster so it's agnostic of the

46
0:03:38.000 --> 0:03:42.080
platform and supported by multiple cloud vendors.

47
0:03:42.080 --> 0:03:46.720
So as you may know Google Enforce data plane V2 on the node is actually psyllium.

48
0:03:46.720 --> 0:03:53.120
Microsoft has recently adopted psyllium as the default CNI for AKS clusters and all their

49
0:03:53.120 --> 0:03:58.240
clusters will be migrated to psyllium and AWS EKS anywhere by default is psyllium so

50
0:03:58.240 --> 0:04:04.880
we see huge adoption in the field of psyllium.

51
0:04:04.880 --> 0:04:07.340
So let's talk about surface mesh.

52
0:04:07.340 --> 0:04:11.960
So obviously if we talk about surface mesh we talk about observing traffic being able

53
0:04:11.960 --> 0:04:19.160
to secure traffic from application to application across clusters doing traffic management building

54
0:04:19.160 --> 0:04:24.120
resilience across applications across clouds.

55
0:04:24.120 --> 0:04:29.520
Surface mesh originally if you needed that capabilities originally you would program

56
0:04:29.520 --> 0:04:34.340
your application either in python or go to get that observability.

57
0:04:34.340 --> 0:04:38.800
That wasn't really useful because you have to maintain all those libraries to get the

58
0:04:38.800 --> 0:04:40.660
information you need.

59
0:04:40.660 --> 0:04:45.320
That's where the sidecars came in right so that they abstract that complexity from the

60
0:04:45.320 --> 0:04:52.800
application to have a standard sidecar implementation to monitor traffic to be able to route traffic

61
0:04:52.800 --> 0:04:56.960
and to be able to extract metrics from the traffic.

62
0:04:56.960 --> 0:05:01.840
However now with psyllium our goal is to move as close to the kernel as we already

63
0:05:01.840 --> 0:05:04.320
run in kernel with eBPF.

64
0:05:04.320 --> 0:05:10.460
So we're moving from a sidecar model to the kernel and where we can we will support it

65
0:05:10.460 --> 0:05:13.640
using eBPF.

66
0:05:13.640 --> 0:05:17.040
The only part which is not yet there is layer 7.

67
0:05:17.040 --> 0:05:24.240
So all the low balancing capabilities routing capabilities in terms of IP to IP metrics

68
0:05:24.240 --> 0:05:28.480
are already available with psyllium using eBPF.

69
0:05:28.480 --> 0:05:37.760
Layer 7 routing is not yet in eBPF for multiple reasons of which one is that eBPF has constraints

70
0:05:37.760 --> 0:05:40.820
in terms of how big a program can be.

71
0:05:40.820 --> 0:05:45.480
Obviously it runs in kernel space so it has constraints for a good reason but in the future

72
0:05:45.480 --> 0:05:52.880
maybe we can even transport complex layer 7 routing in eBPF.

73
0:05:52.880 --> 0:05:58.840
However we already provide layer 7 visibility and ops of ability in using psyllium and eBPF.

74
0:05:58.840 --> 0:06:03.360
We already have the capabilities to inspect traffic using eBPF.

75
0:06:03.360 --> 0:06:08.160
We can already do the low balancing with equip-prosper replacement.

76
0:06:08.160 --> 0:06:14.760
The only part is the layer 7 but the visibility of traffic so HTTP traffic and such is already

77
0:06:14.760 --> 0:06:15.760
there.

78
0:06:15.760 --> 0:06:22.160
So surface mesh capabilities are extending those capabilities moving forward.

79
0:06:22.160 --> 0:06:24.260
So how does it work?

80
0:06:24.260 --> 0:06:29.760
Some of you may know that psyllium runs as an agent as a daemon set on the nodes.

81
0:06:29.760 --> 0:06:35.840
It programs the nodes to be mounting the eBPF programs for the capabilities you need.

82
0:06:35.840 --> 0:06:40.480
We have an embedded envoy running inside the psyllium agent.

83
0:06:40.480 --> 0:06:47.080
This is a narrow down envoy proxy in the agent for networking capabilities.

84
0:06:47.080 --> 0:06:53.460
We leverage this envoy proxy on the node level to do surface mesh capabilities.

85
0:06:53.460 --> 0:06:57.920
So all the things like HTTP path routing and such.

86
0:06:57.920 --> 0:07:02.200
So for each namespace you would create and where you create for example an ingress resource

87
0:07:02.200 --> 0:07:08.760
or a gateway resource that means that a listener will be created through the envoy for that

88
0:07:08.760 --> 0:07:12.520
specific namespace, for that specific work.

89
0:07:12.520 --> 0:07:18.320
And we leverage C groups and stuff to have separation as well for the security reasons

90
0:07:18.320 --> 0:07:23.780
to not be able to have traffic across namespaces as such.

91
0:07:23.780 --> 0:07:29.980
So what is different with psyllium service mesh compared to other service mesh implementations?

92
0:07:29.980 --> 0:07:34.880
First of all our goal is to reduce operational complexity by removing sidecars.

93
0:07:34.880 --> 0:07:40.440
Resource usage reduced better performance and avoids sidecar startup shut down race

94
0:07:40.440 --> 0:07:42.540
conditions.

95
0:07:42.540 --> 0:07:46.560
So obviously if you're not running sidecars at scale this makes a huge difference.

96
0:07:46.560 --> 0:07:51.080
You don't have all the sidecar pods running alongside your normal pods.

97
0:07:51.080 --> 0:07:57.920
It will save memory, it will save CPU, it will save connection tracking etc.

98
0:07:57.920 --> 0:08:00.320
So a lot more efficient.

99
0:08:00.320 --> 0:08:04.680
And also in terms of latency running a sidecar has a cost.

100
0:08:04.680 --> 0:08:09.960
So in this diagram you see that an application wants to send traffic to another application.

101
0:08:09.960 --> 0:08:14.720
What that means technically is that it goes through the TCP IP stack three times with

102
0:08:14.720 --> 0:08:15.960
the sidecar.

103
0:08:15.960 --> 0:08:20.560
First from the app then inbound in the sidecar where the sidecar does its processing and

104
0:08:20.560 --> 0:08:25.600
then external from the sidecar to the physical network device to hit the network to reach

105
0:08:25.600 --> 0:08:28.920
another node.

106
0:08:28.920 --> 0:08:37.840
With eBPF we are able to shortcut that connection and improve the latency because we can detect

107
0:08:37.840 --> 0:08:42.960
the traffic and we can see if it's destined for the physical network or it should be routed

108
0:08:42.960 --> 0:08:46.000
through the proxy.

109
0:08:46.000 --> 0:08:52.200
So once this app opens the socket using eBPF we can shortcut that connection to the physical

110
0:08:52.200 --> 0:08:56.480
network device to be routed on the physical network.

111
0:08:56.480 --> 0:09:02.520
If we need layer seven processing that means that using eBPF we can shortcut the connection

112
0:09:02.520 --> 0:09:06.520
on the socket layer directly to the Envoy proxy where the Envoy proxy on the node does

113
0:09:06.520 --> 0:09:11.480
its HTTP routing and then forwards the traffic again on the physical network.

114
0:09:11.480 --> 0:09:15.040
So a lot less hops there.

115
0:09:15.040 --> 0:09:20.160
And it means that latency is much, much improved because we're not going through this TCP IP

116
0:09:20.160 --> 0:09:22.800
stack multiple times.

117
0:09:22.800 --> 0:09:28.240
In terms of throughput there's also a small difference because we can push more packets.

118
0:09:28.240 --> 0:09:33.800
And in terms of pod ready performance this is also a consideration at scale because when

119
0:09:33.800 --> 0:09:37.800
you're scaling out your applications you always with traditional sidecars you need to wait

120
0:09:37.800 --> 0:09:43.440
for the sidecar to be spun up as well and to be ready to serve connections for that

121
0:09:43.440 --> 0:09:44.900
application.

122
0:09:44.900 --> 0:09:49.040
So without the sidecars with Psyllium Surface Mesh it's already there.

123
0:09:49.040 --> 0:09:53.880
It's running on the node, it's embedded in the proxy so once you scale out your application

124
0:09:53.880 --> 0:10:00.080
the proxy immediately on that node can serve connections.

125
0:10:00.080 --> 0:10:04.120
So in short Psyllium Surface Mesh provides traffic management, observability, security

126
0:10:04.120 --> 0:10:05.520
and resilience.

127
0:10:05.520 --> 0:10:07.440
The goal is to bring your own control plane.

128
0:10:07.440 --> 0:10:11.760
We are not developing a control plane on our own.

129
0:10:11.760 --> 0:10:17.120
What it means is that you can already use Ingress resources with Psyllium 1.13, we'll

130
0:10:17.120 --> 0:10:19.280
support Gateway API.

131
0:10:19.280 --> 0:10:26.360
We are working on Spiffy integration so with the 1.13 release actually the groundwork for

132
0:10:26.360 --> 0:10:30.400
MTLS and Spiffy integration is already there.

133
0:10:30.400 --> 0:10:36.800
You're not really able to use it yet but the goal is to support both MTLS and Spiffy using

134
0:10:36.800 --> 0:10:42.600
Psyllium network policies so you can reference for example a Spiffy ID as a source and destination

135
0:10:42.600 --> 0:10:47.960
using Psyllium network policies and then under the hood the Psyllium agent part will connect

136
0:10:47.960 --> 0:10:55.080
to a Spivey server where that identity is tracked and confirm if that's allowed.

137
0:10:55.080 --> 0:11:00.480
In terms of observability you can leverage the already available observability with Grafana

138
0:11:00.480 --> 0:11:01.560
or Hubble.

139
0:11:01.560 --> 0:11:07.160
If you need to export events you can use scene platforms such as Splunk and OpenTelemetry

140
0:11:07.160 --> 0:11:11.440
is also supported.

141
0:11:11.440 --> 0:11:15.200
If you're new, if you're running new classes you have an option, you can run Psyllium and

142
0:11:15.200 --> 0:11:19.880
you can already use Psyllium Surface Mesh out of the box.

143
0:11:19.880 --> 0:11:27.480
This is obviously the preferred method but if you're running already an Istio based implementation

144
0:11:27.480 --> 0:11:34.160
there's still a lot of benefit to run Psyllium under the hood there as well because for example

145
0:11:34.160 --> 0:11:40.520
we already encrypt the connectivity between the sidecar from an Istio based implementation

146
0:11:40.520 --> 0:11:43.520
towards the destination pod.

147
0:11:43.520 --> 0:11:50.880
What I mean by that is that when you run sidecars, when you run MTLS between applications, that

148
0:11:50.880 --> 0:11:56.720
connectivity may be secure but the connection between the sidecar and the actual destination

149
0:11:56.720 --> 0:11:58.520
is unencrypted on the node.

150
0:11:58.520 --> 0:12:04.320
Anyone with specific privileges on a node could potentially listen on that virtual interface

151
0:12:04.320 --> 0:12:08.680
and eavesdrop traffic and that's obviously not secure.

152
0:12:08.680 --> 0:12:13.520
Running Psyllium under the hood already gives you the benefit because we can encrypt on

153
0:12:13.520 --> 0:12:21.520
layer 4 directly on a socket layer to the destination pod obviously.

154
0:12:21.520 --> 0:12:28.400
With 1.12, so currently we have 1.12 available since I think seven months, we already have

155
0:12:28.400 --> 0:12:32.720
a production ready Psyllium Surface Mesh, a conformant ingress controller which you

156
0:12:32.720 --> 0:12:38.680
can use for HDPath routing, canary releases and such.

157
0:12:38.680 --> 0:12:42.040
You can use Kubernetes as your Surface Mesh control plane.

158
0:12:42.040 --> 0:12:45.800
Prometheus Metrix OpenTelemetry is supported.

159
0:12:45.800 --> 0:12:51.520
For power users we have Psyllium Envoy config and Psyllium Cluster-wide Envoy config CRDs

160
0:12:51.520 --> 0:12:52.520
available.

161
0:12:52.520 --> 0:12:57.360
These are temporarily I would say because the goal is to replace all that capabilities

162
0:12:57.360 --> 0:13:00.700
with Gateway API.

163
0:13:00.700 --> 0:13:04.680
And we're releasing more and more extended Grafana dashboards for layer 7 visibility

164
0:13:04.680 --> 0:13:11.280
so you can actually see between surface to surface what kind of metrics there are and

165
0:13:11.280 --> 0:13:18.720
what the latencies are and what return codes are, so golden signals.

166
0:13:18.720 --> 0:13:24.960
The roadmap for 1.13 and we're very close for releasing 1.13 expected somewhere this

167
0:13:24.960 --> 0:13:27.600
month hopefully.

168
0:13:27.600 --> 0:13:34.640
You can already try a release candidate for Psyllium 1.13 which includes a Gateway API

169
0:13:34.640 --> 0:13:41.880
support for HTTP routing, TLS termination, HTTP traffic splitting and waiting.

170
0:13:41.880 --> 0:13:48.160
So this allows you to do percentage based routing or canary releases as such without

171
0:13:48.160 --> 0:13:51.240
configuring Psyllium Envoy config resources.

172
0:13:51.240 --> 0:13:55.280
And also the capability to have multiple ingresses per load balancer.

173
0:13:55.280 --> 0:14:00.760
What that means is that currently when you create a Psyllium ingress we rely on the load

174
0:14:00.760 --> 0:14:06.280
balancer to attract traffic and forward that to the proxy.

175
0:14:06.280 --> 0:14:10.640
Obviously at scale having a load balancer for each ingress especially in clouds is

176
0:14:10.640 --> 0:14:16.360
expensive so this with an annotation we allow multiple ingresses per load balancer so you

177
0:14:16.360 --> 0:14:20.640
can save cost there.

178
0:14:20.640 --> 0:14:23.560
So how am I doing in time?

179
0:14:23.560 --> 0:14:34.000
So today ingress 1.12 also with services we are having support for annotations so imagine

180
0:14:34.000 --> 0:14:40.800
you have received traffic from your ingress you forward it to a service that means we

181
0:14:40.800 --> 0:14:47.040
support annotations on a simple cluster IP to forward traffic for example to a specific

182
0:14:47.040 --> 0:14:48.500
endpoint.

183
0:14:48.500 --> 0:14:52.640
If you know what Psyllium cluster mesh is we can connect Psyllium across clusters.

184
0:14:52.640 --> 0:14:58.040
With simple annotations you can have even high availability of services across clusters.

185
0:14:58.040 --> 0:15:02.760
Gateway API which I will show a bit later and the Envoy config.

186
0:15:02.760 --> 0:15:06.640
So this is a simple example of ingress and this is also something I will show in the

187
0:15:06.640 --> 0:15:08.360
demo.

188
0:15:08.360 --> 0:15:13.280
You have an ingress and from a specific path you want to forward traffic to a specific

189
0:15:13.280 --> 0:15:14.820
service.

190
0:15:14.820 --> 0:15:21.200
We also support GRPC so you can also have specific GRPC URLs to be forwarded to specific

191
0:15:21.200 --> 0:15:22.800
services.

192
0:15:22.800 --> 0:15:29.160
TLS termination to terminate TLS using secrets using ingress.

193
0:15:29.160 --> 0:15:34.680
A question I get a lot is what about SSL pass through that's on the road map so keep that

194
0:15:34.680 --> 0:15:37.200
in mind.

195
0:15:37.200 --> 0:15:42.080
And obviously new in 1.13 is Gateway API and how it looks like is you will configure a

196
0:15:42.080 --> 0:15:49.180
gateway resource you specify the gateway class name for Psyllium to make sure that the gateway

197
0:15:49.180 --> 0:15:53.080
is created and maintained through Psyllium and then create listeners so in this case

198
0:15:53.080 --> 0:15:56.000
an HTTP listener on port 80.

199
0:15:56.000 --> 0:16:03.260
Then additionally you create multiple HTTP routes one or more and this specify for example

200
0:16:03.260 --> 0:16:09.400
a path prefix for values forward slash details to be forwarded to a backend reference service

201
0:16:09.400 --> 0:16:12.680
called details.

202
0:16:12.680 --> 0:16:15.520
In terms of TLS termination same constructs.

203
0:16:15.520 --> 0:16:21.880
You can also have for example a host name in there to only accept traffic for this given

204
0:16:21.880 --> 0:16:28.640
host name and you reference a secret in the gateway resource and then in the HTTP routes

205
0:16:28.640 --> 0:16:35.640
you will specify the host name, you will reference the gateway you want to use and then again

206
0:16:35.640 --> 0:16:41.260
a path prefix for example to forward to a specific service.

207
0:16:41.260 --> 0:16:48.180
And then traffic splitting very simple also using HTTP routes again referencing your gateway

208
0:16:48.180 --> 0:16:55.460
a path prefix and then you have in this case an echo 1 and echo 2 service where you want

209
0:16:55.460 --> 0:17:02.640
to introduce slowly echo 2 in this case 25% of that traffic will be forwarded to the echo

210
0:17:02.640 --> 0:17:05.360
2 service.

211
0:17:05.360 --> 0:17:10.240
And this is the example what I talked about earlier using simple annotations you can extend

212
0:17:10.240 --> 0:17:16.660
service miss capabilities by annotating services so in this case this service will receive

213
0:17:16.660 --> 0:17:23.960
traffic for gRPC and we can attach load balancing modes for in this case weighted loss request

214
0:17:23.960 --> 0:17:27.480
to be forwarded to backend endpoints.

215
0:17:27.480 --> 0:17:32.720
And using multi cluster capabilities you can extend these capabilities across two or more

216
0:17:32.720 --> 0:17:36.360
clusters depending on your cluster mesh configuration.

217
0:17:36.360 --> 0:17:42.040
And can we roll out so you can even introduce new clusters have the new version of the application

218
0:17:42.040 --> 0:17:46.080
running on the new cluster so we're absolutely sure that you have no resource contention

219
0:17:46.080 --> 0:17:51.920
on your original cluster and then on the service annotate traffic to forward slowly to a remote

220
0:17:51.920 --> 0:17:57.280
cluster before you do the flip over.

221
0:17:57.280 --> 0:18:03.200
So this concludes the presentation part so for example when you want to know more about

222
0:18:03.200 --> 0:18:08.120
cilium go to the cilium community I encourage you to join our slack channel if you have

223
0:18:08.120 --> 0:18:13.840
any questions our team is there as well to answer questions for in slack any issues you

224
0:18:13.840 --> 0:18:17.460
may have or any roadmap or feature requests you may have we're very interested to hear

225
0:18:17.460 --> 0:18:19.120
from you.

226
0:18:19.120 --> 0:18:24.320
You can also contribute so obviously if you want to develop on cilium join the cilium

227
0:18:24.320 --> 0:18:30.720
GitHub and contribute if you want to know more about eBPF go to eBPF.io and if you want

228
0:18:30.720 --> 0:18:35.560
to know more about Isovalent the company who originated cilium and want to for example

229
0:18:35.560 --> 0:18:40.880
work there have a look there we are looking for engineers as well so feel free to have

230
0:18:40.880 --> 0:18:45.920
a look and if you want to know more ask me after the session as well.

231
0:18:45.920 --> 0:18:55.360
All right let me do see how I'm doing with time so in order to run ingress and gator

232
0:18:55.360 --> 0:19:01.760
API you need to set a certain amount of flex on your for example your mvalue style so this

233
0:19:01.760 --> 0:19:10.040
is an example I've run a small demo on GKE so this is a GKE cluster with cilium installed

234
0:19:10.040 --> 0:19:15.520
what you need to do is you need to enable the ingress controller and in this case I'm

235
0:19:15.520 --> 0:19:21.160
also enabling metrics just because it's interesting to see what's going on for gateway API there's

236
0:19:21.160 --> 0:19:28.080
also a value so gateway API enabled through this will trigger gateway API to be enabled

237
0:19:28.080 --> 0:19:35.480
for surface mesh it's important to configure the cube proxy replacement to strict or probe

238
0:19:35.480 --> 0:19:40.000
strict is recommended because you have the full cube proxy replacement capabilities in

239
0:19:40.000 --> 0:19:45.400
your cluster this is also required for surface mesh and that's that's basically it to get

240
0:19:45.400 --> 0:19:53.000
started so for the simple demo this I've created a simple gateway the gateway class name name

241
0:19:53.000 --> 0:20:00.440
cilium so this is running cilium 1.13 release candidate 5 which has the gateway API support

242
0:20:00.440 --> 0:20:06.960
and then a simple HTTP route for the book info example application which has matches

243
0:20:06.960 --> 0:20:14.440
for the details and the default path prefixes so when I go into my environment I want to

244
0:20:14.440 --> 0:20:21.760
show quickly the following so if I do a kubectl get surface you can see I already for sake

245
0:20:21.760 --> 0:20:26.880
for the sake of time created this gateways what I wanted to show you is that obviously

246
0:20:26.880 --> 0:20:34.520
a low balance is required so GKE proficient me a low balancer and low bands IP I can use

247
0:20:34.520 --> 0:20:42.280
to attract traffic in this case I'm demoing a default HTTP gateway and a default HTTPS

248
0:20:42.280 --> 0:20:51.520
gateway so I have two low balancers with each an external IP addresses assigned so this

249
0:20:51.520 --> 0:21:01.360
configuration is applied so if I do a kubectl get a gateway good point you also obviously

250
0:21:01.360 --> 0:21:07.520
your cluster you also need to install the CRDs for gateway API support here you can

251
0:21:07.520 --> 0:21:19.600
see I have my gateway and a TLS gateway and if I do a kubectl kubectl get HTTP routes

252
0:21:19.600 --> 0:21:27.800
I can see I have my book info HTTP route installed and this relates to this part obviously so

253
0:21:27.800 --> 0:21:36.160
with that I can should be able to connect to the details so this is running the bookstore

254
0:21:36.160 --> 0:21:42.080
example so I'm using that public IP as you can see it works and if I go to details I

255
0:21:42.080 --> 0:21:48.800
should be forwarded using the gateway API HTTP routes to that specific detail surface

256
0:21:48.800 --> 0:21:58.560
and that works as well for HTTPS again a simple example I've created that gateway TLS gateway

257
0:21:58.560 --> 0:22:04.520
I've created two listeners so a listener for book info dot psyllium dot rocks and a listener

258
0:22:04.520 --> 0:22:08.680
for hipster shop dot psyllium dot rocks I don't have installed I didn't have installed

259
0:22:08.680 --> 0:22:16.080
the hipster shop but for demo purposes I'm also referencing two secrets so I've using

260
0:22:16.080 --> 0:22:21.080
I've used make cert to create a simple self-signed certificate installed it in my certificate

261
0:22:21.080 --> 0:22:28.400
store and created a secret which I reference using this listener then again HTTP routes

262
0:22:28.400 --> 0:22:36.560
for the TLS gateway for book info dot psyllium dot rocks matches to only the details path

263
0:22:36.560 --> 0:22:45.760
prefix on port 9080 and again a part for the hipster shop so that's what I'm going to show

264
0:22:45.760 --> 0:22:52.280
here so if I do the default URL that doesn't work there's no list there's no HTTP route

265
0:22:52.280 --> 0:22:58.640
configured but for details I can see I can connect it securely and this certificate is

266
0:22:58.640 --> 0:23:08.840
being run from the gateway resource as well obviously this is a self-signed certificate

267
0:23:08.840 --> 0:23:15.520
but obviously you can create signed certificates as well with that that concludes my presentation

268
0:23:15.520 --> 0:23:34.160
and the demo I'm open for questions any questions hi thank you very much for your presentation

269
0:23:34.160 --> 0:23:45.080
okay when you talk about a no layer 7 support in BPF yeah you said maybe in the future and

270
0:23:45.080 --> 0:23:56.200
what you spec is it going to come or not I'm not sure about that HTTP routing requires

271
0:23:56.200 --> 0:24:02.640
quite a lot of memory right so obviously memory is limited to the BPF programs for good little

272
0:24:02.640 --> 0:24:08.800
reasons so it will depend on the EPF foundation and the roadmap there what we can support

273
0:24:08.800 --> 0:24:13.240
technically there's no reason why we shouldn't be able to do that but yeah in terms of memory

274
0:24:13.240 --> 0:24:19.320
we have constraints so if those are being raised we potentially can have parts of even

275
0:24:19.320 --> 0:24:36.280
all parts using eBPF any other yeah hi does it provide or can you provide end-to-end encryption

276
0:24:36.280 --> 0:24:41.720
between especially between the node automatically or not yes so our vision there is that you

277
0:24:41.720 --> 0:24:48.600
should configure for example IPSec or wire guard for node to node encryption in transit

278
0:24:48.600 --> 0:24:54.280
and if you want authentication and authorization on top of that to configure SPF or NTLS between

279
0:24:54.280 --> 0:24:59.340
your applications so it's a multi-layered approach so we're not doing the encryption

280
0:24:59.340 --> 0:25:06.520
on the NTLS part but on the node level as that makes if that makes sense so NTLS again

281
0:25:06.520 --> 0:25:16.800
a SPF is on roadmap hopefully for 1.13 any other no okay thank you thank you

