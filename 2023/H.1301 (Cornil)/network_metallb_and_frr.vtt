WEBVTT

00:00.000 --> 00:26.880
Okay.

00:26.880 --> 00:30.680
So thanks everyone for coming.

00:30.680 --> 00:32.960
Today I'm going to talk about two projects.

00:32.960 --> 00:39.680
One is Metal-B, which I'm currently maintaining, and the other one is FRR, which kind of started

00:39.680 --> 00:44.280
integrating in Metal-B more or less one year and a half ago.

00:44.280 --> 00:46.560
Is anyone using Metal-B?

00:46.560 --> 00:47.560
Awesome.

00:47.560 --> 00:48.560
Okay.

00:48.560 --> 00:56.320
So if you found it less stable in the past two years, that's because of me.

00:56.320 --> 01:02.880
So again, the agenda today is I'll describe what Metal-B is, what matters in the context

01:02.880 --> 01:04.840
of Kubernetes.

01:04.840 --> 01:10.920
Then I'll introduce FRR and then I'll talk about the integration between the two.

01:10.920 --> 01:12.040
Some quick words about me.

01:12.040 --> 01:13.040
I'm Federico.

01:13.040 --> 01:20.800
I work for Red Hat almost for four years now, and I'm part of this networking team on the

01:20.800 --> 01:26.880
OpenShift platform that is in charge of making it suitable to run telco workloads that we

01:26.880 --> 01:31.920
know have slightly different requirements from regular application that runs on the

01:31.920 --> 01:32.920
cloud environments.

01:32.920 --> 01:38.680
Due to that, I contributed to a variety of network related projects.

01:38.680 --> 01:42.960
I touched our primary CNI, which is a BNK.

01:42.960 --> 01:50.000
I wrote a simple CNI plugin for using VRFs.

01:50.000 --> 01:55.560
I put some code in Kubernetes itself and lately my primary focus is on Metal-B.

01:55.560 --> 02:00.600
But please don't think that I'm a networking expert because I'm not.

02:00.600 --> 02:03.440
Who doesn't know anything about Kubernetes?

02:03.440 --> 02:05.000
Okay.

02:05.000 --> 02:12.400
So I'll very briefly introduce the concept of services for those that don't know anything

02:12.400 --> 02:14.840
about Kubernetes.

02:14.840 --> 02:22.240
We have our application deployed as multiple pods where we want to scale our traffic against.

02:22.240 --> 02:27.880
And the concept that Kubernetes gives us is services.

02:27.880 --> 02:31.000
And with the service, we get two things.

02:31.000 --> 02:36.400
One is a cluster IP that is a virtual IP accessible from inside the cluster, and the other part

02:36.400 --> 02:38.320
is the balancing part.

02:38.320 --> 02:45.680
So the client tries to hit the service and then the cluster CNI in some way balances

02:45.680 --> 02:47.800
the traffic across the different endpoints.

02:47.800 --> 02:48.920
It's more than this.

02:48.920 --> 02:53.920
We have IP families, we have parts, but that's the main idea.

02:53.920 --> 02:57.540
What if we want to expose our application outside the cluster?

02:57.540 --> 03:03.400
Because there might be use cases where we want to expose them inside, but it makes sense

03:03.400 --> 03:05.400
to access them from outside.

03:05.400 --> 03:12.160
And the main construct that Kubernetes gives us is a service of type load balancer.

03:12.160 --> 03:17.120
This is the definition taken from the Kubernetes documentation.

03:17.120 --> 03:24.400
A service of type load balancer is exposed externally using a cloud provider's load balancer.

03:24.400 --> 03:29.460
As we saw in the talk before, like there is the cloud provider, Google, AWS, that gives

03:29.460 --> 03:38.880
us an IP that is accessible from outside and that drives the traffic towards our application.

03:38.880 --> 03:43.460
And again, the emphasis here is on the cloud provider.

03:43.460 --> 03:46.080
What happens when we create a service of type load balancer?

03:46.080 --> 03:52.440
We get an external IP that is accessible from outside and we get the load the balancing

03:52.440 --> 03:53.440
part.

03:53.440 --> 03:59.960
If we try to access our application, the network infrastructure of the cloud provider will

03:59.960 --> 04:07.920
drive the traffic toward all the nodes of our cluster so that the CNI can do its part.

04:07.920 --> 04:14.040
And this is important to understand how MetalDB works once the traffic gets to the node.

04:14.040 --> 04:18.280
All the rest is handled by the cluster CNI.

04:18.280 --> 04:21.600
And in this case, this is a real network load balancer.

04:21.600 --> 04:27.000
Again, where we don't have control and it's controlled by the provider.

04:27.000 --> 04:33.960
So just to iterate, we get two things from a load balancer service.

04:33.960 --> 04:39.760
We get a stable IP that we can pin our DNS entries to and we get the load balancing across

04:39.760 --> 04:41.200
all the different nodes.

04:41.200 --> 04:45.680
So now let's move to bare metal and see what happens there.

04:45.680 --> 04:51.140
The first thing that we see when we try to create a service of this type on bare metal

04:51.140 --> 04:55.760
is the fact that the external IP, we are not getting an external IP because there is no

04:55.760 --> 04:58.560
one that is giving that to us.

04:58.560 --> 05:03.280
And the second part is even if we had that IP, who is routing the traffic to the nodes

05:03.280 --> 05:07.800
as the cloud provider's network infrastructure is doing?

05:07.800 --> 05:14.880
And these very same two issues are the issues that MetalDB tries to solve.

05:14.880 --> 05:19.580
MetalDB is a community project now under the CNCF umbrella.

05:19.580 --> 05:23.040
It was originally started by David Anderson.

05:23.040 --> 05:30.000
Then there was a handoff to one red actor, Russell Bryant, and two folks working out

05:30.000 --> 05:34.680
at Kinvalk, Rodrigo Campos and Johannes Lieberman.

05:34.680 --> 05:39.240
And during that phase, MetalDB went more or less in maintenance mode.

05:39.240 --> 05:46.880
They were replying to issues and stuff like that, but it wasn't evolving too much.

05:46.880 --> 05:54.160
And at some point, because things went in a different way, I started leading the project

05:54.160 --> 05:55.640
basically.

05:55.640 --> 05:59.200
One nice thing about MetalDB is this dichotomy.

05:59.200 --> 06:06.680
It's used a lot in home clusters, around Raspberry Pi's, but it's also used by enterprise users

06:06.680 --> 06:11.120
in very complex scenarios.

06:11.120 --> 06:19.000
So the first and most disappointing thing about MetalDB, but please don't run away,

06:19.000 --> 06:25.400
don't leave the room, is the fact that MetalDB is not a network load balancer.

06:25.400 --> 06:28.160
This was disappointing when I started digging into it.

06:28.160 --> 06:33.880
But let's keep in mind those two issues that we want to solve and see how MetalDB tries

06:33.880 --> 06:39.240
to solve them in a, I think in a very elegant way, interacting with an existing network

06:39.240 --> 06:41.240
infrastructure.

06:41.240 --> 06:45.360
And the first part is the address, advertisement, assignment, sorry.

06:45.360 --> 06:47.320
And this part is probably the most boring one.

06:47.320 --> 06:53.000
We have a Kubernetes controller listens for services in the need of an IP and tries to

06:53.000 --> 06:54.240
assign them the IP.

06:54.240 --> 06:59.000
But what IPs are we talking about?

06:59.000 --> 07:00.600
Here we are not on the cloud provider.

07:00.600 --> 07:03.440
We don't have control over the IPs.

07:03.440 --> 07:10.080
And so in this case, the cluster administrator is the entity in charge of providing a pool

07:10.080 --> 07:15.800
of IPs to MetalDB so it can use them and give them to the various services.

07:15.800 --> 07:24.720
And those can be ranges, can be full-siders, we can add multiple ranges and IPv4 and IPv6.

07:24.720 --> 07:35.360
And then there is the probably most networky part of MetalDB, which is address advertisement.

07:35.360 --> 07:42.960
So we have the address and we need to find a way to attract the traffic to the nodes

07:42.960 --> 07:47.240
so that the CNI itself can do its part.

07:47.240 --> 07:49.560
And MetalDB works in two modes.

07:49.560 --> 07:54.960
L2, I briefly described how it works, is more or less like a KIPA-LIV.

07:54.960 --> 08:01.400
It collects one node as the leader of the IP, it replies to R-ProQuest, sends out gratuitous

08:01.400 --> 08:04.720
RPs when a failover happens.

08:04.720 --> 08:08.240
And you can only have one node as the entry point for the service.

08:08.240 --> 08:14.640
And then there is the part that I'll dig more into today, which is the BGP mode.

08:14.640 --> 08:24.800
And BGP will leverage the interaction with BGP-enabled routers in order to advertise them.

08:24.800 --> 08:32.880
So this is taken from the BGP RFC, the primary function of a BGP-speaking system is to exchange

08:32.880 --> 08:35.680
network reachability information with other BGP systems.

08:35.680 --> 08:38.520
So this is exactly what we need.

08:38.520 --> 08:43.240
We need to find a way to say, hey, if you want to reach the service IP which I'm assigning

08:43.240 --> 08:48.640
to my load balancer service, then you should go through this set of nodes because then

08:48.640 --> 08:51.920
again the CNI can do its part.

08:51.920 --> 08:56.360
And this is exactly how MetalDB works.

08:56.360 --> 09:03.640
Each node acts as a mini router establishing BGP sessions with externally configured routers.

09:03.640 --> 09:11.160
You need to make MetalDB aware of the existence of those routers with some configuration.

09:11.160 --> 09:16.600
And then when we create a service, it will start advertising the routes to the router

09:16.600 --> 09:23.480
so that the, and this is a bit bigger for those in the back, so that the router knows

09:23.480 --> 09:32.200
that in order to reach this virtual IP, it needs to route the traffic towards this node.

09:32.200 --> 09:38.360
So again, the traffic gets to the node, thus the end does the rest.

09:38.360 --> 09:47.320
And in this case, compared to the L2 mode, we get fully load balancing through ECMP routes.

09:47.320 --> 09:49.880
And the scenarios can be more complex.

09:49.880 --> 09:53.320
We can have multiple routers.

09:53.320 --> 09:59.360
We have some knobs to drive which routers, which peers we want to advertise a given service.

09:59.360 --> 10:05.160
We have other knobs to say, hey, I want this BGP session to be established only from this

10:05.160 --> 10:10.440
set of nodes because maybe they belong to different, to different RAs.

10:10.440 --> 10:16.600
And of course we can have cascading routers and this is like regular BGP.

10:16.600 --> 10:20.200
The configuration looks something like this.

10:20.200 --> 10:28.560
We still need the set of IPs to get to MetalDB in order to have our services assigned to,

10:28.560 --> 10:31.280
to have them assigned to our services.

10:31.280 --> 10:37.640
And then we have this other item which tries to describe the properties of the BGP session

10:37.640 --> 10:42.240
that needs to be established with the different routers.

10:42.240 --> 10:44.680
And we have a few features here.

10:44.680 --> 10:49.560
We have BFD support.

10:49.560 --> 10:50.880
We have node selectors.

10:50.880 --> 10:55.960
We support IBGP, NDBGP, single and multi-hop.

10:55.960 --> 11:03.120
Not even if we are acting as a mini router because MetalDB's purpose is only to advertise

11:03.120 --> 11:04.120
routes outside.

11:04.120 --> 11:10.680
We refuse any incoming route to do node because again, that is not MetalDB's purpose.

11:10.680 --> 11:14.760
How it works under the hood.

11:14.760 --> 11:16.440
The architecture is pretty simple.

11:16.440 --> 11:23.520
We have one single pod that is the controller that is in charge of the IPAM part of MetalDB.

11:23.520 --> 11:30.400
So it's in charge of reconciliation the services and the configuration with those services,

11:30.400 --> 11:33.000
with those IPs that needs to be assigned to the service.

11:33.000 --> 11:36.880
And again, there is not too much network in this side.

11:36.880 --> 11:38.720
The other part is the speaker.

11:38.720 --> 11:43.960
And the speaker is the part that is in charge of handling the networking side.

11:43.960 --> 11:45.400
We run it as a demo set.

11:45.400 --> 11:47.280
It runs on each node.

11:47.280 --> 11:48.680
It runs on the host network.

11:48.680 --> 11:53.920
So it is in control of the configuration of the host networking.

11:53.920 --> 11:56.760
And it handles the announcement part.

11:56.760 --> 12:02.160
So both the L2 and the BGP1.

12:02.160 --> 12:07.200
And now I will talk a bit about the history.

12:07.200 --> 12:14.280
Originally, the BGP part was done using a native Go implementation that was implementing

12:14.280 --> 12:21.160
a subset of the BGP protocol.

12:21.160 --> 12:27.120
This was before I started to maintain and to contribute the project.

12:27.120 --> 12:34.680
And at some point, there were a bunch of features that were being asked by the users.

12:34.680 --> 12:41.440
And the people that were maintaining the project at the time had this discussion about should

12:41.440 --> 12:47.240
we start extending the Go BGP implementation to cover all these scenarios that the users

12:47.240 --> 12:49.480
are asking.

12:49.480 --> 12:51.840
And the result was we shouldn't.

12:51.840 --> 12:54.200
We should not reinvent the wheel.

12:54.200 --> 12:58.580
We should leverage something that is already doing that for us.

12:58.580 --> 13:00.560
And that thing was FRR.

13:00.560 --> 13:09.400
FRR is Internet Routing Protocol Suite for Linux that is well established.

13:09.400 --> 13:14.840
And it implements all the stuff that we were looking for to add to Metal.ib.

13:14.840 --> 13:22.600
So as the result of this discussion, Metal.ib was extended with an alternative mode that

13:22.600 --> 13:32.280
is turned on by a configuration flag where all the BGP part is handled by FRR.

13:32.280 --> 13:41.640
And after our configuration looks like this, we describe our autonomous system number.

13:41.640 --> 13:47.680
We describe the properties of the neighbors.

13:47.680 --> 13:54.060
And then we describe the prefixes that we want to advertise around.

13:54.060 --> 13:56.520
And we can do more.

13:56.520 --> 14:02.400
We can set rules for each neighbor.

14:02.400 --> 14:07.760
And associated to those rules, we can say, hey, if the IP belongs to this set of IPs,

14:07.760 --> 14:09.960
then we can add communities.

14:09.960 --> 14:12.320
We can have local preferences.

14:12.320 --> 14:18.160
We can block this IP for this particular neighbor.

14:18.160 --> 14:22.200
And this is all the stuff that we had to do.

14:22.200 --> 14:23.560
We required in Metal.ib.

14:23.560 --> 14:29.800
So now in BGP mode, the way Metal.ib works is that we are running multiple containers

14:29.800 --> 14:33.740
inside the speaker pod as we had before.

14:33.740 --> 14:36.760
And one of them is running FRR.

14:36.760 --> 14:43.360
And because all the containers share the host network namespace, then what we need to do

14:43.360 --> 14:51.200
now is to apply some configuration, to apply the proper configuration to FRR so that it

14:51.200 --> 14:54.720
can do its part.

14:54.720 --> 14:58.480
So this is what we have.

14:58.480 --> 15:04.780
On one side, we have all these continuously evolving configurations that we received from

15:04.780 --> 15:06.480
the Kubernetes API.

15:06.480 --> 15:08.980
We have the services that come and go.

15:08.980 --> 15:11.800
We have the new routers that we want to configure.

15:11.800 --> 15:19.480
We have this BGP advertisement that allows us to set some properties on the advertisement

15:19.480 --> 15:22.480
itself on one side.

15:22.480 --> 15:30.160
What we want to achieve is the corresponding FRR configuration so that FRR can do its part.

15:30.160 --> 15:37.240
And this is done by some code that renders all this changing stuff and continuously reconciles

15:37.240 --> 15:39.920
it in some sort of internal data.

15:39.920 --> 15:45.880
We pass it through the Go template engine.

15:45.880 --> 15:53.480
We then generate the configuration for FRR that we want, but we are not finished yet

15:53.480 --> 15:57.840
because then we need to instruct FRR to apply the new changes.

15:57.840 --> 16:09.160
And luckily, what we can leverage is this Python script, FRR.reload, that does a lot

16:09.160 --> 16:10.160
of stuff for us.

16:10.160 --> 16:16.720
So this comes together with FRR and calculates all the delta between the current configuration

16:16.720 --> 16:21.960
and the configuration that we want to achieve and applies all those commands to FRR.

16:21.960 --> 16:29.600
And so again, we get to the right configuration corresponding to the Kubernetes configuration

16:29.600 --> 16:34.160
assuming that we are doing our part here.

16:34.160 --> 16:44.600
So I don't generally add memes to talks, but I thought that this was particularly relevant

16:44.600 --> 16:51.520
because leveraging FRR allowed us to focus on the business logic on what our users were

16:51.520 --> 16:59.320
asking to us without having to worry too much about the protocol implementations.

16:59.320 --> 17:01.700
And that helped us a lot.

17:01.700 --> 17:07.360
In order to add new features to the project, we added the BFD support seamlessly.

17:07.360 --> 17:09.960
We added the VRF support.

17:09.960 --> 17:19.720
IPv6 and dual stack were something that were missing in Metal-L-B and they came out naturally.

17:19.720 --> 17:27.160
But this doesn't mean that we don't have challenges.

17:27.160 --> 17:33.360
Basically the biggest challenge is the fact that on one side we had an existing API that

17:33.360 --> 17:39.680
was already there and was fitting well with the Metal-L-B use case where the focus was

17:39.680 --> 17:40.680
the service.

17:40.680 --> 17:45.240
So I want to apply all these services to all this logic to the service.

17:45.240 --> 17:50.960
On the other hand, FRR thinks in a slightly different way.

17:50.960 --> 17:57.920
So there is a good amount of logic in doing this API contortionism in order to have one

17:57.920 --> 17:59.560
API to fit the other.

17:59.560 --> 18:06.320
And again, that's because we wanted to be backward compatible.

18:06.320 --> 18:13.040
And probably the second most interesting challenge was the fact that Metal-L-B was known to be

18:13.040 --> 18:15.280
super stable.

18:15.280 --> 18:22.800
We came and replaced the core mechanism about the interaction with the routers and we wanted

18:22.800 --> 18:26.200
to make sure that we weren't breaking too much.

18:26.200 --> 18:29.600
On top of that, we started also to add new features.

18:29.600 --> 18:34.680
And again, we were changing a lot in very few times.

18:34.680 --> 18:42.720
And at the time there wasn't a proper CI mechanism that was covering all the cases.

18:42.720 --> 18:48.400
So that was quite a challenge because again, Metal-L-B users were used to having something

18:48.400 --> 18:54.840
that was stable and we were promising that we were replacing the implementation in a

18:54.840 --> 18:56.600
compatible way.

18:56.600 --> 19:02.720
So the problem was we want to be able to test something like this where we have multiple

19:02.720 --> 19:03.760
servers.

19:03.760 --> 19:05.400
You have one router.

19:05.400 --> 19:16.640
You might have multi-hops and you have all the configuration knobs that Metal-L-B has.

19:16.640 --> 19:18.500
And then you have node selectors.

19:18.500 --> 19:20.960
You have the BFT that we're adding.

19:20.960 --> 19:23.600
We have communities and a lot of stuff.

19:23.600 --> 19:34.960
And it was like this was something that wasn't keeping me sleepy.

19:34.960 --> 19:42.440
So we started thinking how do we set up a proper CI for this.

19:42.440 --> 19:46.060
And we use kind.

19:46.060 --> 19:48.240
Does anyone know what kind is?

19:48.240 --> 19:49.240
Of course.

19:49.240 --> 19:50.240
Okay.

19:50.240 --> 19:56.920
So basically in kind we have, with kind we are able, and with kind and with FRR we are

19:56.920 --> 19:59.840
able to replace something like this with something like this.

19:59.840 --> 20:07.120
So each node is running inside the container, the external router is now replaced by FRR.

20:07.120 --> 20:13.720
So we use FRR both inside Metal-L-B to do the implementation but also outside to validate

20:13.720 --> 20:15.280
that everything is working.

20:15.280 --> 20:23.520
And now we have even control on this kind of network because this is the Docker bridge.

20:23.520 --> 20:33.840
So these allowed us to add the test suite where we apply the Metal-L-B configuration

20:33.840 --> 20:39.560
and the FRR configuration corresponding to that Metal-L-B configuration.

20:39.560 --> 20:45.800
And we can inspect the external router so that all the advertisement, all the sessions

20:45.800 --> 20:49.360
are up and all that kind of stuff.

20:49.360 --> 20:53.160
And obviously we can even access the service from outside.

20:53.160 --> 20:57.200
And we can test more complex scenarios.

20:57.200 --> 21:02.840
We can test multi-hops, we can test IPv4, IPv6, dual stack and so on.

21:02.840 --> 21:07.060
And most importantly, this can run on our laptop.

21:07.060 --> 21:13.980
So even the development phase is now easier to move forward.

21:13.980 --> 21:23.760
And also we are able to run this in the AppSim CI as under GitHub actions.

21:23.760 --> 21:35.400
So wrapping up, FRR made us able to focus on the business logic of our applications

21:35.400 --> 21:40.760
instead of having to re-implement all those protocols.

21:40.760 --> 21:47.320
To the point that sometimes writing this test suite takes more time than implementing the

21:47.320 --> 21:51.880
feature itself.

21:51.880 --> 21:58.000
And I think this is a nice example of the interaction between two different projects.

21:58.000 --> 22:01.840
These are some resources.

22:01.840 --> 22:04.800
The first section is about Metal-L-B.

22:04.800 --> 22:09.760
We are trying to be active as much as we can on the Kubernetes luck channel.

22:09.760 --> 22:12.480
We always monitor upstream issues.

22:12.480 --> 22:17.040
If you want to ask questions, that's the right place to do.

22:17.040 --> 22:21.000
Again I'm responding on a daily basis.

22:21.000 --> 22:27.160
And I think that can be said also for the FRR community.

22:27.160 --> 22:31.000
They are super active in their luck channel.

22:31.000 --> 22:37.360
And with that I want to thank them because they made our life super easy.

22:37.360 --> 22:41.640
And I'm done for today.

22:41.640 --> 22:45.560
Any questions?

22:45.560 --> 22:47.320
Hello.

22:47.320 --> 23:06.520
Is it possible in a few words to explain when I should use ARP or PGP?

23:06.520 --> 23:14.680
Like if you are working locally, if you have a home lab and you don't have a BGP enabled

23:14.680 --> 23:24.520
router, that means that the only alternative that you have is L2 and ARP.

23:24.520 --> 23:33.360
Otherwise the BGP mode requires routers but also gives you more power because you have

23:33.360 --> 23:36.720
proper load balancing across all the different nodes.

23:36.720 --> 23:40.560
One more question here.

23:40.560 --> 23:46.920
Hi, thanks for your call.

23:46.920 --> 23:51.080
We have Metal-L-B running on our worker nodes.

23:51.080 --> 23:57.560
And our worker nodes are talking BGP to the host, routing to the host.

23:57.560 --> 24:03.160
So FRR is running on the Metal machines as well.

24:03.160 --> 24:09.360
So can we configure Metal-L-B with FRR routing without conflicting ports?

24:09.360 --> 24:10.360
Because we already have...

24:10.360 --> 24:11.360
Yeah, correct.

24:11.360 --> 24:12.560
You can't.

24:12.560 --> 24:18.080
And that's something that we are trying to think to solve.

24:18.080 --> 24:22.600
One of the ideas that I have in mind, but like this is super early stages.

24:22.600 --> 24:31.800
Again, having Metal-L-B able to use an existing FRR instance, but that comes with challenges

24:31.800 --> 24:37.840
because it expects to reconfigure all the configuration and it might have conflicts

24:37.840 --> 24:40.400
with what you have currently.

24:40.400 --> 24:45.560
So the go-to solution now is to have different ports, I guess.

24:45.560 --> 24:48.840
Okay, thank you.

24:48.840 --> 24:54.560
I have a user having a namespace where he uses a lot of IP addresses without...

24:54.560 --> 24:55.560
A lot?

24:55.560 --> 24:56.560
Sorry?

24:56.560 --> 25:02.640
I have some users that are allocated separate namespaces, Kubernetes, and some of them are

25:02.640 --> 25:05.200
using a lot of IP addresses.

25:05.200 --> 25:08.720
Can you limit the IP addresses that Metal...

25:08.720 --> 25:09.720
For a namespace?

25:09.720 --> 25:10.720
Yeah.

25:10.720 --> 25:11.720
Yeah.

25:11.720 --> 25:16.560
So one of the things that we added over the past year are...

25:16.560 --> 25:21.240
And I think it's gonna go out in the next release.

25:21.240 --> 25:24.880
It's merged domain, but it's not released yet.

25:24.880 --> 25:29.280
It's namespace selectors and service selectors in the IP address pool.

25:29.280 --> 25:32.080
So by doing that, it will solve your...

25:32.080 --> 25:37.000
That's meant to solve the multi-tenant problem where you want to have a given set of pools

25:37.000 --> 25:40.400
associated with a given tenant and not more.

25:40.400 --> 25:48.280
So we are trying to remove the control from the service itself, as it was before, where

25:48.280 --> 25:53.840
you had to set the fixed IP or the pool that you were pulling inside the service and having

25:53.840 --> 25:58.640
it as part of the cluster configuration so that the cluster administrator can do that.

25:58.640 --> 26:04.800
And there won't be cases where applications are abusing of the cluster.

26:04.800 --> 26:10.320
One more here.

26:10.320 --> 26:11.320
Thanks for your talk, also.

26:11.320 --> 26:17.880
I have a question about the possibility to have Metal-LB coexisting with Calico, because

26:17.880 --> 26:21.360
I think in that case, we have two BGP speakers on the same node.

26:21.360 --> 26:24.360
And I had issues with that in the past.

26:24.360 --> 26:25.640
Yeah.

26:25.640 --> 26:31.640
I'm not a Calico expert, so I know that the existing configuration suggested to disable

26:31.640 --> 26:36.760
the Metal-LB BGP part and let Calico advertise the services.

26:36.760 --> 26:41.800
But that's all I know, and I haven't dug into that thing.

26:41.800 --> 26:47.240
One more.

26:47.240 --> 26:52.040
Just to follow up on the last question, is it possible to run Metal-LB on a different

26:52.040 --> 26:55.080
port for BGP?

26:55.080 --> 26:57.280
Yeah.

26:57.280 --> 27:06.920
Actually, it should be, but I don't remember, because it has to be a process parameter.

27:06.920 --> 27:08.840
I should check.

27:08.840 --> 27:11.840
If you catch me out later, I will tell you.

27:11.840 --> 27:18.840
Because for the neighbor part, yes, it's like that is clear, for the Metal-LB part, I don't

27:18.840 --> 27:20.840
remember.

27:20.840 --> 27:22.720
Okay.

27:22.720 --> 27:43.400
Thank you.
