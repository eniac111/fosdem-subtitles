1
0:00:00.000 --> 0:00:10.140
So great to see you all. So many people here. That's awesome. I'm working with my talk.

2
0:00:10.140 --> 0:00:16.040
It's called Decentralized Search with IPFS. Maybe first of all, like a quick pulse, how

3
0:00:16.040 --> 0:00:22.420
many of you have used IPFS? Please raise your hand. Okay. Okay, nice. And how many of you

4
0:00:22.420 --> 0:00:29.340
have heard about IPFS? Okay, all of you. Okay, cool. So you know all about it already. Yeah.

5
0:00:29.340 --> 0:00:34.180
So the talk is called How Does It Work Under the Hood? So we will dive in pretty deep at

6
0:00:34.180 --> 0:00:40.120
some points of the talk. But yeah, first things first. My name is Dennis. I'm a research engineer

7
0:00:40.120 --> 0:00:44.800
at protocol labs. I'm working in a team called Probe Lab, and we're doing network measurements

8
0:00:44.800 --> 0:00:49.720
and protocol optimizations there. I'm also an industrial PhD candidate at the University

9
0:00:49.720 --> 0:00:54.600
of GÃ¶tting, and you can reach me on all these handles on the internet. So if you have any

10
0:00:54.600 --> 0:00:58.920
questions, reach out and let me know your questions, or just hear the venue after the

11
0:00:58.920 --> 0:01:06.360
talk. So what's in for you today? First of all, just in words and numbers, what is IPFS?

12
0:01:06.360 --> 0:01:11.200
Just general overview. And at that point, after we covered that, I would just assume

13
0:01:11.200 --> 0:01:15.640
we have installed a local IPFS node on your computer, and I will walk you through the

14
0:01:15.640 --> 0:01:22.120
different commands from, yeah, we're initializing some of the repository, we're publishing content

15
0:01:22.120 --> 0:01:25.420
to the network, and so on, and we'll explain what happens in each of these steps so that

16
0:01:25.420 --> 0:01:31.080
all of you hopefully get a glimpse on what's going on under the hood. So we are importing

17
0:01:31.080 --> 0:01:35.160
content, we connect to the network, I explain content routing, this is the very technical

18
0:01:35.160 --> 0:01:43.680
part, and at the end some call-offs, basically. So what is IPFS? IPFS stands for the Interplanetary

19
0:01:43.680 --> 0:01:48.680
File System, and generally it's a decentralized storage and delivery network which builds

20
0:01:48.680 --> 0:01:53.240
on peer-to-peer networking and content-based addressing. So peer-to-peer networking, if

21
0:01:53.240 --> 0:01:57.640
you have followed along, or if you've been here earlier today, Max gave a great talk

22
0:01:57.640 --> 0:02:03.480
about LIP-to-P, about browser connectivity in general in peer-to-peer networks, and IPFS

23
0:02:03.480 --> 0:02:09.600
is one of the main users of the LIP-to-P library and builds on top of that. And most importantly,

24
0:02:09.600 --> 0:02:15.120
it's very tiny at the bottom, IPFS is not a blockchain, so also a common misconception,

25
0:02:15.120 --> 0:02:22.600
I'd like to emphasize that. In numbers, given these numbers are from mid-last year, so probably

26
0:02:22.600 --> 0:02:27.000
in need of an update, but its operation has since 2015, that hasn't changed. Numbers of

27
0:02:27.000 --> 0:02:33.800
requests exceed a billion in a week, and hundreds of terabytes of traffic that we see, and tens

28
0:02:33.800 --> 0:02:38.920
of millions of active users also weekly, but a disclaimer, this is just from our vantage

29
0:02:38.920 --> 0:02:43.680
point, in a decentralized network, no one has a complete view of what's going on, so

30
0:02:43.680 --> 0:02:51.880
these numbers could be much higher or just different in general. On ecosystem.ipfs.tech,

31
0:02:51.880 --> 0:02:56.760
you can find some companies that built on top of this tech, and it's all in these different

32
0:02:56.760 --> 0:03:05.520
areas, social media, and so on and so forth, so worth looking up. What's the value proposition

33
0:03:05.520 --> 0:03:11.400
of IPFS? The most important thing that it does, it decouples the content from its host,

34
0:03:11.400 --> 0:03:18.080
and it does this through a concept that's called content addressing. And content addresses

35
0:03:18.080 --> 0:03:24.360
are permanent verifiable links, and this allows you to request content with this, or request

36
0:03:24.360 --> 0:03:28.120
data with the content address, and anyone can serve you the content, and just from the

37
0:03:28.120 --> 0:03:33.800
address that you ask with, you can identify and verify that the content you got served

38
0:03:33.800 --> 0:03:38.960
is actually the one that you requested, and you are not dependent on the authenticity

39
0:03:38.960 --> 0:03:44.560
of the host, as it's the case with HTTP. Because it's a decentralized network, it's also censorship

40
0:03:44.560 --> 0:03:48.880
resistant, and I like to put here that it alleviates backbone addiction, so what do I

41
0:03:48.880 --> 0:03:54.000
mean with that? Let's imagine all of you, or all of us, wanted to download a 100 megabyte

42
0:03:54.000 --> 0:03:58.320
YouTube video here in this room. We would put pressure, so if we were 100 people, we

43
0:03:58.320 --> 0:04:03.480
would put pressure of about 10 gigabytes onto the backbone to just download the video into

44
0:04:03.480 --> 0:04:07.760
this room. Wouldn't it be better if we could just download it once and distribute it across

45
0:04:07.760 --> 0:04:12.920
each other or download different parts and be a little bit more clever about that? In

46
0:04:12.920 --> 0:04:19.360
a similar vein, if we were working on a Google Doc here inside this room, why does it stop

47
0:04:19.360 --> 0:04:25.280
working if we don't have internet connection anymore? It actually should work, it's actually

48
0:04:25.280 --> 0:04:31.720
ridiculous. Also, for us into the same category, this partition tolerance for emerging networks

49
0:04:31.720 --> 0:04:40.160
could also become very important, or if you're just in Apache coffee shop Wi-Fi. All right,

50
0:04:40.160 --> 0:04:46.600
so how can you install IPFS? I put down three different ways here. IPFS in general is not

51
0:04:46.600 --> 0:04:53.120
you don't install IPFS, IPFS is more specification, and there are different implementations of

52
0:04:53.120 --> 0:04:58.000
this specification. The most common one is Kubo, which was formerly known as Go IPFS,

53
0:04:58.000 --> 0:05:02.280
so it's a Go implementation. There's a new one called Iroh, which is in Rust, and I think

54
0:05:02.280 --> 0:05:07.920
the newest one is in JavaScript called Helia. I think that's also the newest kit on the

55
0:05:07.920 --> 0:05:15.480
block. I will talk about Kubo here. The easiest thing to get started is just download IPFS

56
0:05:15.480 --> 0:05:20.120
desktop, which is an electron app that bundles an IPFS node, gives you a nice UI, and you

57
0:05:20.120 --> 0:05:25.440
can already interact and request CIDs from the network and so on. Then there's the IPFS

58
0:05:25.440 --> 0:05:30.000
companion, which is a browser extension that you can install to Firefox or your browser

59
0:05:30.000 --> 0:05:35.880
of choice. Or you directly use Brave or Opera, which comes in with a bundled IPFS node already.

60
0:05:35.880 --> 0:05:41.680
If you enter an IPFS colon slash slash and a CID, it will resolve the content through

61
0:05:41.680 --> 0:05:46.160
the IPFS network. As I said in the beginning, in this talk we will focus on the command

62
0:05:46.160 --> 0:05:51.240
line because we're in a developer conference. I will also assume that we run Kubo, which

63
0:05:51.240 --> 0:05:59.040
is the reference implementation basically. Now we have downloaded Kubo from github.com

64
0:05:59.040 --> 0:06:04.320
slash ipfs slash Kubo, and we want to import some content. We just want to get started.

65
0:06:04.320 --> 0:06:09.280
We downloaded it, and now we have this ipfs command on our machine. The first thing that

66
0:06:09.280 --> 0:06:16.480
we do is run ipfs init. What this does is it generates a public provide key pair per

67
0:06:16.480 --> 0:06:23.640
default in ed25519. It spits out this random string of characters, which is basically your

68
0:06:23.640 --> 0:06:31.400
public key. Formerly it was just the hash of your public key, but now it's just encoded

69
0:06:31.400 --> 0:06:36.840
your public key in here. This is your peer identity, which will become important later

70
0:06:36.840 --> 0:06:42.800
on. Then it also initializes your ipfs repository. It's a default in your home directory under

71
0:06:42.800 --> 0:06:47.520
.ipfs. This is the location where it stores all the files. If you interact with the IPFS

72
0:06:47.520 --> 0:06:52.920
network and request files, it stores it in this directory in a specific format similar

73
0:06:52.920 --> 0:06:59.260
to Git, how Git objects store basically. Importantly, I will point this out a couple

74
0:06:59.260 --> 0:07:02.680
of times. This is just a local operation. We haven't interacted with the network at

75
0:07:02.680 --> 0:07:08.720
all yet. Now we are ready to go. I have a file I want

76
0:07:08.720 --> 0:07:16.680
to add. What I do is I run ipfs add and then my file name. In this case, ipfs gives you

77
0:07:16.680 --> 0:07:21.360
a progress bar, or Kubo gives you a progress bar, and spits out again a random string of

78
0:07:21.360 --> 0:07:26.800
characters, which is the content identifier, the CID, which is the most fundamental ingredient

79
0:07:26.800 --> 0:07:34.680
here. This is the content from its host. As a mental model, you can think about the CID

80
0:07:34.680 --> 0:07:41.160
as a hash with some metadata. It's self-describing, so the metadata is this description part.

81
0:07:41.160 --> 0:07:47.000
You can see the ingredients at the bottom. It's just an encoded version of some information,

82
0:07:47.000 --> 0:07:53.080
like a CID version. We have version 0 and 1 and some other information that I won't

83
0:07:53.080 --> 0:07:58.440
go into right now. Then it's self-certifying. This is the point where if you request some

84
0:07:58.440 --> 0:08:05.680
data from the network, you certify the data that you could serve with the CID itself and

85
0:08:05.680 --> 0:08:14.800
not with the host that served you the content, just reiterating this. It's an immutable identifier.

86
0:08:14.800 --> 0:08:18.600
All this structure, like the CID structure at the bottom and so on, is governed by a

87
0:08:18.600 --> 0:08:26.800
project that's called Multiformets and is also one of protocol lab's projects here.

88
0:08:26.800 --> 0:08:32.040
The talk is called What Happens Under the Hood? What actually happened here? IPFS saw

89
0:08:32.040 --> 0:08:37.880
the file, which is just this white box here, a stream of bytes, and IPFS chunked it up.

90
0:08:37.880 --> 0:08:43.640
It's in different pieces, which is a common technique in networking, actually. This gives

91
0:08:43.640 --> 0:08:49.680
us some nice properties. It allows us to do piecewise transfers, so we can request blocks

92
0:08:49.680 --> 0:08:56.440
from different hosts, actually. It allows for deduplication. If we have two blocks that

93
0:08:56.440 --> 0:09:04.680
are basically the same bytes, we can deduplicate that and save some storage space underneath.

94
0:09:04.680 --> 0:09:09.760
Also if the file was a video file, we also allow for random access so we could start

95
0:09:09.760 --> 0:09:18.160
in the middle of a video and don't need to stream all the previous bytes at all.

96
0:09:18.160 --> 0:09:23.400
After we have chunked that up, what we do now, or what IPFS does now, is we need to

97
0:09:23.400 --> 0:09:29.800
put it together again. What we do here is we hash each individual chunk. Each chunk

98
0:09:29.800 --> 0:09:37.440
gets its own CID, its own content identifier. Then the combination of each CID again gets

99
0:09:37.440 --> 0:09:44.240
another CID, and we do this for both pairs at the bottom. Then the resulting common CIDs

100
0:09:44.240 --> 0:09:50.760
again will be put together yet again to generate the root CID. That's how we call it. This

101
0:09:50.760 --> 0:09:56.120
is actually the CID that you see in the command line up there. We took the chunks, put the

102
0:09:56.120 --> 0:10:02.080
identifiers together to arrive at the final CID at the top. This data structure is actually

103
0:10:02.080 --> 0:10:08.000
called a Merkle tree, but in IPFS land, it's actually a Merkle DAG because in Merkle trees,

104
0:10:08.000 --> 0:10:13.160
your nodes are not allowed to have common parents. A DAG means here a directed acyclic

105
0:10:13.160 --> 0:10:21.320
graph. Let's imagine you didn't add a file, but a directory. How do you encode the directory

106
0:10:21.320 --> 0:10:28.680
structure and not only the bytes and so on? All these formatting and serialization, deserialization

107
0:10:28.680 --> 0:10:33.440
things are governed by yet another project. It's called IPLD, which stands for Interplanetary

108
0:10:33.440 --> 0:10:40.280
Linked Data. IPLD does also a lot of more things, but for now, this is specified in

109
0:10:40.280 --> 0:10:49.120
the scope of this project. Now we have imported the content. We have chunked it up. We've

110
0:10:49.120 --> 0:10:54.960
got the CID, but again, we haven't interacted with the network yet. People think if you

111
0:10:54.960 --> 0:10:58.840
add something to IPFS, you upload it somewhere and someone else takes care of hosting it

112
0:10:58.840 --> 0:11:04.440
for you for free, which is not the case. We added it to our local node. Now it ended up

113
0:11:04.440 --> 0:11:12.040
in this IPFS repository somewhere on our local machine. Only now we connect to the network

114
0:11:12.040 --> 0:11:17.800
and interact with it. For that, we run IPFS daemon, which is a long-running process that

115
0:11:17.800 --> 0:11:23.320
connects to nodes in the network. We see some versioning information with which Go version

116
0:11:23.320 --> 0:11:28.960
was compiled, which Kubo version we actually use. We see the addresses that the Kubo node

117
0:11:28.960 --> 0:11:34.320
listens on and also which ones are announced to the network under which network addresses

118
0:11:34.320 --> 0:11:41.680
we are reachable. It tells us that it started an API server, a web UI, and a gateway. The

119
0:11:41.680 --> 0:11:47.720
API server is just an RPC API that is used by the command line to control the IPFS node.

120
0:11:47.720 --> 0:11:52.880
The web UI is the thing that you saw previously when you saw the screenshot of the IPFS desktop.

121
0:11:52.880 --> 0:12:00.080
Your local Kubo node also serves this web UI. Then the gateway. The gateway is quite

122
0:12:00.080 --> 0:12:07.620
interesting. This bridges the HTTP world with the IPFS world. You can ask under this endpoint

123
0:12:07.620 --> 0:12:13.480
that you can see down there. If you put slash IPFS slash your CID inside the browser or

124
0:12:13.480 --> 0:12:19.320
in your SAD URL, the Kubo node will go ahead and resolve the CID in the network and serve

125
0:12:19.320 --> 0:12:25.800
it to you over HTTP. This is a bridge between both worlds. ProCollapse and Cloudflare and

126
0:12:25.800 --> 0:12:30.880
so on are actually running these gateways on the internet right now, which you can use

127
0:12:30.880 --> 0:12:38.280
just with a low barrier entry to the whole thing. Then the daemon is ready. In this process,

128
0:12:38.280 --> 0:12:42.160
it has also connected to bootstrap nodes, which are hard coded to actually get to know

129
0:12:42.160 --> 0:12:49.080
other peers in the network. You can also override it with your own bootstrap nodes.

130
0:12:49.080 --> 0:12:54.600
Now we are connected to the network. We've added our file to our own machine. Now the

131
0:12:54.600 --> 0:13:01.160
interesting or the problem or the challenge, how do we actually find content hosts for

132
0:13:01.160 --> 0:13:07.200
a given CID? I give my friend a CID. How does the node know that it needs to connect to

133
0:13:07.200 --> 0:13:12.280
me to request the content actually? I put here the solution is simple. We keep a mapping

134
0:13:12.280 --> 0:13:17.000
table. We just have the CID mapped to the actual peer. Every node has this on their

135
0:13:17.000 --> 0:13:23.960
machine. Everyone knows everything basically. As I said, the mapping table gets humongous,

136
0:13:23.960 --> 0:13:29.640
especially if we've split up those files into different chunks. I think the default chunking

137
0:13:29.640 --> 0:13:35.320
size is 256 kilobytes. We have just a lot of entries in this table, so this doesn't

138
0:13:35.320 --> 0:13:40.080
scale. The solution would be to split this table and each participating peer in this

139
0:13:40.080 --> 0:13:46.960
decentralized network holds a separate part of the table. Then we are back to square one.

140
0:13:46.960 --> 0:13:52.400
How do we know which peer holds which piece of this distributed hash table data? The solution

141
0:13:52.400 --> 0:14:01.520
here would be to use a deterministic distribution based on the Cardemna DHT. Cardemna is an

142
0:14:01.520 --> 0:14:10.160
implementation or a specific protocol for a distributed hash table. At this point, many

143
0:14:10.160 --> 0:14:15.240
talks on the internet about IPFS gloss over the DHT and how it works. When I got into

144
0:14:15.240 --> 0:14:22.280
this whole thing, I was lacking something. My experiment would be to just dive even a

145
0:14:22.280 --> 0:14:30.000
little deeper into this. I will cover a bit of Cardemna here. This is very technical,

146
0:14:30.000 --> 0:14:34.560
but at the end I will try to summarize everything so that every one of you guys gets a little

147
0:14:34.560 --> 0:14:40.480
bit out of this. This whole process is called content routing, so this resolution of a CID

148
0:14:40.480 --> 0:14:49.240
to the content host. IPFS uses an adaptation of the Cardemna DHT by using a 256-bit key

149
0:14:49.240 --> 0:14:57.120
space. We are hashing the CID and the peer ID yet again with SHA-256 to arrive in a common

150
0:14:57.120 --> 0:15:02.700
key space. The distributed hash table in IPFS is just a distributed system that maps these

151
0:15:02.700 --> 0:15:09.320
keys to values. The most important records here are provider records which map a CID

152
0:15:09.320 --> 0:15:14.960
to a peer ID. Remember the peer ID is what was generated when we initialize our node.

153
0:15:14.960 --> 0:15:21.840
And peer records which then map the peer ID to actually network addresses like IP addresses

154
0:15:21.840 --> 0:15:28.080
and ports. Looking up a host for a CID is actually a two-step process. First, we need

155
0:15:28.080 --> 0:15:32.800
to resolve the CID to a peer ID and then the peer ID to their network addresses and then

156
0:15:32.800 --> 0:15:39.280
we can connect to each other. The distributed hash table here has two key features. First,

157
0:15:39.280 --> 0:15:46.440
an X or distance metric. That means we have some notion of closeness. What this XR thing

158
0:15:46.440 --> 0:15:51.800
does, if I X or two numbers together, the resulting number or this operation satisfies

159
0:15:51.800 --> 0:15:58.400
the condition and the requirements for a metric. This means I can say a certain peer ID is

160
0:15:58.400 --> 0:16:05.320
closer to a CID than some other peer ID. In this case, peer ID X could be closer to CID

161
0:16:05.320 --> 0:16:17.280
1 than peer ID Y. This allows us to basically sort CIDs with peer IDs together. Then this

162
0:16:17.280 --> 0:16:21.240
tree-based routing mechanism here. In this bottom right diagram, I got this from the

163
0:16:21.240 --> 0:16:27.240
original paper, we have the black node. With this tree-based routing, this is super clever

164
0:16:27.240 --> 0:16:32.320
as in each bubble, so all the peers in the network can actually be considered as in a

165
0:16:32.320 --> 0:16:40.120
big try, a prefix try. If we know only one peer in each of these bubbles, we can guarantee

166
0:16:40.120 --> 0:16:46.640
that we can reach any other peer in the network with O log N lookups by asking for even closer

167
0:16:46.640 --> 0:16:55.080
peers based on this XR routing mechanism here. This was just abstractly what the distributed

168
0:16:55.080 --> 0:16:59.600
hash chain IPFS does. How does it work concretely for IPFS? We started the daemon process. What

169
0:16:59.600 --> 0:17:04.760
happened under the hood was we calculated the chart with 56 of our peer ID, which just

170
0:17:04.760 --> 0:17:10.080
gives us a long string of bits and bytes, or just bits basically in our case. We initialized

171
0:17:10.080 --> 0:17:15.620
the routing table at the bottom. This routing table consists of different buckets. Each

172
0:17:15.620 --> 0:17:24.240
bucket is filled with peers that have a common prefix to our peer ID, the hash from our peer

173
0:17:24.240 --> 0:17:31.360
ID at the top. When our node started up, we asked the bootstrap peers, hey, do you know

174
0:17:31.360 --> 0:17:38.880
anyone who is chart with 56 from peer ID starts with a one? This means we have no common prefix

175
0:17:38.880 --> 0:17:45.240
and we put them those peers in bucket zero. Then we do the same for a prefix of 00 and

176
0:17:45.240 --> 0:17:53.520
011. We go through all the list until 255 and we fill up these buckets. These buckets

177
0:17:53.520 --> 0:17:59.640
are these little blobs in these little circuits that you saw on the previous slide. Why did

178
0:17:59.640 --> 0:18:04.680
we do that? Because when we now want to retrieve contents, as I said, I handed the CID to my

179
0:18:04.680 --> 0:18:12.880
friend and my friend enters the CID in the command line with this IPFS get command. Their

180
0:18:12.880 --> 0:18:17.080
node also calculates the chart with 56 of the CID and then looks in its own routing

181
0:18:17.080 --> 0:18:23.960
table. Cs, okay, I have a prefix of two. I take one peer out of this bucket two and ask,

182
0:18:23.960 --> 0:18:29.040
yeah, locate the appropriate bucket, get a list of all peers. Then I asked all of these

183
0:18:29.040 --> 0:18:33.880
peers in the bucket, hey, do you know anyone? First of all, do you know the provider record

184
0:18:33.880 --> 0:18:40.720
already? Do you know the CID and the peer ID to that CID? If yes, we are done. If not,

185
0:18:40.720 --> 0:18:44.960
we are asking, do you know anyone closer based on this XOR metric? Then this peer yet again

186
0:18:44.960 --> 0:18:50.280
looks in its own routing table. We get closer and closer and closer with this log n property

187
0:18:50.280 --> 0:18:58.320
that I showed you previously. For publishing contents, basically the same. We calculate

188
0:18:58.320 --> 0:19:04.600
the chart with 56 of the CID, locate the appropriate bucket, get a list of all the peers from that,

189
0:19:04.600 --> 0:19:11.280
and then we start parallel queries. Instead of asking for the provider record, we ask

190
0:19:11.280 --> 0:19:19.560
for even closer peers. We terminate when the closest known peers in the query actually

191
0:19:19.560 --> 0:19:28.080
haven't replied with any peer that's closer. It hasn't replied with anyone closer to the

192
0:19:28.080 --> 0:19:33.920
CID than we already know. Then we start the provider record with the 20 closest peers

193
0:19:33.920 --> 0:19:41.040
to that CID and we do it with 20 because there's peer churn. This is a permissionless network.

194
0:19:41.040 --> 0:19:47.280
This means peers can come and go as they wish. If we only stored it with one peer, we would

195
0:19:47.280 --> 0:19:54.000
risk that the provider record is not reachable when the node comes down and in turn all content

196
0:19:54.000 --> 0:20:00.040
is not reachable. This is the very technical part of that.

197
0:20:00.040 --> 0:20:06.240
Let me summarize this. This is probably the easier way to understand all of this. First

198
0:20:06.240 --> 0:20:12.160
of all, we added the content to our node and so this is the file, enters the provider.

199
0:20:12.160 --> 0:20:16.960
The provider looks in its routing table, gets redirected to a peer that is closer to the

200
0:20:16.960 --> 0:20:24.840
CID and gets redirected until it finds the closest peer in this XR key space metric to

201
0:20:24.840 --> 0:20:29.960
the CID and then stores the provider record with that. Then off-band, the CID gets handed

202
0:20:29.960 --> 0:20:37.520
to the requester, to my friend. What I didn't say or told you yet, it's also IPFS maintains

203
0:20:37.520 --> 0:20:44.840
a long list or I don't know how many it is right now, probably 100 or so, constant connections

204
0:20:44.840 --> 0:20:50.320
to other peers and opportunistically just ask them, hey, do you know this CID or the

205
0:20:50.320 --> 0:20:57.280
provider record to the CID? If this resolves, all good, we are done. It's very unlikely

206
0:20:57.280 --> 0:21:03.480
for people to actually know a random CID. Let's assume this didn't work. This requester

207
0:21:03.480 --> 0:21:08.200
also looks in its own routing table, gets redirected, gets redirected even closer to

208
0:21:08.200 --> 0:21:19.440
the peer ID of that CID and then finds the peer that stores the provider record, fetches

209
0:21:19.440 --> 0:21:25.440
the provider record, then does again the same hops to find out the mapping from the peer

210
0:21:25.440 --> 0:21:31.520
ID to the network addresses and then we can actually connect with each other and transfer

211
0:21:31.520 --> 0:21:42.200
the content and we're done. This is the content life cycle and this is already it. Well, already

212
0:21:42.200 --> 0:21:52.640
it is quite involved actually. With that, it's already time for some callouts. Get involved.

213
0:21:52.640 --> 0:21:58.560
IPFS is an open source project. If you're into measurements and so on, we have some

214
0:21:58.560 --> 0:22:05.640
grants open at radius.space. If you want to get involved with some network measurements,

215
0:22:05.640 --> 0:22:11.440
get your applications in, all action is in public. You can follow along our work, especially

216
0:22:11.440 --> 0:22:19.200
my work of our team at this GitHub repository. We have plenty of requests for measurements

217
0:22:19.200 --> 0:22:25.000
that you can dive into and extra ideas are always welcome. In general, IPFS is I think

218
0:22:25.000 --> 0:22:33.640
a very welcoming community, at least for me. That's it. Thank you very much.

219
0:22:33.640 --> 0:22:55.600
Any questions? Is the way you describe using the DHD how all nodes in the network share

220
0:22:55.600 --> 0:23:03.280
files with each other? It's one content routing mechanism. There are multiple ones. It's a

221
0:23:03.280 --> 0:23:08.200
bit, this first thing that I said here, this opportunistic request to your immediate nodes

222
0:23:08.200 --> 0:23:13.040
is also some kind of content routing, so you're resolving the location of content. Then there

223
0:23:13.040 --> 0:23:17.640
are some new efforts for building network indexes, which are just huge nodes that store

224
0:23:17.640 --> 0:23:28.880
the mapping, centralized nodes, which federated centralized nodes, so not as bad. I think

225
0:23:28.880 --> 0:23:36.320
these are the important ones, basically. There are more ways to resolve. Also, MDNS could

226
0:23:36.320 --> 0:23:42.320
also be one part. If you're on the same network, you're broadcasting. No, that's just for... Sorry?

227
0:23:42.320 --> 0:23:45.920
We're still to the DHD and the DNS. We're using the DNS to find the location.

228
0:23:45.920 --> 0:23:52.160
For the local, yeah. Okay, true. Luckily, we have a core maintainer of IPFS here. It's

229
0:23:52.160 --> 0:24:00.600
actually not a joke, but yeah. Sorry. I see that the provider records get replicated,

230
0:24:00.600 --> 0:24:08.400
but does the content actually get replicated across the network too? No. Only if someone

231
0:24:08.400 --> 0:24:14.920
else chooses to. You're publishing the provider record, so it's public somewhere, and anyone

232
0:24:14.920 --> 0:24:21.640
could look that up and also store the record themselves. This is the idea. If content is

233
0:24:21.640 --> 0:24:29.560
popular and you care about the content staying alive in the network, it's called pin, the

234
0:24:29.560 --> 0:24:35.120
CID, and this means you're fetching the content from this other provider and store it yourself

235
0:24:35.120 --> 0:24:40.360
and become a provider yourself. Because of the CID mechanism, which is self-certifying

236
0:24:40.360 --> 0:24:45.840
and so on, other peers that request the content from you don't even need to trust you because

237
0:24:45.840 --> 0:24:53.120
the CID already encodes the trust chain here. But there's nothing that's not happening automatically

238
0:24:53.120 --> 0:24:58.360
here. But you can have multiple providers for the same company. Yeah, definitely. That's

239
0:24:58.360 --> 0:25:05.560
part of it. Another question is how does the project fit in the concept of identity and

240
0:25:05.560 --> 0:25:11.560
trust and personas into IPFS? I'm thinking metadata, reifications about the content and

241
0:25:11.560 --> 0:25:18.320
stuff like that. What do you mean exactly? For instance, just a history of the content

242
0:25:18.320 --> 0:25:26.400
and can you trust that this content is from a certain person or from a certain... I won't

243
0:25:26.400 --> 0:25:32.360
argue this, but this would probably be some mechanism on top of this content and identification.

244
0:25:32.360 --> 0:25:38.120
This is more for IPLD then or for... Perhaps. I would say, if you want to say some content

245
0:25:38.120 --> 0:25:43.920
is from some specific person, then you would work with signatures, so signing the data

246
0:25:43.920 --> 0:25:49.000
and so on, which is something you would bolt on top of IPFS, but nothing I think IPLD has

247
0:25:49.000 --> 0:26:04.240
encoded there right now. It's partly the same question about how it is entered that there

248
0:26:04.240 --> 0:26:11.720
are no collisions in the content ID. No collisions? Yes, because if you publish some other content

249
0:26:11.720 --> 0:26:18.520
with the same content ID, he said it's happening locally, the content ID generation. You could

250
0:26:18.520 --> 0:26:26.880
fake contents. Yeah, but then all these cryptographic hash functions would be broken then, which

251
0:26:26.880 --> 0:26:32.600
would be very bad. If you have a hash collision, then it actually means you have the same content.

252
0:26:32.600 --> 0:26:40.040
That is the assumption right now. We just use a Shadow 56 by default, and you can use

253
0:26:40.040 --> 0:26:44.600
also one like Blake 3, Blake 2, but if you find a collision in Shadow 56, you have bigger

254
0:26:44.600 --> 0:26:56.480
problems and IPFS not working. Exactly this. Follow on on this. How resilient is this against

255
0:26:56.480 --> 0:27:03.640
malicious actors that want to prevent me from reaching the content? It's a big question,

256
0:27:03.640 --> 0:27:12.280
but maybe something. Yes, on peer-to-peer networks, often these kind of civil attacks are an attack

257
0:27:12.280 --> 0:27:18.680
vector that is considered, which means you generate a lot of identities to populate just

258
0:27:18.680 --> 0:27:23.560
some part of the key space to block some requests from reaching the final destination and so

259
0:27:23.560 --> 0:27:36.120
on. From my experience, this is quite hard, and I haven't seen this happening. I cannot

260
0:27:36.120 --> 0:27:44.560
say that it's impossible, probably hard to tell. Max, do you want?

261
0:27:44.560 --> 0:27:57.560
True. This civil thing is just one attack vector, but this is the common one that is

262
0:27:57.560 --> 0:28:02.720
considered. There are many points in the code base where you need to think about what happens

263
0:28:02.720 --> 0:28:10.560
if this attack is going on. One thing that Cardimna does is to prefer long-running nodes,

264
0:28:10.560 --> 0:28:15.760
and the routing table. If someone immediately generates a lot of identities that they don't

265
0:28:15.760 --> 0:28:24.160
end up in your routing table and pollutes your content routing here or interferes with

266
0:28:24.160 --> 0:28:39.000
that. I'm not sure if I want to ask it, but removing content, deleting. We got GDPR, so

267
0:28:39.000 --> 0:28:46.480
is there any solution that can be done? It's hard. That's part of the thing. If you could,

268
0:28:46.480 --> 0:28:54.560
then it's not censorship resistant anymore. One solution, one alleviation maybe, is to

269
0:28:54.560 --> 0:29:04.960
have blacklists of CID that you may publish or may not to say, okay, don't replicate this

270
0:29:04.960 --> 0:29:10.240
CID and so on. If you have such a list, then it's very easy to just look it up and see

271
0:29:10.240 --> 0:29:20.320
what's inside. Deleting content is very tricky. However, I said it's permanent links. The

272
0:29:20.320 --> 0:29:25.760
links are permanent, but actually, content still churns in the IPFS network. These provider

273
0:29:25.760 --> 0:29:34.800
records that you publish into the network expire after 24 hours. If no one actually

274
0:29:34.800 --> 0:29:41.520
reprovides the content or keeps the content, the content is gone as well. A delete operation

275
0:29:41.520 --> 0:29:46.640
doesn't exist, so we just need to hope that no one will be provided anymore, which you

276
0:29:46.640 --> 0:30:01.080
could do with these denialists, for example. Who is able to write into that blacklist?

277
0:30:01.080 --> 0:30:07.600
This is just one, I don't know, to be completely honest, but this is just one, maybe Geropo

278
0:30:07.600 --> 0:30:14.040
knows. There is no blacklist in the network right now. It's some people, it's a few people

279
0:30:14.040 --> 0:30:20.240
that want that. But we have, so, sorry, earlier you said that we have gateways, and gateways

280
0:30:20.240 --> 0:30:26.680
is just a node that publicly is reachable. Those gateway, because many people say that

281
0:30:26.680 --> 0:30:32.720
they find some content illegal on IPFS, and instead of reporting to the actual node, storing

282
0:30:32.720 --> 0:30:36.480
the content on IPFS, they just report it on the gateway, because they know HTTP and they

283
0:30:36.480 --> 0:30:42.440
don't know IPFS. Our gateway has some blacklist that is somewhere, but it's not shared by

284
0:30:42.440 --> 0:30:47.160
the complete network. It's just for our gateway IPFS.io.

285
0:30:47.160 --> 0:30:52.720
CloudFare, for example, and Percolabso, are already writing these gateways, or more, anyone

286
0:30:52.720 --> 0:30:59.800
could operate a gateway. You could file a request for this, don't replicate the CID,

287
0:30:59.800 --> 0:31:04.480
it's a phishing website, for example. Then these CIDs are not served through the gateways,

288
0:31:04.480 --> 0:31:06.680
which is a common way to interact with the network right now.

289
0:31:06.680 --> 0:31:08.960
Just the gateway that follows the list. It's not automatic.

290
0:31:08.960 --> 0:31:12.120
Yeah, of course. Of course, yeah.

291
0:31:12.120 --> 0:31:22.640
Okay, we're running out of time, unless there is one more. I have a question regarding searching

292
0:31:22.640 --> 0:31:32.880
through the stored content. Is there any mechanism on how to go through or index the files that

293
0:31:32.880 --> 0:31:37.040
are there to have some sort of a search engine for that?

294
0:31:37.040 --> 0:31:45.760
There's a project called IPFS Search, and this makes use, among other things, of this

295
0:31:45.760 --> 0:31:52.160
immediate request for CIDs. It's just sitting there connecting to a lot of nodes. As I said,

296
0:31:52.160 --> 0:31:58.040
if someone requests content, you immediately ask your connected peers, and you're connected

297
0:31:58.040 --> 0:32:01.320
to a lot of peers. These IPFS Search nodes are just sitting there listening to these

298
0:32:01.320 --> 0:32:07.240
requests. They see, okay, someone wants the CID, so I go ahead and request that CID as

299
0:32:07.240 --> 0:32:13.680
well, and then index that content on myself. You can then search on this IPFS Search website

300
0:32:13.680 --> 0:32:20.000
for something with Google, and then you see CIDs popping in, and then you can request

301
0:32:20.000 --> 0:32:26.480
those CIDs from the IPFS network. This is one approach to index content.

302
0:32:26.480 --> 0:32:28.840
Okay, thank you.

303
0:32:28.840 --> 0:32:31.840
Thank you.

