WEBVTT

00:00.000 --> 00:09.520
Okay. So hello everyone.

00:10.720 --> 00:14.320
This presentation is about OpenCSD,

00:14.320 --> 00:17.360
which is a computational storage emulation platform.

00:17.360 --> 00:19.320
The reason we're emulating that,

00:19.320 --> 00:20.760
I'll get into shortly.

00:20.760 --> 00:23.440
But first, I think I owe you an explanation of

00:23.440 --> 00:26.200
computational storage and what it's actually is.

00:26.200 --> 00:28.640
Because I don't think many people are familiar with that.

00:28.640 --> 00:30.000
Even in this dev room,

00:30.000 --> 00:33.400
but I'm pretty sure most people are familiar with camera and eBPF.

00:33.400 --> 00:36.000
You can email me, there's a link to the repo.

00:36.000 --> 00:41.520
This has been a long time collaboration with my master's thesis at the food.

00:41.520 --> 00:43.560
So let's get started.

00:43.560 --> 00:45.680
I'm going to briefly explain who am I.

00:45.680 --> 00:48.600
I'm Kone Luka. My handle online is mostly Dentalian.

00:48.600 --> 00:50.920
I'm also a licensed ham radio operator,

00:50.920 --> 00:53.160
Papa Delta 3CR uniform that is.

00:53.160 --> 00:56.600
My expertise is in parallel and distributed system.

00:56.600 --> 00:58.400
I've been in academia for some while,

00:58.400 --> 01:01.160
associate degree, bachelor degree, master's degree.

01:01.160 --> 01:03.880
I've had some experiences throughout that time.

01:03.880 --> 01:06.960
So I've worked on health technology for officially impaired people.

01:06.960 --> 01:09.840
Worked on OpenStack with Cloud optimizations.

01:09.840 --> 01:13.000
I've done computational storage for my master thesis.

01:13.000 --> 01:14.440
That's what this talk is about.

01:14.440 --> 01:19.560
Currently, we're on SCADA systems for the lower two radio telescope at Ostrone.

01:19.560 --> 01:23.120
So why do we actually need computational storage?

01:23.120 --> 01:26.760
That's because we live in a data-driven society nowadays.

01:26.760 --> 01:29.400
So the world is practically exploding with data.

01:29.400 --> 01:34.640
So much so that we're expected to store 200 setabytes of data by 2050.

01:34.640 --> 01:37.240
These high data and throughput requirements

01:37.240 --> 01:39.120
pose significant challenges on

01:39.120 --> 01:42.640
storage interfaces and technologies that we are using today.

01:42.640 --> 01:47.080
So if you look at your traditional computer architecture,

01:47.080 --> 01:49.600
the one that's being used on x86,

01:49.600 --> 01:51.920
it's based on the Von Neumann architecture.

01:51.920 --> 01:54.800
Here, we basically need to move all data into

01:54.800 --> 01:58.080
main system memory before we can begin processing.

01:58.080 --> 02:00.280
So this poses memory bottlenecks and

02:00.280 --> 02:04.520
Internet interconnect bottlenecks on networks or PCI Express.

02:04.520 --> 02:09.280
It also drastically it hinders energy efficiency to an extent.

02:09.280 --> 02:13.120
So how much of a bandwidth gap are we talking here?

02:13.120 --> 02:16.440
Well, if you look at the server from 2021,

02:16.440 --> 02:19.840
say using Epic Milan with 64 SSDs,

02:19.840 --> 02:23.280
we're losing about four and a half times the amount of bandwidth that could be

02:23.280 --> 02:25.600
offered by all the SSDs in tandem but can't be

02:25.600 --> 02:28.680
utilized because we can't move it into memory that fast.

02:28.680 --> 02:31.120
So that's quite significant.

02:31.120 --> 02:33.240
What is this computational sort?

02:33.240 --> 02:34.840
How does this solve this actually?

02:34.840 --> 02:38.520
Well, we fit a computational storage device,

02:38.520 --> 02:41.520
so a flash storage device with its own CPU in memory.

02:41.520 --> 02:44.040
Now the user, the host processor,

02:44.040 --> 02:47.520
can submit small programs to this computational device,

02:47.520 --> 02:50.400
let it execute, and only the result data from

02:50.400 --> 02:54.200
this computation can then be returned over the interconnect into system memory,

02:54.200 --> 02:58.640
thereby reducing data movement and potentially improving energy efficiency.

02:58.640 --> 03:03.040
Because these lower power cores using more specialized hardware are

03:03.040 --> 03:07.760
typically more energy efficient than your general purpose x86 processor.

03:07.760 --> 03:12.000
If we then look at the state of current prototypes as of September 2022,

03:12.000 --> 03:14.400
we see three main impediments.

03:14.400 --> 03:17.600
Firstly is the API between the host and device interface.

03:17.600 --> 03:19.360
There's no standardization here.

03:19.360 --> 03:21.440
People aren't building hardware prototypes,

03:21.440 --> 03:24.160
but not so much looking at the software interfaces.

03:24.160 --> 03:27.680
We also have the problem of a file system because these flash devices,

03:27.680 --> 03:29.520
they have file systems and we want to keep

03:29.520 --> 03:31.680
that synchronized between the host and device.

03:31.680 --> 03:33.320
So how do we achieve that?

03:33.320 --> 03:37.800
We can't use cache-coherent interconnect or shared virtual memory because by the time

03:37.800 --> 03:40.760
we back round trip between the PCI Express interface,

03:40.760 --> 03:44.240
we'll have lost all the performance that we decide to gain.

03:44.240 --> 03:47.200
How do we stick to existing interfaces?

03:47.200 --> 03:49.440
People that access file systems, they read,

03:49.440 --> 03:50.840
they write, they use system calls.

03:50.840 --> 03:52.720
They are very used to this.

03:52.720 --> 03:56.920
If you would suddenly need to link a shared library to access your file system,

03:56.920 --> 03:58.480
people wouldn't be up for that.

03:58.480 --> 04:00.880
So we need some solutions here.

04:00.880 --> 04:04.280
That's what OpenCSD and FluffleFS introduce.

04:04.280 --> 04:06.680
We have a simple and intuitive system.

04:06.680 --> 04:10.320
All the dependencies and the software itself can run in user space.

04:10.320 --> 04:12.720
You don't need any kernel modules or things like this.

04:12.720 --> 04:15.080
We manage to entirely reuse a system,

04:15.080 --> 04:18.600
system calls that are available in all operating systems,

04:18.600 --> 04:21.320
most typical operating systems,

04:21.320 --> 04:23.040
freeBSD, Windows, Mac,

04:23.040 --> 04:24.560
iOS, and Linux.

04:24.560 --> 04:26.160
So I'd say that's pretty good.

04:26.160 --> 04:30.240
We do something that's never been done before in computational storage.

04:30.240 --> 04:35.160
We allow our regular user on the host to access a file concurrently

04:35.160 --> 04:37.120
while a kernel that is executing on

04:37.120 --> 04:40.080
the computational storage device is also accessing that file.

04:40.080 --> 04:42.200
This has never been done before.

04:42.200 --> 04:46.880
We managed to do this using existing open source libraries.

04:46.880 --> 04:48.560
So we've Boost, Cine infuse,

04:48.560 --> 04:50.320
UBPF, and SPDK.

04:50.320 --> 04:54.000
Some of you will be familiar with some of these.

04:54.000 --> 04:57.640
This allows any user like you to after this talk,

04:57.640 --> 04:59.560
try and experience this yourself in

04:59.560 --> 05:01.960
Canvas without buying any additional hardware.

05:01.960 --> 05:04.560
I'll get into that hardware in a second because there's

05:04.560 --> 05:08.040
some specialized hardware that if we want to have this physically in our hands,

05:08.040 --> 05:10.080
we have to do some things.

05:10.080 --> 05:12.280
If you look at the design,

05:12.280 --> 05:14.600
then we see four key components and

05:14.600 --> 05:16.840
a fifth one that I'll explain on the next slide.

05:16.840 --> 05:19.440
We're using a lock structured file system,

05:19.440 --> 05:21.680
which supports no in-place updates.

05:21.680 --> 05:24.120
So everything is appended and appended.

05:24.120 --> 05:28.560
We have a module interface where we have backends and frontends.

05:28.560 --> 05:31.920
So this allows us to experiment and try out new things.

05:31.920 --> 05:35.800
We can basically swap the backends and keep the frontend the same.

05:35.800 --> 05:40.040
We're using this new technology in Flash SSDs that's called zone namespaces.

05:40.040 --> 05:42.840
They are commercially available now,

05:42.840 --> 05:44.600
but they're pretty hard to get still,

05:44.600 --> 05:47.600
but that's going to improve in the future.

05:47.600 --> 05:51.240
The system calls that we managed to reuse,

05:51.240 --> 05:53.320
those are extended attributes.

05:53.320 --> 05:58.520
So extended attributes on any file and directory on most file systems,

05:58.520 --> 06:00.960
on the file system you are using likely now,

06:00.960 --> 06:04.960
you can set arbitrary key value pairs on these files.

06:04.960 --> 06:08.360
We can use this as a hint from the user to

06:08.360 --> 06:12.880
the file system to instruct the file system that something special needs to happen.

06:12.880 --> 06:18.400
Basically, we just reserve some keys there and assign special behavior to them.

06:18.400 --> 06:20.960
Now, let's get back to the topic of

06:20.960 --> 06:24.400
zone namespaces because I own you some explanation here.

06:24.400 --> 06:26.440
Back when we had hard drives,

06:26.440 --> 06:30.440
we could perform arbitrary reads and writes to arbitrary sectors.

06:30.440 --> 06:38.040
Sectors could be rewritten all the time without requiring any erasure beforehand.

06:38.040 --> 06:41.400
This is what is known as the traditional block interface.

06:41.400 --> 06:44.000
But there's a problem and that is that

06:44.000 --> 06:47.000
NAND flash doesn't actually support this behavior.

06:47.000 --> 06:49.800
So when you have NAND flash,

06:49.800 --> 06:56.920
your sectors are concentrated in blocks and this block needs to be linearly written.

06:56.920 --> 07:00.280
Before you can rewrite the information in a block,

07:00.280 --> 07:02.440
the block needs to be erased as a whole.

07:02.440 --> 07:05.720
So in order to accommodate Flash SSDs have to

07:05.720 --> 07:08.680
incorporate what is known as a Flash translation layer,

07:08.680 --> 07:11.280
where basically all these requests that go to

07:11.280 --> 07:15.240
the same sectors are somehow translated and put somewhere else physically,

07:15.240 --> 07:17.160
just so that the user can still use

07:17.160 --> 07:21.280
the same block interface that they have been used to from the time of hard drives.

07:21.280 --> 07:25.840
So there's this physical translation between these logical and physical blocks.

07:25.840 --> 07:29.000
When we try to synchronize the file system from the host with

07:29.000 --> 07:31.080
the device while a kernel is running,

07:31.080 --> 07:33.720
this introduces a whole lot of problems.

07:33.720 --> 07:35.280
So how do we solve this?

07:35.280 --> 07:38.360
Now, you know the answer, it's the sound namespaces.

07:38.360 --> 07:41.560
We basically present an interface that's not a block interface,

07:41.560 --> 07:45.320
and it's an interface that fits to NAND flash behavior.

07:45.320 --> 07:48.040
So when you use the sound namespaces SSD,

07:48.040 --> 07:52.520
you need as a developer of a file system or the kernel,

07:52.520 --> 07:55.600
need to linearly write each sector in the block,

07:55.600 --> 07:57.480
and you need to erase the block as a whole.

07:57.480 --> 08:03.000
So effectively, you become the manager of this SSD,

08:03.000 --> 08:07.200
the flash translation layer and the garbage collection lives on the host,

08:07.200 --> 08:10.360
and we call this whole system host managed.

08:10.360 --> 08:14.440
If we now combine this with a lock-structured file system,

08:14.440 --> 08:17.000
which also didn't have any in-place updates,

08:17.000 --> 08:20.600
and then you naturally see that this becomes a very good fit.

08:20.600 --> 08:23.360
Now, together with these two technologies,

08:23.360 --> 08:27.000
we can finally synchronize the host and the file system,

08:27.000 --> 08:32.960
and we can do that by making the file temporarily immutable while the kernel is running.

08:32.960 --> 08:38.960
And we do that using a snapshot consistency model by creating in-memory snapshots.

08:38.960 --> 08:42.960
So we were able to create a representation of the file as it was on the host,

08:42.960 --> 08:47.960
with metadata, put that to the computational storage device memory,

08:47.960 --> 08:52.240
and we can assure that all the data that is there will remain immutable

08:52.240 --> 08:54.640
during the execution of the kernel.

08:54.640 --> 08:58.440
Meanwhile, the user can actually still write to the file,

08:58.440 --> 09:01.520
and the metadata of the file on the host will differ,

09:01.520 --> 09:03.840
but that's not a problem.

09:03.840 --> 09:06.800
So this is very powerful,

09:06.800 --> 09:11.000
and it allows us to also understand kernel behavior in a way,

09:11.000 --> 09:17.160
because we can now have metadata and send it to the computational storage device that says,

09:17.160 --> 09:20.040
well, actually, if the kernel tries to do this,

09:20.040 --> 09:21.680
remember it's a user-submitted program,

09:21.680 --> 09:22.920
it might be malicious,

09:22.920 --> 09:24.800
then we want to block those actions.

09:24.800 --> 09:27.160
So we have a security interface as well.

09:27.160 --> 09:31.040
The final kick in the bucket for this design is that we want to be

09:31.040 --> 09:34.120
architecture independent and we do that through eBPF,

09:34.120 --> 09:36.920
the system that you're also using for network hooks

09:36.920 --> 09:39.640
and event hooks in the Linux kernel nowadays.

09:39.640 --> 09:46.280
With eBPF, you can define system calls and expose those in a header,

09:46.280 --> 09:48.400
and this is actually the format of how you would do that,

09:48.400 --> 09:49.880
that's a real example,

09:49.880 --> 09:52.720
and then the fender would implement that code,

09:52.720 --> 09:56.520
and you would define in a specification some behavior,

09:56.520 --> 09:59.720
but the fender doesn't have to open source their code,

09:59.720 --> 10:01.240
which in the case of Flash,

10:01.240 --> 10:06.320
SSDs and fenders is pretty important because they don't seem to be that keen on that.

10:06.320 --> 10:08.640
This way, we can still have an interface,

10:08.640 --> 10:14.720
the users can write programs once and reuse them across all fenders without any problem.

10:14.720 --> 10:19.160
The nice thing about eBPF is that this instruction set architecture,

10:19.160 --> 10:21.520
what eBPF essentially is,

10:21.520 --> 10:24.320
is easily implementable in a VM.

10:24.320 --> 10:28.680
So there's even pre-existing open source implementations of this,

10:28.680 --> 10:31.760
and that's what we're using, eBPF.

10:33.040 --> 10:39.640
Now that I've explained all the key components to OpenCSD and FluffleFS,

10:39.640 --> 10:43.120
I want to start with a little demo and show you what are

10:43.120 --> 10:45.760
some of the actual practical use cases for this.

10:45.760 --> 10:51.160
So how can we use such a computational storage system in a way that it

10:51.160 --> 10:55.480
makes sense in terms of data reduction and energy efficiency?

10:55.480 --> 10:58.840
For that, we're going to go to the example of Shannon entropy.

10:58.840 --> 11:02.440
This is heavily used by file systems who can perform

11:02.440 --> 11:08.360
background compression or by just compression programs that compress in the background.

11:08.360 --> 11:12.920
What you basically do is you try to quantify the randomness you have in a file.

11:12.920 --> 11:14.920
Typically, it's between 0 and 1,

11:14.920 --> 11:17.400
but for computers that doesn't really make sense.

11:17.400 --> 11:23.720
So we use this lock B that's over here to normalize this for bytes.

11:23.720 --> 11:27.800
Then we can say what's the distribution of bytes.

11:27.800 --> 11:32.200
So we create, because a byte has 265 different possible values,

11:32.200 --> 11:36.880
we create 265 bins and we submit a program to calculate this,

11:36.880 --> 11:43.080
it runs in the background and only the result is returned to the host operating system.

11:43.080 --> 11:45.480
Then the host operating system is free to

11:45.480 --> 11:49.040
decide whether or not this file should be compressed or not.

11:49.040 --> 11:53.640
So how does such a kernel look like?

11:53.640 --> 11:57.000
The kernel that you actually submit to the computational storage device,

11:57.000 --> 12:00.400
or you can just write them in C and compile them with Clang.

12:00.400 --> 12:02.840
So you write them in C and we have

12:02.840 --> 12:06.720
two individual interfaces here that we are exposing.

12:06.720 --> 12:11.280
The yellow commands, those are introduced by the system calls,

12:11.280 --> 12:14.400
the EBPF ABI that we are defining,

12:14.400 --> 12:15.960
and the purple ones,

12:15.960 --> 12:20.760
those are introduced by a file system.

12:20.760 --> 12:25.560
What that means is that using this system as is now,

12:25.560 --> 12:28.600
that it's not agnostic to the file system.

12:28.600 --> 12:32.920
So it is agnostic to the vendor and the architecture of the vendor.

12:32.920 --> 12:35.760
If it's ARM or x86, that doesn't matter.

12:35.760 --> 12:40.000
But now it's specific to the FluffleFS file system that we have written.

12:40.000 --> 12:44.240
I will address some possible solutions for this at the end.

12:44.240 --> 12:46.840
Other things we need to realize is that

12:46.840 --> 12:49.560
the EBPF stack size is typically very small.

12:49.560 --> 12:52.160
We're talking bytes here instead of kilobytes.

12:52.160 --> 12:55.600
So we need a way to address this.

12:55.600 --> 12:57.720
So what you can do is in UBPF,

12:57.720 --> 13:01.960
you can allocate a heap just as your stack,

13:01.960 --> 13:05.760
and then we have this BPF get mem info

13:05.760 --> 13:08.000
that we have defined as part of the ABI

13:08.000 --> 13:10.200
that allows you to get your heap pointer.

13:10.200 --> 13:13.000
Now, currently, you have to manually offset this,

13:13.000 --> 13:15.280
which is a bit tedious, if you will.

13:15.280 --> 13:18.320
You see that that is actually done here.

13:18.320 --> 13:22.880
To store the bins, we offset the buffer by the sector size,

13:22.880 --> 13:28.320
and then the data from the sector reads is actually stored at the top of the buffer,

13:28.320 --> 13:32.520
and the bins are stored at the offset for precisely one sector size.

13:32.520 --> 13:36.840
Now, when we go to look at the file system interface,

13:36.840 --> 13:38.880
and all the helpers and data structures

13:38.880 --> 13:41.200
and additional function calls that we introduced,

13:41.200 --> 13:44.480
we can later see that we can also make

13:44.480 --> 13:46.640
a basic implementation of malloc and free here,

13:46.640 --> 13:48.360
and then just resolve this.

13:48.360 --> 13:52.600
But for now, for this example, it's a bit tedious.

13:53.200 --> 13:55.880
Now, how do you actually trigger this?

13:55.880 --> 13:57.720
So we had the extended attributes,

13:57.720 --> 13:59.680
we had all these systems in place,

13:59.680 --> 14:01.320
but now you just have this kernel,

14:01.320 --> 14:02.400
you have compiled it,

14:02.400 --> 14:03.960
you have stored it to a file,

14:03.960 --> 14:07.600
and now you want to actually offload your computation,

14:07.600 --> 14:09.480
well, in an emulated fashion,

14:09.480 --> 14:12.320
but you want to see how you do that.

14:12.320 --> 14:16.480
So the first thing you do is you call start on the kernel object.

14:16.480 --> 14:18.840
So this is your compiled die code,

14:18.840 --> 14:21.760
and then you get the inode number.

14:21.760 --> 14:24.720
This inode number you have tend to remember,

14:24.720 --> 14:28.640
and you then open the file that you want to read upon,

14:28.640 --> 14:32.240
or write upon, but for the examples we're using read mostly.

14:32.240 --> 14:34.840
Then you use set extended attributes,

14:34.840 --> 14:37.040
you use our reserved key,

14:37.040 --> 14:40.280
and you set it to the inode number of the kernel file,

14:40.280 --> 14:43.200
and then when you actually issue read commands,

14:43.200 --> 14:45.720
the read commands will actually go to

14:45.720 --> 14:51.040
the computational storage device and they'll run on there.

14:51.040 --> 14:53.880
But when do you actually take these snapshots?

14:53.880 --> 14:56.600
The trick is as soon as you set extended attributes.

14:56.600 --> 14:58.120
This is just by design, right?

14:58.120 --> 15:00.000
Could also be once you call the first read,

15:00.000 --> 15:01.920
or once you execute the first write.

15:01.920 --> 15:03.600
But we have decided to do it at

15:03.600 --> 15:05.840
the moment that you set extended attribute.

15:05.840 --> 15:08.800
That means that if you make any changes to your kernel,

15:08.800 --> 15:11.560
once you've actually set extended attributes,

15:11.560 --> 15:13.560
then nothing changes anymore,

15:13.560 --> 15:16.080
and the same goes to the file.

15:16.240 --> 15:18.480
Now, I want to briefly explain

15:18.480 --> 15:21.160
some different types of kernel that you can have,

15:21.160 --> 15:24.200
and what the example here is mainly

15:24.200 --> 15:26.560
showing is what we call a stream kernel.

15:26.560 --> 15:28.480
So a stream kernel happens in

15:28.480 --> 15:31.080
place of the regular read or write request.

15:31.080 --> 15:33.800
So the regular read or write request doesn't happen,

15:33.800 --> 15:36.480
only the computational storage request

15:36.480 --> 15:39.880
happens on the computational storage device.

15:39.960 --> 15:41.960
With an event kernel,

15:41.960 --> 15:44.400
it's like the opposite way around.

15:44.400 --> 15:47.280
First, the regular event happens normally,

15:47.280 --> 15:49.440
and then the kernel is presented with

15:49.440 --> 15:53.160
the metadata from that request and can do additional things.

15:53.160 --> 15:55.960
This is for databases interesting.

15:55.960 --> 15:58.600
For example, say you're writing a big table,

15:58.600 --> 16:01.720
and you want to know the average or the minimum or the maximum,

16:01.720 --> 16:05.160
and you want to emit that as metadata at the end of your table write.

16:05.160 --> 16:09.800
Well, you could use an event kernel to let it write as is,

16:09.800 --> 16:12.280
then you get presented with the data,

16:12.280 --> 16:14.760
the kernel runs on the computational storage device,

16:14.760 --> 16:17.160
and you emit the metadata after,

16:17.160 --> 16:20.040
and you can store it as like an index.

16:20.040 --> 16:23.880
We have also decided to isolate

16:23.880 --> 16:26.920
the context of this computational storage offloading.

16:26.920 --> 16:29.560
So what is considered once you set

16:29.560 --> 16:32.600
that set its attributes by PID,

16:32.600 --> 16:35.000
but we also could make this by file handle,

16:35.000 --> 16:37.760
or you could even set it for the whole I-node.

16:37.760 --> 16:44.360
More so, we could use specific keys for file handle PID or I-node offloading.

16:44.360 --> 16:47.360
So it's just a matter of semantics here.

16:49.040 --> 16:52.800
Now, I have some source code in Python of

16:52.800 --> 16:55.560
this execution steps that I've just shown here,

16:55.560 --> 16:59.520
because there's a little bit of details that are left out in the brief overview.

16:59.520 --> 17:03.080
The first is that you have to stride your requests,

17:03.080 --> 17:06.000
and those have to be strided by 500 to 12K.

17:06.000 --> 17:07.200
Why is this so?

17:07.200 --> 17:10.120
Well, infuse the amount of kernel pages that are

17:10.120 --> 17:14.880
allocated to move data between the kernel and the user space is statically fixed.

17:14.880 --> 17:16.560
So if you go over this,

17:16.560 --> 17:20.040
then your request will seem fine from the user perspective,

17:20.040 --> 17:23.040
but what the kernel will do is it will chop up your requests.

17:23.040 --> 17:24.680
Why is that problematic?

17:24.680 --> 17:27.640
Well, then multiple kernels spawn,

17:27.640 --> 17:29.680
because from the context of the file system,

17:29.680 --> 17:31.520
every time it sees a read or write request,

17:31.520 --> 17:36.000
it's going to spawn this kernel and move it to the computational storage device.

17:39.200 --> 17:43.840
Then here, you can see how I set the extended attribute and get the kernel,

17:43.840 --> 17:45.520
the I-node number.

17:45.520 --> 17:49.520
What I want to show here at the bottom is that I'm getting

17:49.520 --> 17:56.160
265 integers and that's for each of the buckets of the entropy read,

17:56.160 --> 17:59.440
but I'm having a request of 512K.

17:59.440 --> 18:04.120
So that shows you the amount of data reduction that you can achieve using systems like this.

18:04.120 --> 18:06.080
Do you want to 65 integers?

18:06.080 --> 18:08.920
512K? Pretty good.

18:08.920 --> 18:10.240
Could be better though.

18:10.240 --> 18:13.360
The reason it's not better is floating point support in

18:13.360 --> 18:18.920
EBPF is limited to the fact where you need to implement fixed point match yourself.

18:18.920 --> 18:22.520
So we could do this as part of the file system helpers,

18:22.520 --> 18:25.920
but that's not done for this prototype at the moment.

18:25.920 --> 18:30.040
Now, some limitations.

18:30.040 --> 18:31.800
This was Amostites' work.

18:31.800 --> 18:34.960
This was my first time defining a file system ever.

18:34.960 --> 18:36.880
It's solely a proof of concept.

18:36.880 --> 18:38.320
There's no garbage collection,

18:38.320 --> 18:41.320
no deletion, no space reclaiming.

18:41.320 --> 18:44.040
Please don't use it.

18:44.040 --> 18:46.560
Please don't use it to store your files.

18:46.560 --> 18:52.120
Yeah. EBPF has an endianness just like any ISA would have,

18:52.120 --> 18:54.040
and there's currently no conversions.

18:54.040 --> 18:56.920
So if you happen to use something that uses different endianness,

18:56.920 --> 18:58.480
all your data will be upside down.

18:58.480 --> 19:02.080
So you have to deal with that yourself for now.

19:02.080 --> 19:06.160
But once again, we can make it part of the file system helpers to

19:06.160 --> 19:10.440
help with these data structure layout conversions and the endianess conversions.

19:10.440 --> 19:12.680
As I mentioned briefly earlier,

19:12.680 --> 19:15.560
floating point support in EBPF is practically non-existent,

19:15.560 --> 19:18.480
but we can implement fixed point match.

19:18.480 --> 19:25.040
Currently, I haven't shown any performance examples

19:25.040 --> 19:27.680
because I don't think they are that interesting.

19:27.680 --> 19:31.640
Because what's currently happening when you emulate offloading,

19:31.640 --> 19:35.680
is that it just runs on the host processor as is in EBPF.

19:35.680 --> 19:37.600
So it isn't representative of

19:37.600 --> 19:40.160
the microcontrollers that you would find on SSDs.

19:40.160 --> 19:45.400
So the runtime that it would take to execute these kernels would be much too fast.

19:45.400 --> 19:47.640
So that's something that we need to deal on,

19:47.640 --> 19:50.800
I think, because then we can more easily reason

19:50.800 --> 19:56.480
about what would be the actual performance if we would offload these applications to SSDs.

19:56.480 --> 20:00.440
Frankly, these SSDs do have very capable microcontrollers,

20:00.440 --> 20:02.320
typically even multiple processors.

20:02.320 --> 20:06.360
The reason they do that is because they need to manage the flash sensorias layer.

20:06.360 --> 20:09.760
So they are already fairly capable devices actually.

20:09.760 --> 20:14.760
Only read stream kernels have been fully implemented for this prototype as well.

20:14.760 --> 20:19.480
That's mainly because event kernel performance is problematic.

20:19.480 --> 20:22.840
Because the data from the event kernel,

20:22.840 --> 20:25.640
remember the IO request happens regularly.

20:25.640 --> 20:29.200
So all the data is moved back into the host processor,

20:29.200 --> 20:32.360
and only then is the event kernel started.

20:32.360 --> 20:36.000
But what you really need is a two-stage system where

20:36.000 --> 20:39.120
you prevent the data being moved back from the host.

20:39.120 --> 20:42.120
This requires some more tinkering.

20:42.120 --> 20:47.440
The final thing, we need to make this agnostic to the file system.

20:47.440 --> 20:52.880
We can very easily achieve this using this file system runtime,

20:52.880 --> 20:56.480
where tune ICD, an installable client driver,

20:56.480 --> 21:00.000
much the same way that Vulkan and OpenCL and OpenGL are working,

21:00.000 --> 21:02.560
you can dynamically load a shared library that

21:02.560 --> 21:06.160
implements all the functions you have defined in a header.

21:06.160 --> 21:09.680
This can also dynamically compile your programs and then

21:09.680 --> 21:12.640
store cache versions of this program.

21:12.640 --> 21:17.360
Using statifers, we can easily identify on what file system it is running.

21:17.360 --> 21:20.520
That allows users to write their programs one,

21:20.520 --> 21:22.120
run on any architecture,

21:22.120 --> 21:24.640
and for any computational file system,

21:24.640 --> 21:27.800
which I think is pretty powerful and flexible.

21:27.800 --> 21:30.040
So that's it.

21:30.040 --> 21:32.360
I encourage you to try this.

21:32.360 --> 21:36.640
I've also written a thesis on this that does have some performance metrics.

21:36.640 --> 21:40.120
It also shows you some interesting data structures that we had to

21:40.120 --> 21:44.200
design for the file system to be able to support these in-memory snapshots.

21:44.200 --> 21:47.040
There's a previous work called SetCSD.

21:47.040 --> 21:51.440
It also has some early performance information.

21:51.440 --> 21:56.960
I've written quite an extensive survey on the last decade history or so

21:56.960 --> 21:59.240
of computational flash storage devices,

21:59.240 --> 22:11.680
which also quite interesting. So thank you.

22:11.680 --> 22:13.880
Seven minutes for questions.

22:13.880 --> 22:15.760
It's quite good.

22:15.760 --> 22:21.240
I imagine this is quite difficult, right?

22:21.240 --> 22:23.320
Computational storage, what the fuck's that?

22:23.320 --> 22:29.120
So please don't hesitate to ask questions if anything is unclear.

22:29.120 --> 22:34.560
What's the availability of hardware that can do this?

22:34.560 --> 22:36.640
The computational storage?

22:36.640 --> 22:38.160
Yes, the computational storage.

22:38.160 --> 22:43.200
There's one vendor that is selling a computational storage device

22:43.200 --> 22:46.640
that's not based on zoned namespaces storage.

22:46.640 --> 22:49.200
So it's using conventional SSDs.

22:49.200 --> 22:53.840
It supports computational storage to a network interface.

22:53.840 --> 22:55.840
So you have the normal PCIe interface,

22:55.840 --> 23:00.640
and then there's this transport over needs to do TCP IP,

23:00.640 --> 23:03.200
and then you basically just connect over it to SSH,

23:03.200 --> 23:05.160
and then you can do things on the SSD.

23:05.160 --> 23:07.040
That one's commercially available.

23:07.040 --> 23:11.040
I don't know what they would ask for that product.

23:11.040 --> 23:16.640
What does the CSD have to do with zoned namespaces?

23:16.640 --> 23:18.640
Nothing in principle,

23:18.640 --> 23:21.880
but you need a way to synchronize the file system between the host and device,

23:21.880 --> 23:24.280
and zoned namespaces make that trivial.

23:24.280 --> 23:26.440
Whereas conventional SSDs,

23:26.440 --> 23:29.360
the logical and physical block translation,

23:29.360 --> 23:31.840
severely hinders this,

23:31.840 --> 23:35.040
makes it extremely difficult to perform.

23:36.960 --> 23:42.040
So why didn't you include the performance metrics from your thesis?

23:42.040 --> 23:45.440
Because the performance.

23:45.440 --> 23:46.560
Oh, sorry.

23:46.560 --> 23:49.440
Yeah. Very good.

23:49.440 --> 23:51.200
I forgot that actually all the time.

23:51.200 --> 23:57.240
Yeah. So why didn't I include any performance metrics if I have them?

23:57.240 --> 24:00.600
The answer is because I don't think I would have time,

24:00.600 --> 24:03.160
and I don't think they're interesting enough to include.

24:03.160 --> 24:05.280
This is a very complicated subject.

24:05.280 --> 24:07.680
It's very new for most people, computational storage.

24:07.680 --> 24:09.240
Most people have never heard of it.

24:09.240 --> 24:12.720
So I'd much rather spend the time to explain this properly,

24:12.720 --> 24:17.320
and try to show you that this is a very interesting concept to solve this bandwidth gap,

24:17.320 --> 24:20.560
rather than show you some metrics that are not representative anyway,

24:20.560 --> 24:23.000
because the kernel is running on the host CPU,

24:23.000 --> 24:27.120
and you're not going to have an additional CPU on the Flash SSD.

24:28.120 --> 24:33.240
Can you talk about what kind of test setup you have for your metric?

24:33.240 --> 24:36.680
So I don't have the metrics themselves.

24:36.680 --> 24:38.000
Yeah. So the framework.

24:38.000 --> 24:39.480
Okay. Yeah. Yeah. Very good.

24:39.480 --> 24:45.600
What kind of test setup I had to do all these analysis and to try these things out.

24:45.600 --> 24:48.240
So I run Camu on my own host machine,

24:48.240 --> 24:50.520
just a normal laptop, basically this one.

24:50.520 --> 24:54.520
And Camu then creates a virtual sound namespaces device.

24:54.520 --> 24:57.000
That's actually quite recently introduced to Camu.

24:57.000 --> 25:00.880
So you can now try sound namespaces without owning sound namespaces.

25:00.880 --> 25:03.240
That's the whole reason Camu comes into play,

25:03.240 --> 25:07.760
because otherwise people wouldn't need to buy a sound namespace SSD,

25:07.760 --> 25:09.600
which is quite badly available.

25:09.600 --> 25:12.960
And then you just run the prototype as is.

25:12.960 --> 25:14.320
So that's all you need,

25:14.320 --> 25:18.080
and you really don't need any special hardware.

25:18.080 --> 25:22.280
Yeah. It could be even on an ARM laptop. It doesn't matter.

25:22.280 --> 25:23.560
Did you test them?

25:23.560 --> 25:25.480
No, I did not test it.

25:25.480 --> 25:27.800
But whether or not I tested it,

25:27.800 --> 25:30.400
if it works on ARM, the answer is no, I did not test it.

25:30.400 --> 25:32.800
But I'm pretty sure Camu compiles from ARM.

25:32.800 --> 25:34.600
So I'm pretty sure we're good there.

25:34.600 --> 25:37.920
Because you have to remember that's

25:37.920 --> 25:40.760
maybe not intrinsically clear from this presentation,

25:40.760 --> 25:42.800
but we didn't extend Camu in any way.

25:42.800 --> 25:44.480
It's just a normal Camu accumulation.

25:44.480 --> 25:46.400
You don't even need to custom install it.

25:46.400 --> 25:49.640
You can just get it from the package manager and use this.

25:49.640 --> 25:52.720
I have a lot of questions over here.

25:52.720 --> 25:55.680
Regarding the computational part,

25:55.680 --> 25:59.880
what are the limitations of what kind of CPU or kernel

25:59.880 --> 26:02.200
that it may run on these devices?

26:02.200 --> 26:04.880
I think the main limitations, yeah, yeah.

26:04.880 --> 26:08.960
What are the limitations of the kernels that you run on these devices?

26:08.960 --> 26:12.080
Well, first of all, you need to have data reduction, right?

26:12.080 --> 26:15.400
If you're going to read one gigabyte from the flash storage,

26:15.400 --> 26:18.520
and you're going to return one gigabyte of data to the host,

26:18.520 --> 26:20.680
then there's no real point in offloading this,

26:20.680 --> 26:23.920
because the data is going to be moved anyway.

26:23.920 --> 26:27.240
So the first thing that the limitation is that you have to

26:27.240 --> 26:30.120
find an application that is reductive in nature.

26:30.120 --> 26:33.320
Once you do the computation, you return less data.

26:33.320 --> 26:38.800
The nice thing is that's 99 percent of all workloads, right?

26:38.800 --> 26:40.280
So that's pretty good.

26:40.280 --> 26:44.280
The second thing is that if it's time in critical,

26:44.280 --> 26:46.320
and the computation takes a long time,

26:46.320 --> 26:48.400
then it's probably not that interesting,

26:48.400 --> 26:51.440
because the latency will then be too bad,

26:51.440 --> 26:53.080
because the performance of these cores

26:53.080 --> 26:55.240
is much less than your host processor.

26:55.240 --> 27:00.600
But you can implement specialized instructions that could be

27:00.600 --> 27:03.560
very efficient in doing database filtering or things like this,

27:03.560 --> 27:07.720
and that is where the whole ASIC and FPGA part would come into play.

27:07.720 --> 27:12.040
But if it's not time in critical and it's in the background,

27:12.040 --> 27:14.520
like the Shannon-Manser pre-compassion,

27:14.520 --> 27:16.320
those are ideal cases,

27:16.320 --> 27:20.000
reduction in data and not time in critical.

27:20.000 --> 27:23.880
So what you mean is we can have software kernels with

27:23.880 --> 27:29.440
a back-end in hardware so we can also program the hardware?

27:29.440 --> 27:34.240
So like maybe a core CPU board or GPU board?

27:34.240 --> 27:36.240
To repeat the question,

27:36.240 --> 27:40.920
whether or not it's just software or whether we also program the hardware.

27:40.920 --> 27:44.000
Of course, FPGAs can be reprogrammed on the fly,

27:44.000 --> 27:46.160
and we have seen prototypes in the past for

27:46.160 --> 27:48.680
computational storage devices where they do just that.

27:48.680 --> 27:53.240
From the host device, the user sends a bitstream that dynamically reprograms

27:53.240 --> 27:55.560
the FPGA and then the kernel starts running.

27:55.560 --> 27:57.960
That's not what we're trying to achieve here.

27:57.960 --> 27:59.760
What I envision in this study,

27:59.760 --> 28:06.040
FPGA has specialized logic to do certain computations,

28:06.040 --> 28:09.560
and then from the EBPF ABI,

28:09.560 --> 28:12.240
once the Fender code triggers those instructions,

28:12.240 --> 28:15.240
we'll utilize the FPGA to do those computations.

28:15.240 --> 28:18.760
But it would be defined in the specification beforehand.

28:18.760 --> 28:22.080
Because typically, re-flashing

28:22.080 --> 28:24.640
FPGA with a new bitstream takes quite some time.

28:24.640 --> 28:26.440
So in the interest of performance,

28:26.440 --> 28:28.840
it might not be that interesting.

28:28.840 --> 28:36.360
So I'm going to ask a question.

28:36.360 --> 28:39.280
So you might have mentioned it,

28:39.280 --> 28:43.320
but are there like close source competitors?

28:43.320 --> 28:47.360
If there are close source competitors in the space of computational storage.

28:47.360 --> 28:50.680
Well, actually, that's one of the things that's been growing really well.

28:50.680 --> 28:55.760
In the scene, I'd say the vast majority of everything is open source.

28:55.760 --> 29:00.920
At least if you look at the recent things.

29:00.920 --> 29:02.680
If you look at the past decade,

29:02.680 --> 29:05.040
then it's a bit worse because there's a lot of research

29:05.040 --> 29:07.960
published that doesn't actually publish the source codes,

29:07.960 --> 29:10.120
or rather the source codes is published,

29:10.120 --> 29:12.560
but everything is a hardware prototype and they didn't publish

29:12.560 --> 29:15.480
the bitstreams or the VHDL or the Fairlock.

29:15.480 --> 29:17.120
So you then stuck as well,

29:17.120 --> 29:19.520
but it didn't have any PCB designs,

29:19.520 --> 29:22.880
or you can't reproduce the work, if you will.

29:22.880 --> 29:25.840
I say this is a much bigger problem than just

29:25.580 --> 29:50.760
complexity in terms of?

29:25.840 --> 29:28.360
computational storage in the field of academia,

29:28.360 --> 29:32.160
but it's also present here.

29:34.160 --> 29:35.480
Yes.

29:35.480 --> 29:38.040
Can we go back to the Python code?

29:38.040 --> 29:40.920
Which one? The Python code.

29:40.920 --> 29:44.800
Yeah, this one. We were talking about the moments and questions.

29:44.800 --> 29:50.760
I was just wondering about the complexity of the

29:50.760 --> 29:53.360
I have a nested loop there.

29:57.400 --> 30:00.920
The reason this is a nested loop,

30:00.920 --> 30:03.160
in the phase of performance,

30:03.160 --> 30:04.240
I have a nested loop here.

30:04.240 --> 30:05.480
So why that?

30:05.480 --> 30:07.720
In the terms of performance, how?

30:07.720 --> 30:08.560
The Python.

30:08.560 --> 30:11.080
Python. Yeah. The trick is,

30:11.080 --> 30:13.160
this is just for demonstration purposes.

30:13.160 --> 30:16.200
There's one you can easily make this example in C or C++,

30:16.200 --> 30:18.240
and you shit if you care about performance.

30:18.240 --> 30:20.560
The trick is that this program is already spending

30:20.560 --> 30:22.600
99 percent of its time in IO weight,

30:22.600 --> 30:24.280
because it's waiting for the kernel to complete.

30:24.280 --> 30:26.880
So in the face of that, it's not that interesting.

30:26.880 --> 30:29.960
The reason we have a nested loop is because

30:29.960 --> 30:35.800
the floating points in eBPF is not

30:35.800 --> 30:38.800
existent or at least I didn't implement the fixed point mod.

30:38.800 --> 30:40.920
So what I have to do after this,

30:40.920 --> 30:46.120
at the bottom what you don't see here is that from all these buckets of these bins,

30:46.120 --> 30:49.440
I'm actually computing the distribution using

30:49.440 --> 30:52.160
fixed point using floating point mat in Python,

30:52.160 --> 30:55.520
which is why I don't get a single number from this kernel.

30:55.520 --> 31:00.440
Because if I would have floating point implementation in eBPF,

31:00.440 --> 31:05.360
I could already do that computation in eBPF and only return

31:05.360 --> 31:11.320
a single 32-bit float as a result instead of these 265 integers.

31:12.280 --> 31:16.080
But the reason this is a loop is because I still have to

31:16.080 --> 31:19.640
try for the read request because I can't go above 512K,

31:19.640 --> 31:22.920
even if my file is bigger than 512K.

31:32.680 --> 31:36.360
Well, the trick is,

31:36.360 --> 31:41.040
couldn't I implement multi-threading here?

31:41.040 --> 31:45.760
Currently, the eBPF VM runs as a single process.

31:45.760 --> 31:49.160
So even if you submit multiple kernels,

31:49.160 --> 31:51.240
only one will execute at a time.

31:51.240 --> 31:53.840
Why? It's a thesis prototype, right?

31:53.840 --> 31:57.440
Time and things like this. Okay.

31:57.440 --> 31:58.760
Thank you very much.

31:58.760 --> 32:16.320
No worries.
