WEBVTT

00:00.000 --> 00:11.920
Hello, everyone. Good afternoon. I am Thanos from the University of Manchester. And today

00:11.920 --> 00:16.960
I have the pleasure to present you Tornado VM, what is the state of Tornado VM at this

00:16.960 --> 00:24.440
moment. And in fact, I want to focus also on a slogan that's very known to everyone.

00:24.440 --> 00:32.900
The right ones run anywhere for Java. So we'll start with that. So this is a known slogan

00:32.900 --> 00:40.720
derived since the 90s from microsystems in a way to advertise that Java language and

00:40.720 --> 00:47.200
the JVM in particular, it is a platform that can ensure portability across different CPU

00:47.200 --> 00:54.160
structures and architectures. So the idea is that programmers can run their code, they

00:54.160 --> 01:03.040
can compile it once, and it can run transparently on different hardware architectures. However,

01:03.040 --> 01:09.320
hardware has changed in the last years. It is evolving. And perhaps this is not sufficient

01:09.320 --> 01:21.520
for the new types of hardware resources that are coming. So lately we have GPUs and FPGAs

01:21.520 --> 01:27.920
which are coming to complement the power of the CPUs in a way to maximize performance

01:27.920 --> 01:36.080
and reduce the energy consumption. These are good, but there is some challenges that are

01:36.080 --> 01:43.600
deriving and these are mainly posed in programmability. So how programmers can harness this power

01:43.600 --> 01:50.600
from these resources. I don't know if you have experience with OpenCL and CUDA, but

01:50.600 --> 01:55.120
mainly these programming models that have been designed for these hardware types to

01:55.120 --> 02:01.520
get access to these hardware types, they are mainly focused on the C and C++ world. So

02:01.520 --> 02:07.320
there are different programming models from different companies like SQL, OneAPI, NVIDIA

02:07.320 --> 02:16.720
CUDA, OpenCL, which is a standard that can run on all the devices. And if you have FPGA

02:16.720 --> 02:23.600
expertise, then perhaps you can write RTL and Verilog, which is a hardware description

02:23.600 --> 02:27.520
language. But this is very low level. And here we are talking about Java, so we want

02:27.520 --> 02:37.600
to go high level. So if you are a Java developer, then you use the JVM and you go to the CPU.

02:37.600 --> 02:44.920
If you want to have access to these devices, then you need to write your own native interfaces

02:44.920 --> 02:51.760
in the JNI and then tap into the C and C++ world. But still, you need to be aware of

02:51.760 --> 02:58.160
how these programming models are written. So you need to be familiar with this. And

02:58.160 --> 03:04.440
this is exactly the problem that Tornado VM has been designed to solve. So Tornado VM,

03:04.440 --> 03:14.920
it is a plugin to existing OpenJDK distributions like Amazon, and others. And the way that

03:14.920 --> 03:23.000
it is built, it is to enable hardware acceleration in an easy manner. So it offers a Java API.

03:23.000 --> 03:31.520
And it has inside a JIT compiler for the hardware devices that I saw in this figure. Our compiler

03:31.520 --> 03:37.960
inside, it can automatically translate the Java bytecodes to run on CPUs, multi code

03:37.960 --> 03:46.400
CPUs, GPUs integrated or discrete GPUs and FPGAs. And the compiler in the backend, it

03:46.400 --> 03:53.440
has three different backend types. It can emit OpenCLC, PTX, which is assembly for the

03:53.440 --> 04:02.000
CUDA for the NVIDIA GPUs. And it has recently also the SPIRV backend, which enables to utilize

04:02.000 --> 04:11.440
the level zero dispatcher from the one API. So Tornado VM, it is a technology that can

04:11.440 --> 04:17.840
be used as a JVM plugin to enable hardware acceleration for JVMs. And some of the key

04:17.840 --> 04:25.400
features is that it has a lightweight Java API. It is coded in a platform agnostic manner.

04:25.400 --> 04:32.920
So one command can be the same no matter which device it will be executed to program. And

04:32.920 --> 04:39.640
it can transparently at the compile time specialize the code. Because the code that is generated

04:39.640 --> 04:47.640
for the GPU, it is completely different from a code that is generated for an FPGA. So regarding

04:47.640 --> 04:54.640
the compiler, we have different phases that will be enabled for GPUs and different phases

04:54.640 --> 05:05.800
that will be enabled to specialize the code for an FPGA. Our code is available in GitHub.

05:05.800 --> 05:11.280
So we encourage everyone who wants to have a look to fork it, download, play with examples,

05:11.280 --> 05:20.000
or even create their own examples. And also to come back to us, I mean, to feel free to

05:20.000 --> 05:24.800
use the discussions to trigger the discussion if you have questions or to open issues if

05:24.800 --> 05:31.080
something is broken in order to fix it. And we have also available Docker images for NVIDIA

05:31.080 --> 05:41.040
GPUs and Intel integrated GPUs. Now the next part that I want to talk is regarding the

05:41.040 --> 05:49.400
API. Two weeks ago, we released a new version of Tornado VM, the version 0.15. And this

05:49.400 --> 05:57.640
comes with many new changes in the API level. So our goal was to make the API easier for

05:57.640 --> 06:04.000
Java programmers in order to use it in a comprehensive manner. So to know how to use and how to express

06:04.000 --> 06:11.760
parallelism from Java. But first, I have to make you familiar with the programming model

06:11.760 --> 06:18.680
of Tornado VM. And this programming model comes, it is inspired from the heterogeneous

06:18.680 --> 06:25.680
programming models like OpenCL and CUDA, the way that these programming models are operating.

06:25.680 --> 06:32.040
And in this sense, a Java program can be composed of two parts. The first part is the host code

06:32.040 --> 06:38.280
where it is the actual core of the Java application. And the second part, it is the accelerated

06:38.280 --> 06:44.440
code which actually it is the method or the set of methods that will be offloaded for

06:44.440 --> 06:51.280
execution on a GPU. Once we have made this clear, then we can move with the execution

06:51.280 --> 06:58.800
model which it requires first because the processing will take place on a device, it

06:58.800 --> 07:06.360
will have first to move the data from the host code from the CPU to the actual device.

07:06.360 --> 07:11.400
Then perform the processing. And once the processing is finished, then the data, the

07:11.400 --> 07:20.720
result will have to be transferred back to the host code. Now in Tornado VM, in the API

07:20.720 --> 07:26.480
of Tornado VM, we have exposed the set of objects and annotations for each of these

07:26.480 --> 07:32.280
two parts, the host code and the accelerated code. In the host code, we have the task graph

07:32.280 --> 07:38.960
object and the Tornado execution plan. The task graph corresponds to what to run on a

07:38.960 --> 07:45.680
GPU. And the Tornado execution plan, it is how to run on the GPU. And for the accelerated

07:45.680 --> 07:54.960
code, we have a set of annotations and objects that I will show you later. So let's start

07:54.960 --> 08:00.680
with the task graph. What to run? Assuming that you are a Java programmer, then you want

08:00.680 --> 08:07.920
to offload the execution of a method. In this example, method A to the GPU. This method

08:07.920 --> 08:16.240
has some input and some output. Now this method corresponds to what in the Tornado VM terminology

08:16.240 --> 08:23.640
call a task. So each method that will be offloaded for execution on hardware acceleration, it

08:23.640 --> 08:31.920
is a task. And it has the input data and the output data. And then we have a group of tasks

08:31.920 --> 08:38.000
which is the task graph. Now task graph can be a group of tasks that may have dependency

08:38.000 --> 08:43.600
or may not have dependency. And the programmers, they want to offload them all for hardware

08:43.600 --> 08:51.240
acceleration. In this particular example, I have put one task in this task graph. Once

08:51.240 --> 08:57.160
we have defined what to run, one question that comes, it is how often to transfer the

08:57.160 --> 09:05.200
data between the host, CPU and the device. And this can have a tremendous impact because

09:05.200 --> 09:11.560
it can affect the data transfer time. So it can have a long execution time. So it can

09:11.560 --> 09:18.640
affect performance. But can also affect energy. The power consumption. So how to transfer

09:18.640 --> 09:25.160
data, it matters. It depends on the pattern of the application. So one application may

09:25.160 --> 09:31.720
need to copy only the first execution if the data are read only. Then always or only in

09:31.720 --> 09:38.160
the last execution, for example, for the output, for the result. And here is a code snippet

09:38.160 --> 09:45.400
of how the task graph can be used to define this functionality in the Tornado API. So

09:45.400 --> 09:50.840
we create a new object, the task graph. We assign a name, which is a string. In this

09:50.840 --> 09:58.480
particular example, it is TG. And then we utilize the exposed methods of the API in

09:58.480 --> 10:05.840
order to fulfill the execution model. At first, we use the transfer to device which has two

10:05.840 --> 10:13.200
inputs. The first argument that we put, it is a data transfer mode which will be used

10:13.200 --> 10:20.560
to trigger how often the data will be moved. In this particular example, it is first execution

10:20.560 --> 10:25.200
shown in the first execution, the data will be moved. And then we'll have the parameter

10:25.200 --> 10:32.200
which is the input, which is the input array. The second method, it is the task and it defines

10:32.200 --> 10:39.560
which method will be used for hardware acceleration. The first parameter, it is a name, a string

10:39.560 --> 10:46.720
actually of the method. It could be any name. And this is associated for the dynamic configuration

10:46.720 --> 10:51.720
which I will show you later. The second parameter, it is the method reference. So the reference

10:51.720 --> 10:58.400
to the method that will be offloaded to the GPU for acceleration. And then it is the list

10:58.400 --> 11:05.440
of parameters of this method that corresponds to the method signature. And the last method,

11:05.440 --> 11:12.640
it is the transfer to host. And this, again, this method, it is configured the first argument

11:12.640 --> 11:18.640
to be the data transfer mode. In this example, we will copy the data, the output in every

11:18.640 --> 11:29.120
execution. Okay. And once we have defined the task through the task graph, this task

11:29.120 --> 11:37.680
can be appended, can be updated. We can add a new task, a second task. We can change the

11:37.680 --> 11:44.000
way that the data transfer will be triggered in every execution only in the first execution.

11:44.000 --> 11:53.000
Then the next step, it is to define the immutable state of the task graph. So how to preserve

11:53.000 --> 11:59.720
the shape of a task graph. And this is done by taking a snapshot of the task graph, by

11:59.720 --> 12:06.480
using the snapshot method in the task graph object. Then we retrieve back an immutable

12:06.480 --> 12:13.000
task graph. And this means that this can be used for JIT compilation and execution on

12:13.000 --> 12:19.760
the hardware. And this ensures that the Java programmers, they can create different versions

12:19.760 --> 12:25.920
of their task graph. They can update it. And then the code that will have Internet of VM,

12:25.920 --> 12:31.720
it can store all these versions. It doesn't need to recompile and override the generated

12:31.720 --> 12:41.640
code. And this is the final step before we move to the actual execution plan. We have

12:41.640 --> 12:47.200
the mutable state of the task graph that can be modified and the immutable task graph that

12:47.200 --> 12:53.880
it cannot be modified anymore. So if the users, they want to do a change, they can still change

12:53.880 --> 13:03.320
the task graph and get a new snapshot for a second version of their code. And now we

13:03.320 --> 13:09.720
move to how to run, how to execute the task graph. And this is done through the execution

13:09.720 --> 13:18.480
plan. Here is a snippet of a tornado execution plan. We create a new object that accepts

13:18.480 --> 13:24.320
as input the mutable task graph. That doesn't change anymore. And then we can either directly

13:24.320 --> 13:30.800
execute it in the default execution mode by invoking the execution plan.execute method

13:30.800 --> 13:38.000
or we can configure it with some various optimizations. In this particular example, I have enabled

13:38.000 --> 13:45.160
the configuration to run with dynamic reconfiguration, which is a feature in tornado VM that will

13:45.160 --> 13:51.560
launch a Java thread to get compiled and execute the application per device that is available

13:51.560 --> 13:58.240
on the system. So we can have a CPU, a GPU, an FPGA. Java thread will run for all the

13:58.240 --> 14:03.080
devices. And then it is triggered with a policy of performance, which means that the first

14:03.080 --> 14:08.680
device that will finish the execution, it will be the best. And the rest, Java threads

14:08.680 --> 14:19.640
will be killed. Now I have concluded the part of the host code. We can briefly go to the

14:19.640 --> 14:25.320
accelerated code, which is the way to express parallelism within the kernel, within the

14:25.320 --> 14:33.000
method or the tornado VM task, as we call it. We have two ways, two APIs. The first

14:33.000 --> 14:40.080
one is called loop parallel API. And in a sense, we expose the add parallel annotations

14:40.080 --> 14:46.720
that can be used by programmers to as a hint to the tornado VM compiler that these loops

14:46.720 --> 14:55.120
can run in parallel. And the second one is the kernel API, which is an API exposed to

14:55.120 --> 15:01.680
the users through the kernel context object. And in a sense, the meaning of this API, it

15:01.680 --> 15:09.160
is meant for open CL and programmers or Java programmers who know open CL in a way to get

15:09.160 --> 15:16.160
more freedom on how to code things so they can get access to local memory, which is equivalent

15:16.160 --> 15:24.080
to the memory of the CPU for GPUs. So they have more freedom on what to express. And

15:24.080 --> 15:34.040
in fact, I have used this API to port existing kernels written in open CL and CUDA to Java.

15:34.040 --> 15:39.280
For more information, you can use this link, which is the actual documentation of tornado

15:39.280 --> 15:47.360
VM and describes some examples. I will briefly go to one example of a matrix multiplication,

15:47.360 --> 15:54.600
which I presented last year in force them. So in this example, we have the accelerated

15:54.600 --> 16:02.720
code and the host code. The matrix multiplication method implements matrix multiplication over

16:02.720 --> 16:09.840
a flattened arrays in two dimensions. And the way to annotate and express parallelism

16:09.840 --> 16:15.360
using the add parallel annotation, it would be to add the add parallel annotation inside

16:15.360 --> 16:23.720
the four loops. That means that we indicate that these loops could be executed in parallel.

16:23.720 --> 16:31.120
And now regarding the second API, the kernel API, we would use the kernel context object.

16:31.120 --> 16:40.480
And in particular, we would use the global ID, X and Y, which correspond to the two dimensions

16:40.480 --> 16:53.640
that we have. So in a sense, it is like having the thread ID that will execute on the GPU.

16:53.640 --> 17:01.880
Here are some use cases that we used tornado VM. And concluding this talk, I would like

17:01.880 --> 17:08.440
to focus on a feature that we implemented in a project that we are working. It is called

17:08.440 --> 17:13.520
Elegant and the idea is to create a software stack that unifies development for big data

17:13.520 --> 17:21.920
and IOT deployment. And there, tornado VM is used as a technology to enable acceleration

17:21.920 --> 17:28.200
as a service. So we have implemented the REST API. It is still a prototype. But the programmers,

17:28.200 --> 17:33.400
they can write a method, they can specify a method, they can specify the characteristics

17:33.400 --> 17:40.120
of the targeted device. And then the service will return back open CL code that it is meant

17:40.120 --> 17:47.240
to run parallel this function. The interesting part is that this code, the open CL code,

17:47.240 --> 17:53.960
it is generated to be portable across different programming languages. So it doesn't only

17:53.960 --> 18:03.760
bind to Java, it can run also through C++, Python, because it is open CL. And this means

18:03.760 --> 18:12.000
that in this particular example, we have Java, we use OpenJDK, we take the byte code and

18:12.000 --> 18:17.920
we pass the byte code to tornado VM. And tornado VM is running on an experimental feature which

18:17.920 --> 18:23.840
is called code interoperability mode. And in this mode, it converts this byte code to

18:23.840 --> 18:29.720
open CL that can run from any programming language and run time. Therefore, it is like

18:29.720 --> 18:39.840
prototyping in Java for parallel programming. Wrapping up, we would like to receive feedback

18:39.840 --> 18:46.380
and we are looking also for collaborations if we can help to port and use cases or for

18:46.380 --> 18:55.040
any other issues. And summarizing this talk, I briefly went through the right ones run

18:55.040 --> 19:00.960
anywhere in the context of heterogeneous hardware acceleration. I have familiarized you with

19:00.960 --> 19:07.260
tornado VM, which is an open source project. And the code base is available in GitHub.

19:07.260 --> 19:14.120
And familiarize you with the programming model of tornado VM and the new API, how to use

19:14.120 --> 19:22.960
it. And more are about to come in the FUJ blog with a new blog. So, finally, just to

19:22.960 --> 19:28.000
acknowledge the projects that they have supported our research in the University of Manchester

19:28.000 --> 19:44.280
and I'm ready for questions.
