1
0:00:00.000 --> 0:00:07.000
Check, one, two, hello.

2
0:00:07.000 --> 0:00:08.000
Hello.

3
0:00:08.000 --> 0:00:10.000
Hi, where's Malte?

4
0:00:10.000 --> 0:00:13.000
Hi, nice to meet you.

5
0:00:13.000 --> 0:00:18.000
Okay, sorry, just like one of my hacker friends has been working with me on the project.

6
0:00:18.000 --> 0:00:22.000
I've actually never met him in person, so nice to meet you.

7
0:00:22.000 --> 0:00:28.000
Anyway, today we're going to be talking about Aura or Aude, however you want to pronounce it, is fine,

8
0:00:28.000 --> 0:00:31.000
which we're temporarily calling a distributed systems runtime,

9
0:00:31.000 --> 0:00:38.000
and that's the name that has caused the least amount of friction over the past few months.

10
0:00:38.000 --> 0:00:42.000
Okay, so my least favorite slide, my slide about me.

11
0:00:42.000 --> 0:00:47.000
So I'm an engineer, I work at GitHub, I help keep GitHub.com online.

12
0:00:47.000 --> 0:00:52.000
Sorry about the Shaw thing last week.

13
0:00:52.000 --> 0:00:55.000
Yeah, so I keep a lot of systems online.

14
0:00:55.000 --> 0:00:59.000
Some of you may or may not use them, all of you hopefully have good opinions of them.

15
0:00:59.000 --> 0:01:04.000
And then if you want to follow me on the Fetaverse, there's where you can follow me.

16
0:01:04.000 --> 0:01:07.000
So I'll do overview and context.

17
0:01:07.000 --> 0:01:13.000
So if you want to go to the GitHub repo, you can grab a photo of this or just remember it.

18
0:01:13.000 --> 0:01:18.000
The link to the slides are there right now, I just forced push to main like two seconds ago.

19
0:01:18.000 --> 0:01:24.000
So you can go and you can see the slides and there's like links to everything there that I'll be going over today.

20
0:01:24.000 --> 0:01:27.000
So if you want to grab that, go ahead and grab that.

21
0:01:27.000 --> 0:01:30.000
Okay, so we're going to start off, I'll do a little bit of context.

22
0:01:30.000 --> 0:01:33.000
I'll answer the question, what is Aura, what does it do?

23
0:01:33.000 --> 0:01:39.000
And then we'll spend the last two thirds of the presentation talking about Rust

24
0:01:39.000 --> 0:01:45.000
and why we decided to use Rust for the project and some reports about how it's going so far

25
0:01:45.000 --> 0:01:47.000
and some of my experience as well.

26
0:01:47.000 --> 0:01:52.000
Okay, so just show of hands, who here has heard of Aura before?

27
0:01:52.000 --> 0:01:56.000
Oh, God, okay.

28
0:01:56.000 --> 0:01:59.000
Well, thank you for following my project.

29
0:01:59.000 --> 0:02:04.000
That makes me very happy but also a little terrified.

30
0:02:04.000 --> 0:02:10.000
So anyway, Aura, it's an open source Rust project and it's aimed at simplifying node management at scale.

31
0:02:10.000 --> 0:02:18.000
And so when I talk about it, I usually say it's basically a generic execution engine for containers, VMs and processes.

32
0:02:18.000 --> 0:02:24.000
The really quick pitch that I'll give on Aura is all of these things, containers, VMs,

33
0:02:24.000 --> 0:02:29.000
hypervisors and basic process management is all that I do at GitHub

34
0:02:29.000 --> 0:02:32.000
and all that I have done in my career for the past 10 years.

35
0:02:32.000 --> 0:02:38.000
And I have used a plethora of tools to do this and I was tired of learning and managing all these different tools

36
0:02:38.000 --> 0:02:45.000
and so I hope that this will be the last tool I ever have to work on in my career.

37
0:02:45.000 --> 0:02:52.000
So I wrote a thesis about the project and I'm trying hard to continually reevaluate this thesis

38
0:02:52.000 --> 0:02:57.000
and basically it says that by bringing some deliberate runtime controls to a node,

39
0:02:57.000 --> 0:03:00.000
we can unlock a new generation of higher order distributed systems.

40
0:03:00.000 --> 0:03:05.000
And what I mean by that is in my experience, a lot of the things we do on a node are organic

41
0:03:05.000 --> 0:03:08.000
and grew over the past 30 years or so.

42
0:03:08.000 --> 0:03:12.000
And this is more of a deliberate set of what do we need in the enterprise

43
0:03:12.000 --> 0:03:16.000
and what do we need at a bare minimum on the node and I think that if we get that right,

44
0:03:16.000 --> 0:03:20.000
we're actually going to have a much more interesting conversations in the coming decades.

45
0:03:20.000 --> 0:03:27.000
So I also believe that simplifying the execution stack will foster and secure observable systems

46
0:03:27.000 --> 0:03:29.000
while reducing complexity and risk.

47
0:03:29.000 --> 0:03:35.000
And complexity, if you have ever ran Kubernetes, is the name of the game.

48
0:03:35.000 --> 0:03:39.000
Cool, so I'll be talking about these things called nodes today.

49
0:03:39.000 --> 0:03:44.000
So node is a keyword and when I say node pretty much always in life,

50
0:03:44.000 --> 0:03:50.000
but very specifically in this talk, what I mean is a single compute unit in a set.

51
0:03:50.000 --> 0:03:56.000
So this would be one or more computers that we're trying to group together and manage as a set of computers.

52
0:03:56.000 --> 0:04:02.000
So when we do one thing to a node, the sort of assumption here is you want to go and do this twice

53
0:04:02.000 --> 0:04:07.000
or three times or 10,000 times sometimes or so on.

54
0:04:07.000 --> 0:04:13.000
So when we say node, I want you to think of a set of computers or an array of computers.

55
0:04:13.000 --> 0:04:17.000
OK, so what does Aura do?

56
0:04:17.000 --> 0:04:22.000
So the thesis here is this is going to be a central control for every runtime process on a node.

57
0:04:22.000 --> 0:04:27.000
So whether you're running PID1 or a container or a virtual machine,

58
0:04:27.000 --> 0:04:32.000
the hope is that all of this can be funneled through the Aura binary at runtime

59
0:04:32.000 --> 0:04:39.000
and Aura will have the ability to not only manage it but also observe it and control it and start it and stop it.

60
0:04:39.000 --> 0:04:44.000
And who knows, maybe even one day debug it if I'm very lucky.

61
0:04:44.000 --> 0:04:48.000
It runs as a minimal init system. So this is important.

62
0:04:48.000 --> 0:04:52.000
A lot of folks want to compare Aura to systemd and the more I think about it,

63
0:04:52.000 --> 0:04:56.000
the more I think that I really believe Aura and systemd have different goals.

64
0:04:56.000 --> 0:04:59.000
Aura doesn't really want to become a desktop manager.

65
0:04:59.000 --> 0:05:05.000
In fact, it kind of wants to be the opposite of that. It wants to be as lightweight and as minimal as possible.

66
0:05:05.000 --> 0:05:09.000
In a perfect world, there would be no user space on an Aura system

67
0:05:09.000 --> 0:05:13.000
because we wouldn't actually want users touching a single computer.

68
0:05:13.000 --> 0:05:16.000
Remember, we're managing sets of computers.

69
0:05:16.000 --> 0:05:22.000
And so the hope here is that we can make this as lightweight as possible.

70
0:05:22.000 --> 0:05:25.000
Additionally, we want this thing to have a remote API.

71
0:05:25.000 --> 0:05:32.000
So the idea of a single person sitting at a desk and operating on a single node is kind of irrelevant here.

72
0:05:32.000 --> 0:05:37.000
So everything that we do on the node, whether it's scheduling another process like a bash shell

73
0:05:37.000 --> 0:05:42.000
or it's scheduling a container, should all come through this remote API.

74
0:05:42.000 --> 0:05:48.000
And we're going to learn more about this API in Rust specifically later on in the talk.

75
0:05:48.000 --> 0:05:55.000
Also, it runs on Linux. It right now is tightly coupled to the Linux kernel.

76
0:05:55.000 --> 0:05:59.000
So what doesn't it do? So it doesn't do generic desktop support.

77
0:05:59.000 --> 0:06:03.000
So that's just completely out of scope. I don't want to deal with your Bluetooth drivers.

78
0:06:03.000 --> 0:06:08.000
I don't want to deal with your sound drivers. I don't want to manage your desktop interface.

79
0:06:08.000 --> 0:06:12.000
I don't care. In a perfect world, this hooks up to the network,

80
0:06:12.000 --> 0:06:17.000
and that's about the most advanced user interface we're going to have to one of these nodes in a set.

81
0:06:17.000 --> 0:06:20.000
Additionally, higher order scheduling is out of scope.

82
0:06:20.000 --> 0:06:27.000
So when we talk about enterprise management, whether it's some sort of orchestration system like Kubernetes or not,

83
0:06:27.000 --> 0:06:31.000
a lot of those discussions very quickly go into the scheduling discussion.

84
0:06:31.000 --> 0:06:35.000
There was a really good article, I think it was yesterday or the day before, on Hacker News

85
0:06:35.000 --> 0:06:39.000
that came out of fly.io about their orchestrator experience with Nomad.

86
0:06:39.000 --> 0:06:42.000
I see somebody shaking their head. Yeah, you read the article.

87
0:06:42.000 --> 0:06:48.000
It was a great article, and maybe we can find a link to it and put it in the video or something for folks.

88
0:06:48.000 --> 0:06:55.000
But that conversation was very much about how do we make scheduling decisions with available resources today.

89
0:06:55.000 --> 0:06:59.000
And that is pretty much all I do at my day job at GitHub,

90
0:06:59.000 --> 0:07:03.000
and that's all I've been doing managing Kubernetes for the past five or six years.

91
0:07:03.000 --> 0:07:07.000
And so while I'm very interested in having that conversation,

92
0:07:07.000 --> 0:07:14.000
my hope is that by simplifying the node, we can make those scheduling conversations easier in the future.

93
0:07:14.000 --> 0:07:19.000
And what I mean by that is that we will have less to say about what we actually do on a node,

94
0:07:19.000 --> 0:07:22.000
and we can effectively make nodes boring.

95
0:07:22.000 --> 0:07:25.000
So it doesn't run on Darwin, it doesn't run on Windows.

96
0:07:25.000 --> 0:07:27.000
Like I said, we're tightly coupled to the Linux kernel,

97
0:07:27.000 --> 0:07:34.000
which if you haven't pieced it together yet is why Rust is very exciting for the project.

98
0:07:34.000 --> 0:07:39.000
Okay, so again in summary, where did ORIT come from?

99
0:07:39.000 --> 0:07:43.000
It came with challenges with complexity at scale, so we just want the node to be boring.

100
0:07:43.000 --> 0:07:49.000
And it became, there was this desire to simplify and secure the stack.

101
0:07:49.000 --> 0:07:53.000
So I do deeply believe that with simple systems come secure systems.

102
0:07:53.000 --> 0:08:00.000
Every hack that I've been a part of in the industry has usually started with some sort of disparate

103
0:08:00.000 --> 0:08:04.000
and un-node fragmented attack surface that somebody's been able to exploit

104
0:08:04.000 --> 0:08:08.000
and do some sort of lateral movement once they're into the system.

105
0:08:08.000 --> 0:08:13.000
So if we can simplify that and we can just make the conversation involve less moving pieces,

106
0:08:13.000 --> 0:08:16.000
my hope is that we can actually secure the stack.

107
0:08:16.000 --> 0:08:19.000
I also want there to be a stronger node API.

108
0:08:19.000 --> 0:08:24.000
So who here has ever debugged the kublet API before?

109
0:08:24.000 --> 0:08:26.000
Who here even knows what this is?

110
0:08:26.000 --> 0:08:28.000
Okay, so we have a handful of people.

111
0:08:28.000 --> 0:08:34.000
So the kublet is Kubernetes version of we're going to go run an agent on a node.

112
0:08:34.000 --> 0:08:36.000
It does have an API.

113
0:08:36.000 --> 0:08:41.000
Last I checked it was undocumented and it was tightly coupled with the Kubernetes control plane.

114
0:08:41.000 --> 0:08:42.000
We hope to break that.

115
0:08:42.000 --> 0:08:47.000
We hope to just have a generic API that you could use to run a single process remotely

116
0:08:47.000 --> 0:08:50.000
or you could schedule millions of processes remotely.

117
0:08:50.000 --> 0:08:57.000
And we want that to be a very strong and thoughtful API.

118
0:08:57.000 --> 0:09:02.000
One of the big lessons of running large distributed systems at scale is that

119
0:09:02.000 --> 0:09:07.000
the bigger you get, the less trust that you can have in the people working on your systems.

120
0:09:07.000 --> 0:09:13.000
So as I've grown either like my small mastodon server that's grown into a medium-sized mastodon server

121
0:09:13.000 --> 0:09:16.000
or even dealing with thousands of nodes at scale,

122
0:09:16.000 --> 0:09:22.000
one of the lessons that I've noticed is that all workloads tend to this untrusted banality.

123
0:09:22.000 --> 0:09:26.000
So the bigger you get, the less you can trust a single workload.

124
0:09:26.000 --> 0:09:29.000
And even if these workloads are on the same team as you,

125
0:09:29.000 --> 0:09:32.000
you really want to start looking at them as an isolation zone

126
0:09:32.000 --> 0:09:41.000
that you don't want to trust too much from the centralized control plane perspective.

127
0:09:41.000 --> 0:09:43.000
So we started off Aura with a few guiding principles.

128
0:09:43.000 --> 0:09:45.000
Number one, I wanted to be boring.

129
0:09:45.000 --> 0:09:47.000
So we're targeting a single binary.

130
0:09:47.000 --> 0:09:50.000
We want this binary to be polymorphic in nature.

131
0:09:50.000 --> 0:09:54.000
Who here is familiar with Busybox?

132
0:09:54.000 --> 0:09:55.000
Great. Yeah, Busybox.

133
0:09:55.000 --> 0:09:57.000
It's a good binary in my opinion.

134
0:09:57.000 --> 0:09:58.000
I really like what it does.

135
0:09:58.000 --> 0:10:02.000
There's a switch on arg zero and it basically behaves like however you call it.

136
0:10:02.000 --> 0:10:07.000
So we're trying to get some similar functionality into the Aura binary as well.

137
0:10:07.000 --> 0:10:09.000
And we also want this thing to be lightweight

138
0:10:09.000 --> 0:10:14.000
and have a very strong scope and be as low risk as possible.

139
0:10:14.000 --> 0:10:16.000
Additionally, we wanted this thing to be attainable.

140
0:10:16.000 --> 0:10:17.000
We wanted to play nice with others.

141
0:10:17.000 --> 0:10:20.000
So I knew that I wanted this to fit in neatly with Kubernetes.

142
0:10:20.000 --> 0:10:22.000
I knew I wanted this to fit in neatly with Linux.

143
0:10:22.000 --> 0:10:26.000
And I knew I wanted pretty much everyone in this room to feel realistically

144
0:10:26.000 --> 0:10:31.000
like they could be running this thing on their laptops one day as the project grows.

145
0:10:31.000 --> 0:10:36.000
And so in order to do that, the API was going to be the majority of what we were talking about

146
0:10:36.000 --> 0:10:39.000
as we began developing the project.

147
0:10:39.000 --> 0:10:41.000
And ultimately, I wanted to be functional.

148
0:10:41.000 --> 0:10:44.000
I don't want it to subserve the needs of a corporation.

149
0:10:44.000 --> 0:10:47.000
I don't want it to serve the needs of a higher order control plane.

150
0:10:47.000 --> 0:10:53.000
I literally just want a standard library for executing processes in containers and VMs at scale.

151
0:10:53.000 --> 0:10:55.000
What we do with that is out of scope.

152
0:10:55.000 --> 0:10:58.000
I just want it to work first and foremost.

153
0:10:58.000 --> 0:11:00.000
So ultimately, I want boring systems.

154
0:11:00.000 --> 0:11:06.000
And if you see in the background here, there's all of these subtle distributed system propaganda

155
0:11:06.000 --> 0:11:13.000
notes that you can go look at if you want to look at the slides later.

156
0:11:13.000 --> 0:11:16.000
So ultimately, I wanted the thing to be safe.

157
0:11:16.000 --> 0:11:20.000
So when we're looking at tenant security, one of the questions I ask is,

158
0:11:20.000 --> 0:11:22.000
how do we make it easy to do the right thing?

159
0:11:22.000 --> 0:11:25.000
And I think that comes from the underlying infrastructure.

160
0:11:25.000 --> 0:11:28.000
And in our case, Aura is the underlying infrastructure.

161
0:11:28.000 --> 0:11:35.000
And we intended to build a very strong project here that would unlock this sort of safe paradigm

162
0:11:35.000 --> 0:11:40.000
that we could give a team a binary, and they would be able to run their applications on top of it.

163
0:11:40.000 --> 0:11:44.000
And we wouldn't really have to worry about anybody sneaking out of their container

164
0:11:44.000 --> 0:11:48.000
or accessing any parts of the system so we didn't want them to access.

165
0:11:48.000 --> 0:11:55.000
So tenant security is a strong motivator for this as well.

166
0:11:55.000 --> 0:12:01.000
OK, so about six months ago on Twitch, which I do a Twitch stream,

167
0:12:01.000 --> 0:12:04.000
you should maybe follow me if you want to learn more about the project.

168
0:12:04.000 --> 0:12:06.000
But I started to write this paper.

169
0:12:06.000 --> 0:12:11.000
And it was mostly as some bro in chat was like, yo, why don't you just go rebuild system D?

170
0:12:11.000 --> 0:12:13.000
And I was just like, maybe I will.

171
0:12:13.000 --> 0:12:15.000
And so anyway, I ended up writing this paper.

172
0:12:15.000 --> 0:12:17.000
And so, well, here we are.

173
0:12:17.000 --> 0:12:19.000
And so the paper really grew.

174
0:12:19.000 --> 0:12:23.000
And it started to answer a bunch of questions about like, why should we should go write it and go?

175
0:12:23.000 --> 0:12:27.000
No, no, no, we should go write it in C, because C is going to be the most common language

176
0:12:27.000 --> 0:12:29.000
that will interface neatly with the kernel.

177
0:12:29.000 --> 0:12:31.000
And we can do EVPF probes and so on.

178
0:12:31.000 --> 0:12:33.000
No, no, no, no, we should go write it in Rust.

179
0:12:33.000 --> 0:12:34.000
And you can go look.

180
0:12:34.000 --> 0:12:35.000
There's a Google Doc.

181
0:12:35.000 --> 0:12:38.000
And it's just got all these comments of people from all over the internet and all over the industry

182
0:12:38.000 --> 0:12:40.000
arguing about what we should do.

183
0:12:40.000 --> 0:12:49.000
And eventually we settled on we want a lightweight node daemon and thus became the Aura runtime project.

184
0:12:49.000 --> 0:12:50.000
Okay.

185
0:12:50.000 --> 0:12:56.000
So this is where we shift from the conceptual what is Aura, how did we get here, what problems does it solve.

186
0:12:56.000 --> 0:13:00.000
And we start to get a little deeper into the code.

187
0:13:00.000 --> 0:13:05.000
So when we originally started the project, we started writing it in Go, the Go programming language.

188
0:13:05.000 --> 0:13:12.000
And there's two kind of predecessor projects that later turned into Aura, which is written in Rust.

189
0:13:12.000 --> 0:13:18.000
This first one that we call Aura Legacy, which up until about five, well, I guess 15 minutes ago now,

190
0:13:18.000 --> 0:13:23.000
but right before I walked into the room, this was a private GitHub repo.

191
0:13:23.000 --> 0:13:25.000
And I've gone ahead and actually opened it up.

192
0:13:25.000 --> 0:13:31.000
So if you want to go see the original code in Go, there's some really interesting things in there.

193
0:13:31.000 --> 0:13:38.000
We did some libp2p bit torrent style routing between nodes where you can build a mesh of nodes and things.

194
0:13:38.000 --> 0:13:47.000
But you can really see where this runtime daemon started and some of the original concepts that we were tinkering around with.

195
0:13:47.000 --> 0:13:51.000
Ultimately, though, we ran into a lot of the same problems that I ran into in Kubernetes,

196
0:13:51.000 --> 0:13:57.000
which was I needed to start recreating these objects and I needed to start reading some config,

197
0:13:57.000 --> 0:14:03.000
whether that be YAML or JSON or something similar, and then marshal that onto a struct in memory

198
0:14:03.000 --> 0:14:08.000
and then go and do arbitrary things with that, in our case, schedule a pod.

199
0:14:08.000 --> 0:14:12.000
And one of the things that was kind of outstanding in the back of my mind was what about access to libc?

200
0:14:12.000 --> 0:14:19.000
I knew as soon as we started scheduling containers and DMs, we absolutely were going to need native access to libc.

201
0:14:19.000 --> 0:14:24.000
Additionally, there's this project called NAML, which is basically Turing Complete Kubernetes Config.

202
0:14:24.000 --> 0:14:27.000
It's written in Go and it just uses the Go SDK.

203
0:14:27.000 --> 0:14:34.000
And that was yet another way of sort of validating this idea of we need to start making our system stronger

204
0:14:34.000 --> 0:14:39.000
and building stronger interfaces for teams to manage different parts of the stack.

205
0:14:39.000 --> 0:14:46.000
So those are the two sort of precursors to the Aura runtime as it exists today.

206
0:14:46.000 --> 0:14:53.000
But of course, writing in Go came with some challenges.

207
0:14:53.000 --> 0:14:57.000
The big one here is obviously native access to libc.

208
0:14:57.000 --> 0:15:01.000
We were going to be creating C groups against the Linux kernel.

209
0:15:01.000 --> 0:15:05.000
We definitely wanted to use the clone3 system call.

210
0:15:05.000 --> 0:15:12.000
And the container run times of today had some assumptions about how we were going to be executing the clone3 system call

211
0:15:12.000 --> 0:15:16.000
that, of course, I had to disagree with because, hi, have you met me?

212
0:15:16.000 --> 0:15:19.000
I have to disagree with everything.

213
0:15:19.000 --> 0:15:23.000
And we also wanted to implement some P trace functionality as well.

214
0:15:23.000 --> 0:15:29.000
So obviously, Go was going to give us some challenges here when it came to using C Go.

215
0:15:29.000 --> 0:15:38.000
So Rust became very exciting and definitely got a lot of attention very quickly as we were writing the Go side of things.

216
0:15:38.000 --> 0:15:41.000
We also wanted EVPF for networking.

217
0:15:41.000 --> 0:15:46.000
I personally want it for security and maybe for some other interesting service mesh ideas.

218
0:15:46.000 --> 0:15:51.000
But I do think that having EVPF for networking as a non-negotiable,

219
0:15:51.000 --> 0:15:56.000
we're definitely going to want to simplify what Kubernetes refers to as kubeproxy,

220
0:15:56.000 --> 0:16:00.000
that we can now invent our own name and hopefully simplify that later.

221
0:16:00.000 --> 0:16:02.000
But I digress.

222
0:16:02.000 --> 0:16:05.000
We also wanted some access to native virtualization library.

223
0:16:05.000 --> 0:16:07.000
So all the KVM stuff is written in C.

224
0:16:07.000 --> 0:16:13.000
And if you go look at the Firecracker code base, that is also written in Rust that vendors the KVM bindings.

225
0:16:13.000 --> 0:16:21.000
And so we knew we would want to access these three components and all three of these were going to be problematic with Go.

226
0:16:21.000 --> 0:16:29.000
Update as of about an hour ago, I went to the state of the Go room across the hall here.

227
0:16:29.000 --> 0:16:31.000
Did anybody else go to the Go talk this morning?

228
0:16:31.000 --> 0:16:34.000
Yeah, we got three or four hands up here.

229
0:16:34.000 --> 0:16:35.000
So this kind of pissed me off.

230
0:16:35.000 --> 0:16:40.000
Go has unwrap now as of 1.2.0.

231
0:16:40.000 --> 0:16:43.000
And they also freaking have.clone.

232
0:16:43.000 --> 0:16:46.000
And I was just like, bro, get off our keywords.

233
0:16:46.000 --> 0:16:50.000
This is totally like, this is our thing.

234
0:16:50.000 --> 0:16:56.000
So anyway, it's really exciting to see Go taking these concepts a little more seriously.

235
0:16:56.000 --> 0:17:01.000
And if you've ever written Rust before, who here has written unwrap in Rust?

236
0:17:01.000 --> 0:17:02.000
Put your hand.

237
0:17:02.000 --> 0:17:03.000
No, no, put your hands down.

238
0:17:03.000 --> 0:17:04.000
We're not supposed to do that.

239
0:17:04.000 --> 0:17:05.000
We're supposed to use other things.

240
0:17:05.000 --> 0:17:06.000
I don't know what we're supposed to use now.

241
0:17:06.000 --> 0:17:09.000
I just get so much shit on my Twitch stream every time I write unwrap.

242
0:17:09.000 --> 0:17:15.000
But yes, we do have unwrap and clone in Go now,

243
0:17:15.000 --> 0:17:19.000
which is just a strong indicator that we're likely doing something right with Rust.

244
0:17:19.000 --> 0:17:22.000
So anyway, I made the decision to move to Rust.

245
0:17:22.000 --> 0:17:25.000
And I didn't know very much about Rust when I made the decision.

246
0:17:25.000 --> 0:17:29.000
And I literally just started at the main function and said, we'll figure it out as we go.

247
0:17:29.000 --> 0:17:35.000
And I ordered the Rust book and just jumped in and started to write code with the hope

248
0:17:35.000 --> 0:17:39.000
of accessing kernel constructs and C groups and EBPF probes.

249
0:17:39.000 --> 0:17:43.000
So what could possibly go wrong here?

250
0:17:43.000 --> 0:17:46.000
OK, so how are we doing on time, by the way?

251
0:17:46.000 --> 0:17:47.000
We're 15 minutes in.

252
0:17:47.000 --> 0:17:49.000
OK, cool.

253
0:17:49.000 --> 0:17:54.000
So Rust, to help us solve the YAML problem,

254
0:17:54.000 --> 0:17:57.000
I suspect we're all familiar with feeding YAML to machines.

255
0:17:57.000 --> 0:18:00.000
We've all done this before at some point in our lifetime.

256
0:18:00.000 --> 0:18:04.000
OK, so this is a thing I do a lot working in large distributed systems.

257
0:18:04.000 --> 0:18:06.000
And I work with people who do this a lot.

258
0:18:06.000 --> 0:18:10.000
And we do it so much, we've tried to get really good at doing it.

259
0:18:10.000 --> 0:18:13.000
And that, I think, that's an interesting discussion.

260
0:18:13.000 --> 0:18:18.000
So in my opinion, so warning, Chris Nova opinions here,

261
0:18:18.000 --> 0:18:24.000
in my opinion, all config ultimately is going to drift towards Turing completion.

262
0:18:24.000 --> 0:18:28.000
So I see this C++ templates.

263
0:18:28.000 --> 0:18:29.000
Anybody?

264
0:18:29.000 --> 0:18:30.000
Anybody C++ templates?

265
0:18:30.000 --> 0:18:31.000
OK.

266
0:18:31.000 --> 0:18:37.000
Helm charts, customizing Kubernetes, any of the templating rendering languages

267
0:18:37.000 --> 0:18:39.000
that you see in web dev and front-end work,

268
0:18:39.000 --> 0:18:43.000
there's all kinds of interesting Python libraries that will allow you

269
0:18:43.000 --> 0:18:46.000
to interpolate your config and so on.

270
0:18:46.000 --> 0:18:51.000
In my opinion, a good balance is kind of something like Bash that is Turing complete,

271
0:18:51.000 --> 0:18:53.000
but it just comes with some strong guarantees.

272
0:18:53.000 --> 0:18:57.000
And so I knew very quickly that I didn't want to be feeding YAML to Aura.

273
0:18:57.000 --> 0:19:00.000
I definitely didn't want to recreate this idea of, like,

274
0:19:00.000 --> 0:19:03.000
we're going to have to manage 1,000 pieces of YAML

275
0:19:03.000 --> 0:19:06.000
because we have 1,000 different nodes.

276
0:19:06.000 --> 0:19:10.000
So I wanted to explore more about what are some options that we have here,

277
0:19:10.000 --> 0:19:13.000
so we're not just feeding YAML to machines anymore.

278
0:19:13.000 --> 0:19:17.000
So thus became this really interesting project of mine.

279
0:19:17.000 --> 0:19:22.000
We'll see if this pans out, which is this binary called AuraScript.

280
0:19:22.000 --> 0:19:25.000
So AuraScript is a Rust binary.

281
0:19:25.000 --> 0:19:27.000
We have it compiling with muzzle today,

282
0:19:27.000 --> 0:19:33.000
and it embeds all of the connection logic for a single machine.

283
0:19:33.000 --> 0:19:37.000
And so we'll talk more about the semantics of AuraScript in a second.

284
0:19:37.000 --> 0:19:42.000
But ultimately, what you need to understand to kind of get the initial motivation here

285
0:19:42.000 --> 0:19:49.000
is that this aims to be an alternative to managing YAML at scale.

286
0:19:49.000 --> 0:19:55.000
So I found this really fascinating TypeScript runtime called Dino.

287
0:19:55.000 --> 0:19:57.000
Have folks heard of Dino before?

288
0:19:57.000 --> 0:19:58.000
Can I swear in here?

289
0:19:58.000 --> 0:20:01.000
I effin love Dino.

290
0:20:01.000 --> 0:20:05.000
I'm sorry. I really like this project.

291
0:20:05.000 --> 0:20:07.000
If you want a good example of, like,

292
0:20:07.000 --> 0:20:10.000
hey, I just want to see a really successful Rust project

293
0:20:10.000 --> 0:20:12.000
that has a really strong community,

294
0:20:12.000 --> 0:20:15.000
I would encourage you to just go look at the Dino project.

295
0:20:15.000 --> 0:20:16.000
I think their code is beautiful.

296
0:20:16.000 --> 0:20:18.000
I think what it does is beautiful.

297
0:20:18.000 --> 0:20:21.000
I think the way that they manage the project is beautiful.

298
0:20:21.000 --> 0:20:23.000
It's just a really good quality project,

299
0:20:23.000 --> 0:20:25.000
and it solves a problem for us with Aura.

300
0:20:25.000 --> 0:20:28.000
And so Dino is basically, it's a runtime for TypeScript,

301
0:20:28.000 --> 0:20:30.000
and it's written in Rust.

302
0:20:30.000 --> 0:20:32.000
And the way the project is set up,

303
0:20:32.000 --> 0:20:36.000
that you can go and you can add your own custom interpretive logic,

304
0:20:36.000 --> 0:20:38.000
and you can build fancy things into the binary,

305
0:20:38.000 --> 0:20:43.000
and you can do things with the TypeScript interpretation at runtime,

306
0:20:43.000 --> 0:20:47.000
which is precisely what we needed to do with Aura.

307
0:20:47.000 --> 0:20:49.000
So here is the model now.

308
0:20:49.000 --> 0:20:52.000
So instead of feeding YAML to a single node,

309
0:20:52.000 --> 0:20:55.000
we now have this higher order set of libraries

310
0:20:55.000 --> 0:20:58.000
that we can statically compile into a binary,

311
0:20:58.000 --> 0:21:02.000
and we can interpret it directly on a machine.

312
0:21:02.000 --> 0:21:06.000
So in order for you to interface with an Aura node or a set of nodes,

313
0:21:06.000 --> 0:21:08.000
all you need is one binary,

314
0:21:08.000 --> 0:21:12.000
mtlsconfig, and then whatever TypeScript you want to write.

315
0:21:12.000 --> 0:21:16.000
And this is an alternative to, like, any of the Nomad command line tools,

316
0:21:16.000 --> 0:21:18.000
or the Mesos command line tools,

317
0:21:18.000 --> 0:21:21.000
or the Kubernetes kubectl kubectl command line tool.

318
0:21:21.000 --> 0:21:25.000
And now you can just write it all directly in TypeScript.

319
0:21:25.000 --> 0:21:31.000
So this is actually a concrete example of what would be,

320
0:21:31.000 --> 0:21:34.000
what systemd would call a unit file,

321
0:21:34.000 --> 0:21:36.000
what Kubernetes would call a manifest,

322
0:21:36.000 --> 0:21:40.000
and what Aura just calls a freaking TypeScript file,

323
0:21:40.000 --> 0:21:43.000
because we don't have fancy names for our stuff yet.

324
0:21:43.000 --> 0:21:46.000
So you can see here at the top,

325
0:21:46.000 --> 0:21:49.000
we basically contact the Aura standard library,

326
0:21:49.000 --> 0:21:51.000
we get a new client,

327
0:21:51.000 --> 0:21:55.000
and then we can allocate this thing called a cell.

328
0:21:55.000 --> 0:21:58.000
A cell is basically an abstraction for a C group.

329
0:21:58.000 --> 0:22:00.000
We cordon off a section of the system,

330
0:22:00.000 --> 0:22:02.000
and we say, like, we want to use a certain percentage

331
0:22:02.000 --> 0:22:04.000
of the available CPUs on a node,

332
0:22:04.000 --> 0:22:07.000
and I want it to only let processes run,

333
0:22:07.000 --> 0:22:09.000
in this case, for 0.4 seconds,

334
0:22:09.000 --> 0:22:11.000
and then we'll use the kernel to just kill the process

335
0:22:11.000 --> 0:22:13.000
if it runs longer than that.

336
0:22:13.000 --> 0:22:16.000
And so the first thing we would do is we would allocate that,

337
0:22:16.000 --> 0:22:20.000
which is an improvement over Kubernetes as it exists today,

338
0:22:20.000 --> 0:22:22.000
because we can allocate resources before we actually

339
0:22:22.000 --> 0:22:24.000
start anything in that area,

340
0:22:24.000 --> 0:22:28.000
and then we can go ahead and actually start whatever we want.

341
0:22:28.000 --> 0:22:31.000
And so you can see I simplified the example just for today,

342
0:22:31.000 --> 0:22:34.000
but it's just, it's remote command injection as a service.

343
0:22:34.000 --> 0:22:37.000
So this whole talk was just basically like how to go

344
0:22:37.000 --> 0:22:42.000
and run a bash command on a server.

345
0:22:42.000 --> 0:22:45.000
And so now you can express your commands and similar primitives

346
0:22:45.000 --> 0:22:50.000
that you would see in other run times directly in TypeScript.

347
0:22:50.000 --> 0:22:54.000
The interesting thing here is TypeScript is just natively

348
0:22:54.000 --> 0:22:59.000
more expressive than a lot of the AML things that we see today.

349
0:22:59.000 --> 0:23:01.000
In this case, we can actually do math,

350
0:23:01.000 --> 0:23:04.000
but I'm sure you can imagine you can do other things as well.

351
0:23:04.000 --> 0:23:07.000
You can access logic, loops, if statements,

352
0:23:07.000 --> 0:23:10.000
there's if branching, and so on.

353
0:23:10.000 --> 0:23:13.000
And so we were able to actually solve some of these like

354
0:23:13.000 --> 0:23:15.000
templatey, rendery style problems by just doing things

355
0:23:15.000 --> 0:23:19.000
natively in a well-known and easy to understand language

356
0:23:19.000 --> 0:23:22.000
such as TypeScript.

357
0:23:22.000 --> 0:23:24.000
So patterns started to emerge.

358
0:23:24.000 --> 0:23:28.000
So Rust gave us the ability to generate the TypeScript binary

359
0:23:28.000 --> 0:23:31.000
with all of the magic behind the scenes

360
0:23:31.000 --> 0:23:34.000
MTLS security config that we wanted.

361
0:23:34.000 --> 0:23:37.000
And so now the conversation was a little more like this,

362
0:23:37.000 --> 0:23:40.000
which is how do I manage a small set of TypeScript,

363
0:23:40.000 --> 0:23:43.000
and it's much more flexible, and you can start to actually

364
0:23:43.000 --> 0:23:46.000
express things the way that we used to and just express

365
0:23:46.000 --> 0:23:49.000
things statically, and then you can have all of your Turing

366
0:23:49.000 --> 0:23:52.000
complete logical components below, and you can mix

367
0:23:52.000 --> 0:23:56.000
and match these however you want.

368
0:23:56.000 --> 0:24:02.000
Okay, so in addition to addressing the YAML problem

369
0:24:02.000 --> 0:24:07.000
with DNO and TypeScript, Rust also helped us to solve

370
0:24:07.000 --> 0:24:10.000
the Sidecar problem, and by us I mean this is our hope

371
0:24:10.000 --> 0:24:14.000
as we operate our Mastodon servers and our various other

372
0:24:14.000 --> 0:24:17.000
ridiculous side projects that we operate,

373
0:24:17.000 --> 0:24:20.000
both in my basement and in a colo in Germany.

374
0:24:20.000 --> 0:24:24.000
So talking about Sidecars, who here knows what a Sidecar is?

375
0:24:24.000 --> 0:24:27.000
Show of hands. Okay, most folks do.

376
0:24:27.000 --> 0:24:30.000
Okay, so a Sidecar that is always available with the same

377
0:24:30.000 --> 0:24:33.000
features as a host. So this is going to sound a little bit

378
0:24:33.000 --> 0:24:36.000
weird, and this slide's going to look a little bit weird,

379
0:24:36.000 --> 0:24:38.000
but just bear with me as we kind of like unpack what's

380
0:24:38.000 --> 0:24:40.000
actually going on here.

381
0:24:40.000 --> 0:24:44.000
What we want, that I don't think we're talking about,

382
0:24:44.000 --> 0:24:48.000
is that sentence. I actually think what we want is we want

383
0:24:48.000 --> 0:24:52.000
a Sidecar to sit along our applications that does literally

384
0:24:52.000 --> 0:24:56.000
the exact same things we have to do on a given host whenever

385
0:24:56.000 --> 0:25:00.000
we're managing these workloads at scale.

386
0:25:00.000 --> 0:25:04.000
As I began looking into writing Sidecars at the host level,

387
0:25:04.000 --> 0:25:07.000
I began drilling deeper and deeper into the C programming

388
0:25:07.000 --> 0:25:10.000
language as I was writing this in Rust and just made the

389
0:25:10.000 --> 0:25:13.000
connection that memory safety was going to be key because

390
0:25:13.000 --> 0:25:16.000
we're going to be running these demons right alongside of

391
0:25:16.000 --> 0:25:21.000
your workload. And so unpacking the need to do this really

392
0:25:21.000 --> 0:25:26.000
helps you understand why we shifted over to Rust.

393
0:25:26.000 --> 0:25:31.000
So again, another Chris Nova opinion. Any sufficiently

394
0:25:31.000 --> 0:25:35.000
mature infrastructure service will evolve into a Sidecar.

395
0:25:35.000 --> 0:25:38.000
So if you have done any sort of structured logging, in my

396
0:25:38.000 --> 0:25:41.000
opinion, if you will continue to build structured logging

397
0:25:41.000 --> 0:25:44.000
and you will continue to ship logs, that will eventually

398
0:25:44.000 --> 0:25:46.000
turn into a Sidecar that you're going to want to go run

399
0:25:46.000 --> 0:25:48.000
beside your app so you have this transparent logging

400
0:25:48.000 --> 0:25:51.000
experience. You can rinse and repeat that paradigm for

401
0:25:51.000 --> 0:25:54.000
pretty much anything, secrets, authentication data, and so

402
0:25:54.000 --> 0:25:59.000
on. And so I started to see these patterns kind of surface.

403
0:25:59.000 --> 0:26:02.000
And very specifically, I started to look at how would I

404
0:26:02.000 --> 0:26:06.000
solve these with Rust. And as it turns out, the Rust

405
0:26:06.000 --> 0:26:10.000
ecosystem had a plethora of pleasant surprises for me as I

406
0:26:10.000 --> 0:26:14.000
started to explore what putting some of these features into

407
0:26:14.000 --> 0:26:18.000
a binary would look like. Logging was boring because we

408
0:26:18.000 --> 0:26:22.000
could just use Tokyo streams. Auth N and Auth Z was boring

409
0:26:22.000 --> 0:26:25.000
because all I had to do was just use the Rust derived

410
0:26:25.000 --> 0:26:28.000
primitives to just start applying Auth Z to each of our

411
0:26:28.000 --> 0:26:31.000
units in the source code. Identity was boring because I

412
0:26:31.000 --> 0:26:34.000
didn't even get to fight with open SSL anymore. We just had

413
0:26:34.000 --> 0:26:37.000
to use Rust TLS and that was easy. And so the network was

414
0:26:37.000 --> 0:26:41.000
also easy because we had native access to Linux and libc so

415
0:26:41.000 --> 0:26:45.000
we could just very boringly schedule a Linux device and we

416
0:26:45.000 --> 0:26:49.000
got a Linux device and it was pretty straightforward. So we

417
0:26:49.000 --> 0:26:55.000
were able to create this at the node and now my question was

418
0:26:55.000 --> 0:26:59.000
how do we bring this into the workload level at scale and I

419
0:26:59.000 --> 0:27:02.000
think this is where most of the conversations you start

420
0:27:02.000 --> 0:27:06.000
talking about things like Istio and service meshes and

421
0:27:06.000 --> 0:27:10.000
structured logging and so forth. And I actually think that

422
0:27:10.000 --> 0:27:14.000
we can simplify that conversation too. And so what we

423
0:27:14.000 --> 0:27:18.000
were able to do with Aura is we just spawned the root daemon

424
0:27:18.000 --> 0:27:21.000
and used that as the new PID one in any of our nested

425
0:27:21.000 --> 0:27:25.000
isolation zones. And when I say spawn I very directly mean

426
0:27:25.000 --> 0:27:28.000
like we literally read the byte code from the kernel and

427
0:27:28.000 --> 0:27:32.000
we build an image at runtime with the byte for byte, the

428
0:27:32.000 --> 0:27:35.000
same byte code that's running on the host and then we can

429
0:27:35.000 --> 0:27:38.000
just go and execute whatever we want against the same API

430
0:27:38.000 --> 0:27:42.000
as the original host runs and all of this is memory safe.

431
0:27:42.000 --> 0:27:46.000
So I can put this right next to your application in the same

432
0:27:46.000 --> 0:27:49.000
name spaces running in a container or running in a

433
0:27:49.000 --> 0:27:52.000
virtual machine and there's a relatively low risk of any

434
0:27:52.000 --> 0:27:57.000
sort of binary exploitation at scale. So here's a model of

435
0:27:57.000 --> 0:28:02.000
what that looks like. So on the left big side here we have

436
0:28:02.000 --> 0:28:05.000
the Aura host daemon and on the right we have the three

437
0:28:05.000 --> 0:28:08.000
types of isolation zones that you can run with a daemon.

438
0:28:08.000 --> 0:28:12.000
You have a cell sandbox which is effectively a C group, a

439
0:28:12.000 --> 0:28:15.000
pod sandbox which is a group of containers running in

440
0:28:15.000 --> 0:28:19.000
unique Linux name spaces and a virtual machine which is

441
0:28:19.000 --> 0:28:23.000
effectively a container with a kernel and some virtualization

442
0:28:23.000 --> 0:28:27.000
technology. All of this is possible with Rust natively

443
0:28:27.000 --> 0:28:31.000
and all of this was made possible by spawning the binary

444
0:28:31.000 --> 0:28:36.000
and creating these nested isolation zones at run time.

445
0:28:36.000 --> 0:28:39.000
Additionally, Rust was able to help solve the untrusted

446
0:28:39.000 --> 0:28:43.000
workload problem because of the memory safety and that Rust

447
0:28:43.000 --> 0:28:47.000
offers and because of this really interesting model that

448
0:28:47.000 --> 0:28:51.000
we have right here. So this is a zoomed in model that might

449
0:28:51.000 --> 0:28:54.000
look familiar if you've ever done any container escapes

450
0:28:54.000 --> 0:28:57.000
before and in this model basically what we're saying is

451
0:28:57.000 --> 0:29:01.000
we're replacing any sort of like pause or initialization

452
0:29:01.000 --> 0:29:05.000
sequence in an isolation zone with the same daemon we run

453
0:29:05.000 --> 0:29:08.000
on the host. So I think the Rust binary for Aura right now

454
0:29:08.000 --> 0:29:12.000
is about 40 megabytes and we can just copy that into a

455
0:29:12.000 --> 0:29:14.000
container and run that alongside your application. So it's

456
0:29:14.000 --> 0:29:20.000
a relatively small application run time that will sit right

457
0:29:20.000 --> 0:29:26.000
alongside of your app. So managing memory from

458
0:29:26.000 --> 0:29:31.000
MTLS and AuraD. So as I'm writing Rust, one of the things

459
0:29:31.000 --> 0:29:33.000
I notice is I start paying attention to memory management

460
0:29:33.000 --> 0:29:37.000
more. Every time I try to clone something or the freakin'

461
0:29:37.000 --> 0:29:40.000
borrow checker yells at me that kind of like is a small like

462
0:29:40.000 --> 0:29:44.000
grim reminder of my roots as a C developer. This is an

463
0:29:44.000 --> 0:29:47.000
interesting takeaway. The only memory that we need to share

464
0:29:47.000 --> 0:29:50.000
that multiple parts of the system have access to in this

465
0:29:50.000 --> 0:29:54.000
entire model whether we're creating containers or VMs is

466
0:29:54.000 --> 0:29:57.000
the shared MTLS config. So this is the only bit of shared

467
0:29:57.000 --> 0:30:01.000
memory that we really have to manage and Rust very clearly

468
0:30:01.000 --> 0:30:04.000
called that out and to be candid I don't think I would be

469
0:30:04.000 --> 0:30:07.000
able to be as comfortable with this model if I was doing

470
0:30:07.000 --> 0:30:11.000
this in something like Go. So Rust was able to help us

471
0:30:11.000 --> 0:30:15.000
solve the maintainability problem. So did somebody say

472
0:30:15.000 --> 0:30:19.000
Rust macros? So we have a really brilliant guy, Future

473
0:30:19.000 --> 0:30:24.000
Highway, who helps us work on the project and Future Highway

474
0:30:24.000 --> 0:30:27.000
is our resident macro guy. Does everybody here have a

475
0:30:27.000 --> 0:30:32.000
macro guy on your team? Because you should. He has made

476
0:30:32.000 --> 0:30:35.000
things a lot simpler for us. So one of the things we

477
0:30:35.000 --> 0:30:38.000
struggled with with Go and Kubernetes specifically was like

478
0:30:38.000 --> 0:30:42.000
how do we generate objects with unique logic? Rust macros

479
0:30:42.000 --> 0:30:45.000
were a solution to this for us. So if you've ever looked at

480
0:30:45.000 --> 0:30:47.000
the Kubernetes code base you can see we've created these

481
0:30:47.000 --> 0:30:50.000
things called CRDs that started out as third party resources

482
0:30:50.000 --> 0:30:54.000
and we've built this entire bespoke API machinery system

483
0:30:54.000 --> 0:30:58.000
that basically is a glorified macro system that allows us

484
0:30:58.000 --> 0:31:03.000
to generate Go in the project. So we're allowed to use Rust

485
0:31:03.000 --> 0:31:06.000
macros now and it's a very simple model in the code base.

486
0:31:06.000 --> 0:31:09.000
We basically have a combinatorics problem where we're

487
0:31:09.000 --> 0:31:11.000
able to map the different primitives to the different

488
0:31:11.000 --> 0:31:15.000
logical systems that are unique to us and we can generate

489
0:31:15.000 --> 0:31:21.000
our source code as needed. And so our source code ends up

490
0:31:21.000 --> 0:31:25.000
looking like this, which I think we've successfully achieved

491
0:31:25.000 --> 0:31:30.000
boring for a low level runtime. This is a fairly straightforward

492
0:31:30.000 --> 0:31:33.000
call and then we can be confident that the code it generates

493
0:31:33.000 --> 0:31:37.000
is unique to the project and encapsulates all of our

494
0:31:37.000 --> 0:31:41.000
concerns as maintainers. So really the whole conversation

495
0:31:41.000 --> 0:31:44.000
now is just the proto conversation. Everything can be

496
0:31:44.000 --> 0:31:47.000
generated by Rust macros. The whole project really is

497
0:31:47.000 --> 0:31:50.000
pretty much on auto-gen at this point. You can just go

498
0:31:50.000 --> 0:31:53.000
introduce a new field in the API and then you can spit out

499
0:31:53.000 --> 0:31:56.000
a new client. It'll plumb itself into the runtime. It'll

500
0:31:56.000 --> 0:31:58.000
plumb itself into the Aura Script library and everything

501
0:31:58.000 --> 0:32:03.000
is given to us for free just because of macros in Rust.

502
0:32:03.000 --> 0:32:07.000
And so this is our code path and the way that we're able

503
0:32:07.000 --> 0:32:10.000
to take advantage of macros. We do a lot of manual work.

504
0:32:10.000 --> 0:32:13.000
We fight with the borrow checker. We make some improvements.

505
0:32:13.000 --> 0:32:16.000
And then we get done and we encapsulate it into a macro.

506
0:32:16.000 --> 0:32:19.000
And we can simplify our code path by just replacing all of

507
0:32:19.000 --> 0:32:23.000
that with a macro after we've been done. And so this is

508
0:32:23.000 --> 0:32:27.000
the Aura project as it exists today, which again I'm very

509
0:32:27.000 --> 0:32:31.000
stoked to say that this is a very boring exercise.

510
0:32:31.000 --> 0:32:35.000
So a quick update and then I'll be done with my talk here.

511
0:32:35.000 --> 0:32:39.000
There's a few components, all of which are written in Rust

512
0:32:39.000 --> 0:32:42.000
here. Number one, the Aura D daemon is the main static

513
0:32:42.000 --> 0:32:44.000
binary that's written in Rust and compiled with muzzle.

514
0:32:44.000 --> 0:32:47.000
So we can ship that without any of the shared objects on

515
0:32:47.000 --> 0:32:51.000
the host directly into an isolation zone. AER is

516
0:32:51.000 --> 0:32:54.000
completely generated from proto client. So this is exciting.

517
0:32:54.000 --> 0:32:59.000
We can actually call a GRPC API directly from the client.

518
0:32:59.000 --> 0:33:02.000
We don't have to do any of the runtime plumbing. So if

519
0:33:02.000 --> 0:33:06.000
we add a bool to the proto file, we get dash dash bool

520
0:33:06.000 --> 0:33:09.000
directly in the client compiled for free without typing

521
0:33:09.000 --> 0:33:12.000
a single line of code. So this is a very exciting primitive

522
0:33:12.000 --> 0:33:15.000
for us. So we can just begin to have API conversations and

523
0:33:15.000 --> 0:33:18.000
not necessarily care about the internals of the program

524
0:33:18.000 --> 0:33:21.000
anymore. Aura script is completely generated and we

525
0:33:21.000 --> 0:33:25.000
have this exciting project down here, which is AE, which is

526
0:33:25.000 --> 0:33:30.000
an alternative command line client written in Go.

527
0:33:30.000 --> 0:33:33.000
So ultimately the lesson here is Rust was able to help us

528
0:33:33.000 --> 0:33:36.000
solve the boring problem. We have a very complicated, very

529
0:33:36.000 --> 0:33:39.000
obscure piece of technology that is you don't really have

530
0:33:39.000 --> 0:33:43.000
to do much to work on it anymore. Most of it's on autopilot

531
0:33:43.000 --> 0:33:46.000
at this point and most of the conversations are very

532
0:33:46.000 --> 0:33:49.000
philosophical in nature and not necessarily about how to

533
0:33:49.000 --> 0:33:52.000
implement things in the software.

534
0:33:52.000 --> 0:33:55.000
So takeaways about the project. Aura is completely

535
0:33:55.000 --> 0:33:58.000
stateless. So you can restart a node and it's basically

536
0:33:58.000 --> 0:34:01.000
empty until you push config to it, which means all of our

537
0:34:01.000 --> 0:34:04.000
systems are declarative like NixOS now and you can just

538
0:34:04.000 --> 0:34:07.000
pass things like TypeScript or JSON to them and it makes it

539
0:34:07.000 --> 0:34:11.000
easy to manage things like containers.

540
0:34:11.000 --> 0:34:14.000
Next we have some to-dos for the project and I would

541
0:34:14.000 --> 0:34:17.000
encourage you all to get involved. And if you want to

542
0:34:17.000 --> 0:34:21.000
see a demo of all this, I'll be out here in the hallway

543
0:34:21.000 --> 0:34:24.000
after the talk and you can come and you can track me

544
0:34:24.000 --> 0:34:28.000
down and I'm happy to give you a demo. So anyway, I think

545
0:34:28.000 --> 0:34:31.000
we have a few minutes for questions and five minutes for

546
0:34:31.000 --> 0:34:34.000
questions. So I'll take questions and if you want to

547
0:34:34.000 --> 0:34:38.000
get involved, here's how to get involved and I'm Chris

548
0:34:38.000 --> 0:34:40.000
Nova. Please clap.

549
0:34:40.000 --> 0:34:56.000
You mentioned the size of the binary being 40 megabytes.

550
0:34:56.000 --> 0:34:59.000
Is that with size optimization or no?

551
0:34:59.000 --> 0:35:01.000
Sorry, say that again.

552
0:35:01.000 --> 0:35:04.000
Is the size of the binary at 40 megabytes with size

553
0:35:04.000 --> 0:35:06.000
optimization applied already or not?

554
0:35:06.000 --> 0:35:09.000
No, that's completely unoptimized. That is just straight

555
0:35:09.000 --> 0:35:15.000
out of the compiler without any aftermarket tuning.

556
0:35:15.000 --> 0:35:18.000
Amazing talk. Quick question. So if I want to have just

557
0:35:18.000 --> 0:35:22.000
enough Linux to like pixie boot into this thing, do you guys

558
0:35:22.000 --> 0:35:25.000
have any templates because it feels like a shame to run it

559
0:35:25.000 --> 0:35:28.000
on something like Crel. I just need like enough of Linux to

560
0:35:28.000 --> 0:35:30.000
just pixie boot into that.

561
0:35:30.000 --> 0:35:33.000
Yeah, so the question is basically can we pixie boot this

562
0:35:33.000 --> 0:35:36.000
and then you mentioned Crel. Where we're going we don't

563
0:35:36.000 --> 0:35:41.000
need Red Hat. So I guess what I would say is in theory all

564
0:35:41.000 --> 0:35:45.000
you need to run is a static Linux kernel and Aura and a

565
0:35:45.000 --> 0:35:49.000
network connection and some NTLS config. And so everything

566
0:35:49.000 --> 0:35:52.000
else at that point, all of your packages, your services,

567
0:35:52.000 --> 0:36:03.000
your commands are passed to it via the API.

568
0:36:03.000 --> 0:36:07.000
Hi. You mentioned that you use a lot of macros. I've also

569
0:36:07.000 --> 0:36:11.000
run into problems where you have a combinatorial explosion

570
0:36:11.000 --> 0:36:16.000
of templates in C++ speak or something like that.

571
0:36:16.000 --> 0:36:19.000
What are your thoughts on generics for generating some of

572
0:36:19.000 --> 0:36:22.000
this rather than macros in order to be a bit more type safe

573
0:36:22.000 --> 0:36:24.000
I suppose?

574
0:36:24.000 --> 0:36:27.000
Personally I got a little drunk with generics, I'm not going

575
0:36:27.000 --> 0:36:30.000
to lie. When I first moved over from Go because I was just

576
0:36:30.000 --> 0:36:34.000
so excited about it, the reason I like macros is because we

577
0:36:34.000 --> 0:36:37.000
can add logic to them. So we have like, to give you an

578
0:36:37.000 --> 0:36:40.000
example, we have containers and we have VMs. So we'll have a

579
0:36:40.000 --> 0:36:44.000
section of the macro dedicated just to VMs that manages the

580
0:36:44.000 --> 0:36:47.000
kernel. And that's irrelevant to the container systems in

581
0:36:47.000 --> 0:36:50.000
the project because containers run on the host kernel. And so

582
0:36:50.000 --> 0:36:53.000
we can embed those small branches directly into the

583
0:36:53.000 --> 0:36:56.000
macro code so that macros generate slightly different

584
0:36:56.000 --> 0:37:00.000
outputs based off of the inputs that are given to them. So

585
0:37:00.000 --> 0:37:03.000
for Aura, when you're dealing with similar systems of code

586
0:37:03.000 --> 0:37:08.000
that have small nuances like we are, macros really, in my

587
0:37:08.000 --> 0:37:12.000
opinion, are the way to go. Did I answer your question?

588
0:37:12.000 --> 0:37:16.000
Looks like?

589
0:37:16.000 --> 0:37:21.000
Simple question. So can I actually give the configuration

590
0:37:21.000 --> 0:37:26.000
instead of our script or type of script just in Rust?

591
0:37:26.000 --> 0:37:32.000
Yeah, of course. So we have this Rust client here. It's

592
0:37:32.000 --> 0:37:36.000
basically a Rust SDK. And then we have a tool called AER

593
0:37:36.000 --> 0:37:39.000
which takes it a step further and is actually automatically

594
0:37:39.000 --> 0:37:43.000
generated with macros. And it's a compiled binary that you

595
0:37:43.000 --> 0:37:46.000
can just use from the command line. So you can just type

596
0:37:46.000 --> 0:37:50.000
commands directly into it and it will run against the server

597
0:37:50.000 --> 0:37:52.000
on the back end.

598
0:37:52.000 --> 0:37:54.000
Yeah, there's also an SDK. So you could write your own Rust

599
0:37:54.000 --> 0:37:58.000
code. And it's gRPC. So you could generate, you could write

600
0:37:58.000 --> 0:38:01.000
it in Go, and we do, and you could write it in Python or Ruby

601
0:38:01.000 --> 0:38:04.000
or realistically anything, any client you want.

602
0:38:04.000 --> 0:38:08.000
Hi. I was wondering when you talk about the remote API, have

603
0:38:08.000 --> 0:38:13.000
you considered a future direction to make this a unique

604
0:38:13.000 --> 0:38:14.000
kernel?

605
0:38:14.000 --> 0:38:18.000
A unique kernel. I have a slide for this. So I added a bunch

606
0:38:18.000 --> 0:38:21.000
of FAQ slides to the end because I knew that we were going to

607
0:38:21.000 --> 0:38:25.000
get all these good questions. The answer is it depends. Hold

608
0:38:25.000 --> 0:38:28.000
on. Let's see if I can't find it. You guys get to see. There

609
0:38:28.000 --> 0:38:33.000
it is. It depends. What does unique kernel mean to you? I

610
0:38:33.000 --> 0:38:35.000
think the most minimal system we could do would be a Linux

611
0:38:35.000 --> 0:38:39.000
kernel as it exists today, like good old-fashioned stock Linux

612
0:38:39.000 --> 0:38:41.000
giant make file to hold nine yards, and then the or a D

613
0:38:41.000 --> 0:38:44.000
daemon. And that would be the minimal system. Everything else

614
0:38:44.000 --> 0:38:47.000
you would need to pass to it at run time.

615
0:38:47.000 --> 0:38:55.000
I think we have time for about one more question.

616
0:38:55.000 --> 0:38:58.000
So you said it doesn't do any higher order scheduling. I

617
0:38:58.000 --> 0:39:01.000
guess I'm kind of curious what, if you want to do things like

618
0:39:01.000 --> 0:39:04.000
resilience or steering or, you know, if the job dies, bring

619
0:39:04.000 --> 0:39:07.000
up what are people typically using with ora?

620
0:39:07.000 --> 0:39:10.000
So ora is still very new. I think that my hope for the

621
0:39:10.000 --> 0:39:13.000
project is kind of like the same hope I had with my book,

622
0:39:13.000 --> 0:39:17.000
like solve the lower layer first and then that is going to

623
0:39:17.000 --> 0:39:20.000
open the door for higher order conversations in the future.

624
0:39:20.000 --> 0:39:23.000
My hope is that there's a whole ecosystem of schedulers,

625
0:39:23.000 --> 0:39:25.000
right? You change your scheduler, you change your

626
0:39:25.000 --> 0:39:30.000
socks. Well, maybe not that often, but the point would be

627
0:39:30.000 --> 0:39:33.000
that that's very specific to the needs of the current

628
0:39:33.000 --> 0:39:36.000
organization that's working on it. And I would hope that we

629
0:39:36.000 --> 0:39:39.000
can still use the Kubernetes scheduler or the Nomad

630
0:39:39.000 --> 0:39:43.000
scheduler to schedule jobs on ora.

631
0:39:43.000 --> 0:39:46.000
I know there's also some machine learning folks who have

632
0:39:46.000 --> 0:39:48.000
some data resiliency problems that are interested in ora

633
0:39:48.000 --> 0:39:54.000
right now and plan on using some weird global mesh that

634
0:39:54.000 --> 0:39:57.000
will do a peer-to-peer network around the world, kind of

635
0:39:57.000 --> 0:40:00.000
bit torrent, and then they intend to use ora for that.

636
0:40:00.000 --> 0:40:03.000
So I think there's some opportunities there. The project

637
0:40:03.000 --> 0:40:07.000
itself won't ever have an opinion on a scheduler. Maybe I

638
0:40:07.000 --> 0:40:09.000
personally will start another project to do that in the

639
0:40:09.000 --> 0:40:12.000
future or something, but this is the scope for now.

640
0:40:12.000 --> 0:40:15.000
So that's all the time we have.

641
0:40:15.000 --> 0:40:16.000
Okay.

642
0:40:16.000 --> 0:40:31.000
Can we hear it again?

