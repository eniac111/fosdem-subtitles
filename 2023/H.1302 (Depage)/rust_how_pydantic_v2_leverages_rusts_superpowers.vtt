WEBVTT

00:00.000 --> 00:13.880
So, we have Samuel here to talk about Pydantic 2 and how it leverages Rust superpowers.

00:13.880 --> 00:16.080
Thank you very much.

00:16.080 --> 00:18.080
Can you hear me at the back?

00:18.080 --> 00:19.080
Great.

00:19.080 --> 00:21.800
So, a bit about me.

00:21.800 --> 00:22.800
I'm Samuel.

00:22.800 --> 00:26.400
I've been a software developer for 10 years, among other things.

00:26.400 --> 00:30.480
I've been doing open source quite a lot for the last five years.

00:30.480 --> 00:35.680
Mostly Python projects, but moving a bit into Rust over the last few years.

00:35.680 --> 00:41.000
The most high profile Python project that I maintain is Pydantic, which I started back

00:41.000 --> 00:47.640
in 2017 and has subsequently kind of taken over my life.

00:47.640 --> 00:50.960
I've been working on it full time for the last year.

00:50.960 --> 00:55.160
So, what I'm going to talk about today.

00:55.160 --> 01:00.760
I'm going to give you an introduction to Pydantic, some hype numbers for vanity, but also for

01:00.760 --> 01:05.000
some context of why making Pydantic better is worthwhile.

01:05.000 --> 01:08.600
I'm going to explain why I decided to rebuild Pydantic completely.

01:08.600 --> 01:14.680
I'm going to talk about how I've done that with Rust and I guess most importantly why

01:14.680 --> 01:16.760
doing it in Rust is the right choice.

01:16.760 --> 01:20.320
I guess I'm preaching to the converted, but hey.

01:20.320 --> 01:25.720
What I'm not going to do is hello world, this is how you would build a Python extension

01:25.720 --> 01:26.720
in Rust.

01:26.720 --> 01:28.000
There were lots of other talks on that.

01:28.000 --> 01:29.000
They're great.

01:29.000 --> 01:30.920
And also the PYO3 documentation is amazing.

01:30.920 --> 01:38.000
So I think it's more interesting to go into a bit of depth on the challenges, the advantages,

01:38.000 --> 01:42.240
than just to do the hello world example again.

01:42.240 --> 01:43.240
What is it?

01:43.240 --> 01:46.760
Well, Pydantic is a data validation library in Python.

01:46.760 --> 01:47.760
It's not the first.

01:47.760 --> 01:49.280
It's definitely not the last.

01:49.280 --> 01:53.160
It started off as a side project like so many open source projects.

01:53.160 --> 01:54.160
Nothing special.

01:54.160 --> 01:55.160
I maintained it in my spare time.

01:55.160 --> 01:57.200
People came along occasionally.

01:57.200 --> 01:59.400
Said nice things, reported bugs.

01:59.400 --> 02:00.600
Occasionally said not very nice things.

02:00.600 --> 02:03.880
And then something weird happened.

02:03.880 --> 02:05.640
And its usage went crazy.

02:05.640 --> 02:10.160
So the first thing that happened, which you can't really see on this graph in 2018, my

02:10.160 --> 02:15.640
friend Sebastian Ramirez started the fast API project, which is a web framework in Python

02:15.640 --> 02:22.200
which uses Pydantic and has now got, I don't know, how many thousand stars, 60,000 stars

02:22.200 --> 02:23.200
or something.

02:23.200 --> 02:24.200
It's got a lot of attention.

02:24.200 --> 02:27.240
You can see fast API's growth there.

02:27.240 --> 02:31.520
That got a lot of people, I think, to first find out about Pydantic.

02:31.520 --> 02:37.360
But something else happened at the beginning of 2021 to cause Pydantic's download numbers

02:37.360 --> 02:38.360
to go crazy.

02:38.360 --> 02:44.400
Now, I'm well aware that Aaron Armin's speech talk earlier kind of pre-trolled me before

02:44.400 --> 02:48.280
I even made my talk saying that download numbers are a terrible metric.

02:48.280 --> 02:49.840
But they are the only metric.

02:49.840 --> 02:52.000
So that's what we have to use.

02:52.000 --> 02:56.400
It's also worth saying that I have actually looked at Pydantic's downloads in terms of

02:56.400 --> 03:00.200
as a dependency and as a direct download.

03:00.200 --> 03:05.240
It's not that easy to do with PyPI, but it looks like about 15 million downloads a month

03:05.240 --> 03:07.480
are from as a dependency of another package.

03:07.480 --> 03:12.800
And the remaining 25 or so million are people installing Pydantic directly.

03:12.800 --> 03:17.440
So it seems like people are using it not just as a dependency of another library.

03:17.440 --> 03:22.160
I've included Django on there as the middle line because it's the most high profile, most

03:22.160 --> 03:25.800
well-known web framework in Python.

03:25.800 --> 03:26.800
Not to be critical of it.

03:26.800 --> 03:27.800
It's amazing.

03:27.800 --> 03:28.800
It's like changed my life.

03:28.800 --> 03:33.960
I mean, no disrespect by saying we've overtaken it, but just that Pydantic's usage has gone

03:33.960 --> 03:36.000
mad.

03:36.000 --> 03:41.080
In terms of how it's used, it's used by lots of organizations you would expect.

03:41.080 --> 03:46.680
All the fan companies, something like 19 out of the top 25 companies in NASDAQ, but also

03:46.680 --> 03:51.160
by organizations which you wouldn't expect like JP Morgan.

03:51.160 --> 03:52.160
Use it quite a lot.

03:52.160 --> 03:54.160
I don't know in what regard.

03:54.160 --> 03:55.160
But it's quite interesting.

03:55.160 --> 03:59.920
If you have an open source project, if you look in analytics at the referrers, lots of

03:59.920 --> 04:04.800
those big, very security-centric companies forget to turn off the referrer header from

04:04.800 --> 04:06.300
their internal systems.

04:06.300 --> 04:11.760
And they name their internal systems things like github.jpmorgan.net.

04:11.760 --> 04:16.000
So you can see which companies are using your dependencies by looking at those referrers.

04:16.000 --> 04:21.560
So, for example, Apple have no public demonstration of using Pydantic at all, but six different

04:21.560 --> 04:26.200
enterprise instances of GitHub within Apple use Pydantic, and you can even see which ones

04:26.200 --> 04:27.200
they are.

04:27.200 --> 04:36.280
They're like maps.github.maps.apple.com, github.siri.apple.com, et cetera.

04:36.280 --> 04:40.760
It's also used by some cool organizations, used by NASA for processing imagery from James

04:40.760 --> 04:48.480
Webb and used by the International Panel on Climate Change for processing the data they

04:48.480 --> 04:52.200
give to the UN on climate change every month, which is the stuff I'm most proud of and why

04:52.200 --> 04:56.040
I want to make Pydantic better.

04:56.040 --> 04:57.800
So what's so great about Pydantic?

04:57.800 --> 04:59.200
Why are so many people using it?

04:59.200 --> 05:01.280
The short answer is I don't know.

05:01.280 --> 05:02.760
Because you can't go and ask those people.

05:02.760 --> 05:05.000
You can look at a graph, but it can't really tell you.

05:05.000 --> 05:10.680
But we can look at Pydantic and what people say, and we can guess at what's made it popular.

05:10.680 --> 05:12.560
So this is, I know we're in the Rust room.

05:12.560 --> 05:13.560
We've got some Python code.

05:13.560 --> 05:14.560
Don't worry.

05:14.560 --> 05:15.960
We'll get to Rust later.

05:15.960 --> 05:19.320
This is some Python code that demonstrates what Pydantic does.

05:19.320 --> 05:24.520
So we have a model which represents a talk which has four fields in this case.

05:24.520 --> 05:26.360
Obviously, title is a string.

05:26.360 --> 05:28.040
Attendance is an integer.

05:28.040 --> 05:32.800
Number of people who came when, which is a date time or none and has a default value

05:32.800 --> 05:33.800
of none.

05:33.800 --> 05:39.600
And then the mistakes I make, which is a list of tuples with the time they were made at

05:39.600 --> 05:42.600
and a description.

05:42.600 --> 05:47.840
And then lastly, last line, we instantiate an instance of talk using that data.

05:47.840 --> 05:49.800
And if there was a mistake, we would get an error.

05:49.800 --> 05:51.880
If there wasn't a mistake, we'd obviously get the instance.

05:51.880 --> 05:55.640
The first thing that makes Pydantic special and the reason that people like it is because

05:55.640 --> 05:59.440
we use Python type hints to define the types.

05:59.440 --> 06:01.200
That's become reasonably commonplace now.

06:01.200 --> 06:05.520
There are a whole suite of different libraries that do the same thing.

06:05.520 --> 06:07.520
Either because it's obvious or because they're copying Pydantic.

06:07.520 --> 06:13.760
But Pydantic was the first to do that because type hints were kind of new in 2017.

06:13.760 --> 06:16.280
And obviously, the main advantage is it's easy to learn.

06:16.280 --> 06:19.040
You don't need to learn a new kind of DSL to define stuff.

06:19.040 --> 06:23.720
But it's also compatible with static type checking with all the rest of your code, with

06:23.720 --> 06:24.920
your IDE.

06:24.920 --> 06:29.520
Once you defined your model, and if Pydantic's worked correctly, then you know you've got

06:29.520 --> 06:31.760
a proper instance of talk.

06:31.760 --> 06:37.120
So the frustration that caused me to create Pydantic was that type annotations existed.

06:37.120 --> 06:39.440
They sat there in the code.

06:39.440 --> 06:41.160
You could read them, but they did nothing at runtime.

06:41.160 --> 06:46.320
And so effectively, could we try and make them work?

06:46.320 --> 06:51.160
The second and slightly more controversial thing that Pydantic does, which I think is

06:51.160 --> 06:55.280
one of the reasons that people find it easy to use, is because we default to coercion.

06:55.280 --> 06:58.200
So you can see a tendency there, although it needs to be an integer.

06:58.200 --> 07:00.960
It's defined as a string.

07:00.960 --> 07:05.800
Pydantic will automatically coerce from, for example, a valid string to an integer.

07:05.800 --> 07:09.960
But it'll also do other coersions that are a bit more commonplace, like coercing a string

07:09.960 --> 07:13.640
as an ISO date format into a date time object.

07:13.640 --> 07:17.280
And same for the durations.

07:17.280 --> 07:18.280
Some people hate that.

07:18.280 --> 07:19.280
Some people complain about it a lot.

07:19.280 --> 07:24.760
I suspect lots of people who don't even realize they're using it.

07:24.760 --> 07:31.080
They process environment variables, or JSON, or URL arguments, and they're always strings.

07:31.080 --> 07:33.320
And Pydantic just works, and they don't even see it.

07:33.320 --> 07:35.680
A few other reasons I think were quite popular.

07:35.680 --> 07:36.680
We're fast-ish.

07:36.680 --> 07:38.640
We're friendly-ish on the bug tracker.

07:38.640 --> 07:41.600
I don't promise not to ever be cross of people.

07:41.600 --> 07:44.880
And we're reasonably feature complete.

07:44.880 --> 07:45.880
So that was Pydantic.

07:45.880 --> 07:46.880
It's great.

07:46.880 --> 07:47.880
Lots of people are using it.

07:47.880 --> 07:48.880
What's the problem?

07:48.880 --> 07:51.560
Well, it started off as a side project for me.

07:51.560 --> 07:54.960
It wasn't designed to be successful, and the internals stink.

07:54.960 --> 07:58.080
I'm very proud of what Pydantic is doing in terms of how it's being used.

07:58.080 --> 08:00.240
I'm not proud of what's under the hood.

08:00.240 --> 08:04.440
And so I've been keen for a long time to fix the internals.

08:04.440 --> 08:10.840
Also second way in which Armin kind of trolled me before my talk was talking about API compatibility.

08:10.840 --> 08:13.760
We're going to have to break a lot of things in Pydantic v2 to get it right.

08:13.760 --> 08:18.680
But that's the right thing to do, I think, to get the future API to be correct and stable

08:18.680 --> 08:21.960
and not break again.

08:21.960 --> 08:26.560
And while we're building v2, why don't we do some other stuff?

08:26.560 --> 08:27.560
So make it even faster.

08:27.560 --> 08:28.680
It's already quite fast.

08:28.680 --> 08:33.680
But if you think about that number of downloads, you think about the number of CPU cycles globally

08:33.680 --> 08:38.200
every day devoted to doing validation with Python.

08:38.200 --> 08:40.320
That's currently in with Pydantic.

08:40.320 --> 08:42.080
That's currently all in Python.

08:42.080 --> 08:46.000
That's probably quite a lot of carbon dioxide that's being released effectively unnecessarily

08:46.000 --> 08:48.920
because we could make Pydantic significantly faster.

08:48.920 --> 08:53.680
Strict mode I already talked about because, well, often you don't need it.

08:53.680 --> 08:57.240
There are legitimate cases where you want strict mode.

08:57.240 --> 09:01.960
We have functional validators which is effectively running some Python code to validate a field.

09:01.960 --> 09:05.680
They're useful, but they would be more useful if they could operate like an onion.

09:05.680 --> 09:10.720
So like middleware where you take both a value and a handler and call the handler if you

09:10.720 --> 09:11.720
want to.

09:11.720 --> 09:14.480
Once you've done some processing of the value, that would be super valuable.

09:14.480 --> 09:16.880
Another thing we could add, composability.

09:16.880 --> 09:21.000
So Pydantic, as I showed you earlier, is based on a Pydantic model.

09:21.000 --> 09:24.760
Often your root type doesn't need to be a Pydantic model or shouldn't be a Pydantic model.

09:24.760 --> 09:30.440
It might be a list, might be a tuple, it might be a list of models, it might be a type dict

09:30.440 --> 09:33.800
which is a common new type in Python.

09:33.800 --> 09:35.760
And then lastly, maintainability.

09:35.760 --> 09:38.720
Since I maintain Pydantic, I want maintaining it to be fun.

09:38.720 --> 09:44.720
So about a year ago last March, I started as a kind of experiment, could I rebuild some

09:44.720 --> 09:46.440
of it in Rust a year later?

09:46.440 --> 09:52.040
I'm still working on it full time and we're nearly there.

09:52.040 --> 09:57.840
So what does it mean to do some validation, to validate Python data in Rust?

09:57.840 --> 09:58.840
What's the process?

09:58.840 --> 10:05.960
Well, phase one, we need to take a Pydantic model and convert it to a Rust structure.

10:05.960 --> 10:14.680
So unlike libraries like Curd, we're not compiling models in Rust.

10:14.680 --> 10:17.740
The compiled Rust code doesn't know anything about the models it's going to receive because

10:17.740 --> 10:21.560
obviously Python developers don't want to be compiling Rust code to get their model

10:21.560 --> 10:22.560
to work.

10:22.560 --> 10:26.760
So we have to have a, in Rust terms, dynamic definition of our schema which we could then

10:26.760 --> 10:29.160
use for validation.

10:29.160 --> 10:34.600
The way we build that is effectively these validators which are structs that contain

10:34.600 --> 10:40.800
both characteristics of what they're going to validate but also other validators recursively

10:40.800 --> 10:43.480
such that you can define complex structures.

10:43.480 --> 10:50.280
So in this case, our outermost validator is a model validator which effectively just instantiates

10:50.280 --> 10:54.960
an instance of talk and sets its attributes from a dictionary.

10:54.960 --> 11:00.840
It contains another validator which is a typedict validator which contains the definition of

11:00.840 --> 11:05.680
all the fields which have effectively the key that they're going to look for and then

11:05.680 --> 11:08.680
a validator that they're going to run.

11:08.680 --> 11:10.240
The first two are reasonably obvious.

11:10.240 --> 11:15.480
I've added a few constraints to show how you would manage those constraints.

11:15.480 --> 11:21.160
And then the third one, the when field is obviously a union which in turn contains a

11:21.160 --> 11:28.800
vec of validators to run effectively in turn to try and find the value.

11:28.800 --> 11:33.040
And then the last one which is the kind of more complex type contains this list validator

11:33.040 --> 11:37.040
which contains two-pool validator which contains two more validators.

11:37.040 --> 11:43.080
And we can build up effectively infinitely complex schemas from a relatively simple,

11:43.080 --> 11:47.960
I say relatively simple, principle at the outset which is we have a validator, it's

11:47.960 --> 11:50.200
going to contain some other stuff.

11:50.200 --> 11:54.600
So what does that look like in code?

11:54.600 --> 11:56.080
I said I was going to show you some Rust code.

11:56.080 --> 12:01.760
I'm going to show you some Rust code because I think this is the most clear way of explaining

12:01.760 --> 12:03.240
what it is that we do.

12:03.240 --> 12:08.320
So the root of everything is this trait validator which contains effectively three things.

12:08.320 --> 12:14.240
It contains a const, a static string which is used for defining, as I'll show you later,

12:14.240 --> 12:20.280
which validator we're going to use for a given bit of data build which is a simple function

12:20.280 --> 12:24.120
to construct an instance of itself in the generic sense.

12:24.120 --> 12:28.680
And then the validate function that goes off and does the validation.

12:28.680 --> 12:34.400
We then take all of those, well, we then implement that trait for all of the common types that

12:34.400 --> 12:35.400
we want.

12:35.400 --> 12:39.800
So I think we have 58, 48 or so different validators.

12:39.800 --> 12:43.240
And then we bang all of them into one massive enum.

12:43.240 --> 12:49.640
Then the magic bit which is provided by enum dispatch which is a Rust crate that effectively

12:49.640 --> 12:55.880
implements a trait on an enum if every member of that enum implements that trait.

12:55.880 --> 13:02.600
Effectively it goes and does a big procedural macro to create an implementation of the function

13:02.600 --> 13:07.200
which is just a big match choosing which function to call.

13:07.200 --> 13:12.440
But it's significantly faster than DINE.

13:12.440 --> 13:17.680
And in fact in some cases it can abstract away everything and be as fast as just calling

13:17.680 --> 13:22.720
the implementation directly.

13:22.720 --> 13:26.720
So I said earlier that we needed to use this constant, the expected type.

13:26.720 --> 13:33.880
We use that in another effectively big enum to go through and we take the type attribute

13:33.880 --> 13:38.160
out of this schema which is a Python dictionary and we use that to effectively look up which

13:38.160 --> 13:40.120
validator we're going to go and build.

13:40.120 --> 13:43.320
And again, I've shown a few here but there's obviously a bunch more.

13:43.320 --> 13:48.240
This in real life is not implemented as a big match statement like this.

13:48.240 --> 13:53.860
It's a macro that builds this function but it's clearer here if you get the idea.

13:53.860 --> 13:58.940
So I showed you earlier this validate function and I kind of skipped over the input argument.

13:58.940 --> 14:02.900
So the input argument is just an implementation of a trait.

14:02.900 --> 14:08.880
That trait input is like the beginnings of which are defined here.

14:08.880 --> 14:11.960
And it effectively gives you all the things that the validation functions are going to

14:11.960 --> 14:13.040
need on a value.

14:13.040 --> 14:20.160
So is none, strict string, lack string, int, float, et cetera, et cetera, but also more

14:20.160 --> 14:25.920
complex types like date, date time, dictionary, et cetera, et cetera.

14:25.920 --> 14:33.280
And then we implement that trait on both a Python value and on a JSON value which means

14:33.280 --> 14:39.040
that we can parse Rust directly without having to go via Python.

14:39.040 --> 14:41.440
That's super valuable for two reasons.

14:41.440 --> 14:43.320
One for performance reasons.

14:43.320 --> 14:49.960
So if our input is a string and if we were to then parse it into Python objects and then

14:49.960 --> 14:54.600
take that into our validator and run all of that, that would be much lower than parsing

14:54.600 --> 14:58.680
in Rust and then running the validator in Rust straight away.

14:58.680 --> 15:02.120
The other big advantage is to do with strict mode.

15:02.120 --> 15:05.560
So I said earlier that people want strict mode but they say they want strict mode but

15:05.560 --> 15:06.560
often they don't.

15:06.560 --> 15:11.760
So what people will say is I want totally strict Python, why isn't it strict?

15:11.760 --> 15:15.280
And then you'll say do you want to load data from JSON and they say of course I do.

15:15.280 --> 15:18.960
And you say how are you going to define a date?

15:18.960 --> 15:22.560
And they'll say I use a standard date format.

15:22.560 --> 15:23.560
That's not strict then.

15:23.560 --> 15:24.560
You're parsing a string.

15:24.560 --> 15:27.360
And they're like oh, that's fine because it should know in that case it's coming from

15:27.360 --> 15:28.360
JSON.

15:28.360 --> 15:30.600
Well, obviously, how are we going to do that?

15:30.600 --> 15:37.440
By parsing JSON directly and in future potentially other types, we can implement our strict date

15:37.440 --> 15:45.000
method both on that JSON input, we can say well, we're in JSON, we don't have a date

15:45.000 --> 15:48.600
type so we're going to have to do something so we're going to parse a string.

15:48.600 --> 15:53.760
And the strict date implementation for JSON will parse a string.

15:53.760 --> 15:57.760
And therefore we can have a strict mode that's actually useful which we wouldn't have had

15:57.760 --> 16:02.800
if we couldn't have had in Python where the validation logic doesn't know anything about

16:02.800 --> 16:04.240
where the data is coming from.

16:04.240 --> 16:09.040
Even if we have a parse JSON function, all it's doing is parsing JSON to Python and then

16:09.040 --> 16:12.880
doing validation.

16:12.880 --> 16:14.080
So then that's all very well.

16:14.080 --> 16:16.160
That defines effectively how we do our validation.

16:16.160 --> 16:17.680
What's the interface to Python?

16:17.680 --> 16:26.360
So that's where we have this schema validator Rust struct which using the PyClass decorator

16:26.360 --> 16:29.600
is also available as a Python class.

16:29.600 --> 16:35.680
And all it really contains is a validator which of course can in turn contain other

16:35.680 --> 16:38.120
validators as I said earlier.

16:38.120 --> 16:43.840
And it's implementation which are all then exposed as Python methods are new which just

16:43.840 --> 16:49.640
construct it so we call the build validator and get back an instance of our validator

16:49.640 --> 16:53.280
which we then store and return the type.

16:53.280 --> 16:55.120
Actually this is much more complicated.

16:55.120 --> 17:01.440
One of the cleverest and most infuriating bits of Python core is that we, as you can

17:01.440 --> 17:05.080
imagine this schema for defining validation becomes quite complex.

17:05.080 --> 17:06.840
It's very easy to make a mistake.

17:06.840 --> 17:11.800
So we validate it using Python core itself.

17:11.800 --> 17:16.720
Which when it works is magic and when it doesn't work leads to impossible errors because obviously

17:16.720 --> 17:22.360
all of the things that you're looking at as members of dictionaries are in turn the names

17:22.360 --> 17:23.800
of bits of validation.

17:23.800 --> 17:29.360
So it's complete hell but it works and it makes it very hard to build an invalid or

17:29.360 --> 17:33.160
not build the validator that you want.

17:33.160 --> 17:37.880
And then we have these two implementations, two functions which do validate Python object

17:37.880 --> 17:43.200
as I said earlier which core validate and same with JSON where we pass a JSON using

17:43.200 --> 17:48.860
Serd to a JSON value and then call validate again with that input.

17:48.860 --> 17:51.720
This code is obviously heavily simplified so that it fits.

17:51.720 --> 17:54.560
It doesn't fit on the page but nearly fits on the page.

17:54.560 --> 17:58.520
So not everything is exactly as it really is but I think that kind of gives you an idea

17:58.520 --> 18:02.240
of how we build up these validators.

18:02.240 --> 18:06.280
The other thing missed here, we also do the whole thing again for serialization.

18:06.280 --> 18:13.320
So the serialization from both a Python model to a Python dictionary and from a Python model

18:13.320 --> 18:19.660
straight to JSON is all written in Rust and it does useful things like filtering out elements

18:19.660 --> 18:26.320
as you go along and it's effectively the same structure, uses the same schema but it's just

18:26.320 --> 18:32.760
dedicated to serialization rather than rather than validation.

18:32.760 --> 18:35.520
So what does the Python interface then look like?

18:35.520 --> 18:40.800
So this is so what I didn't explain earlier is that Python v2 which is going to be released

18:40.800 --> 18:44.440
fingers crossed in Q1 this year is made up of two packages.

18:44.440 --> 18:49.000
We have Python itself which is a pure Python package and then we have Python core which

18:49.000 --> 18:51.040
is almost all Rust code.

18:51.040 --> 18:55.400
We have a little bit of a shim of Python to explain what's going on but it's really just

18:55.400 --> 18:57.280
the Rust code I've been showing you.

18:57.280 --> 19:03.480
So what Pydantic now does, all that Pydantic effectively takes care of is converting those

19:03.480 --> 19:09.440
type annotations I showed you earlier into a Pydantic core schema and then building a

19:09.440 --> 19:10.440
validator.

19:10.440 --> 19:15.120
So looking at an example here, we obviously import schema validator which I just showed

19:15.120 --> 19:28.000
you from, that's come up at the wrong time, from Pydantic core and then we, the base level,

19:28.000 --> 19:32.760
the base validator which in turn, the schema for the base validator is model and it contains

19:32.760 --> 19:37.800
as I said earlier a class which is the Python class to instantiate and another schema which

19:37.800 --> 19:48.040
in turn defines the fields and that inner validator is then defined by a typedict validator

19:48.040 --> 19:49.640
as I said earlier.

19:49.640 --> 19:54.120
So this is completely valid Python code, this will run now.

19:54.120 --> 19:58.760
So yeah, we have a typedict validator which contains fields which in turn are those fields

19:58.760 --> 20:00.320
which I showed you earlier.

20:00.320 --> 20:04.280
So title attendance is of type int.

20:04.280 --> 20:11.080
The most interesting thing here is if you look at the when validator, it gets a bit

20:11.080 --> 20:12.080
confusing.

20:12.080 --> 20:19.440
It is, its schema is of type default which in turn contains another schema which is of

20:19.440 --> 20:24.680
type nullable which is a kind of the simplest union, either a value or none.

20:24.680 --> 20:32.760
The default validator contains another member which is the default value, in this case none.

20:32.760 --> 20:36.680
The inner schema is then nullable which in turn contains another inner schema which is

20:36.680 --> 20:37.880
then the date time.

20:37.880 --> 20:44.240
So that's how we define effectively default values and null or nullable.

20:44.240 --> 20:49.160
So one of the other mistakes in PyLantic in the past was that we kind of conflate, effectively

20:49.160 --> 20:54.600
Python made a mistake about ten years ago where they used, they had a alias for union

20:54.600 --> 21:00.040
of something and none that they called optional which then meant that I didn't want to have

21:00.040 --> 21:03.080
a thing that was called optional but was not optional.

21:03.080 --> 21:10.160
So we conflated nullable with optional in PyLantic and rightly it confused everyone.

21:10.160 --> 21:16.440
So the solution from Python was to start using the pipe operator for unions and to basically

21:16.440 --> 21:17.440
ignore optional.

21:17.440 --> 21:22.960
They can't really get rid of it but they just pretend it didn't really happen.

21:22.960 --> 21:29.200
My solution is to define default and nullable as completely separate things and we're not

21:29.200 --> 21:33.800
going to use the optional type anywhere in our docs, we're just going to use union of

21:33.800 --> 21:39.120
thing and none to avoid that confusion.

21:39.120 --> 21:41.600
And then I think mistakes, I hope it kind of makes sense to you.

21:41.600 --> 21:45.480
Again, it's this like schema within schema within schema which become validator within

21:45.480 --> 21:48.480
validator.

21:48.480 --> 21:53.640
And we take our code as I showed you earlier, run validation, in this case we call validate

21:53.640 --> 21:57.440
Python because we've got some Python code but we could just as well have a JSON string

21:57.440 --> 22:02.800
and call validate JSON and then we have a talk instance which lets us access the members

22:02.800 --> 22:08.560
of it as you normally would.

22:08.560 --> 22:11.920
So where does Rust excel in these applications?

22:11.920 --> 22:14.400
Why build this in Rust?

22:14.400 --> 22:20.160
There are a bunch of obvious reasons to use Rust, performance being the number one, multi-threading

22:20.160 --> 22:24.720
and not having the global interpreter lock in Python is another one.

22:24.720 --> 22:30.800
The third is using high quality existing Rust libraries to build libraries in Python instead

22:30.800 --> 22:32.280
of implementing it yourself.

22:32.280 --> 22:37.960
So I maintain two other Python libraries written in Rust watch files which uses the notify

22:37.960 --> 22:45.000
crate to do file watching and then rtoml which as you can guess is a Toml parser using the

22:45.000 --> 22:47.640
Toml library from Rust.

22:47.640 --> 22:57.360
And the rtoml library is the fastest Python, well Python, Toml parser out there.

22:57.360 --> 22:59.840
And actually watch files is becoming more and more popular, it's the default now with

22:59.840 --> 23:03.780
uvehicle which is one of the web servers.

23:03.780 --> 23:07.960
But perhaps less obviously in terms of where Rust fits in best.

23:07.960 --> 23:11.880
Deeply recursive code as I've just showed you with these validators within validators,

23:11.880 --> 23:15.880
there's no stack and so we don't have a penalty for recursion.

23:15.880 --> 23:20.160
We do have to be very, very careful because as I'm sure you will know if you have recursion

23:20.160 --> 23:22.880
in Rust and you don't catch it, you just get a segfault.

23:22.880 --> 23:26.600
And that would be very, very upsetting to Python developers who have never seen one

23:26.600 --> 23:29.280
before.

23:29.280 --> 23:34.640
So there's an enormous amount of code in Python dedicated to catching recursion.

23:34.640 --> 23:39.680
We have to have two or three different sorts of guard to protect against recursion in all

23:39.680 --> 23:43.800
possible different situations because it's effectively the worst thing that we can have

23:43.800 --> 23:48.080
is that there is some data structure that you can pass to Python which causes your entire

23:48.080 --> 23:54.000
Python process to segfault and you wouldn't know where to even start looking.

23:54.000 --> 23:58.760
So that's a blessing, the lack of a stack is a blessing and a curse.

23:58.760 --> 24:04.240
And then the second big advantage I think of where Rust excels is in these small modular

24:04.240 --> 24:05.240
components.

24:05.240 --> 24:11.080
So where I was showing you before these relatively small in terms of code footprint validators

24:11.080 --> 24:17.080
which in turn hold other ones, there's no performance penalty for having these functions

24:17.080 --> 24:18.080
in Rust.

24:18.080 --> 24:23.320
I say almost because we actually have to use box around validators because they hold themselves

24:23.320 --> 24:29.720
so there is a bit of an overhead of going into the heap but it's relatively small particularly

24:29.720 --> 24:31.280
compared to Python.

24:31.280 --> 24:35.080
And then the lastly, complex error handling.

24:35.080 --> 24:38.280
Obviously in Python you don't know what's going to error and what exceptions you're

24:38.280 --> 24:41.920
going to get in Rust.

24:41.920 --> 24:45.320
Putting to one side the comment about panic earlier, you can in general know what errors

24:45.320 --> 24:51.160
you're going to get and catch them and construct validation errors in the case of pedantic

24:51.160 --> 24:57.440
which is a great deal easier than it would ever have been to write that code in Python.

24:57.440 --> 25:03.080
So the way I want to think about the future development of Python is not as Python versus

25:03.080 --> 25:08.920
Rust but as Python as the user interface for Rust or the application developer interface

25:08.920 --> 25:11.040
for Rust.

25:11.040 --> 25:13.840
So I'd love to see more and more libraries do what we've done with pedantic core and

25:13.840 --> 25:20.160
effectively implement their low level components in Rust.

25:20.160 --> 25:25.760
So I kind of my dream is a world in which thinking about the life cycle of a HTTP request

25:25.760 --> 25:32.840
but you could think the same about some NL pipeline or many other applications, we effectively

25:32.840 --> 25:39.440
the vast majority of the execution is Rust or C but then all of the application logic

25:39.440 --> 25:40.440
can be in Python.

25:40.440 --> 25:44.600
So effectively we get to a point where we have 100% of developer time spent in high

25:44.600 --> 25:50.840
level languages but only 1% of CPU dedicated to actually running Python code which is slower

25:50.840 --> 25:52.120
and is always going to be slower.

25:52.120 --> 25:55.240
I don't think there's ever a world in which someone's going to come up with a language

25:55.240 --> 25:59.920
that is as fast and as safe as Rust but also as quick to write as Python.

25:59.920 --> 26:02.000
So I don't think it should be one versus the other.

26:02.000 --> 26:06.600
It should be building the low level, building the rails, perhaps a bad term but the rails

26:06.600 --> 26:12.200
in Rust and building the train in Python.

26:12.200 --> 26:16.240
It doesn't work but you get where I'm coming from.

26:16.240 --> 26:18.960
On that note, thank you very much.

26:18.960 --> 26:24.160
A few links there, particularly thanks to the PyO3 team who built the bindings for Rust

26:24.160 --> 26:29.080
in Python which is amazing and if you want a laugh there's a very, very funny issue on

26:29.080 --> 26:33.520
GitHub where a very angry man says why we should never use Rust.

26:33.520 --> 26:34.520
So if you want to read that.

26:34.520 --> 26:38.560
I then took some time to take them to pieces which was quite satisfying although a waste

26:38.560 --> 26:39.560
of time.

26:39.560 --> 26:41.160
So have a look at that.

26:41.160 --> 26:42.160
Questions?

26:42.160 --> 27:08.200
First, especially for the sanitation, are you thinking to publish a library of Rust?

27:08.200 --> 27:15.160
The job is already done and you could have a public API in a library already to validate

27:15.160 --> 27:16.160
Rust data.

27:16.160 --> 27:18.840
I'm sorry, I don't understand quite what...

27:18.840 --> 27:19.840
Sorry.

27:19.840 --> 27:27.120
So you wrote the library in Rust so could you publish just an API to validate JSON for

27:27.120 --> 27:31.040
example from Rust instead of through Python?

27:31.040 --> 27:36.520
Absolutely you could and it would be useful if you wanted to somehow construct the schema

27:36.520 --> 27:43.480
at runtime fast but it's never going to be anywhere near as performant as because you

27:43.480 --> 27:45.720
were not compiling...

27:45.720 --> 27:47.640
We can't do anything at compile time.

27:47.640 --> 27:52.600
Secondly, it's currently all completely intertwined with the PyO3 library and the Python types.

27:52.600 --> 27:58.480
So there is a future nascent possible project, Tydantic, which is Pydantic for TypeScript

27:58.480 --> 28:02.680
where we take the PyO3 types, we effectively replace them with a new library which has

28:02.680 --> 28:08.200
a compile time switch between the Python bindings and the JavaScript bindings or the Wasm bindings

28:08.200 --> 28:10.360
and then we can build Tydantic.

28:10.360 --> 28:12.760
That's a future plan but a long way off.

28:12.760 --> 28:15.640
Right now it wouldn't really be worth it because you would get lots to slow down from Python

28:15.640 --> 28:17.080
and from compile time.

28:17.080 --> 28:20.880
So we really need a completely different library just for Rust like you're saying.

28:20.880 --> 28:22.240
Yeah, CERT is amazing.

28:22.240 --> 28:25.600
I don't think I'm going to go and try and compete with that.

28:25.600 --> 28:28.400
At least it's great for that application.

28:28.400 --> 28:32.360
Hi, thanks for the talk.

28:32.360 --> 28:37.560
Personally I think the Python library cryptography introduced Rust and had some complaints from

28:37.560 --> 28:41.080
people using obscure build processes where Rust didn't work.

28:41.080 --> 28:43.440
Are you expecting anything from that?

28:43.440 --> 28:45.960
So I will actually bring up now...

28:45.960 --> 28:47.920
Now I'm going to get into how to...

28:47.920 --> 28:52.760
Effectively go and read that issue where among other things I...

28:52.760 --> 28:58.680
Oh, how do I get out of this mode?

28:58.680 --> 29:06.440
So rant, rant, rant, rant from him, effectively I went through the just over a quarter of

29:06.440 --> 29:10.880
a billion downloads over the last 12 months of Pydantic and I worked out looking at the

29:10.880 --> 29:19.120
distribution of the different operating systems and libc implementations etcetera that 99.9859%

29:19.120 --> 29:23.240
of people would have got a binary if they'd installed Pydantic core then.

29:23.240 --> 29:27.760
That number will be higher now because there will be fewer esoteric operating systems.

29:27.760 --> 29:34.120
Most of the failed ones if you look are actually installing, say they're installing Python

29:34.120 --> 29:35.440
onto iOS.

29:35.440 --> 29:40.720
I don't know what that means or whether it could ever work but...

29:40.720 --> 29:43.440
The other thing I would say is Pydantic core is already compiled to WebAssembly so you

29:43.440 --> 29:45.120
can already run it in the browser.

29:45.120 --> 29:49.360
So I understand why people complained but I think it's not a concern.

29:49.360 --> 29:51.240
It's a straw man for most people.

29:51.240 --> 29:55.040
It's like satisfying slap down.

29:55.040 --> 29:59.600
But again if there's another distribution that we don't, we currently release 60 different

29:59.600 --> 30:08.320
binaries, if there's another one we'll try and compile for it and release the binary.

30:08.320 --> 30:12.720
There's a question right at the back.

30:12.720 --> 30:23.720
I'll get back to the talk rather than...

30:23.720 --> 30:32.680
Is there a way to use the Django models as Pydantic models?

30:32.680 --> 30:33.680
Say again?

30:33.680 --> 30:41.480
To use the Django to have binding or to translate the Django model directly into a Pydantic model?

30:41.480 --> 30:42.760
There's no way at the moment.

30:42.760 --> 30:47.280
There's a number of different ORMs, six I know of, built on top of Pydantic which effectively

30:47.280 --> 30:48.600
allow that.

30:48.600 --> 30:53.680
If you were wanting specifically Django there's a project called Django Ninja that makes extensive

30:53.680 --> 30:55.040
use of Pydantic.

30:55.040 --> 30:56.640
I don't know that much about it.

30:56.640 --> 31:00.400
But if you actually wanted Pydantic models you'd probably want some kind of code reformatter

31:00.400 --> 31:03.160
to convert them.

31:03.160 --> 31:06.440
So I look at Django Ninja, I'm sure what they're doing is the best of what's possible right

31:06.440 --> 31:13.960
now.

31:13.960 --> 31:19.320
If you had additional time, say after finishing Pydantic, are there any other projects where

31:19.320 --> 31:26.480
you'd like to follow this vision of a Rust core with Python user space or API?

31:26.480 --> 31:29.240
There are a number of ones.

31:29.240 --> 31:33.840
There's already OR JSON which is a very, very fast if unsafe in the sense of listed with

31:33.840 --> 31:38.080
unsafe JSON parser which is very, very fast.

31:38.080 --> 31:43.560
There are the obvious one is a web framework where you do like I kind of showed here the

31:43.560 --> 31:48.200
HTTP parsing, the routing all in Rust.

31:48.200 --> 31:51.960
That's not very easy using ASGI.

31:51.960 --> 31:55.240
There are already a few projects doing that.

31:55.240 --> 31:57.680
So that would be the obvious one but there's no winner yet.

31:57.680 --> 32:03.200
Currently the best low level web framework is Starlet which FastAPI is built on which

32:03.200 --> 32:10.280
I think it does use Rust for, it uses a Rust library for HTTP parsing or a C library.

32:10.280 --> 32:16.480
So some of it's already happening but no obvious candidate right now.

32:16.480 --> 32:21.600
What I would say though is libraries like rich, no criticism of will but rich is incredibly

32:21.600 --> 32:25.760
complicated, it's for terminal output, it's not so much performance critical but it's

32:25.760 --> 32:28.120
really quite involved in complex logic.

32:28.120 --> 32:31.960
I would much prefer to write that logic in Rust than Python.

32:31.960 --> 32:39.760
I think there are lots of candidates.

32:39.760 --> 32:48.000
Yeah, what do you mean by Python as the application layer?

32:48.000 --> 32:54.920
I guess I could have added some example code here but you can imagine a Python function

32:54.920 --> 33:01.760
which is a viewpoint in a web framework which takes in some validated arguments done by

33:01.760 --> 33:02.920
Python.

33:02.920 --> 33:08.200
You then decide in Python to make a query to the database to get back the user's name

33:08.200 --> 33:14.080
from the ID and then you return the JSON object containing data about the user.

33:14.080 --> 33:19.840
If you think about that, all of the code outside the Python functions could be written in a

33:19.840 --> 33:25.120
faster language whether it be the database query accessing the database, TSL termination,

33:25.120 --> 33:31.840
HTTP parsing, routing, validation but effectively using Python to define as a way to effectively

33:31.840 --> 33:47.400
configure Rust code or configure compile code.

33:47.400 --> 33:48.400
Yes hello.

33:48.400 --> 33:52.080
I have a question, just a pedantic one.

33:52.080 --> 33:56.600
Is there any support or are you planning any support for alternative schema types like

33:56.600 --> 34:00.760
protobuf or gRPC or Avro?

34:00.760 --> 34:06.520
In the future, what I have a plan for is I don't want to build them into pedantic.

34:06.520 --> 34:11.760
Pedantic is already big but you can parse them to Python now and validate them as a

34:11.760 --> 34:13.000
Python object.

34:13.000 --> 34:34.080
There is a plan effectively to take the this value which you would then construct in Rust,

34:34.080 --> 34:38.760
parse as a Python value into pedantic core which would then extract the raw underlying

34:38.760 --> 34:44.400
Rust instance and then validate that and that would allow you to get basically zero Python

34:44.400 --> 34:46.920
validation effectively.

34:46.920 --> 34:51.120
But without having to us having to either have compile time dependencies or build it

34:51.120 --> 34:53.960
all into pedantic core.

34:53.960 --> 34:57.160
I think that's our last question that we have time for.

34:57.160 --> 35:05.400
One comment we did get from matrix was that this code is a bit small on the display so

35:05.400 --> 35:06.400
if you upload the.

35:06.400 --> 35:07.400
I will do.

35:07.400 --> 35:08.400
Yeah perfect.

35:08.400 --> 35:13.040
So if you're watching the stream, slides will be uploaded and you can read the code.

35:13.040 --> 35:16.120
Oh I'll put them on Twitter as well but yeah definitely I'll upload them as well.

35:16.120 --> 35:17.120
Awesome.

35:17.120 --> 35:40.200
Thank you very much.
