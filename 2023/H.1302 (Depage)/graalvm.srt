1
0:00:00.000 --> 0:00:14.280
Just two classical optimizations that will help modern but mature virtual machine where

2
0:00:14.280 --> 0:00:18.680
we have that powers native images.

3
0:00:18.680 --> 0:00:20.720
Why is it important?

4
0:00:20.720 --> 0:00:21.720
And who I am?

5
0:00:21.720 --> 0:00:23.720
My name is Mitre Chukov.

6
0:00:23.720 --> 0:00:29.800
I work with a company named Bellsoft which actively participates in open JDK community

7
0:00:29.800 --> 0:00:37.360
and we release our own JDK distribution which you probably met if you have ever built a

8
0:00:37.360 --> 0:00:40.360
Spring Boot container with default build pack.

9
0:00:40.360 --> 0:00:42.600
So it's in there.

10
0:00:42.600 --> 0:00:50.320
And now Spring Boot, since version 3, supports containers with native images.

11
0:00:50.320 --> 0:00:56.320
It can be built as a native image and if you do that the compiler being used is the Barricot

12
0:00:56.320 --> 0:01:02.400
native image kit which is a Bellsoft distribution of GraalVM.

13
0:01:02.400 --> 0:01:15.560
So that's another project that we participate and GraalVM itself can be seen as different

14
0:01:15.560 --> 0:01:20.160
things at least two major modes that we can observe.

15
0:01:20.160 --> 0:01:29.480
It can run as a JIT where compiler is Graal or we can build a native image with a static

16
0:01:29.480 --> 0:01:39.360
compilation and it will utilize a special chief virtual machine substrate VM.

17
0:01:39.360 --> 0:01:49.120
And here it's different from the traditional Java, traditional way of how we run it.

18
0:01:49.120 --> 0:01:55.520
Well another interesting and peculiar point here is that it is written in Java.

19
0:01:55.520 --> 0:02:04.520
So it is a complex project but most of the code is Java and this is beautiful.

20
0:02:04.520 --> 0:02:12.560
So you have a virtual machine and a compiler for JVM languages and Java in particular written

21
0:02:12.560 --> 0:02:14.280
in Java.

22
0:02:14.280 --> 0:02:21.600
So if you look at Java itself, why is it so beautiful?

23
0:02:21.600 --> 0:02:24.720
Well not so beautiful compared to Kotlin as we know, right?

24
0:02:24.720 --> 0:02:30.520
But still both Java and Kotlin they share those concepts.

25
0:02:30.520 --> 0:02:36.880
So from the very beginning there is a way to write correct parallel programs.

26
0:02:36.880 --> 0:02:43.160
So then to write parallel programs we need some means of synchronization or to orchestrate

27
0:02:43.160 --> 0:02:46.280
our threads if we share data.

28
0:02:46.280 --> 0:02:48.840
But most typically we do that.

29
0:02:48.840 --> 0:02:56.840
And also it's a managed runtime where we don't have to worry that much about pre-memory because

30
0:02:56.840 --> 0:03:04.560
we have garbage collection and garbage is collected for us and our programs just, they

31
0:03:04.560 --> 0:03:10.480
can have memory but you have to work hard to get one.

32
0:03:10.480 --> 0:03:21.560
And having that native image implementation makes our final binaries very sometimes, makes

33
0:03:21.560 --> 0:03:23.680
them very performant.

34
0:03:23.680 --> 0:03:25.880
Of course we have an instant startup.

35
0:03:25.880 --> 0:03:29.280
It was mentioned today several times.

36
0:03:29.280 --> 0:03:31.560
But we can also have a very good peak performance.

37
0:03:31.560 --> 0:03:36.120
In certain cases that's not a rule but it can happen.

38
0:03:36.120 --> 0:03:44.320
Like it happens here on this plot, that's just a simple Spring Boot application and

39
0:03:44.320 --> 0:03:47.480
we just ping the same endpoint.

40
0:03:47.480 --> 0:03:54.440
And here the native image works better and also it warms up instantly and it has very

41
0:03:54.440 --> 0:03:55.980
good latency.

42
0:03:55.980 --> 0:04:01.660
So for this small amount of memory that it takes, so this is a small service, it takes

43
0:04:01.660 --> 0:04:05.400
small amount of memory, very small heap.

44
0:04:05.400 --> 0:04:06.800
And it also has low latency.

45
0:04:06.800 --> 0:04:16.360
And under the hood it uses serial GC and we'll talk about that later.

46
0:04:16.360 --> 0:04:21.800
What about relationship between Graal VM and OpenJDK?

47
0:04:21.800 --> 0:04:31.160
Well we're here in the Friends of OpenJDK room and Graal has been integrated as an additional

48
0:04:31.160 --> 0:04:34.980
experimental compiler in JDK 9.

49
0:04:34.980 --> 0:04:39.560
But well, it has been removed from recent JDKs.

50
0:04:39.560 --> 0:04:41.360
But what's the left over?

51
0:04:41.360 --> 0:04:43.640
It's an interface to plug it in.

52
0:04:43.640 --> 0:04:49.360
So now it's going to be a second attempt to do that.

53
0:04:49.360 --> 0:04:54.480
So here on slides it's mentioned that there is a discussion about project, new project

54
0:04:54.480 --> 0:04:55.480
they all had.

55
0:04:55.480 --> 0:05:03.560
But last week it was already called for votes in OpenJDK to start the project of bringing

56
0:05:03.560 --> 0:05:15.080
the most sweet parts of this technology into OpenJDK, back into OpenJDK.

57
0:05:15.080 --> 0:05:18.000
It's something that happens right now.

58
0:05:18.000 --> 0:05:23.960
So that default garbage collector that sometimes shows very good latency even compared to parallel

59
0:05:23.960 --> 0:05:26.400
GC or G1 in hotspot.

60
0:05:26.400 --> 0:05:29.160
Well on small heaps.

61
0:05:29.160 --> 0:05:35.240
Well it's a kind of garbage collector we can easily understand.

62
0:05:35.240 --> 0:05:37.880
And it's a generational stop the world collection.

63
0:05:37.880 --> 0:05:47.080
So here only one survivor space but actually it's 16 by default.

64
0:05:47.080 --> 0:05:55.720
But anyway, so we stop all our application threads and we collect garbage in a single

65
0:05:55.720 --> 0:05:56.720
thread.

66
0:05:56.720 --> 0:06:01.560
So this is kind of a basic garbage collector right.

67
0:06:01.560 --> 0:06:07.280
From the other hand it's reliable and it's very effective especially if we have only

68
0:06:07.280 --> 0:06:10.240
a single core available.

69
0:06:10.240 --> 0:06:12.800
So you see the problem.

70
0:06:12.800 --> 0:06:19.960
We have some CPU which may be enough to run many threads but we run only one of them.

71
0:06:19.960 --> 0:06:21.600
At least for garbage collection.

72
0:06:21.600 --> 0:06:26.440
Garbage collection can take significant time during our application execution.

73
0:06:26.440 --> 0:06:28.960
Well that's obvious.

74
0:06:28.960 --> 0:06:31.640
Well what would we do?

75
0:06:31.640 --> 0:06:39.180
Of course we would like to do exactly the same thing but in parallel.

76
0:06:39.180 --> 0:06:45.240
To decrease the time garbage collection takes to reduce the garbage collection pause.

77
0:06:45.240 --> 0:06:50.480
Because it still stops the world pause but we reduce it because we process data with

78
0:06:50.480 --> 0:06:51.880
multiple threads.

79
0:06:51.880 --> 0:06:54.320
So that's the idea of parallel garbage collection.

80
0:06:54.320 --> 0:07:00.960
The idea is not new but surprisingly this modern runtime doesn't have it yet.

81
0:07:00.960 --> 0:07:08.920
Well we decided to implement it and it's still being under review and some implementation

82
0:07:08.920 --> 0:07:12.240
details well they change.

83
0:07:12.240 --> 0:07:15.560
But the idea is very simple.

84
0:07:15.560 --> 0:07:24.480
You just say pass the garbage collection selection during the creation of your native image.

85
0:07:24.480 --> 0:07:29.840
For instance if you use some main or gradual configuration for your Spring Boot container

86
0:07:29.840 --> 0:07:32.080
you also can do that.

87
0:07:32.080 --> 0:07:42.200
And then you have some grips in runtime which you also can twist when you run your application.

88
0:07:42.200 --> 0:07:46.640
And well you enable that implementation.

89
0:07:46.640 --> 0:07:49.840
I'll show some performance results later.

90
0:07:49.840 --> 0:07:57.160
But basically the implementation itself well it can be analyzed as a change in a big Java

91
0:07:57.160 --> 0:08:00.200
program which Braavium is.

92
0:08:00.200 --> 0:08:07.440
And there are now two GC interfaces and implementations.

93
0:08:07.440 --> 0:08:17.880
And this functionality just reuse existing things in a very I would say smart way.

94
0:08:17.880 --> 0:08:25.960
Just to keep what is all about the parallelization as a code.

95
0:08:25.960 --> 0:08:33.480
So everything else is reused from serial GC.

96
0:08:33.480 --> 0:08:43.600
Basically there's a problem of how do we synchronize and share the work because parallel threads

97
0:08:43.600 --> 0:08:48.880
for garbage collection they also have the same problem because they work on the same

98
0:08:48.880 --> 0:08:54.720
data so they have contention or may have contention.

99
0:08:54.720 --> 0:08:59.080
So we need to share in some smart manner.

100
0:08:59.080 --> 0:09:06.360
Well it's implemented with a work divided in its volume.

101
0:09:06.360 --> 0:09:14.800
So every thread operates its local memory and it's a chunk of memory of one megabyte.

102
0:09:14.800 --> 0:09:21.920
So if we need an extra memory like we scan objects and we fulfill some set of data that

103
0:09:21.920 --> 0:09:23.800
we operate on.

104
0:09:23.800 --> 0:09:31.080
And then we have an extra chunk we can just put it aside so someone else can pick it.

105
0:09:31.080 --> 0:09:36.440
So that's the stack that contains the chunks of work.

106
0:09:36.440 --> 0:09:44.280
And then the work is finished the thread just takes next chunk of work.

107
0:09:44.280 --> 0:09:54.240
There may be a situation when threads similar several threads try to copy to promote the same object.

108
0:09:54.240 --> 0:09:56.360
And this is actually solved very simply.

109
0:09:56.360 --> 0:10:02.200
They just reserve some space for the object and then tries to install forward pointer

110
0:10:02.200 --> 0:10:05.440
using an atomic operation.

111
0:10:05.440 --> 0:10:11.400
And as this is an atomic operation only one thread succeeds so others just roll back and

112
0:10:11.400 --> 0:10:16.280
this is lightweight operation.

113
0:10:16.280 --> 0:10:25.840
Again this is Java this is not a strict email sorry but still all existing places that manage

114
0:10:25.840 --> 0:10:31.840
memory were reused without changing the architecture of Gralys so.

115
0:10:31.840 --> 0:10:38.000
There are already possibility to add garbage collectors so if you want to implement one

116
0:10:38.000 --> 0:10:40.520
it's not that complex.

117
0:10:40.520 --> 0:10:52.200
The major problem is to be correct then you deal with memory then you deal with concurrency

118
0:10:52.200 --> 0:10:59.320
and then you inject your code into this virtual machine because it's all declarative magic

119
0:10:59.320 --> 0:11:03.320
that requires you to be careful.

120
0:11:03.320 --> 0:11:13.040
Well some performance results with relatively large heaps with serial GC you can have pauses

121
0:11:13.040 --> 0:11:18.200
of several seconds which is long of course.

122
0:11:18.200 --> 0:11:24.200
And there's a big difference if you have a two or three or four second pause or if you

123
0:11:24.200 --> 0:11:26.200
decrease it by one second.

124
0:11:26.200 --> 0:11:32.000
So that's possible that this implementation already.

125
0:11:32.000 --> 0:11:36.320
So that's the order of this improvement.

126
0:11:36.320 --> 0:11:43.320
With another benchmark is Hyper-Alog you see that latency here latency of pauses can be

127
0:11:43.320 --> 0:11:47.240
decreased like two times.

128
0:11:47.240 --> 0:11:54.840
Those pauses are not that big and we have frequent collections here so X axis is apple

129
0:11:54.840 --> 0:12:05.960
so each point is a garbage collection and Y axis is time in I believe milliseconds.

130
0:12:05.960 --> 0:12:09.760
Well that's parallel GC.

131
0:12:09.760 --> 0:12:18.060
So we can obviously improve many applications and many installations where we have an option

132
0:12:18.060 --> 0:12:20.840
to use several CPUs.

133
0:12:20.840 --> 0:12:25.840
If you use one CPU of course we won't see much difference.

134
0:12:25.840 --> 0:12:32.880
There is some increase in memory used for service needs but that's kind of moderate.

135
0:12:32.880 --> 0:12:37.160
So other parts of this complex system.

136
0:12:37.160 --> 0:12:45.640
I mentioned synchronization and well synchronization is useful but it has trade-offs because if

137
0:12:45.640 --> 0:12:52.000
we implement the non-synchronization we need to save our CPU resources to put aside threads

138
0:12:52.000 --> 0:12:54.480
that won't get the resource.

139
0:12:54.480 --> 0:13:02.640
We need to stop them to queue them to manage that queues to wake them up to involve operating

140
0:13:02.640 --> 0:13:04.960
system in that process.

141
0:13:04.960 --> 0:13:08.280
So that's not cheap.

142
0:13:08.280 --> 0:13:15.320
But there are situations that that's another queue right and that even influences the design

143
0:13:15.320 --> 0:13:22.400
of standard library because like we all know string buffer and string builder right.

144
0:13:22.400 --> 0:13:29.480
One class appeared because well another one wasn't very pleasant in terms of performance.

145
0:13:29.480 --> 0:13:35.120
Yeah we need it sometimes but in many cases we need a non-synchronized implementation

146
0:13:35.120 --> 0:13:39.520
saying like hash table and hash map whoever uses hash table right.

147
0:13:39.520 --> 0:13:43.320
But it's very good synchronized.

148
0:13:43.320 --> 0:13:50.640
But not all classes that have any synchronization in them have their twins without synchronization.

149
0:13:50.640 --> 0:13:52.800
That makes no sense right.

150
0:13:52.800 --> 0:14:01.840
So there's a well-known technology how to deal with the case where accesses to our data

151
0:14:01.840 --> 0:14:09.320
structures to our classes are mostly sequential than at any point in time only a single thread

152
0:14:09.320 --> 0:14:12.080
owns and operates with an object.

153
0:14:12.080 --> 0:14:15.680
Let's call bias logging or thin logging.

154
0:14:15.680 --> 0:14:23.040
Well why is it simpler and more lightweight?

155
0:14:23.040 --> 0:14:28.080
Because we don't want to manage all the complex cases.

156
0:14:28.080 --> 0:14:35.040
We know that we are in a good situation and if not yes we can fall back and it's called

157
0:14:35.040 --> 0:14:39.280
inflate our monitor.

158
0:14:39.280 --> 0:14:45.400
Well it existed in OpenJDK for ages and it has been removed from OpenJDK.

159
0:14:45.400 --> 0:14:54.960
First deprecated then no one noticed I believe because still are there too many people using

160
0:14:54.960 --> 0:14:58.600
something newer than JDK11.

161
0:14:58.600 --> 0:15:06.520
Well some consequences were noticed probably too late.

162
0:15:06.520 --> 0:15:12.000
Well what are the reasons first of all what are the reasons to remove bias logging from

163
0:15:12.000 --> 0:15:15.120
OpenJDK from hotspot JVM?

164
0:15:15.120 --> 0:15:22.720
Well to ease the implementation of virtual threads to deliver project loom to decrease

165
0:15:22.720 --> 0:15:25.400
the amount of work there.

166
0:15:25.400 --> 0:15:32.440
So some consequences here an issue is discovered.

167
0:15:32.440 --> 0:15:42.200
In certain cases things like input streams can be slowed down like here it's 8x or something

168
0:15:42.200 --> 0:15:45.440
that's enormously slow.

169
0:15:45.440 --> 0:15:54.040
And for growl VM there is a mode that you say during static compilation.

170
0:15:54.040 --> 0:16:01.680
Okay this native image doesn't try to work with many cores it's a single threaded program.

171
0:16:01.680 --> 0:16:07.800
So it's simple and it works really better in these circumstances.

172
0:16:07.800 --> 0:16:09.920
So there is an optimization for that.

173
0:16:09.920 --> 0:16:14.800
But you have to know it in advance then you compile your program.

174
0:16:14.800 --> 0:16:19.440
Well and there is of course a runtime option that supports all kinds of situations and

175
0:16:19.440 --> 0:16:21.000
it's complex.

176
0:16:21.000 --> 0:16:29.240
So the missing part is in the left lower corner well to dynamically be able to process the

177
0:16:29.240 --> 0:16:33.920
situation of sequential access pattern.

178
0:16:33.920 --> 0:16:48.360
So we implemented quite a classical approach to this problem that helps to that brings

179
0:16:48.360 --> 0:16:50.640
that thin locking to growl VM.

180
0:16:50.640 --> 0:17:02.840
The initial idea was operating with object header so where it already contains a pointer

181
0:17:02.840 --> 0:17:05.880
to a fat monitor object.

182
0:17:05.880 --> 0:17:12.520
But it can be treated as well as some words we can atomically access and put some information

183
0:17:12.520 --> 0:17:15.880
there.

184
0:17:15.880 --> 0:17:20.620
Probably close to final implementation that we have right now still or again uses a point

185
0:17:20.620 --> 0:17:29.760
or because it turned to be not so easy to keep correctness across the whole VM with

186
0:17:29.760 --> 0:17:37.440
some memory that you treat as a point or as a word depending on the situation.

187
0:17:37.440 --> 0:17:46.560
Well anyway inside that part of header or inside that special object we can have 64

188
0:17:46.560 --> 0:17:48.360
bits of information.

189
0:17:48.360 --> 0:17:55.520
And we can mark it as a thin lock this is a flag right then we can do it atomically.

190
0:17:55.520 --> 0:18:06.120
We can keep the ID of an owner thread which we can obtain then the work the threads and

191
0:18:06.120 --> 0:18:12.800
a count of recursive locks that we currently hold.

192
0:18:12.800 --> 0:18:19.920
That by the way means that after a certain amount of recursive locks we have to inflate

193
0:18:19.920 --> 0:18:28.480
monitor because we can store more information in that part of this work.

194
0:18:28.480 --> 0:18:40.000
So again it's a pure Java implementation where we work with some atomic magic and we update

195
0:18:40.000 --> 0:18:41.000
this information.

196
0:18:41.000 --> 0:18:46.160
But what we've got and the most recent numbers are even better.

197
0:18:46.160 --> 0:18:51.480
So we see that effect on exactly that example the streams.

198
0:18:51.480 --> 0:19:00.320
We can speed them up and even in a very kind of nano benchmark kind of measurement you

199
0:19:00.320 --> 0:19:16.000
also see the improvement and even in multi-threaded case there is no difference with the original.

