1
0:00:00.000 --> 0:00:11.920
Hello, everyone. Good afternoon. I am Thanos from the University of Manchester. And today

2
0:00:11.920 --> 0:00:16.960
I have the pleasure to present you Tornado VM, what is the state of Tornado VM at this

3
0:00:16.960 --> 0:00:24.440
moment. And in fact, I want to focus also on a slogan that's very known to everyone.

4
0:00:24.440 --> 0:00:32.900
The right ones run anywhere for Java. So we'll start with that. So this is a known slogan

5
0:00:32.900 --> 0:00:40.720
derived since the 90s from microsystems in a way to advertise that Java language and

6
0:00:40.720 --> 0:00:47.200
the JVM in particular, it is a platform that can ensure portability across different CPU

7
0:00:47.200 --> 0:00:54.160
structures and architectures. So the idea is that programmers can run their code, they

8
0:00:54.160 --> 0:01:03.040
can compile it once, and it can run transparently on different hardware architectures. However,

9
0:01:03.040 --> 0:01:09.320
hardware has changed in the last years. It is evolving. And perhaps this is not sufficient

10
0:01:09.320 --> 0:01:21.520
for the new types of hardware resources that are coming. So lately we have GPUs and FPGAs

11
0:01:21.520 --> 0:01:27.920
which are coming to complement the power of the CPUs in a way to maximize performance

12
0:01:27.920 --> 0:01:36.080
and reduce the energy consumption. These are good, but there is some challenges that are

13
0:01:36.080 --> 0:01:43.600
deriving and these are mainly posed in programmability. So how programmers can harness this power

14
0:01:43.600 --> 0:01:50.600
from these resources. I don't know if you have experience with OpenCL and CUDA, but

15
0:01:50.600 --> 0:01:55.120
mainly these programming models that have been designed for these hardware types to

16
0:01:55.120 --> 0:02:01.520
get access to these hardware types, they are mainly focused on the C and C++ world. So

17
0:02:01.520 --> 0:02:07.320
there are different programming models from different companies like SQL, OneAPI, NVIDIA

18
0:02:07.320 --> 0:02:16.720
CUDA, OpenCL, which is a standard that can run on all the devices. And if you have FPGA

19
0:02:16.720 --> 0:02:23.600
expertise, then perhaps you can write RTL and Verilog, which is a hardware description

20
0:02:23.600 --> 0:02:27.520
language. But this is very low level. And here we are talking about Java, so we want

21
0:02:27.520 --> 0:02:37.600
to go high level. So if you are a Java developer, then you use the JVM and you go to the CPU.

22
0:02:37.600 --> 0:02:44.920
If you want to have access to these devices, then you need to write your own native interfaces

23
0:02:44.920 --> 0:02:51.760
in the JNI and then tap into the C and C++ world. But still, you need to be aware of

24
0:02:51.760 --> 0:02:58.160
how these programming models are written. So you need to be familiar with this. And

25
0:02:58.160 --> 0:03:04.440
this is exactly the problem that Tornado VM has been designed to solve. So Tornado VM,

26
0:03:04.440 --> 0:03:14.920
it is a plugin to existing OpenJDK distributions like Amazon, and others. And the way that

27
0:03:14.920 --> 0:03:23.000
it is built, it is to enable hardware acceleration in an easy manner. So it offers a Java API.

28
0:03:23.000 --> 0:03:31.520
And it has inside a JIT compiler for the hardware devices that I saw in this figure. Our compiler

29
0:03:31.520 --> 0:03:37.960
inside, it can automatically translate the Java bytecodes to run on CPUs, multi code

30
0:03:37.960 --> 0:03:46.400
CPUs, GPUs integrated or discrete GPUs and FPGAs. And the compiler in the backend, it

31
0:03:46.400 --> 0:03:53.440
has three different backend types. It can emit OpenCLC, PTX, which is assembly for the

32
0:03:53.440 --> 0:04:02.000
CUDA for the NVIDIA GPUs. And it has recently also the SPIRV backend, which enables to utilize

33
0:04:02.000 --> 0:04:11.440
the level zero dispatcher from the one API. So Tornado VM, it is a technology that can

34
0:04:11.440 --> 0:04:17.840
be used as a JVM plugin to enable hardware acceleration for JVMs. And some of the key

35
0:04:17.840 --> 0:04:25.400
features is that it has a lightweight Java API. It is coded in a platform agnostic manner.

36
0:04:25.400 --> 0:04:32.920
So one command can be the same no matter which device it will be executed to program. And

37
0:04:32.920 --> 0:04:39.640
it can transparently at the compile time specialize the code. Because the code that is generated

38
0:04:39.640 --> 0:04:47.640
for the GPU, it is completely different from a code that is generated for an FPGA. So regarding

39
0:04:47.640 --> 0:04:54.640
the compiler, we have different phases that will be enabled for GPUs and different phases

40
0:04:54.640 --> 0:05:05.800
that will be enabled to specialize the code for an FPGA. Our code is available in GitHub.

41
0:05:05.800 --> 0:05:11.280
So we encourage everyone who wants to have a look to fork it, download, play with examples,

42
0:05:11.280 --> 0:05:20.000
or even create their own examples. And also to come back to us, I mean, to feel free to

43
0:05:20.000 --> 0:05:24.800
use the discussions to trigger the discussion if you have questions or to open issues if

44
0:05:24.800 --> 0:05:31.080
something is broken in order to fix it. And we have also available Docker images for NVIDIA

45
0:05:31.080 --> 0:05:41.040
GPUs and Intel integrated GPUs. Now the next part that I want to talk is regarding the

46
0:05:41.040 --> 0:05:49.400
API. Two weeks ago, we released a new version of Tornado VM, the version 0.15. And this

47
0:05:49.400 --> 0:05:57.640
comes with many new changes in the API level. So our goal was to make the API easier for

48
0:05:57.640 --> 0:06:04.000
Java programmers in order to use it in a comprehensive manner. So to know how to use and how to express

49
0:06:04.000 --> 0:06:11.760
parallelism from Java. But first, I have to make you familiar with the programming model

50
0:06:11.760 --> 0:06:18.680
of Tornado VM. And this programming model comes, it is inspired from the heterogeneous

51
0:06:18.680 --> 0:06:25.680
programming models like OpenCL and CUDA, the way that these programming models are operating.

52
0:06:25.680 --> 0:06:32.040
And in this sense, a Java program can be composed of two parts. The first part is the host code

53
0:06:32.040 --> 0:06:38.280
where it is the actual core of the Java application. And the second part, it is the accelerated

54
0:06:38.280 --> 0:06:44.440
code which actually it is the method or the set of methods that will be offloaded for

55
0:06:44.440 --> 0:06:51.280
execution on a GPU. Once we have made this clear, then we can move with the execution

56
0:06:51.280 --> 0:06:58.800
model which it requires first because the processing will take place on a device, it

57
0:06:58.800 --> 0:07:06.360
will have first to move the data from the host code from the CPU to the actual device.

58
0:07:06.360 --> 0:07:11.400
Then perform the processing. And once the processing is finished, then the data, the

59
0:07:11.400 --> 0:07:20.720
result will have to be transferred back to the host code. Now in Tornado VM, in the API

60
0:07:20.720 --> 0:07:26.480
of Tornado VM, we have exposed the set of objects and annotations for each of these

61
0:07:26.480 --> 0:07:32.280
two parts, the host code and the accelerated code. In the host code, we have the task graph

62
0:07:32.280 --> 0:07:38.960
object and the Tornado execution plan. The task graph corresponds to what to run on a

63
0:07:38.960 --> 0:07:45.680
GPU. And the Tornado execution plan, it is how to run on the GPU. And for the accelerated

64
0:07:45.680 --> 0:07:54.960
code, we have a set of annotations and objects that I will show you later. So let's start

65
0:07:54.960 --> 0:08:00.680
with the task graph. What to run? Assuming that you are a Java programmer, then you want

66
0:08:00.680 --> 0:08:07.920
to offload the execution of a method. In this example, method A to the GPU. This method

67
0:08:07.920 --> 0:08:16.240
has some input and some output. Now this method corresponds to what in the Tornado VM terminology

68
0:08:16.240 --> 0:08:23.640
call a task. So each method that will be offloaded for execution on hardware acceleration, it

69
0:08:23.640 --> 0:08:31.920
is a task. And it has the input data and the output data. And then we have a group of tasks

70
0:08:31.920 --> 0:08:38.000
which is the task graph. Now task graph can be a group of tasks that may have dependency

71
0:08:38.000 --> 0:08:43.600
or may not have dependency. And the programmers, they want to offload them all for hardware

72
0:08:43.600 --> 0:08:51.240
acceleration. In this particular example, I have put one task in this task graph. Once

73
0:08:51.240 --> 0:08:57.160
we have defined what to run, one question that comes, it is how often to transfer the

74
0:08:57.160 --> 0:09:05.200
data between the host, CPU and the device. And this can have a tremendous impact because

75
0:09:05.200 --> 0:09:11.560
it can affect the data transfer time. So it can have a long execution time. So it can

76
0:09:11.560 --> 0:09:18.640
affect performance. But can also affect energy. The power consumption. So how to transfer

77
0:09:18.640 --> 0:09:25.160
data, it matters. It depends on the pattern of the application. So one application may

78
0:09:25.160 --> 0:09:31.720
need to copy only the first execution if the data are read only. Then always or only in

79
0:09:31.720 --> 0:09:38.160
the last execution, for example, for the output, for the result. And here is a code snippet

80
0:09:38.160 --> 0:09:45.400
of how the task graph can be used to define this functionality in the Tornado API. So

81
0:09:45.400 --> 0:09:50.840
we create a new object, the task graph. We assign a name, which is a string. In this

82
0:09:50.840 --> 0:09:58.480
particular example, it is TG. And then we utilize the exposed methods of the API in

83
0:09:58.480 --> 0:10:05.840
order to fulfill the execution model. At first, we use the transfer to device which has two

84
0:10:05.840 --> 0:10:13.200
inputs. The first argument that we put, it is a data transfer mode which will be used

85
0:10:13.200 --> 0:10:20.560
to trigger how often the data will be moved. In this particular example, it is first execution

86
0:10:20.560 --> 0:10:25.200
shown in the first execution, the data will be moved. And then we'll have the parameter

87
0:10:25.200 --> 0:10:32.200
which is the input, which is the input array. The second method, it is the task and it defines

88
0:10:32.200 --> 0:10:39.560
which method will be used for hardware acceleration. The first parameter, it is a name, a string

89
0:10:39.560 --> 0:10:46.720
actually of the method. It could be any name. And this is associated for the dynamic configuration

90
0:10:46.720 --> 0:10:51.720
which I will show you later. The second parameter, it is the method reference. So the reference

91
0:10:51.720 --> 0:10:58.400
to the method that will be offloaded to the GPU for acceleration. And then it is the list

92
0:10:58.400 --> 0:11:05.440
of parameters of this method that corresponds to the method signature. And the last method,

93
0:11:05.440 --> 0:11:12.640
it is the transfer to host. And this, again, this method, it is configured the first argument

94
0:11:12.640 --> 0:11:18.640
to be the data transfer mode. In this example, we will copy the data, the output in every

95
0:11:18.640 --> 0:11:29.120
execution. Okay. And once we have defined the task through the task graph, this task

96
0:11:29.120 --> 0:11:37.680
can be appended, can be updated. We can add a new task, a second task. We can change the

97
0:11:37.680 --> 0:11:44.000
way that the data transfer will be triggered in every execution only in the first execution.

98
0:11:44.000 --> 0:11:53.000
Then the next step, it is to define the immutable state of the task graph. So how to preserve

99
0:11:53.000 --> 0:11:59.720
the shape of a task graph. And this is done by taking a snapshot of the task graph, by

100
0:11:59.720 --> 0:12:06.480
using the snapshot method in the task graph object. Then we retrieve back an immutable

101
0:12:06.480 --> 0:12:13.000
task graph. And this means that this can be used for JIT compilation and execution on

102
0:12:13.000 --> 0:12:19.760
the hardware. And this ensures that the Java programmers, they can create different versions

103
0:12:19.760 --> 0:12:25.920
of their task graph. They can update it. And then the code that will have Internet of VM,

104
0:12:25.920 --> 0:12:31.720
it can store all these versions. It doesn't need to recompile and override the generated

105
0:12:31.720 --> 0:12:41.640
code. And this is the final step before we move to the actual execution plan. We have

106
0:12:41.640 --> 0:12:47.200
the mutable state of the task graph that can be modified and the immutable task graph that

107
0:12:47.200 --> 0:12:53.880
it cannot be modified anymore. So if the users, they want to do a change, they can still change

108
0:12:53.880 --> 0:13:03.320
the task graph and get a new snapshot for a second version of their code. And now we

109
0:13:03.320 --> 0:13:09.720
move to how to run, how to execute the task graph. And this is done through the execution

110
0:13:09.720 --> 0:13:18.480
plan. Here is a snippet of a tornado execution plan. We create a new object that accepts

111
0:13:18.480 --> 0:13:24.320
as input the mutable task graph. That doesn't change anymore. And then we can either directly

112
0:13:24.320 --> 0:13:30.800
execute it in the default execution mode by invoking the execution plan.execute method

113
0:13:30.800 --> 0:13:38.000
or we can configure it with some various optimizations. In this particular example, I have enabled

114
0:13:38.000 --> 0:13:45.160
the configuration to run with dynamic reconfiguration, which is a feature in tornado VM that will

115
0:13:45.160 --> 0:13:51.560
launch a Java thread to get compiled and execute the application per device that is available

116
0:13:51.560 --> 0:13:58.240
on the system. So we can have a CPU, a GPU, an FPGA. Java thread will run for all the

117
0:13:58.240 --> 0:14:03.080
devices. And then it is triggered with a policy of performance, which means that the first

118
0:14:03.080 --> 0:14:08.680
device that will finish the execution, it will be the best. And the rest, Java threads

119
0:14:08.680 --> 0:14:19.640
will be killed. Now I have concluded the part of the host code. We can briefly go to the

120
0:14:19.640 --> 0:14:25.320
accelerated code, which is the way to express parallelism within the kernel, within the

121
0:14:25.320 --> 0:14:33.000
method or the tornado VM task, as we call it. We have two ways, two APIs. The first

122
0:14:33.000 --> 0:14:40.080
one is called loop parallel API. And in a sense, we expose the add parallel annotations

123
0:14:40.080 --> 0:14:46.720
that can be used by programmers to as a hint to the tornado VM compiler that these loops

124
0:14:46.720 --> 0:14:55.120
can run in parallel. And the second one is the kernel API, which is an API exposed to

125
0:14:55.120 --> 0:15:01.680
the users through the kernel context object. And in a sense, the meaning of this API, it

126
0:15:01.680 --> 0:15:09.160
is meant for open CL and programmers or Java programmers who know open CL in a way to get

127
0:15:09.160 --> 0:15:16.160
more freedom on how to code things so they can get access to local memory, which is equivalent

128
0:15:16.160 --> 0:15:24.080
to the memory of the CPU for GPUs. So they have more freedom on what to express. And

129
0:15:24.080 --> 0:15:34.040
in fact, I have used this API to port existing kernels written in open CL and CUDA to Java.

130
0:15:34.040 --> 0:15:39.280
For more information, you can use this link, which is the actual documentation of tornado

131
0:15:39.280 --> 0:15:47.360
VM and describes some examples. I will briefly go to one example of a matrix multiplication,

132
0:15:47.360 --> 0:15:54.600
which I presented last year in force them. So in this example, we have the accelerated

133
0:15:54.600 --> 0:16:02.720
code and the host code. The matrix multiplication method implements matrix multiplication over

134
0:16:02.720 --> 0:16:09.840
a flattened arrays in two dimensions. And the way to annotate and express parallelism

135
0:16:09.840 --> 0:16:15.360
using the add parallel annotation, it would be to add the add parallel annotation inside

136
0:16:15.360 --> 0:16:23.720
the four loops. That means that we indicate that these loops could be executed in parallel.

137
0:16:23.720 --> 0:16:31.120
And now regarding the second API, the kernel API, we would use the kernel context object.

138
0:16:31.120 --> 0:16:40.480
And in particular, we would use the global ID, X and Y, which correspond to the two dimensions

139
0:16:40.480 --> 0:16:53.640
that we have. So in a sense, it is like having the thread ID that will execute on the GPU.

140
0:16:53.640 --> 0:17:01.880
Here are some use cases that we used tornado VM. And concluding this talk, I would like

141
0:17:01.880 --> 0:17:08.440
to focus on a feature that we implemented in a project that we are working. It is called

142
0:17:08.440 --> 0:17:13.520
Elegant and the idea is to create a software stack that unifies development for big data

143
0:17:13.520 --> 0:17:21.920
and IOT deployment. And there, tornado VM is used as a technology to enable acceleration

144
0:17:21.920 --> 0:17:28.200
as a service. So we have implemented the REST API. It is still a prototype. But the programmers,

145
0:17:28.200 --> 0:17:33.400
they can write a method, they can specify a method, they can specify the characteristics

146
0:17:33.400 --> 0:17:40.120
of the targeted device. And then the service will return back open CL code that it is meant

147
0:17:40.120 --> 0:17:47.240
to run parallel this function. The interesting part is that this code, the open CL code,

148
0:17:47.240 --> 0:17:53.960
it is generated to be portable across different programming languages. So it doesn't only

149
0:17:53.960 --> 0:18:03.760
bind to Java, it can run also through C++, Python, because it is open CL. And this means

150
0:18:03.760 --> 0:18:12.000
that in this particular example, we have Java, we use OpenJDK, we take the byte code and

151
0:18:12.000 --> 0:18:17.920
we pass the byte code to tornado VM. And tornado VM is running on an experimental feature which

152
0:18:17.920 --> 0:18:23.840
is called code interoperability mode. And in this mode, it converts this byte code to

153
0:18:23.840 --> 0:18:29.720
open CL that can run from any programming language and run time. Therefore, it is like

154
0:18:29.720 --> 0:18:39.840
prototyping in Java for parallel programming. Wrapping up, we would like to receive feedback

155
0:18:39.840 --> 0:18:46.380
and we are looking also for collaborations if we can help to port and use cases or for

156
0:18:46.380 --> 0:18:55.040
any other issues. And summarizing this talk, I briefly went through the right ones run

157
0:18:55.040 --> 0:19:00.960
anywhere in the context of heterogeneous hardware acceleration. I have familiarized you with

158
0:19:00.960 --> 0:19:07.260
tornado VM, which is an open source project. And the code base is available in GitHub.

159
0:19:07.260 --> 0:19:14.120
And familiarize you with the programming model of tornado VM and the new API, how to use

160
0:19:14.120 --> 0:19:22.960
it. And more are about to come in the FUJ blog with a new blog. So, finally, just to

161
0:19:22.960 --> 0:19:28.000
acknowledge the projects that they have supported our research in the University of Manchester

162
0:19:28.000 --> 0:19:44.280
and I'm ready for questions.

