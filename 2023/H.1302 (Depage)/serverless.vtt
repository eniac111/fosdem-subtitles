WEBVTT

00:00.000 --> 00:15.360
I'm a Dromihaly. I used to work at Pyra, so maybe some of you know me from my six years

00:15.360 --> 00:25.600
at Pyra company. Now I work at Omnifish, and we, with our cofounders and employees,

00:25.600 --> 00:31.680
were a support class fish server. So back to the roots, kind of. But this time I'd like

00:31.680 --> 00:40.440
to talk about Java, plain Java and Jakarta EE, and how it all fits together when we combine

00:40.440 --> 00:55.680
that with AWS. So first, before I talked about AWS, let's ask why do we want to have Java

00:55.680 --> 01:06.800
fast? Or do we want to have Java start fast? I think everybody wants that, but why? Because

01:06.800 --> 01:14.880
it's cool. Or because we need it. So there were times when we really didn't need that,

01:14.880 --> 01:20.080
when we had the application servers, it was a pain, that it took a while to start. But

01:20.080 --> 01:26.040
in production it was already running, so there was no real business need for that, only to

01:26.040 --> 01:32.960
make developers happy and be more productive with developing code. But now we have several

01:32.960 --> 01:38.960
use cases where it's really needed, because the more time it takes for Java program to

01:38.960 --> 01:47.280
start, it costs more money, and it's not user friendly. And one example, a perfect example

01:47.280 --> 01:55.120
of this is AWS Lambda. So now, what is AWS Lambda? It's basically a service to which

01:55.120 --> 02:02.280
you can deploy your code, and this service runs your code only when it's needed, and

02:02.280 --> 02:10.240
it also charges you, because we need to pay for the cloud environment. But if we run the

02:10.240 --> 02:16.200
code in Lambda, we are charged only for the time when the code is running. And that's

02:16.200 --> 02:24.520
pretty nice, especially if we have code that usually just sits there and responds to users

02:24.520 --> 02:30.200
just once in a while, or only during certain periods of time, especially during the day

02:30.200 --> 02:38.560
or in the morning when there is some business activity. So how does AWS Lambda do that?

02:38.560 --> 02:45.360
It basically creates environment and deploys our code when it needs to be executed. And

02:45.360 --> 02:52.560
for that, if the code is not already deployed, it needs to create the runtime and initialize

02:52.560 --> 02:59.160
our so-called function, because this is how our code is called. It's called a function,

02:59.160 --> 03:04.440
because it's basically just called by the runtime, it gives some result, and then it's

03:04.440 --> 03:13.680
thrown away. In reality, it's not always thrown away, because AWS Lambda tries to cache our

03:13.680 --> 03:21.040
code so that it doesn't have to reinitialize it every time when it's run more frequently.

03:21.040 --> 03:27.520
So sometimes it stays there, and then AWS Lambda can skip the initialization phase.

03:27.520 --> 03:33.320
This is called warm start, because the code is already prepared to serve things. But if

03:33.320 --> 03:38.520
this doesn't happen and the code is not available, it has to initialize everything, and this

03:38.520 --> 03:44.720
is usually referred to as cold start, just start from scratch.

03:44.720 --> 03:50.200
So the whole lifecycle of AWS Lambda is on the slide. You can see there's in its phase.

03:50.200 --> 03:57.160
This is only when the code or the function is not initialized. So in case of cold start,

03:57.160 --> 04:05.560
when there is a warm phase, which happens even for warm startups, there is this invoke

04:05.560 --> 04:12.240
phase, which actually is the only productive phase from these three. It actually does some

04:12.240 --> 04:18.440
job. The first phase only initialized gets some things ready before the application can

04:18.440 --> 04:24.520
process requests. Then the invoke phase does the job. And then when AWS Lambda service

04:24.520 --> 04:33.760
decides that it no longer needs our application running, because it's not doing anything right

04:33.760 --> 04:42.880
now and AWS wants to use resources in some other way, it will tear everything away. So

04:42.880 --> 04:51.520
it will shut down the environment. And then we're at square zero. And next invocation

04:51.520 --> 04:57.920
needs to go through the initial initialization phase.

04:57.920 --> 05:06.040
So let's now go back to the roots with plain Java application. And let's see or let's think

05:06.040 --> 05:14.640
about how fast we can get with Java on AWS Lambda. Can we start Java really fast? I tried

05:14.640 --> 05:23.240
to start a very simple Java program on my local machine. And if you do that, too, on

05:23.240 --> 05:30.560
your computers, you will see that Java really starts fast. In my case, it was 50 milliseconds,

05:30.560 --> 05:40.600
0.05 seconds. So very small fraction of a second, where JVM started, printed something

05:40.600 --> 05:47.960
on output and finished. So we see on local computer, plain Java doesn't take very long

05:47.960 --> 05:58.160
to start. If we compare exactly what's going on in AWS Lambda, because AWS Lambda needs

05:58.160 --> 06:05.240
to initialize the environment and only then it can run Java function, it takes a bit longer

06:05.240 --> 06:09.720
in reality. But when we compare it to other languages, I haven't done this. This is done

06:09.720 --> 06:17.160
by some other guy who is more experienced with AWS Lambda than me and compared performance

06:17.160 --> 06:24.600
in a more sophisticated way than just running on the computer or just several measurements.

06:24.600 --> 06:31.560
He did a lot of measurements across all or various different languages, various different

06:31.560 --> 06:38.160
run times provided by AWS Lambda. And he found out that Java basically is on the same level

06:38.160 --> 06:43.520
as JavaScript, Python and a lot of other languages. There's not much difference. There's a small

06:43.520 --> 06:52.000
difference that at that time C sharp was a bit slower. But as AWS improves continually,

06:52.000 --> 06:57.720
the AWS Lambda, even these numbers would be probably better now. And C sharp and Docker

06:57.720 --> 07:04.800
would be maybe more even with the rest because the technology running AWS Lambda is continuously

07:04.800 --> 07:11.480
improving. But this is just to compare and show that Java itself or even the implementation

07:11.480 --> 07:19.440
of Java AWS function or the environment isn't worse than other languages.

07:19.440 --> 07:26.560
So now what is the problem actually? Why a lot of people perceive that Java starts very

07:26.560 --> 07:35.240
slow? The problem, at least how I see it, is that many people don't think about Java

07:35.240 --> 07:41.600
in this simple way, that it's a simple application. A lot of people think about Java as a language

07:41.600 --> 07:47.560
that runs enterprise applications. And with enterprise applications we're used to use

07:47.560 --> 07:53.320
frameworks that do a lot of job for us. We run the applications on application servers

07:53.320 --> 08:00.360
which are slow to start. And this is what we think about when we say Java or when we

08:00.360 --> 08:11.640
talk about Java. So now we're coming to that thing. If we basically can run our applications

08:11.640 --> 08:20.600
that are similar to what we were used to before, but if we can start them fast, we could solve

08:20.600 --> 08:26.080
a problem with Java cold starts, at least as we use Java now.

08:26.080 --> 08:31.800
So the question I have now is Jakarta E or some other frameworks like Springboard or

08:31.800 --> 08:42.360
something like that, can that be as fast as plain Java? Can we run that in AWS Lambda

08:42.360 --> 08:50.640
to get good performance and fast startup? And the answer is there are such frameworks

08:50.640 --> 08:56.400
and solutions to that. There are several ones. I don't have much time to talk about all of

08:56.400 --> 09:03.720
them. So I picked one that I personally like. And it's called Piranha Cloud Framework. And

09:03.720 --> 09:11.400
this one is based on entirely Jakarta EE APIs. Previously it was called Java EE. So it's

09:11.400 --> 09:18.560
a very well-known API that a lot of people already know, a lot of tools out there already

09:18.560 --> 09:27.200
use. So it's interoperable with existing code base. But the thing with Piranha Cloud

09:27.200 --> 09:36.200
is that the implementation, actually the engine of the framework is new, very flexible, and

09:36.200 --> 09:46.200
allows our application to start very fast. Piranha Cloud is based on a lot of existing

09:46.200 --> 09:52.080
components. A lot of them come from the Glassfish server, which actually sort of proves that

09:52.080 --> 09:58.080
the server is not the problem or Jakarta EE is not the problem. The components are there.

09:58.080 --> 10:03.400
They are quite fast. But the problem how they are assembled in traditional Jakarta EE servers,

10:03.400 --> 10:10.120
that is the problem. Because an application server usually has a lot of other things that

10:10.120 --> 10:18.920
we don't need in Lambda, like monitoring a lot of vendor features and go on an administration

10:18.920 --> 10:29.920
console and a lot of other things. So here's an example. It's basically nothing else than

10:29.920 --> 10:39.520
a servlet. But this is an application using already some Jakarta EE APIs. And this application,

10:39.520 --> 10:45.960
this servlet, you can run on any Jakarta EE server. You can run it on Tomcat. You can

10:45.960 --> 10:51.800
run it on Glassfish. You can run it on anything that supports servlets. So the only difference

10:51.800 --> 10:58.400
if we run it with Piranha Cloud is that it starts fast and it uses Piranha's own servlet

10:58.400 --> 11:06.240
container, which was designed from scratch. And it's very flexible and fast. What is also

11:06.240 --> 11:14.480
nice about Piranha's servlet container is that it can be embedded very easily. And that's

11:14.480 --> 11:22.400
the crucial point. When we want to use Jakarta EE in Lambda, we need to basically shave off

11:22.400 --> 11:28.480
everything that we don't need. And in AWS Lambda, we don't even need an HTTP listener. Because

11:28.480 --> 11:36.000
AWS Lambda basically only wants a method from us that will be called returns some response.

11:36.000 --> 11:41.560
And then AWS Lambda is responsible for mapping the HTTP request to an object that it passes

11:41.560 --> 11:49.000
to our method. And then the returned object should be mapped to an HTTP response. And

11:49.000 --> 11:54.520
not only HTTP requests and responses, but Lambda can handle any type of basically JSON

11:54.520 --> 12:01.880
messages, JSON events. So the only thing that our application needs is to parse some input

12:01.880 --> 12:15.400
object and return some output event. And with Piranha, we can create an engine and map our

12:15.400 --> 12:26.440
servlet onto it and just listen on some object. This object is usually called or the request

12:26.440 --> 12:33.800
cycle is invoked by a service method, which accepts a request object and returns a response

12:33.800 --> 12:40.200
object. And this is exactly how we can use it in AWS Lambda. We just need to add one

12:40.200 --> 12:49.160
additional layer to map AWS request object to Piranha, request object and back. If we

12:49.160 --> 12:56.600
run Piranha Cloud, this simple servlet, which is comparable to our plain Java we were running

12:56.600 --> 13:03.320
before, if you remember with plain Java on my computer, I had startup times. Actually,

13:03.320 --> 13:09.520
it was not only startup times, but until the program ended and printed some message and

13:09.520 --> 13:19.560
finished, it was around 50 milliseconds. With Piranha, it's a bit longer time, but this

13:19.560 --> 13:26.400
already includes the first request. So it's very similar to the plain Java application.

13:26.400 --> 13:32.800
It's not only that the engine starts, but it actually serves the request response with

13:32.800 --> 13:42.080
text message through HTTP stack. And with that, it takes still comparable time around

13:42.080 --> 13:55.760
130 milliseconds. Now we can compare how it works in AWS Lambda. In AWS Lambda, I have

13:55.760 --> 14:01.520
a picture, but I hope I will be able to show you in a minute. As I said before, it takes

14:01.520 --> 14:08.680
a bit longer when we start the function first time. Because this doesn't really matter if

14:08.680 --> 14:18.000
we run Java or any other runtime, AWS Lambda first needs to create some environment to

14:18.000 --> 14:25.280
execute our code in. And it takes a little bit of time. But together with creating this

14:25.280 --> 14:32.840
environment and running our code, our example Piranha function, it takes under one second

14:32.840 --> 14:38.880
to serve the request. Even if nothing was ready before, even on the first time we tried

14:38.880 --> 14:46.640
to run the function, it still serves the response under one second. If we tried again, again,

14:46.640 --> 14:54.760
again, then the response times are much faster. This is on the right side here. It's under

14:54.760 --> 15:01.040
two milliseconds. Because this is only the code that needs to serve the request. Everything

15:01.040 --> 15:07.320
was initialized. Environment was initialized. The Piranha engine was initialized. It's cached

15:07.320 --> 15:14.160
in a static variable. So it's part of the process that is already live. AWS just executes

15:14.160 --> 15:21.160
a method basically on the Piranha engine that goes through the servlet and creates the servlet

15:21.160 --> 15:26.800
response. And that's it. That's why it takes only two milliseconds. This is only the time

15:26.800 --> 15:37.320
required to serve the actual response. So I'll try. I think I have a link here. Hope

15:37.320 --> 15:58.880
it works. So this is the actual AWS console where I already deployed the application,

15:58.880 --> 16:08.480
the function. And AWS console has a nice feature called test button. With that, we can directly

16:08.480 --> 16:14.800
invoke the lambda. Normally, we would have to create an API gateway and map it to lambda

16:14.800 --> 16:22.720
so that we can access lambda from the HTTP from outside. AWS can also generate some URL

16:22.720 --> 16:28.680
that we can use to invoke the lambda. But this is like directly executing the lambda

16:28.680 --> 16:37.120
without actually invoking an HTTP request. So with this, there is some example data.

16:37.120 --> 16:44.000
But the application doesn't read anything from the request. It just responds with some

16:44.000 --> 16:51.120
hello world message. And if we execute it, you see it takes a bit of time. And this is

16:51.120 --> 17:04.000
what I had in my slides. Here is even shorter. 850 milliseconds. But if we try it again,

17:04.000 --> 17:16.840
it's already pre-warmed because AWS caches the environment. And now it's just two milliseconds.

17:16.840 --> 17:28.320
So now the question is when the cold starts happening. They happen. I don't have an experience

17:28.320 --> 17:33.520
how much they have an impact. I heard that it's not much of an impact because they happen

17:33.520 --> 17:41.480
only once in a while. So the response is once in a while takes one more second. So on top

17:41.480 --> 17:49.240
of request processing. But if it takes five seconds, which can happen with normal Spring

17:49.240 --> 17:55.440
Boot application or traditional frameworks or even application servers, I don't know,

17:55.440 --> 18:01.080
sometimes some application servers can be embedded. Then you can run them in AWS lambda.

18:01.080 --> 18:07.600
But some of them really are hard to basically map to the method call. So you have to install

18:07.600 --> 18:12.360
application server. And for that, it's not even possible to run application servers in

18:12.360 --> 18:19.400
lambda. But if you did, it would take 10, 20 seconds with some application servers.

18:19.400 --> 18:27.080
And that's really a difference. You pay for the execution time, but you also have exposure

18:27.080 --> 18:34.920
users to waiting for a couple of seconds. If it's a user facing lambda. If it's not,

18:34.920 --> 18:40.000
you maybe don't care so much. If it's something that some bad job that takes two, three minutes

18:40.000 --> 18:53.920
to finish, then a couple of seconds don't really matter. So here's a slide about the

18:53.920 --> 19:02.040
piranha cloud. In short, piranha cloud is basically, as I said, based on a new servlet

19:02.040 --> 19:09.040
container designed from scratch and a lot of components built on top of it. The servlet

19:09.040 --> 19:17.480
container being servlet implementation can run any servlet out there. And a lot of jakatai

19:17.480 --> 19:22.920
technologies are created as servlets. So for example, Jersey, as a servlet, can be deployed

19:22.920 --> 19:32.640
on piranha. And that's quite an easy way how to get a REST endpoint or REST library on

19:32.640 --> 19:40.000
piranha to deploy Jersey as a servlet. And then we have everything that Jersey provides.

19:40.000 --> 19:45.360
We can embed piranha, as I did in my demo. But we can also build a war application and

19:45.360 --> 19:50.360
run the war application with piranha on command line. This is using jakatai distributions,

19:50.360 --> 19:57.560
which already contain this distribution of packages, distribution of auxiliary of piranha

19:57.560 --> 20:04.220
that are mostly used. And the last thing, it's plain Java. There's no real magic. There's

20:04.220 --> 20:10.720
no generated code. Everything is just clean code written by clever people, I think. At

20:10.720 --> 20:14.840
least jesing on the code there when I looked at the code, it looks like the people were

20:14.840 --> 20:21.760
very clever. So with piranha cloud, we were able to achieve

20:21.760 --> 20:27.480
quite fast startup times. But it still takes a couple of milliseconds, 100, 200. It depends

20:27.480 --> 20:34.680
on how our application is complex. It may end up to two seconds, even if we add all

20:34.680 --> 20:43.680
the jakatai functionality that piranha cloud provides. If we want to reduce that even further,

20:43.680 --> 20:49.200
we have some general Java options to do that. We can first increase the CPU and RAM on the

20:49.200 --> 20:59.840
lambda, which we can always do with any language. But we can also use a faster JVM. On the last

20:59.840 --> 21:07.280
slide, I have a table where I compared running the same application with Java 11 and Java

21:07.280 --> 21:14.520
17. If you look at the numbers, Java 17 is mostly, most of the time, a bit faster. So

21:14.520 --> 21:23.560
just by deciding which Java version we use, we can get a bit better startup time. Then

21:23.560 --> 21:29.680
the last option here is basically a combination. I did some experiments which options work

21:29.680 --> 21:35.240
well regarding to the startup time or reducing the startup time. And in the end, not many

21:35.240 --> 21:41.760
things matter. But what matters is class data sharing, which basically caches class information.

21:41.760 --> 21:49.120
So it doesn't have to be loaded and processed in the beginning. It's already pre-computed

21:49.120 --> 21:57.480
before a cold start. And tinkering with compiler, we can disable second level just-in-time compiler

21:57.480 --> 22:06.320
if we want to really focus on startup time. And then there are other more magical options,

22:06.320 --> 22:12.520
but they can even reduce performance or reduce startup time almost to zero, either compiling

22:12.520 --> 22:19.600
the code to GraalVM with GraalVM to a native binary, which runs the application almost

22:19.600 --> 22:29.360
instantly. Or we can use crack, which is a coordinated restore and checkpoint mechanism.

22:29.360 --> 22:38.480
The next talk will be about it also. And what is also nice is that AWS Lambda integrated

22:38.480 --> 22:46.320
that basically in one of their Java runtimes. And it's called SnapStart. So you can get

22:46.320 --> 22:54.160
it for free, but only with Java 11. But hopefully Java 17 support will be coming soon. And this

22:54.160 --> 22:59.600
works in a way that your application basically stores or you at the build time can store

22:59.600 --> 23:05.160
checkpoints of your application with all the memory or all the information basically like

23:05.160 --> 23:11.920
hibernates. You can hibernate your application. And then it started again and again. And cold

23:11.920 --> 23:16.400
start and warm start in that case basically don't make a difference because they start

23:16.400 --> 23:24.120
from the same point. That's all from me. If you have any questions, let me know. Thank

23:24.120 --> 23:42.640
you for watching.
