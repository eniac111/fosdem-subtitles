WEBVTT

00:00.000 --> 00:00.000
All right.

00:02.000 --> 00:03.000
So anyone thank you.

00:03.000 --> 00:10.000
Let's welcome Francesco on the present and future of that.

00:10.000 --> 00:11.000
Okay.

00:11.000 --> 00:16.000
Thanks, everyone for this session is about office talk.

00:16.000 --> 00:17.000
Okay.

00:17.000 --> 00:18.000
Thank you.

00:18.000 --> 00:19.000
Thank you.

00:19.000 --> 00:20.000
Thank you.

00:20.000 --> 00:21.000
Thank you.

00:21.000 --> 00:22.000
Thank you.

00:22.000 --> 00:23.000
Thank you.

00:23.000 --> 00:24.000
Thank you.

00:24.000 --> 00:25.000
Thank you.

00:25.000 --> 00:26.000
Thank you.

00:26.000 --> 00:27.000
Thank you.

00:27.000 --> 00:28.000
Thank you.

00:28.000 --> 00:29.000
Thank you.

00:29.000 --> 00:40.000
So what we're going to talk about office talk and self integration with a quick glance to the Kubernetes world is a quick agenda.

00:40.000 --> 00:51.000
I'm going to talk about what what have been the integration with staff in the office talk system in the office of community in general was the status of the heart.

00:51.000 --> 00:58.000
Has been changed with the Cephi DM in the Burmese world and what means having Kubernetes in this picture.

00:58.000 --> 01:07.000
There is also a demo which is I'm not going to show the demo but it's linked in the session so you can feel free to take it later.

01:07.000 --> 01:12.000
So for reasons not familiar with the office project is just pretty old at this point.

01:12.000 --> 01:16.000
It's the infrastructure as a service.

01:16.000 --> 01:22.000
As you can see on the right side there are a lot of projects being part of the office talk ecosystem.

01:22.000 --> 01:30.000
Each of them provide interfaces to each other and they cooperate together to provide processing storage network resources.

01:30.000 --> 01:36.000
You basically can build your cloud infrastructure using all these projects together.

01:36.000 --> 01:53.000
It's open source. It's now 10 years, 13 years old. So it's there are a lot of projects that are stable now and safe is part of this picture in the sense that you can probably don't see this picture very well.

01:53.000 --> 02:08.000
But in both computes in the storage part we have Ceph that is basically can be used as a back end for Nova which is the compute processing component.

02:08.000 --> 02:11.000
So we can provide the female disk using Ceph as a back end.

02:11.000 --> 02:25.000
We have Manila providing object. There is a good providing file. Swift providing object. There is a long story with the integration with Rados gateway.

02:25.000 --> 02:41.000
Glance for images. So basically all this component you see there in the storage area they can be can use Ceph as a back end and this is a justification to have this integration with these two technologies.

02:41.000 --> 03:04.000
So why the integration and why is relevant? There is the HCI. I will convert the infrastructures operators who can save hardware resources collocating the compute part of the infrastructure and OSD nodes.

03:04.000 --> 03:18.000
This means that you can save hardware. You can have both technologies together serving as an infrastructure as a full operational infrastructure and a couple things.

03:18.000 --> 03:29.000
I just this is funny. I just asked a chat GPT why Ceph is relevant and why the integration of these two projects are interesting.

03:29.000 --> 03:42.000
And I just put there you cannot see probably this this part on the right. But it's basically my marketing sentence on why this thing makes sense.

03:42.000 --> 03:51.000
And there is scalability resiliency. It's scalability and all this kind of keywords that we like a lot.

03:51.000 --> 04:03.000
But this session is about orchestrating services deploying always stuck and safe together. There have been a lot of deployment tools and strategies over the past.

04:03.000 --> 04:17.000
And I want just to focus on the fancy ball and safe atm because the fancy ball has been the official tool the most useful tool in the past and now it's a FADM things have changed a bit especially in the office ecosystem.

04:17.000 --> 04:26.000
So previously the approach was I need my infrastructure as a service. I need to deploy office stock and safe.

04:26.000 --> 04:37.000
I want to describe my entire cluster with a lot of tons of tons of variables and I push that magic button that make it so.

04:37.000 --> 04:49.000
So the fancy ball was there in this picture during the deploying of office stock. There was this particular part where it's a fancy ball was triggered to deploy stuff behind the scene.

04:49.000 --> 05:06.000
And so the drawback of this solution is that if you need to change anything in your safe cluster you have to run again the playbook the safe answer would playbook because there is this answerable layer that manage everything for you.

05:06.000 --> 05:20.000
And it needs to stay consistent with the status of the cluster. So basically variables and are the human operator is supposed to provide variables and maintain those variables which is a bit different.

05:20.000 --> 05:28.000
Also it affects scale down scale up operation and day to operation especially day to operation.

05:28.000 --> 05:40.000
I want to change something in my safe cluster. I need to run the deployment again because I can rely on the answerable lead in potency basically.

05:40.000 --> 05:48.000
But with FADM things are a bit different. The status of the cluster is not made by tons of answerable variables.

05:48.000 --> 06:01.000
There is a FADM which is able to keep the status of the cluster watch the cluster and take an action. I want to deploy a new SD whenever I attach a new disk.

06:01.000 --> 06:19.000
I can do that and I don't have to run deployment again or doing any fancy day to operation with my tool that is supposed to manage my infrastructure in general which is made by both of a stock and safe.

06:19.000 --> 06:37.000
And this changes a bit because we had the safe answerable world where we describe all our clouds infrastructure. We push that magic button and everything was deployed and sometimes broken.

06:37.000 --> 06:52.000
But now operators are more aware about the steps so things have changed because you have to provide networks and networks means that you want to manage your hardware.

06:52.000 --> 07:04.000
You want to build your networks. You want to use this specifically for the trivial project where we integrated the fancy one in the past and now we're moving to safe FADM.

07:04.000 --> 07:12.000
And now people should provide networks should provide metal the description of the nodes and they are just separated steps.

07:12.000 --> 07:23.000
The storage safe is deployed first so you can start deploying your office like infrastructure with a minimal safe cluster already deployed.

07:23.000 --> 07:35.000
And when you deploy office talk you can say I need I have seen there I need volumes I need the volume spool and I can finalize my cluster creating and configuring the safe cluster accordingly.

07:35.000 --> 07:51.000
I have Manila I need self-efface I can deploy MDS doing other stuff behind the scene but it's basically you're basically aware that according to the service you're deploying in your office stock infrastructure.

07:51.000 --> 08:03.000
You can enable pieces in your safe cluster and you can just tune hit accordingly at that point we still have the answerable layer managing open stock and all the infrastructure in general.

08:03.000 --> 08:08.000
But at that point the safe cluster is seen as a separated entity.

08:08.000 --> 08:16.000
So it's like having an external safe cluster even though you have the same tool deploying both technologies together.

08:16.000 --> 08:28.000
And Saff FADM and the manager the orchestrator is that piece in safe that is supposed to provide interfaces where the operators can interact with.

08:28.000 --> 08:53.000
And it's basically this light with the lab answerable doing everything on top but the orchestrator interface is what you can rely on to make changes in your cluster without having to redeploy everything again around the answerable that can take a lot of time if your infrastructure is large.

08:53.000 --> 09:01.000
And we don't have any the operator is not supposed to keep and maintain any variable in the answerable system.

09:01.000 --> 09:16.000
You can just interact with the Saff FADM CLI create a spec which is a little definition of the self service that will be translated by the orchestrator and it will be deployed as a new demon in the safe cluster.

09:16.000 --> 09:34.000
So this is about why it's so easy because you have multiple at some point you can just boost up a minimal safe cluster with this common bootstrap providing a monitor repair address because networks are already there at that stage.

09:34.000 --> 09:53.000
And you can create a spec file that is basically the description of the nodes that should be enrolled in safe and you can just apply them. It's even easier rather than running a nonceable with a lot of roles execution time which is long enough.

09:53.000 --> 10:02.000
And they do operation are complicated are complicated because not just because of this slide and this is the interaction with the Saff FADM CLI.

10:02.000 --> 10:15.000
You can query the cluster and see the study of status. You can see where demons are placed. You can list the host and manage the labels and assign roles to this host.

10:15.000 --> 10:30.000
You can do a lot of things. But the real point here is that with Saff FADM we don't have the need to run again all the deployment of the office stock infrastructure.

10:30.000 --> 10:40.000
An example of this of our projects are impacted by this this change is Manila is not just because we need a new version of liberal B.D.

10:40.000 --> 10:58.000
We need to be compatible and we we are changing from safe until to safe FADM. But just because we are doing architectural changes to the use cases provided by office stock Manila is that service that curves out safe face volumes and provide them

10:58.000 --> 11:10.000
to the virtual machine within tenants which means that you have a dedicated network that you can use to mount your shares and behind the scene we have Saff FADM or NFS with Ganesha.

11:10.000 --> 11:34.000
In the past Manila was supposed to use the bus to interact with the NFS Ganesha and it was complicated because it we had to run privileged containers. We had to use this interface to update and manage shares within using Ganesha as a gateway.

11:34.000 --> 11:51.000
And from an architectural point of view we had the inactive passive model with made by a piecemaker and system D. So you basically had this maker honing the virtual IP as an entry point and then one active Ganesha even though you have more than one instance.

11:51.000 --> 11:57.000
So and and with some constraints with system D.

11:57.000 --> 12:08.000
Now it's FADM there is an interface with the manager with the NFS cluster and Manila can use a new driver to interact with with this component.

12:08.000 --> 12:14.000
So we don't have to do the bus anymore. We don't have to do the bus to the Ganesha container anymore.

12:14.000 --> 12:37.000
And that's the new model where we rely on the ingress daemon provided by Saff FADM and this ingress daemon is made by HAProxy and KepaLivD. KepaLivD honing the V as an entry point HAProxy for load balancing across the distributing the load across the NFS Ganesha server.

12:37.000 --> 12:54.000
It's a better approach. We still have some limitation on this area because considering that Manila is an infrastructure service for Obestack but providing shares within the tenant virtual machines which meet with a dedicated network.

12:54.000 --> 13:16.000
We need client restrictions to avoid other tenants mounting the same share. And there is an effort doing the proxy protocol in Ganesha to make sure that we can use client restriction with this new model which is the current limitation basically.

13:16.000 --> 13:28.000
Or at least there is some effort to provide floating stable IP addresses to the ingress daemon and skip the HAProxy layer which is always an additional hope.

13:28.000 --> 13:36.000
And in terms of performances this can be something that has an impact of course.

13:36.000 --> 13:54.000
Lastly, at this point we had Saff Fadm what Kubernetes means in this world. We have a look at the way to deploy Saff within Kubernetes regardless of what Obestack is deployed.

13:54.000 --> 14:07.000
We have several combinations of these two things together. You can have converged infrastructure where Obestack control plane is virtualized or it can be containerized.

14:07.000 --> 14:19.000
So basically using the same model, the same approach to deploy both it can be useful because it's a unified approach to manage your infrastructure.

14:19.000 --> 14:29.000
It's easy, deployable and reproducible because Kubernetes poses a standard approach to deploy things.

14:29.000 --> 14:37.000
So we don't have any more flexibility that today is trivial but it's easier from that point of view.

14:37.000 --> 14:44.000
And the same Saff cluster can be shared between Obestack and Kubernetes. We have different workloads.

14:44.000 --> 14:56.000
Kubernetes is PVC interfaces provided by Rook. Obestack is mostly RBD and their workload runs on virtual machines and it's usually outside.

14:56.000 --> 15:10.000
So you have to reach from the compute node the Rook cluster, the Saff cluster deployed by Rook within Kubernetes which poses some networking challenges that are...

15:10.000 --> 15:28.000
They can be managed using OST networking through. So you're using Kubernetes as a platform to deploy your Saff cluster but you're still relying on the OST networking to reach it and provide RBD to the outside workload.

15:28.000 --> 15:32.000
And that's the point of this slide.

15:32.000 --> 15:51.000
There are some things that are not mentioned here like some tuning in Rook that is supposed to be done to make sure that there is Kubernetes in the middle so it's not natively, the native Saff cluster we usually have.

15:51.000 --> 16:07.000
So at this point the thing is that we should do some tuning especially in the HCI world because iberconverged is still one of the most popular use cases and HCI is provided out of the box by Kubernetes.

16:07.000 --> 16:15.000
You can tag your infra nodes, you can deploy Rook, you can assign those nodes for OSDs. That's it.

16:15.000 --> 16:25.000
But at that point you have to make sure that both your cluster and the connection is available for the outside workload.

16:25.000 --> 16:37.000
So this can be done. It's done by this demo. I'm not going to show this but it's all there. It's all described. It's all I was describing in this slide.

16:37.000 --> 16:52.000
So you can have your your Obestack infrastructure deployed with DevStack or Triple O and it's still bare metal and it can consume a Saff cluster deployed by Rook using RBD.

16:52.000 --> 17:06.000
You can still use the CSI actually to provide PVC interface. It's not something that it's mutually exclusive but it's just a good exercise to see how these two technologies

17:06.000 --> 17:12.000
can work together in the future probably.

17:12.000 --> 17:28.000
And yeah, just some additional resources for who is interested in looking at these slides offline and some contacts for people in the Obestack world if you want to dig more into these experiments.

17:28.000 --> 17:36.000
And that's it. Thank you very much.
