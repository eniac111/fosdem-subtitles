WEBVTT

00:00.000 --> 00:09.960
So, everyone will hear about Rook.

00:09.960 --> 00:16.640
Let's welcome Alex and Dova to Mahan.

00:16.640 --> 00:20.480
Hi, everyone.

00:20.480 --> 00:24.920
Hope you're doing well, not feeling enough sleepy after the lunch.

00:24.920 --> 00:31.840
We're here to talk about introduction to Ceph on Kubernetes using Rook.

00:31.840 --> 00:32.840
Here's Alexander.

00:32.840 --> 00:34.600
Alexander will introduce himself.

00:34.600 --> 00:39.040
I'm Gaurav, Cloud Storage Engineer at Kubernetes Technologies.

00:39.040 --> 00:44.300
And I'm also a community ambassador for the Ceph project from Indian region and been working

00:44.300 --> 00:49.640
with the Ceph and Rook project since a long time and now a contributor to the Rook project.

00:49.640 --> 00:55.600
I'm Alexander Trost, founding engineer of Cotechnologies Inc.

00:55.600 --> 01:00.000
I'm a maintainer of the Rook project as well.

01:00.000 --> 01:06.400
And, yeah, we want to talk about Rook for everyone who doesn't know it.

01:06.400 --> 01:08.400
Want to get you started with storage.

01:08.400 --> 01:13.080
Who doesn't need fast, reliable storage nowadays with the cloud native applications?

01:13.080 --> 01:20.200
We're obviously talking about a bit more, well, more performance storage, I guess, depending

01:20.200 --> 01:24.000
on who you're asking.

01:24.000 --> 01:35.600
Well, the point of Rook in end is that with Kubernetes being kind of like with the shipping

01:35.600 --> 01:43.360
container ship here, you have your Kubernetes that kind of abstracts everything, tries to,

01:43.360 --> 01:49.440
well, provide you this one API for most to all things, depending on how far you want

01:49.440 --> 01:52.120
to go with it.

01:52.120 --> 01:55.720
And I guess for most people running Kubernetes, it kind of looks like that.

01:55.720 --> 01:59.080
You have your big giant ship running your production applications and you have your

01:59.080 --> 02:06.720
automation and CI CD process that kind of just try to keep it running.

02:06.720 --> 02:13.000
And that's where the question with storage more and more comes into frame for people,

02:13.000 --> 02:20.400
especially with local storage already, like, I think a year or so, a year or two even ago,

02:20.400 --> 02:27.680
coming into, let's say, being better supported in Kubernetes in a native way and not just

02:27.680 --> 02:35.400
having things around Kubernetes to try to make that an easier endeavor.

02:35.400 --> 02:40.760
We have the question of how can I, for example, get my self storage talking with Kubernetes

02:40.760 --> 02:44.680
so that I have storage for my applications.

02:44.680 --> 02:49.640
And well, that's the simple interface.

02:49.640 --> 02:55.280
It's more or less great that it's nowadays mainly one interface there called CSI, the

02:55.280 --> 03:00.960
storage interface, which for, well, for storage vendors basically means they only need to

03:00.960 --> 03:07.360
implement one storage or just they only need to implement one interface.

03:07.360 --> 03:12.000
And as well for Kubernetes slash you as a user, you have one interface or like one way

03:12.000 --> 03:14.200
on how you can get storage.

03:14.200 --> 03:18.800
For somebody, if you want storage on Kubernetes, you have the way of using the Kubernetes

03:18.800 --> 03:20.400
persistent volume claims.

03:20.400 --> 03:26.840
We basically from application perspective claim storage and Kubernetes will take care

03:26.840 --> 03:34.800
of, for example, talking with self storage and provisioning the volume and subsequently

03:34.800 --> 03:40.240
the CSI driver from self will take care of the whole mapping the volume, mounting the

03:40.240 --> 03:43.280
volume so that it is completely transparent to your application.

03:43.280 --> 03:45.720
And the whole thing is with the CSI interface there.

03:45.720 --> 03:51.840
It's like this one way for any storage vendor to also get their storage running.

03:51.840 --> 03:58.560
Like there's, there's obviously more than self, but well, obviously with Rook self,

03:58.560 --> 04:00.920
we're going to focus on self here.

04:00.920 --> 04:03.680
And that's exactly kind of like a connector in between there.

04:03.680 --> 04:09.280
So if you want storage, doesn't really matter if it's just self.

04:09.280 --> 04:14.680
The point of obviously self is that we have the self-cis I that's disconnecting bits from

04:14.680 --> 04:19.320
Kubernetes from the applications container side for your storage.

04:19.320 --> 04:26.080
And that's already a point where kind of Rook or starting to talk about Rook here is that

04:26.080 --> 04:29.600
you can run your self storage cluster.

04:29.600 --> 04:32.080
Well on most to any hardware.

04:32.080 --> 04:35.600
I don't know what we could run it on a Raspberry Pi as well.

04:35.600 --> 04:36.600
Right.

04:36.600 --> 04:37.600
Yes.

04:37.600 --> 04:38.600
Easily.

04:38.600 --> 04:44.480
Well, I think I've heard people run it on some Android phones or something even as well.

04:44.480 --> 04:50.040
But it's like the well, you know, just because you can doesn't necessarily mean you should,

04:50.040 --> 04:51.360
but that's the whole other discussion.

04:51.360 --> 04:55.040
The point being you can technically have your self storage anywhere.

04:55.040 --> 05:03.200
So it doesn't really matter if it's well, if it's on the metal in your own data center

05:03.200 --> 05:08.360
or if it's just a few laptops thrown together, doesn't, that's the thing with staff in general.

05:08.360 --> 05:10.480
It's like, you don't need the best hardware.

05:10.480 --> 05:15.560
Like you don't need to buy that big box from the one storage hardware vendor, maybe to

05:15.560 --> 05:21.520
explicitly go into that direction to have storage.

05:21.520 --> 05:29.320
And that's the thing where kind of the combination of using Kubernetes and staff might come into

05:29.320 --> 05:37.840
play or adjust for having storage for your applications, but also as a point for how

05:37.840 --> 05:45.360
should I put it, for running SAV.

05:45.360 --> 05:48.600
That's what RUG is about.

05:48.600 --> 05:55.160
It's about running SAV, obviously the connecting part, setting that connection up between SAV

05:55.160 --> 05:56.920
and Kubernetes as well.

05:56.920 --> 06:03.440
But the idea is that RUG runs SAV in Kubernetes, in containers.

06:03.440 --> 06:08.480
I think I mainly saw it from SAVADM last time we deployed a cluster on bare metal directly

06:08.480 --> 06:15.880
that like SAVADM also one other way maybe to put like that to install, deploy, even

06:15.880 --> 06:16.880
configure.

06:16.880 --> 06:18.040
Easy to manage.

06:18.040 --> 06:19.760
It's easily manageable.

06:19.760 --> 06:22.160
It's one way to just install, run it.

06:22.160 --> 06:28.600
It's kind of the same point for like RUG, where RUG is basically a SAV operator for

06:28.600 --> 06:29.680
Kubernetes.

06:29.680 --> 06:34.360
I'm going to go into a little bit more what I operated us because that's one of the vital

06:34.360 --> 06:41.360
points in general just from running certain applications on Kubernetes.

06:41.360 --> 06:48.400
Just again as we had it like running SAV on Kubernetes, with the operator pattern that

06:48.400 --> 06:56.600
we have in Kubernetes, we can easily have most things that cause quite some pain depending

06:56.600 --> 07:01.080
on how big you scale your storage cluster as well.

07:01.080 --> 07:04.320
Obviously deployment, bootstrapping, configuration upgrades and everything.

07:04.320 --> 07:10.920
That's all processes that I think there's probably 5 million Ansible playbooks to install

07:10.920 --> 07:11.920
SAV.

07:11.920 --> 07:14.160
There's well obviously SAVADM.

07:14.160 --> 07:16.320
There's what it was called?

07:16.320 --> 07:17.320
SAV deploy was there earlier.

07:17.320 --> 07:21.320
SAV deploy as well, which is SAVADM I think now it is right?

07:21.320 --> 07:29.520
SAV deploy earlier, SAVADM is now more advanced than the latest tool that everyone is using

07:29.520 --> 07:32.440
these days.

07:32.440 --> 07:37.720
There's more, I can already just think about 5 more tools on how to deploy SAV.

07:37.720 --> 07:45.600
And ironically for the people that have looked into Kubernetes a bit more already, it's kind

07:45.600 --> 07:50.040
of the same story for deploying Kubernetes.

07:50.040 --> 07:54.440
But because of Kubernetes being kind of like this abstraction layer on top of hardware

07:54.440 --> 08:00.800
to some degree, not like abstracting everything away.

08:00.800 --> 08:08.000
But let me, if we skip this page.

08:08.000 --> 08:11.240
It allows the root operator, that's exactly where this image comes in.

08:11.240 --> 08:13.440
It's orchestrating a cluster.

08:13.440 --> 08:19.200
It's not just a deployment obviously as well, but it's about using the Kubernetes APIs to

08:19.200 --> 08:23.360
easily just take care of everything, so to say.

08:23.360 --> 08:27.560
You want add a new node into your storage cluster.

08:27.560 --> 08:29.080
What do you do technically speaking?

08:29.080 --> 08:35.760
You just add it to Kubernetes and well, if everything goes well, 10 seconds later the

08:35.760 --> 08:41.480
operator will be like, oh, new node, got to do my job, run the preparing job and everything,

08:41.480 --> 08:42.480
get the node ready.

08:42.480 --> 08:48.560
And a few seconds even later from that, the new SAV components, the OSDs on the disks,

08:48.560 --> 08:54.360
depending on what disks are obviously as well, are taken care of.

08:54.360 --> 08:59.840
To make this full circle there with Kubernetes side is like what the operator flow kind of

08:59.840 --> 09:02.960
pattern it's mostly called is about.

09:02.960 --> 09:05.560
It's about observing.

09:05.560 --> 09:10.720
The operator is observing a status or even in Kubernetes case, custom resources.

09:10.720 --> 09:19.720
I just think about it as like YAML, let's just give it a depth what it is, object of

09:19.720 --> 09:25.920
YAML in Kubernetes, which the operator can watch on, as a user, either make a change

09:25.920 --> 09:31.280
or even like my automatic CI CD process makes a change to it like, oh, a new node has been

09:31.280 --> 09:36.640
added or something or I want to tweak something in the configuration of the cluster.

09:36.640 --> 09:52.120
And so the operators are observing that and when there's a change or when there's even

09:52.120 --> 09:57.360
in like the Kubernetes cluster, there's a change like a node missing or something, it

09:57.360 --> 09:59.120
analyzes that change.

09:59.120 --> 10:04.560
For example, if a node is gone or it's just not ready in Kubernetes terms anymore, let's

10:04.560 --> 10:11.200
say network outage for like two of your nodes, the operator would analyze, well, observe

10:11.200 --> 10:15.920
it first of all, analyze that and start acting upon that.

10:15.920 --> 10:22.360
For example, in Kubernetes terms, it would take care of setting certain so-called, just

10:22.360 --> 10:26.720
to have that term out there, potter disruption budgets, which kind of try to prevent other

10:26.720 --> 10:34.320
nodes from potentially stopping the components of the SAV storage cluster.

10:34.320 --> 10:38.840
And the point is really just that it's this like observe, analyze, act kind of loop because

10:38.840 --> 10:41.640
in the end it just repeats itself all over again.

10:41.640 --> 10:45.240
It's a whole deal with Kubernetes operators.

10:45.240 --> 10:54.200
It's again, if like I want to, for I guess the people more already into SAV, if you want

10:54.200 --> 11:01.200
to scale up some more SAV monitors, or well, SAV Mons, you just edit the object in Kubernetes

11:01.200 --> 11:05.920
in the API and just crank the number from one count from three to five or something.

11:05.920 --> 11:10.800
And again, this change is detected by the operator, analyzes it and acts upon it.

11:10.800 --> 11:16.040
And that makes it quite convenient as well.

11:16.040 --> 11:18.320
Again here, oh, the perfect way to YAML.

11:18.320 --> 11:21.200
Sorry, it's a, I'm just this guy and come a little bit of clarification.

11:21.200 --> 11:26.120
I don't have it mirrored on my screen, so it's a bit hard.

11:26.120 --> 11:28.360
But that's exactly the YAML that we talked about.

11:28.360 --> 11:37.080
Like as an example, I have my cluster running and let's say new SAV release.

11:37.080 --> 11:42.360
What I would need to do to upgrade my cluster, I would basically go ahead and just change

11:42.360 --> 11:45.760
the image to be well not 17.2.3, let's say.

11:45.760 --> 11:48.560
17.2.5, that's the latest.

11:48.560 --> 11:50.960
As an example, yeah, 17.2.5 as an example.

11:50.960 --> 11:55.840
And again, operate with detected, analyze if every component is up to date or not, and

11:55.840 --> 12:01.640
then even start the, well, I don't want to say complicated upgrade process, but there's,

12:01.640 --> 12:06.240
especially with something like SAV, there's more than just, ah, let me just restart it.

12:06.240 --> 12:11.840
There's checks before every component is restarted for SAV native ways of like, it's basically

12:11.840 --> 12:17.000
commands that are, well, okay to stop, they're basically called like that.

12:17.000 --> 12:21.080
And that's the whole idea there, that the operator helps you with that and in the end

12:21.080 --> 12:22.520
just fully takes care of it.

12:22.520 --> 12:27.360
So that in the end, for the main part of your work, you can just sit back, change it in

12:27.360 --> 12:36.200
the YAML in a few minutes or it can even be hours depending on how big the cluster is.

12:36.200 --> 12:40.760
The operator will take care of that.

12:40.760 --> 12:44.320
As I mentioned before, like the example of the monitor count, for example, if you want

12:44.320 --> 12:45.720
to change that, change it.

12:45.720 --> 12:50.080
A few seconds later, the operator will pick that up and start making the changes necessary

12:50.080 --> 12:56.160
or even if you would want to scale it down from like five to three or three to one, which

12:56.160 --> 13:00.880
not recommended, we need highly availability there.

13:00.880 --> 13:05.240
Or another option, the operator again watches it, takes care of doing it.

13:05.240 --> 13:11.720
Or even if you want to specifically say on this one node, please use this one device

13:11.720 --> 13:19.200
or even for this then, disk or NVMe, for example, use more than one storage team, OSD team

13:19.200 --> 13:20.440
for that.

13:20.440 --> 13:28.000
These things are possible and quite easily just by writing some lines of YAML in the

13:28.000 --> 13:29.000
end.

13:29.000 --> 13:33.120
According to your workload, you can easily just customize your, according to your workload,

13:33.120 --> 13:34.640
you can easily customize your YAMLs.

13:34.640 --> 13:37.840
That will make your life easier.

13:37.840 --> 13:44.480
And we've mainly talked about having like the cluster running or even setting up the

13:44.480 --> 13:48.640
cluster with the YAML definition of a self-cluster object.

13:48.640 --> 13:57.240
But if you would, for example, want to, well, run some Prometheus in your Kubernetes cluster

13:57.240 --> 13:59.280
and need storage for them.

13:59.280 --> 14:04.160
To be able to use storage in SAV, you need to have a storage pool, for example, RBD storage,

14:04.160 --> 14:06.280
block storage basically.

14:06.280 --> 14:12.320
We also again just go ahead, create a SAV block pool object, which is simply containing

14:12.320 --> 14:20.120
the information of, if we go from here, failure domain, where, well, you basically tell SAV

14:20.120 --> 14:27.120
to only store data on different hosts to keep it simple for now.

14:27.120 --> 14:34.000
The replicated size that there will be free total replica or free copies of data in your

14:34.000 --> 14:35.800
cluster that require a safe replica.

14:35.800 --> 14:36.800
Let's just skip to for now.

14:36.800 --> 14:43.400
There's like a safe replica size, and even that you could technically set the compression

14:43.400 --> 14:44.400
mode for this pool.

14:44.400 --> 14:50.080
The point is, again, we can just write this in YAML, apply it against the Kubernetes API,

14:50.080 --> 14:56.320
and a few seconds later, also for like the other objects, same way you need the SAV file

14:56.320 --> 15:03.480
system, SAV object storage, same way the operator takes care of creating all the components.

15:03.480 --> 15:09.960
For example, the MDS for a file system, we have the standard components, like the manager,

15:09.960 --> 15:17.080
the monitors, operator, the OSDs, and even for the object store, for example, the RGW

15:17.080 --> 15:18.240
components.

15:18.240 --> 15:21.760
The operator simply takes care of that, and again, if you change the SAV version, a few

15:21.760 --> 15:26.400
seconds to maybe a minute or two later, depending on what the state of your cluster is, operator

15:26.400 --> 15:32.120
will take care of doing the update.

15:32.120 --> 15:43.160
We've talked about deploying RookSAV cluster, mainly right now.

15:43.160 --> 15:51.320
We want to highlight in that point as well the crew plugin that RookSAV is building and

15:51.320 --> 16:03.120
providing, it allows you to have certain processes automated, even certain disaster recovery

16:03.120 --> 16:06.640
cases are easier to handle with that.

16:06.640 --> 16:19.760
And GURF will talk a bit about that.

16:19.760 --> 16:21.200
So what's crew, right?

16:21.200 --> 16:26.600
Crew is basically a package manager for kubectl plugins.

16:26.600 --> 16:33.600
It makes the management of Kubernetes easier, and that's how the core developers and maintainers

16:33.600 --> 16:40.880
came together and thought we can definitely write a plugin to make the lives of our developers

16:40.880 --> 16:43.520
and administrators more easier.

16:43.520 --> 16:50.240
Crew was the way to go since it's the de facto package manager for kubectl plugins.

16:50.240 --> 16:58.800
So I mean, you can just do a kubectl install, kubectl crew install RookSAV.

16:58.800 --> 17:01.040
That's how the plugin will be installed.

17:01.040 --> 17:04.880
And just if you can see, we just ran the help command.

17:04.880 --> 17:09.000
It shows a bunch of things that you could do.

17:09.000 --> 17:14.760
You can just run a whole bunch of sef commands, rbd commands right now.

17:14.760 --> 17:16.320
Also check the health of your cluster.

17:16.320 --> 17:21.120
You could just do a bunch of things, like even if you want to remove an OSD.

17:21.120 --> 17:28.160
So the need actually arises because, for example, if you want to use underlying tools like sef

17:28.160 --> 17:33.320
object store or something like that to debug core troubleshooting issues and issues at

17:33.320 --> 17:35.080
core OSD level.

17:35.080 --> 17:41.280
I mean, crew plugin is definitely a great way to go as it provides common management

17:41.280 --> 17:43.520
and troubleshooting tools for sef.

17:43.520 --> 17:47.160
It's currently, I mean, a lot of things work.

17:47.160 --> 17:49.640
We'll show you.

17:49.640 --> 17:53.840
It's just like I mentioned, you just need to run kubectl crew install RookSAV.

17:53.840 --> 17:57.440
It goes ahead quickly, installs the plugin.

17:57.440 --> 18:01.360
It's I mean, way easier that you just don't need to earlier.

18:01.360 --> 18:07.560
I mean, you had to go inside the toolbox pod to debug and troubleshoot every issue with

18:07.560 --> 18:08.560
crew.

18:08.560 --> 18:15.560
It provides such an ease of access that it makes, I mean, lives easier and troubleshooting

18:15.560 --> 18:17.320
makes is definitely easier.

18:17.320 --> 18:23.040
You could just override the cluster configuration just at the runtime and some of the disaster

18:23.040 --> 18:25.600
recovery scenarios are also addressed.

18:25.600 --> 18:29.280
Some of the troubleshooting scenarios that were addressed is mon recovery.

18:29.280 --> 18:33.480
Suppose let's say you were have default three months in the cluster and majority of them

18:33.480 --> 18:35.080
lose code.

18:35.080 --> 18:38.000
I mean, recovering months from one maps.

18:38.000 --> 18:45.560
I mean, just doing a bunch of tasks could be, if not done carefully, it could lead to

18:45.560 --> 18:52.520
more disasters, but certainly with more automation in place when things are definitely working,

18:52.520 --> 18:55.840
this is also made easier with the crew plugin.

18:55.840 --> 19:04.600
And even if you want to troubleshoot CSI issues, it makes it easier for sure.

19:04.600 --> 19:14.320
So yeah, I mean, just like if you want to restore mods with OSDs and even if we just

19:14.320 --> 19:20.040
delete the RookSAV cluster after accidental deletion of custom resources, that could be

19:20.040 --> 19:23.640
also restored.

19:23.640 --> 19:28.520
And one of the common goals in the roadmap is also automating core dump collection,

19:28.520 --> 19:33.640
because let's say if there's an issue that happens with the safe daemon and we want to

19:33.640 --> 19:39.720
collect the core dump of the process for further investigation to share it with the developers

19:39.720 --> 19:44.320
and with the community to understand what issues we are facing.

19:44.320 --> 19:49.680
It can easily do well if you want to just do a performance profiling of a process with

19:49.680 --> 19:52.120
GDB that could be made easier as well.

19:52.120 --> 19:54.000
So these are some of the goals.

19:54.000 --> 20:00.560
The current plugin is written in bash, but there's a work going on to rewrite the whole

20:00.560 --> 20:08.720
plugin in Golang so that it's definitely more scalable and much more easier to manage and

20:08.720 --> 20:11.160
even for contributors.

20:11.160 --> 20:18.120
So yeah, just like that.

20:18.120 --> 20:37.400
So, I guess the point we're more or less just trying to make is that if you have Kubernetes

20:37.400 --> 20:46.840
or even run a distribution of, well, what is the range or open shift obviously as well

20:46.840 --> 20:53.680
on your hardware and I wouldn't even put it to some degree as like you're confident enough

20:53.680 --> 20:57.960
with Kubernetes to run it.

20:57.960 --> 21:04.000
You can have it quite easy running a self cluster as well on top of that.

21:04.000 --> 21:07.520
Obviously to some degree you need some self knowledge, but that's with everything.

21:07.520 --> 21:15.320
If you run it in, if you want to run it in production, it's just that with this abstraction

21:15.320 --> 21:19.280
layer again with Kubernetes, it makes it easier for you.

21:19.280 --> 21:24.360
It's more of like you're kind of stamped in general there to think of more of like, oh,

21:24.360 --> 21:31.360
well I have some nodes and they're simply there to take care of the components that

21:31.360 --> 21:34.200
you need to run for the self cluster.

21:34.200 --> 21:40.240
And especially with the root self operator obviously, it makes the process easier by

21:40.240 --> 21:48.000
well a get ups approach for example, where you can just throw your Yamls into well into

21:48.000 --> 21:55.240
get most of the time and have that automatic mechanism basically take care of this deployment

21:55.240 --> 21:56.240
process.

21:56.240 --> 22:02.120
So that again, the operator just takes this YAML takes care of it and makes the changes

22:02.120 --> 22:09.280
necessary and with the root self crew plugin just to get that summarized real quick again.

22:09.280 --> 22:18.040
It's a way for us to have certain automatic processes in the hand of admins when they

22:18.040 --> 22:23.960
need to and not just as like, hey, here's like a 100 line bash script, please run that

22:23.960 --> 22:26.840
one comment at a time.

22:26.840 --> 22:30.320
And it simply allows it again, because we have this access to communities where we can

22:30.320 --> 22:33.440
just ask Kubernetes, hey, where's the monitor running?

22:33.440 --> 22:38.800
Oh, it's on node A and all that because well, we have an API that can tell us most of this

22:38.800 --> 22:42.880
information and also for recovery scenarios there.

22:42.880 --> 22:49.240
We can just ask Kubernetes to run a new pod or to well, have a new monitor for example,

22:49.240 --> 22:53.600
then running with this old information from the other monitors to have this forum recovered

22:53.600 --> 22:56.120
is required there.

22:56.120 --> 23:05.480
And regarding root self is like a general outlook for the future.

23:05.480 --> 23:09.960
One of the major points we're currently looking at is that we want to improve the cluster

23:09.960 --> 23:13.240
manageability even more than we already have it at.

23:13.240 --> 23:15.080
We want to make it easier.

23:15.080 --> 23:16.320
We're using the root self plugin.

23:16.320 --> 23:21.920
Right now you still need to do quite a lot of manual YAML editing of the objects that

23:21.920 --> 23:29.640
we have in the API, but we would like to have, well, some more crew plugin commands there

23:29.640 --> 23:34.440
again to extend that functionality, make it simply easier.

23:34.440 --> 23:40.160
As well, improved security by having the operator and other components that are running in the

23:40.160 --> 23:46.120
cluster, use separate access credentials to the cluster just to have there a bit more,

23:46.120 --> 23:52.080
well, I guess to some degree, even transparency if you would look at like audit logging of

23:52.080 --> 24:01.480
the self cluster and as well as encryption support for self-affairs and OSDs on partitions.

24:01.480 --> 24:04.760
And as with everything, there's more.

24:04.760 --> 24:10.400
Feel free to check out the roadmap MD file on the GitHub, github.com slash root.

24:10.400 --> 24:13.380
The link will be shown as well.

24:13.380 --> 24:16.600
If you want to get involved, if you want to contribute, if you have questions or anything,

24:16.600 --> 24:20.840
we have, well, obviously to get up, there's even the GitHub discussions open.

24:20.840 --> 24:28.960
If you have any, well, any more, more questions, I guess, then that might not fit on Slack.

24:28.960 --> 24:32.960
Well, we have Twitter account, obviously.

24:32.960 --> 24:40.720
We also have community meetings if you have any more pressing concerns to talk about.

24:40.720 --> 24:46.400
And well, just kind of from that side as well, we're, as Gerv and I mentioned, we're from

24:46.400 --> 24:47.720
Code Technology Sync.

24:47.720 --> 24:52.520
We're building a company that wants to create a product around Rook7 and just in general,

24:52.520 --> 24:54.760
try to help the community out there.

24:54.760 --> 24:58.080
If we do talk with us or contact us as well.

24:58.080 --> 25:04.320
And for now, thank you for listening and we'll gladly take some questions and can simply

25:04.320 --> 25:08.800
take the last, I think you showed 50 minutes for questions or even just talking a bit about

25:08.800 --> 25:11.120
certain scenarios here with everyone, I guess.

25:11.120 --> 25:14.880
I would just like to add one more last thing before we go.

25:14.880 --> 25:16.960
I'll just take it.

25:16.960 --> 25:19.400
It's not a good idea that there's two like.

25:19.400 --> 25:21.360
Yeah, I would just like to add one last thing.

25:21.360 --> 25:27.920
If you want to check a demo and more troubleshooting scenarios, we did a talk at SEF virtual,

25:27.920 --> 25:34.200
Summit 2022, which is already there on YouTube where we demoed a couple troubleshooting scenarios

25:34.200 --> 25:35.200
and crew plugin.

25:35.200 --> 25:39.920
I'll definitely share a reference and add a reference to here, but that'll be good to

25:39.920 --> 25:44.240
check out as well if you want to check out a live demo.

25:44.240 --> 25:45.240
Yeah.

25:45.240 --> 25:46.240
Thanks.

25:46.240 --> 25:47.240
Thanks.

25:47.240 --> 25:48.240
Any questions?

25:48.240 --> 25:49.240
Yeah.

25:49.240 --> 25:50.240
Thank you.

25:50.240 --> 25:51.240
Thank you.

25:51.240 --> 25:52.240
Thank you.

25:52.240 --> 25:53.240
Thank you.

25:53.240 --> 25:54.240
Thank you.

25:54.240 --> 25:55.240
Thank you.

25:55.240 --> 25:56.240
Thank you.

25:56.240 --> 25:57.240
Thank you.

25:57.240 --> 25:58.240
Thank you for this talk.

25:58.240 --> 26:08.520
I was wondering a bit about the downsides of using Rook with SEF because SEF is known

26:08.520 --> 26:16.040
to be really hard in configuring getting the right performance to be some kind of functionality

26:16.040 --> 26:17.040
there.

26:17.040 --> 26:23.320
So if I summarize correctly, the question is what are the downsides?

26:23.320 --> 26:29.080
I would more or less maybe put it at advantages, disadvantages of using Rook to run SEF on

26:29.080 --> 26:31.800
Kubernetes, especially with SEF being quite complex.

26:31.800 --> 26:32.800
Yeah.

26:32.800 --> 26:33.800
Right.

26:33.800 --> 26:41.680
In general, so if you have a decision on the loss of control that may have a concept,

26:41.680 --> 26:46.520
if there is a...

26:46.520 --> 26:48.560
If there's a loss of control on SEF side?

26:48.560 --> 26:49.560
Yeah.

26:49.560 --> 26:53.480
SEF has a lot of nubs and things to configure.

26:53.480 --> 26:56.320
Are there some that we lose and we use Rook?

26:56.320 --> 26:57.320
Oh, I see.

26:57.320 --> 26:58.320
Okay.

26:58.320 --> 27:00.760
And added to that question, if there's anything that's...

27:00.760 --> 27:04.280
Well, you lose when you use Rook SEF.

27:04.280 --> 27:12.720
I guess as a major downside that most people see as well is because you have an additional

27:12.720 --> 27:17.800
layer with Kubernetes being that.

27:17.800 --> 27:24.760
I guess maybe to address that a bit more from what is at least I know the error for some

27:24.760 --> 27:25.760
of SEF ADM.

27:25.760 --> 27:27.400
I think SEF ADM is for Rimmel, correct?

27:27.400 --> 27:30.240
Uses Docker to run containers basically as well, right?

27:30.240 --> 27:31.240
Pudman.

27:31.240 --> 27:32.240
Yeah.

27:32.240 --> 27:33.240
SEF ADM, right?

27:33.240 --> 27:34.240
Yeah.

27:34.240 --> 27:35.440
SEF ADM, for example, at least uses...

27:35.440 --> 27:41.320
Kind of also introduces a scenario, so to say, with Docker slash Pudman.

27:41.320 --> 27:47.120
One that runs container insert here.

27:47.120 --> 27:53.880
It has more or less in regards to installing SEF, for example, in my eyes, but I'm very

27:53.880 --> 27:57.500
biased to containers, obviously.

27:57.500 --> 28:03.120
It has this aspect of here's the SEF image and it should work unless you have something

28:03.120 --> 28:08.120
weird with the host OS going on.

28:08.120 --> 28:14.680
Downside is, again, if Kubernetes just goes completely crazy, the SEF class is probably

28:14.680 --> 28:21.080
also going to have a bad time, but that's kind of then like the weighing of, are you

28:21.080 --> 28:27.960
confident enough, I guess, to run Kubernetes and even running a Kubernetes just for long

28:27.960 --> 28:28.960
term?

28:28.960 --> 28:34.080
Especially with Kubernetes, there's more of this talk about, what's it again, pets versus

28:34.080 --> 28:35.080
cattle.

28:35.080 --> 28:40.040
Instead of just having a cluster for every application or something even and just, oh,

28:40.040 --> 28:46.200
we're done throwing it away versus for, well, obviously something as persistent and important

28:46.200 --> 28:50.960
as a SEF cluster, you can just throw it away then.

28:50.960 --> 28:55.920
From experience so far, I can tell that it is possible to run a Ruke SEF cluster over

28:55.920 --> 28:56.920
multiple years.

28:56.920 --> 28:57.920
I think I...

28:57.920 --> 28:59.920
When did I start mine?

28:59.920 --> 29:05.440
I think I had it running for two years and the only reason I shut it down was because

29:05.440 --> 29:10.880
I had gotten new hardware in another location, I kind of just said it was like, do I migrate

29:10.880 --> 29:11.880
it or do I not mine?

29:11.880 --> 29:16.840
It was just, okay, let's just start from scratch, but it's also because that cluster I'm talking

29:16.840 --> 29:22.360
about there had like 50 other applications running where it's just like, okay, let's

29:22.360 --> 29:25.820
start from scratch anyway, so to say.

29:25.820 --> 29:31.360
In regards to losing control, it's not necessarily.

29:31.360 --> 29:36.120
You don't really have like a...

29:36.120 --> 29:41.920
Like you don't have like a use this disk manual really way besides putting it in the YAML

29:41.920 --> 29:47.880
and fingers crossed the operator takes care of preparing and then applying an OSD to that

29:47.880 --> 29:51.840
disk or even partition.

29:51.840 --> 29:56.500
But it's like, again, I think with most tools there that take away certain aspects at least

29:56.500 --> 30:04.960
in regards to the installation or configuration.

30:04.960 --> 30:09.320
That those points are taking away, but it lies in regards to configuring staff or even

30:09.320 --> 30:11.880
certain aspects.

30:11.880 --> 30:21.560
You can do everything as normal and at least from experience with staff, I guess to put

30:21.560 --> 30:26.920
it like that has gotten a lot better as with the...

30:26.920 --> 30:29.680
Now the config store.

30:29.680 --> 30:36.440
The config store as it basically says, you have a config store in the monitors where

30:36.440 --> 30:40.680
you can just easily set for certain components instead of always having to manually make

30:40.680 --> 30:45.880
changes to any config files on the servers on your storage nodes themselves.

30:45.880 --> 30:48.480
And it has gotten better.

30:48.480 --> 30:49.480
That's awesome.

30:49.480 --> 30:57.520
I would just like to say at a lot of places it gives you a control as well, right?

30:57.520 --> 31:05.880
Because I mean, operator is responsible for reconciliation and taking charge when...

31:05.880 --> 31:10.480
I mean, automated scenarios where we want recovery to happen, right?

31:10.480 --> 31:14.560
And, Seth, the goal is to improve recovery and introductions.

31:14.560 --> 31:18.080
You don't need any unexpected loss of control as well, right?

31:18.080 --> 31:21.680
We would want to give admins a certain level of control.

31:21.680 --> 31:25.520
We don't want them to go ahead and play around with the OSD.

31:25.520 --> 31:34.080
So I think in many of the production scenarios, you need a certain set of control as well,

31:34.080 --> 31:36.640
which Rook actually provides.

31:36.640 --> 31:44.760
So at that point, I would certainly recommend and consider it as an advantage as well.

31:44.760 --> 31:48.480
One, two, three.

31:48.480 --> 31:49.480
Yeah.

31:49.480 --> 31:50.480
Okay.

31:50.480 --> 31:51.480
Yeah.

31:51.480 --> 31:52.480
Yeah.

31:52.480 --> 31:53.480
Yeah.

31:53.480 --> 31:54.480
Please.

31:54.480 --> 31:55.480
Sure.

31:55.480 --> 31:56.480
Please.

31:56.480 --> 31:59.480
We need three guys up here.

31:59.480 --> 32:01.480
Okay, please go ahead.

32:01.480 --> 32:02.480
We need forum.

32:02.480 --> 32:03.480
Yeah, exactly.

32:03.480 --> 32:04.480
Please go ahead.

32:04.480 --> 32:09.480
It's a similar question, but do you expect much more performance here, rather than setting

32:09.480 --> 32:15.080
up Kubernetes versus running some time?

32:15.080 --> 32:26.280
Question is, if there's going to be a performance hit in regards to running, Seth and Kubernetes?

32:26.280 --> 32:28.360
Depends on how you run it.

32:28.360 --> 32:33.800
If you run it, like I'm personally preferring running the myRook, Seth clusters always with

32:33.800 --> 32:38.040
host network, but you kind of can already, depending on how far you're with container

32:38.040 --> 32:41.880
or Kubernetes, it goes over, well, host network.

32:41.880 --> 32:43.880
Some like that, some don't.

32:43.880 --> 32:48.320
I personally do more or less just do it because I don't want the traffic to go over the overlay

32:48.320 --> 32:49.320
network.

32:49.320 --> 32:54.480
I have some plug-in, some CNI, container network interface where anyone who wants to look into

32:54.480 --> 32:57.720
that, that takes care of the network between your nodes.

32:57.720 --> 32:58.840
So it more or less depends.

32:58.840 --> 33:03.800
There's a lot of people using, well, just having Rook, Seth talk over the overlay network

33:03.800 --> 33:04.800
as well.

33:04.800 --> 33:06.320
It works fine as well.

33:06.320 --> 33:11.200
It's just a preference, I would really more or less put it at.

33:11.200 --> 33:15.720
Depending on what your network looks like, if you have 10G or something and your overlay

33:15.720 --> 33:20.120
network in the end maybe brings the, like in the IPERF test at least, brings it down

33:20.120 --> 33:29.040
to like 9.0 something, is it worth exposing that traffic to the host network then versus

33:29.040 --> 33:31.920
just having it go over the overlay network?

33:31.920 --> 33:35.480
But again, if you think about it, just another layout to consider.

33:35.480 --> 33:40.200
If you want that, if you don't, and if you don't want that, there's also options like

33:40.200 --> 33:48.600
maltos to allow you some more fine-grained network connections or config in regards to

33:48.600 --> 33:52.960
the interfaces you want to pass in, like different VLANs or something.

33:52.960 --> 33:55.680
But that's like, again, it depends.

33:55.680 --> 33:57.280
Yeah, yeah.

33:57.280 --> 34:03.320
Can you still manage your Seth cluster via the Seth dashboard or is it another dashboard

34:03.320 --> 34:06.720
or do you need two dashboards?

34:06.720 --> 34:10.840
The question was if you can still use the Seth dashboard, maybe just to expand on that

34:10.840 --> 34:16.880
Seth manager dashboard, just to manage your Seth cluster.

34:16.880 --> 34:26.800
To some degree, there is currently not a functionality to add new OSDs, I think, if I remember correctly.

34:26.800 --> 34:30.880
That's one thing as well with the future roadmap part, with the more manager-ability, where

34:30.880 --> 34:35.760
I also kind of looked at the dashboard and was like, wait, I have a create button.

34:35.760 --> 34:36.760
Why?

34:36.760 --> 34:38.760
Oh, why don't we?

34:38.760 --> 34:42.440
But then it's the typical, oh, there's some roadblocks that we just need to get out of

34:42.440 --> 34:47.120
the way and make sure that we are all, especially with operator and even Sethadium and others

34:47.120 --> 34:51.600
out there, we're all aligned on the same way or if there's a manager interface, because

34:51.600 --> 34:54.080
there is even one.

34:54.080 --> 34:57.480
And I think if I understood you correctly or if I heard from the meetings correctly,

34:57.480 --> 35:02.920
they are even looking into improving that interface further, it will hopefully be easier,

35:02.920 --> 35:08.320
thankfully also faster, to have the dashboard as this point of contact as well.

35:08.320 --> 35:12.680
Yeah, there's a lot of work that is currently going on.

35:12.680 --> 35:15.600
I'll just keep it out.

35:15.600 --> 35:22.440
I'll just say that there's a lot of work going on currently in the usability space from the

35:22.440 --> 35:29.760
recent discussions in upstream that we have had to improve dashboard as well from both

35:29.760 --> 35:35.840
Qubectl, from both Kubernetes and all standalone Seth perspective.

35:35.840 --> 35:43.080
It's to make sure that, I mean, you can easily manage and monitor Seth even in the CNCA fold.

35:43.080 --> 35:51.040
There has been recent discussions that have happened to improve it as well from Rook space.

35:51.040 --> 35:53.880
So a lot of work is going on in the usability space.

35:53.880 --> 35:58.080
But if you have any ideas, it'll be most welcome.

35:58.080 --> 36:03.160
And really, it would be great to have, I mean, usability is one thing that really matters

36:03.160 --> 36:04.160
a lot, right?

36:04.160 --> 36:10.080
I mean, user experience is one thing that, I mean, we would certainly cater to improve

36:10.080 --> 36:11.080
in Rook.

36:11.080 --> 36:14.080
Should we have time for one more question?

36:14.080 --> 36:15.080
Yeah.

36:15.080 --> 36:22.080
Is your main use case to provide storage within a cluster where, for example, other applications

36:22.080 --> 36:31.080
or is the use case more to use Kubernetes as an orchestrator for Seth that I put next

36:31.080 --> 36:35.080
to my cluster where my main applications will have basically just the orchestration the

36:35.080 --> 36:38.680
same way I'm not within the cluster.

36:38.680 --> 36:46.080
So the question is, could I maybe modify it a bit more into the direction of, like, how

36:46.080 --> 36:47.920
can you run Rook, I guess?

36:47.920 --> 36:49.240
I think that plays into that as well.

36:49.240 --> 36:53.960
Like you can run Rook, Seth.

36:53.960 --> 37:00.640
You can run Rook, Seth in a way that you connected to an existing Seth cluster that it doesn't

37:00.640 --> 37:05.600
even matter if it's a Rook, Seth cluster, just a Seth cluster works as well.

37:05.600 --> 37:11.240
It mainly takes care of them just setting up the CSI driver then.

37:11.240 --> 37:13.480
I know people use that to some degree as well.

37:13.480 --> 37:18.040
If they have an existing or even an existing Rook, Seth cluster that they want to share

37:18.040 --> 37:20.880
with the others.

37:20.880 --> 37:26.800
There's also in this external mode the possibility of the Rook, Seth operator to manage certain

37:26.800 --> 37:27.800
components.

37:27.800 --> 37:31.600
So that, for example, if you want a file system, you could run those MDS demons that you need

37:31.600 --> 37:37.540
for the file system in that cluster that your Kubernetes is running on then.

37:37.540 --> 37:38.540
That works as well.

37:38.540 --> 37:40.880
It's kind of like those two main external modes.

37:40.880 --> 37:45.320
And obviously the case of running it in the same cluster.

37:45.320 --> 37:51.240
That's kind of like this either you just share what you have or share and allow, like, Seth

37:51.240 --> 37:57.840
file system or Seth objects or you can just run the demons in the same cluster then.

37:57.840 --> 37:59.600
Both works for all the operators.

37:59.600 --> 38:00.600
Does that answer that?

38:00.600 --> 38:01.600
Yeah.

38:01.600 --> 38:02.600
Thank you.

38:02.600 --> 38:03.600
Done?

38:03.600 --> 38:04.600
Any other questions?

38:04.600 --> 38:05.600
There are no questions.

38:05.600 --> 38:09.600
There are a bunch of stickers here for everyone.

38:09.600 --> 38:10.600
Yeah.

38:10.600 --> 38:11.600
Stickers?

38:11.600 --> 38:12.600
And?

38:12.600 --> 38:13.600
Do we have some more?

38:13.600 --> 38:14.600
If you've asked the question just now, just come see me.

38:14.600 --> 38:15.600
You've got a T-shirt too.

38:15.600 --> 38:33.600
And maybe there's some leftover after that.
