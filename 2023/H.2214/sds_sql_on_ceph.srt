1
0:00:00.000 --> 0:00:00.000
Welcome everyone.

2
0:00:02.000 --> 0:00:03.000
Start with the next session.

3
0:00:04.000 --> 0:00:05.000
Try to find a place.

4
0:00:06.000 --> 0:00:08.000
Feel free to use the reserved spaces if nobody comes.

5
0:00:10.000 --> 0:00:11.000
Let's go.

6
0:00:12.000 --> 0:00:13.000
Okay.

7
0:00:14.000 --> 0:00:15.000
Welcome everyone.

8
0:00:16.000 --> 0:00:17.000
Start with the next session.

9
0:00:18.000 --> 0:00:20.000
So, try to find a place.

10
0:00:22.000 --> 0:00:24.000
Feel free to use the reserved spaces if nobody comes.

11
0:00:26.000 --> 0:00:27.000
Let's go.

12
0:00:27.000 --> 0:00:29.000
Let's go.

13
0:00:30.000 --> 0:00:33.000
Let's welcome Patrick on his talk on SQL on Sepulveda.

14
0:00:36.000 --> 0:00:37.000
All right. Hey everybody.

15
0:00:38.000 --> 0:00:40.000
I'm Patrick Donnelly.

16
0:00:41.000 --> 0:00:48.000
I know I got Red Hat slides up, but I'm actually part of the storage group at Red Hat that got moved over to IBM as of this year.

17
0:00:49.000 --> 0:00:50.000
So technically I'm at IBM now.

18
0:00:51.000 --> 0:00:53.000
If that matters for anybody who wants to ask me questions.

19
0:00:53.000 --> 0:00:57.000
Today I'm going to be talking about SQL on Seph.

20
0:00:58.000 --> 0:01:00.000
It's kind of a small project that started about two years ago.

21
0:01:01.000 --> 0:01:10.000
It was actually a COVID project for me while I was dealing with a newborn baby that had lots of, or some time on my hands.

22
0:01:11.000 --> 0:01:16.000
But anyway, this is kind of like an overview for what we're going to talk about.

23
0:01:17.000 --> 0:01:18.000
Yeah. Go ahead.

24
0:01:18.000 --> 0:01:22.000
Yeah, I do.

25
0:01:23.000 --> 0:01:24.000
We don't have a PA, unfortunately.

26
0:01:25.000 --> 0:01:27.000
Oh, yeah. I just have to speak up.

27
0:01:28.000 --> 0:01:34.000
So, I'm naturally soft spoken, so if you can't hear me in the back, just wave your hand and I'll try to speak up more, okay?

28
0:01:35.000 --> 0:01:36.000
I'm okay right now.

29
0:01:37.000 --> 0:01:38.000
All right.

30
0:01:39.000 --> 0:01:40.000
Where was I?

31
0:01:41.000 --> 0:01:44.000
So, just a quick canvassing in the audience.

32
0:01:45.000 --> 0:01:46.000
Who's pro?

33
0:01:46.000 --> 0:01:47.000
Who's used Seph before?

34
0:01:48.000 --> 0:01:49.000
Oh, wow. Okay.

35
0:01:50.000 --> 0:01:52.000
Who's used SQLite before?

36
0:01:53.000 --> 0:01:54.000
Fewer people. That's interesting.

37
0:01:55.000 --> 0:01:56.000
Okay. But not much fewer.

38
0:01:57.000 --> 0:02:03.000
All right. So, I'm going to quickly talk about Seph and what it is.

39
0:02:04.000 --> 0:02:07.000
I won't spend too much time on it due to the time I have available in my talk.

40
0:02:08.000 --> 0:02:11.000
I'll give you a brief introduction to RATOS for anyone who's not familiar with it.

41
0:02:12.000 --> 0:02:14.000
And I'm going to talk a little bit about that in a minute.

42
0:02:14.000 --> 0:02:16.000
I'm going to talk a little bit about SQLite.

43
0:02:17.000 --> 0:02:22.000
Then some typical storage patterns we use for storing data on RATOS.

44
0:02:24.000 --> 0:02:28.000
I'll give you an introduction to this new library, libsefsqlite.

45
0:02:29.000 --> 0:02:35.000
And then I'm going to talk about how we're going to use that today within Seph.

46
0:02:36.000 --> 0:02:41.000
Just to show that this library is not being used by anyone.

47
0:02:41.000 --> 0:02:44.000
Although I am interested if anyone's using it in the community.

48
0:02:45.000 --> 0:02:52.000
I'll go through a brief tutorial, an interactive tutorial using the library.

49
0:02:53.000 --> 0:02:58.000
And then I'll end the talk with some retrospective and talk about future work.

50
0:02:59.000 --> 0:03:00.000
So, what's Seph?

51
0:03:01.000 --> 0:03:03.000
Seph is an ecosystem for distributed object storage.

52
0:03:04.000 --> 0:03:09.000
So, it's composed of numerous projects centered around managing a large storage cluster.

53
0:03:09.000 --> 0:03:16.000
The underpinning of Seph is RATOS, which I'll talk about on the next slide.

54
0:03:17.000 --> 0:03:19.000
But it's basically a distributed object store.

55
0:03:20.000 --> 0:03:23.000
Most people don't use it, use RATOS directly.

56
0:03:24.000 --> 0:03:27.000
What they use instead is the storage abstractions we built on top of RATOS,

57
0:03:28.000 --> 0:03:32.000
which provide the more popular storage mechanisms that people are familiar with,

58
0:03:33.000 --> 0:03:36.000
including SephFS, which gives you your file storage as a distributed file system.

59
0:03:36.000 --> 0:03:42.000
RGW, providing the S3 object storage gateway.

60
0:03:43.000 --> 0:03:48.000
And RBD, which gives you your block device storage on top of RATOS.

61
0:03:49.000 --> 0:03:54.000
Seph has kind of evolved more and more recently to become more user friendly.

62
0:03:55.000 --> 0:03:58.000
If you had maybe poor experiences in the past with Seph,

63
0:03:59.000 --> 0:04:00.000
I encourage you to give it another shot.

64
0:04:00.000 --> 0:04:07.000
The dev team has dedicated a lot of time recently to improving the user experience.

65
0:04:08.000 --> 0:04:16.000
And also, taking the hassle out of managing your storage cluster out of the experience.

66
0:04:17.000 --> 0:04:20.000
There are things like monitoring device health.

67
0:04:21.000 --> 0:04:25.000
We now have a very mature dashboard for interacting with the Seph cluster.

68
0:04:25.000 --> 0:04:31.000
And the cluster management itself is now largely being done through Seph ADM,

69
0:04:32.000 --> 0:04:38.000
which, as you saw in the previous talk, you can start up a Seph cluster with just a simple command

70
0:04:39.000 --> 0:04:43.000
and then start adding hosts to it. It's never been simpler.

71
0:04:46.000 --> 0:04:47.000
Oops, went backwards.

72
0:04:47.000 --> 0:04:54.000
So what's RATOS? RATOS is a number of object storage demons that run on physical disks.

73
0:04:55.000 --> 0:04:58.000
They can be hard disks, SSDs, NVMe's.

74
0:04:59.000 --> 0:05:03.000
And on top of these object storage devices, we have this concept of pools,

75
0:05:04.000 --> 0:05:08.000
which allows you to have various administrative policies regarding like what kind of hardware the pool should use,

76
0:05:09.000 --> 0:05:10.000
how the data should be replicated.

77
0:05:10.000 --> 0:05:18.000
Clients of RATOS talk directly to the primary object storage device for a given object.

78
0:05:19.000 --> 0:05:26.000
And you can look up which object storage device an object belongs to in constant time using a library called Crush.

79
0:05:27.000 --> 0:05:30.000
You don't need to use that directly. That's just under the covers.

80
0:05:31.000 --> 0:05:37.000
And then, as part of the name suggests, reliable autonomic object storage.

81
0:05:37.000 --> 0:05:44.000
Distribute object storage. The cluster heals, self-heals, it's autonomic.

82
0:05:45.000 --> 0:05:49.000
And the replication is done automatically. You don't have to worry about how any of that works.

83
0:05:51.000 --> 0:05:53.000
Sorry. So what's an object?

84
0:05:56.000 --> 0:06:01.000
The object storage device is composed of a number of objects,

85
0:06:01.000 --> 0:06:06.000
and that is the logical unit you have when you're storing things in RATOS.

86
0:06:07.000 --> 0:06:10.000
An object is composed of three different parts. You can use one or all.

87
0:06:11.000 --> 0:06:15.000
They have the data blob, which is analogous to like a regular file.

88
0:06:16.000 --> 0:06:18.000
Like you put data in the file, you get data out of the file.

89
0:06:19.000 --> 0:06:29.000
You have key value X-SATRs. This is sort of an older technology that was used in the early days with CephFS for storing certain information about files,

90
0:06:29.000 --> 0:06:35.000
which is typically very small data. It's not usually used anymore, except in some parts of CephFS.

91
0:06:36.000 --> 0:06:44.000
Now, the key value store that's used most often in CephFS, also RGW, is OMAP.

92
0:06:45.000 --> 0:06:49.000
And that is much more of a general purpose key value store used today.

93
0:06:50.000 --> 0:06:55.000
So this is how you interact with RATOS through these objects.

94
0:06:55.000 --> 0:07:02.000
Now, it's not that simple to take like a number of objects, distribute all over the cluster, and try to build something with that,

95
0:07:03.000 --> 0:07:05.000
because you've got consistency issues that you have to deal with.

96
0:07:06.000 --> 0:07:09.000
You've got to manage how you're going to stripe the data across all these objects,

97
0:07:10.000 --> 0:07:18.000
which is why we have these more popular abstractions that I talked about, CephFS, RBD, RGW, which is how you typically interact with RATOS.

98
0:07:18.000 --> 0:07:28.000
So what I'm going to talk about today is a SQL lite library that operates alongside these other three storage abstractions,

99
0:07:29.000 --> 0:07:33.000
gives you something on top of libRATOS, but you can also now run SQL on Ceph.

100
0:07:36.000 --> 0:07:41.000
So how do you typically do application storage on RATOS?

101
0:07:42.000 --> 0:07:45.000
Well, we have various bindings you can use to talk to RATOS.

102
0:07:45.000 --> 0:07:51.000
We have the typical CC++ bindings, which are part of the broader project, also used within Ceph.

103
0:07:52.000 --> 0:07:57.000
We also have a Python interface, which is used for manipulating the objects.

104
0:07:58.000 --> 0:08:08.000
And that's used in now, that's somewhat used in the broader community for various projects, but also within Ceph we use it for some of the new Ceph Manager daemon,

105
0:08:09.000 --> 0:08:10.000
which I'll talk about more later.

106
0:08:10.000 --> 0:08:17.000
And again, it's not that simple to stripe data across the objects, which is why we have these other abstractions.

107
0:08:18.000 --> 0:08:25.000
One of the more notable exceptions is this libRATOS striper, which is one of the ways you can create a file concept on top of objects,

108
0:08:26.000 --> 0:08:34.000
where you open and close, read and write and sync to a number of objects and it looks like a regular file.

109
0:08:34.000 --> 0:08:42.000
That was developed by some folks at CERN and it's mostly, I think, in terms of use, it's stayed confined to that space.

110
0:08:42.000 --> 0:09:04.000
So, even though we do have these other storage abstractions, it's still useful to talk to RATOS directly because sometimes you want to do something that is not dependent on these other storage abstractions,

111
0:09:04.000 --> 0:09:13.000
which may, in the case of within Ceph's internals, may not actually be available, which is why a number of Ceph Manager plugins,

112
0:09:14.000 --> 0:09:20.000
the Ceph Manager has a number of Python modules and they talk directly to RATOS.

113
0:09:21.000 --> 0:09:29.000
So, this was something I wanted to address because it was a little bit awkward and I'll talk about that more.

114
0:09:29.000 --> 0:09:40.000
So, a quick overview of SQLite. For those who've never used it before, it's a user application library for allowing you to, that acts as a SQL engine,

115
0:09:41.000 --> 0:09:49.000
and lets you store a SQL database as a regular file, usually two files. There'll be a journal and then the database object itself.

116
0:09:50.000 --> 0:09:54.000
And depending on how you use it, the journal's transient may come and go.

117
0:09:54.000 --> 0:10:01.000
It's widely recognized as one of the most used SQLite engines in the world, it's very popular.

118
0:10:02.000 --> 0:10:08.000
They estimate on their website there's billions of SQLite databases in use.

119
0:10:09.000 --> 0:10:13.000
It's at least tens of billions at this point because it's in every Android phone.

120
0:10:14.000 --> 0:10:21.000
So, it was an easy choice to make. It's a very simple library and bindings exist for numerous ecosystems.

121
0:10:21.000 --> 0:10:24.000
In particular of interest to me was Python.

122
0:10:25.000 --> 0:10:29.000
Actually extending SQLite is fairly simple.

123
0:10:30.000 --> 0:10:33.000
They have this VFS concept, Virtual File System concept.

124
0:10:34.000 --> 0:10:39.000
Lets you swap in different virtual file systems as needed.

125
0:10:40.000 --> 0:10:47.000
The basic one is the Unix VFS, that's what comes with SQLite by default and it's very intuitive.

126
0:10:47.000 --> 0:10:53.000
It just passes on open, read, write, close off to the local file system for execution.

127
0:10:57.000 --> 0:11:08.000
Libsef SQLite is a library for a SQLite VFS library that lets you put a SQLite database in RATOs.

128
0:11:09.000 --> 0:11:15.000
It's composed of two parts, libsef sqlite and simple RATO striper.

129
0:11:15.000 --> 0:11:18.000
I'll talk about simple RATO striper on the next slide.

130
0:11:19.000 --> 0:11:24.000
The use of this library does not require any application modification.

131
0:11:25.000 --> 0:11:31.000
That's kind of like the killer feature here because you can just set some environment variables and modify the database URI

132
0:11:32.000 --> 0:11:35.000
and you can automatically start storing your database in Ceph.

133
0:11:36.000 --> 0:11:42.000
All of these, the journal objects, the database objects are striped across OSDs.

134
0:11:42.000 --> 0:11:44.000
You don't need to do anything differently.

135
0:11:45.000 --> 0:11:50.000
The simple RATO striper is based loosely off of the lib RATO striper developed by CERN.

136
0:11:51.000 --> 0:11:58.000
The main reason I didn't end up using CERN's library was because it had some locking behavior that was not really desirable

137
0:11:59.000 --> 0:12:02.000
for a highly asynchronous use case.

138
0:12:03.000 --> 0:12:08.000
I didn't want to modify their library out from under their feet so I just wrote a simple version.

139
0:12:08.000 --> 0:12:20.000
It provides the primitives that SQLite needs, open, read, write, close, sync, and all the writes are done asynchronously.

140
0:12:21.000 --> 0:12:25.000
And then the sync call that comes from SQLite actually flushes them all out.

141
0:12:26.000 --> 0:12:36.000
These are all stored across RATOs with these names, foo.db, and it's got the block number associated with the database, and so on.

142
0:12:36.000 --> 0:12:39.000
Using libsef-sqlite, again, it's very easy.

143
0:12:40.000 --> 0:12:44.000
You just have to load a VFS library.

144
0:12:45.000 --> 0:12:54.000
This is done with the sqlite command,.load libsef-sqlite, and then you just provide a URI for the database.

145
0:12:55.000 --> 0:13:01.000
This is the pool ID or the pool name, the namespace within that pool, which is optional,

146
0:13:01.000 --> 0:13:08.000
and then you give the database a name and specify the VFS as Ceph, and that's it. It just works.

147
0:13:09.000 --> 0:13:25.000
You may have to specify some environment variables if you're using the sqlite binary to tell it which Ceph cluster to use or which Ceph configs to read, things like that, but that's all fairly unobtrusive, not obtrusive.

148
0:13:25.000 --> 0:13:37.000
Within the Ceph manager, so the Ceph manager is one of the newer demons in Ceph that takes care of certain details of managing your Ceph cluster and trying to provide easier interfaces.

149
0:13:38.000 --> 0:13:48.000
Of particular interest to us is one that handles health metrics that come from the OSDs, giving the Ceph manager information about the smart data associated with the disks,

150
0:13:48.000 --> 0:14:10.000
being able to anticipate failures in disks, again Ceph trying to reduce the management burden of storage clusters, and then also a portal to higher level commands like managing volumes within Ceph that is like a subvolume concept that's used by OpenStack or Kubernetes CSI.

151
0:14:10.000 --> 0:14:23.000
Within the Ceph manager daemon, what I observed was that there were several modules that were just storing data in the omap key value of a particular object, and it turned out this doesn't scale very well.

152
0:14:24.000 --> 0:14:31.000
We know it won't scale well because if you have more than 10,000 key value pairs in a single object, the performance starts to degrade.

153
0:14:31.000 --> 0:14:42.000
In fact, you'll start getting cluster warnings that there's objects with too many key value pairs. So it was also pretty awkward in terms of how it was being used.

154
0:14:43.000 --> 0:14:57.000
And just by how we were managing the data, it was a perfect match for a SQL database, except it was not very easy to put a SQL database on Ceph at the time.

155
0:14:57.000 --> 0:15:13.000
In fact, Jan here worked on SnapSchedule, which is a module for creating snapshots and maintaining snapshots in CephFS and handling retention policies.

156
0:15:13.000 --> 0:15:27.000
And that actually used a SQLite database that was flushed to RATOS objects and then loaded in anticipation of the project that I'm working on now. And that's all been updated now to use the Ceph SQLite library.

157
0:15:27.000 --> 0:15:41.000
All right. So in terms of how it actually looks within the Ceph Manager, on the left we have a schema. It's fairly simple.

158
0:15:42.000 --> 0:15:54.000
Just creating a table with a device ID is the primary key, and then another table with device health metrics with the time we got the metrics, the device ID associated with that metric, and then the raw smart text.

159
0:15:54.000 --> 0:16:09.000
And then they actually put the device metrics in the database. It's as simple as this within the Manager. I've taken out a few unnecessary lines just for space, or unnecessary keywords for space in the SQL.

160
0:16:10.000 --> 0:16:22.000
You create the device ID, which just calls another SQL statement to insert into this table, and actually execute the SQL statement with the Epic dev ID and data. It's that simple.

161
0:16:22.000 --> 0:16:25.000
And now that's stored, persisted in RATOS.

162
0:16:26.000 --> 0:16:31.000
So here's a quick libcefsqlite in action series of GIFs I've created.

163
0:16:32.000 --> 0:16:43.000
Here we're running the Ceph status command, just showing us the state of the cluster. We have two pools right now at.manager and an A pool that I'm creating for this demo.

164
0:16:43.000 --> 0:16:56.000
Here I'm purging A just to show that there's nothing in it. It removed one object. And here I'm just listing all the objects within this pool. There's none because I just purged it.

165
0:16:57.000 --> 0:17:11.000
So that's just a starter. And then here we're actually going to run some libcefsqlite. So to do that, again I mentioned there were some environment variables. If I'm using the SQLite command directly, I have to specify some environment variables.

166
0:17:11.000 --> 0:17:19.000
So the library knows what to do. Here, because this is a dev cluster, I have to tell it to use the library path associated with my build.

167
0:17:20.000 --> 0:17:28.000
I specify which Ceph config to load, which keyring associated with the admin user that I'm going to specify here.

168
0:17:29.000 --> 0:17:34.000
I was actually going to also add some logging data, but ended up not doing that.

169
0:17:34.000 --> 0:17:47.000
Just to save space. And then here I'm actually running a SQLite command. I'm loading the libcefsqlite library. That's one of the first commands that libceqlite is going to run.

170
0:17:48.000 --> 0:17:59.000
And here I'm opening a database in pool A, namespace B within that pool, and then a database named A.db with vfscef.

171
0:17:59.000 --> 0:18:10.000
Alright, now I'm in SQLite. Here I create a simple table with an integer column. There's the schema. Exactly what I wrote.

172
0:18:11.000 --> 0:18:17.000
And then we're going to insert into the table one value. And then dump it.

173
0:18:17.000 --> 0:18:32.000
So it's now in RATOS. And now just to confirm that, I'm going to run the RATOS command on the pool A, list all the objects in the pool. You can see in namespace B I have this A.db.

174
0:18:33.000 --> 0:18:44.000
So now I'm going to use this striper command. So I'm actually, if this database were composed of many objects, you can use the striper command to actually pull the database out.

175
0:18:44.000 --> 0:18:51.000
And you can see here I've done that. It's an 8K database. It's small because there's just one table with one value.

176
0:18:52.000 --> 0:19:08.000
And I loaded that locally. I pulled it out of RATOS. Sorry, the gif loops. I pulled it out of RATOS. And then I now have the database as a local file. I ran SQLite on that local file database.

177
0:19:08.000 --> 0:19:17.000
And just dumped it to confirm that it actually wrote the data out to RATOS correctly. And I can pull it out of RATOS and verify that it actually worked.

178
0:19:18.000 --> 0:19:30.000
So here's another demo with just rerunning the same SQLite command I had earlier. And sorry, this is going to be a big paste.

179
0:19:30.000 --> 0:19:44.000
But I'm creating a table. This is just some magic in SQLite to basically create an infinite loop. And I'm just going to insert a number, I think it's 100,000 integers into the table.

180
0:19:45.000 --> 0:19:50.000
And just see how many objects are in the database now. There's four objects composed of that database.

181
0:19:50.000 --> 0:19:59.000
So I think for time reasons, I'm not going to go through the performance notes, but it's on the slide if you want to look at it later.

182
0:20:00.000 --> 0:20:12.000
And just as a retrospective for Quincy, when the database got used live, it's being used in the two manager modules right now, the device health and the snap scheduling module.

183
0:20:12.000 --> 0:20:21.000
It's been fairly successful. We had a few minor hiccups that weren't really too much related to the library.

184
0:20:24.000 --> 0:20:35.000
And just for some future work, I want to add support for supporting concurrent readers. That's not yet possible right now. All readers and writers obtained exclusive locks when accessing the database.

185
0:20:35.000 --> 0:20:49.000
There's not a technical reason why we can't add concurrent reader support. And then I also want to look at adding read ahead performance, improving read ahead performance because right now every read call in libsef SQLite is synchronous.

186
0:20:52.000 --> 0:20:58.000
So that's the end of my talk. Thank you. Do we have any time for questions? No? Okay.

187
0:20:58.000 --> 0:21:06.000
Sorry, no time for questions this session. We have to find Patrick and ask him.

