1
0:00:00.000 --> 0:00:09.960
So, everyone will hear about Rook.

2
0:00:09.960 --> 0:00:16.640
Let's welcome Alex and Dova to Mahan.

3
0:00:16.640 --> 0:00:20.480
Hi, everyone.

4
0:00:20.480 --> 0:00:24.920
Hope you're doing well, not feeling enough sleepy after the lunch.

5
0:00:24.920 --> 0:00:31.840
We're here to talk about introduction to Ceph on Kubernetes using Rook.

6
0:00:31.840 --> 0:00:32.840
Here's Alexander.

7
0:00:32.840 --> 0:00:34.600
Alexander will introduce himself.

8
0:00:34.600 --> 0:00:39.040
I'm Gaurav, Cloud Storage Engineer at Kubernetes Technologies.

9
0:00:39.040 --> 0:00:44.300
And I'm also a community ambassador for the Ceph project from Indian region and been working

10
0:00:44.300 --> 0:00:49.640
with the Ceph and Rook project since a long time and now a contributor to the Rook project.

11
0:00:49.640 --> 0:00:55.600
I'm Alexander Trost, founding engineer of Cotechnologies Inc.

12
0:00:55.600 --> 0:01:00.000
I'm a maintainer of the Rook project as well.

13
0:01:00.000 --> 0:01:06.400
And, yeah, we want to talk about Rook for everyone who doesn't know it.

14
0:01:06.400 --> 0:01:08.400
Want to get you started with storage.

15
0:01:08.400 --> 0:01:13.080
Who doesn't need fast, reliable storage nowadays with the cloud native applications?

16
0:01:13.080 --> 0:01:20.200
We're obviously talking about a bit more, well, more performance storage, I guess, depending

17
0:01:20.200 --> 0:01:24.000
on who you're asking.

18
0:01:24.000 --> 0:01:35.600
Well, the point of Rook in end is that with Kubernetes being kind of like with the shipping

19
0:01:35.600 --> 0:01:43.360
container ship here, you have your Kubernetes that kind of abstracts everything, tries to,

20
0:01:43.360 --> 0:01:49.440
well, provide you this one API for most to all things, depending on how far you want

21
0:01:49.440 --> 0:01:52.120
to go with it.

22
0:01:52.120 --> 0:01:55.720
And I guess for most people running Kubernetes, it kind of looks like that.

23
0:01:55.720 --> 0:01:59.080
You have your big giant ship running your production applications and you have your

24
0:01:59.080 --> 0:02:06.720
automation and CI CD process that kind of just try to keep it running.

25
0:02:06.720 --> 0:02:13.000
And that's where the question with storage more and more comes into frame for people,

26
0:02:13.000 --> 0:02:20.400
especially with local storage already, like, I think a year or so, a year or two even ago,

27
0:02:20.400 --> 0:02:27.680
coming into, let's say, being better supported in Kubernetes in a native way and not just

28
0:02:27.680 --> 0:02:35.400
having things around Kubernetes to try to make that an easier endeavor.

29
0:02:35.400 --> 0:02:40.760
We have the question of how can I, for example, get my self storage talking with Kubernetes

30
0:02:40.760 --> 0:02:44.680
so that I have storage for my applications.

31
0:02:44.680 --> 0:02:49.640
And well, that's the simple interface.

32
0:02:49.640 --> 0:02:55.280
It's more or less great that it's nowadays mainly one interface there called CSI, the

33
0:02:55.280 --> 0:03:00.960
storage interface, which for, well, for storage vendors basically means they only need to

34
0:03:00.960 --> 0:03:07.360
implement one storage or just they only need to implement one interface.

35
0:03:07.360 --> 0:03:12.000
And as well for Kubernetes slash you as a user, you have one interface or like one way

36
0:03:12.000 --> 0:03:14.200
on how you can get storage.

37
0:03:14.200 --> 0:03:18.800
For somebody, if you want storage on Kubernetes, you have the way of using the Kubernetes

38
0:03:18.800 --> 0:03:20.400
persistent volume claims.

39
0:03:20.400 --> 0:03:26.840
We basically from application perspective claim storage and Kubernetes will take care

40
0:03:26.840 --> 0:03:34.800
of, for example, talking with self storage and provisioning the volume and subsequently

41
0:03:34.800 --> 0:03:40.240
the CSI driver from self will take care of the whole mapping the volume, mounting the

42
0:03:40.240 --> 0:03:43.280
volume so that it is completely transparent to your application.

43
0:03:43.280 --> 0:03:45.720
And the whole thing is with the CSI interface there.

44
0:03:45.720 --> 0:03:51.840
It's like this one way for any storage vendor to also get their storage running.

45
0:03:51.840 --> 0:03:58.560
Like there's, there's obviously more than self, but well, obviously with Rook self,

46
0:03:58.560 --> 0:04:00.920
we're going to focus on self here.

47
0:04:00.920 --> 0:04:03.680
And that's exactly kind of like a connector in between there.

48
0:04:03.680 --> 0:04:09.280
So if you want storage, doesn't really matter if it's just self.

49
0:04:09.280 --> 0:04:14.680
The point of obviously self is that we have the self-cis I that's disconnecting bits from

50
0:04:14.680 --> 0:04:19.320
Kubernetes from the applications container side for your storage.

51
0:04:19.320 --> 0:04:26.080
And that's already a point where kind of Rook or starting to talk about Rook here is that

52
0:04:26.080 --> 0:04:29.600
you can run your self storage cluster.

53
0:04:29.600 --> 0:04:32.080
Well on most to any hardware.

54
0:04:32.080 --> 0:04:35.600
I don't know what we could run it on a Raspberry Pi as well.

55
0:04:35.600 --> 0:04:36.600
Right.

56
0:04:36.600 --> 0:04:37.600
Yes.

57
0:04:37.600 --> 0:04:38.600
Easily.

58
0:04:38.600 --> 0:04:44.480
Well, I think I've heard people run it on some Android phones or something even as well.

59
0:04:44.480 --> 0:04:50.040
But it's like the well, you know, just because you can doesn't necessarily mean you should,

60
0:04:50.040 --> 0:04:51.360
but that's the whole other discussion.

61
0:04:51.360 --> 0:04:55.040
The point being you can technically have your self storage anywhere.

62
0:04:55.040 --> 0:05:03.200
So it doesn't really matter if it's well, if it's on the metal in your own data center

63
0:05:03.200 --> 0:05:08.360
or if it's just a few laptops thrown together, doesn't, that's the thing with staff in general.

64
0:05:08.360 --> 0:05:10.480
It's like, you don't need the best hardware.

65
0:05:10.480 --> 0:05:15.560
Like you don't need to buy that big box from the one storage hardware vendor, maybe to

66
0:05:15.560 --> 0:05:21.520
explicitly go into that direction to have storage.

67
0:05:21.520 --> 0:05:29.320
And that's the thing where kind of the combination of using Kubernetes and staff might come into

68
0:05:29.320 --> 0:05:37.840
play or adjust for having storage for your applications, but also as a point for how

69
0:05:37.840 --> 0:05:45.360
should I put it, for running SAV.

70
0:05:45.360 --> 0:05:48.600
That's what RUG is about.

71
0:05:48.600 --> 0:05:55.160
It's about running SAV, obviously the connecting part, setting that connection up between SAV

72
0:05:55.160 --> 0:05:56.920
and Kubernetes as well.

73
0:05:56.920 --> 0:06:03.440
But the idea is that RUG runs SAV in Kubernetes, in containers.

74
0:06:03.440 --> 0:06:08.480
I think I mainly saw it from SAVADM last time we deployed a cluster on bare metal directly

75
0:06:08.480 --> 0:06:15.880
that like SAVADM also one other way maybe to put like that to install, deploy, even

76
0:06:15.880 --> 0:06:16.880
configure.

77
0:06:16.880 --> 0:06:18.040
Easy to manage.

78
0:06:18.040 --> 0:06:19.760
It's easily manageable.

79
0:06:19.760 --> 0:06:22.160
It's one way to just install, run it.

80
0:06:22.160 --> 0:06:28.600
It's kind of the same point for like RUG, where RUG is basically a SAV operator for

81
0:06:28.600 --> 0:06:29.680
Kubernetes.

82
0:06:29.680 --> 0:06:34.360
I'm going to go into a little bit more what I operated us because that's one of the vital

83
0:06:34.360 --> 0:06:41.360
points in general just from running certain applications on Kubernetes.

84
0:06:41.360 --> 0:06:48.400
Just again as we had it like running SAV on Kubernetes, with the operator pattern that

85
0:06:48.400 --> 0:06:56.600
we have in Kubernetes, we can easily have most things that cause quite some pain depending

86
0:06:56.600 --> 0:07:01.080
on how big you scale your storage cluster as well.

87
0:07:01.080 --> 0:07:04.320
Obviously deployment, bootstrapping, configuration upgrades and everything.

88
0:07:04.320 --> 0:07:10.920
That's all processes that I think there's probably 5 million Ansible playbooks to install

89
0:07:10.920 --> 0:07:11.920
SAV.

90
0:07:11.920 --> 0:07:14.160
There's well obviously SAVADM.

91
0:07:14.160 --> 0:07:16.320
There's what it was called?

92
0:07:16.320 --> 0:07:17.320
SAV deploy was there earlier.

93
0:07:17.320 --> 0:07:21.320
SAV deploy as well, which is SAVADM I think now it is right?

94
0:07:21.320 --> 0:07:29.520
SAV deploy earlier, SAVADM is now more advanced than the latest tool that everyone is using

95
0:07:29.520 --> 0:07:32.440
these days.

96
0:07:32.440 --> 0:07:37.720
There's more, I can already just think about 5 more tools on how to deploy SAV.

97
0:07:37.720 --> 0:07:45.600
And ironically for the people that have looked into Kubernetes a bit more already, it's kind

98
0:07:45.600 --> 0:07:50.040
of the same story for deploying Kubernetes.

99
0:07:50.040 --> 0:07:54.440
But because of Kubernetes being kind of like this abstraction layer on top of hardware

100
0:07:54.440 --> 0:08:00.800
to some degree, not like abstracting everything away.

101
0:08:00.800 --> 0:08:08.000
But let me, if we skip this page.

102
0:08:08.000 --> 0:08:11.240
It allows the root operator, that's exactly where this image comes in.

103
0:08:11.240 --> 0:08:13.440
It's orchestrating a cluster.

104
0:08:13.440 --> 0:08:19.200
It's not just a deployment obviously as well, but it's about using the Kubernetes APIs to

105
0:08:19.200 --> 0:08:23.360
easily just take care of everything, so to say.

106
0:08:23.360 --> 0:08:27.560
You want add a new node into your storage cluster.

107
0:08:27.560 --> 0:08:29.080
What do you do technically speaking?

108
0:08:29.080 --> 0:08:35.760
You just add it to Kubernetes and well, if everything goes well, 10 seconds later the

109
0:08:35.760 --> 0:08:41.480
operator will be like, oh, new node, got to do my job, run the preparing job and everything,

110
0:08:41.480 --> 0:08:42.480
get the node ready.

111
0:08:42.480 --> 0:08:48.560
And a few seconds even later from that, the new SAV components, the OSDs on the disks,

112
0:08:48.560 --> 0:08:54.360
depending on what disks are obviously as well, are taken care of.

113
0:08:54.360 --> 0:08:59.840
To make this full circle there with Kubernetes side is like what the operator flow kind of

114
0:08:59.840 --> 0:09:02.960
pattern it's mostly called is about.

115
0:09:02.960 --> 0:09:05.560
It's about observing.

116
0:09:05.560 --> 0:09:10.720
The operator is observing a status or even in Kubernetes case, custom resources.

117
0:09:10.720 --> 0:09:19.720
I just think about it as like YAML, let's just give it a depth what it is, object of

118
0:09:19.720 --> 0:09:25.920
YAML in Kubernetes, which the operator can watch on, as a user, either make a change

119
0:09:25.920 --> 0:09:31.280
or even like my automatic CI CD process makes a change to it like, oh, a new node has been

120
0:09:31.280 --> 0:09:36.640
added or something or I want to tweak something in the configuration of the cluster.

121
0:09:36.640 --> 0:09:52.120
And so the operators are observing that and when there's a change or when there's even

122
0:09:52.120 --> 0:09:57.360
in like the Kubernetes cluster, there's a change like a node missing or something, it

123
0:09:57.360 --> 0:09:59.120
analyzes that change.

124
0:09:59.120 --> 0:10:04.560
For example, if a node is gone or it's just not ready in Kubernetes terms anymore, let's

125
0:10:04.560 --> 0:10:11.200
say network outage for like two of your nodes, the operator would analyze, well, observe

126
0:10:11.200 --> 0:10:15.920
it first of all, analyze that and start acting upon that.

127
0:10:15.920 --> 0:10:22.360
For example, in Kubernetes terms, it would take care of setting certain so-called, just

128
0:10:22.360 --> 0:10:26.720
to have that term out there, potter disruption budgets, which kind of try to prevent other

129
0:10:26.720 --> 0:10:34.320
nodes from potentially stopping the components of the SAV storage cluster.

130
0:10:34.320 --> 0:10:38.840
And the point is really just that it's this like observe, analyze, act kind of loop because

131
0:10:38.840 --> 0:10:41.640
in the end it just repeats itself all over again.

132
0:10:41.640 --> 0:10:45.240
It's a whole deal with Kubernetes operators.

133
0:10:45.240 --> 0:10:54.200
It's again, if like I want to, for I guess the people more already into SAV, if you want

134
0:10:54.200 --> 0:11:01.200
to scale up some more SAV monitors, or well, SAV Mons, you just edit the object in Kubernetes

135
0:11:01.200 --> 0:11:05.920
in the API and just crank the number from one count from three to five or something.

136
0:11:05.920 --> 0:11:10.800
And again, this change is detected by the operator, analyzes it and acts upon it.

137
0:11:10.800 --> 0:11:16.040
And that makes it quite convenient as well.

138
0:11:16.040 --> 0:11:18.320
Again here, oh, the perfect way to YAML.

139
0:11:18.320 --> 0:11:21.200
Sorry, it's a, I'm just this guy and come a little bit of clarification.

140
0:11:21.200 --> 0:11:26.120
I don't have it mirrored on my screen, so it's a bit hard.

141
0:11:26.120 --> 0:11:28.360
But that's exactly the YAML that we talked about.

142
0:11:28.360 --> 0:11:37.080
Like as an example, I have my cluster running and let's say new SAV release.

143
0:11:37.080 --> 0:11:42.360
What I would need to do to upgrade my cluster, I would basically go ahead and just change

144
0:11:42.360 --> 0:11:45.760
the image to be well not 17.2.3, let's say.

145
0:11:45.760 --> 0:11:48.560
17.2.5, that's the latest.

146
0:11:48.560 --> 0:11:50.960
As an example, yeah, 17.2.5 as an example.

147
0:11:50.960 --> 0:11:55.840
And again, operate with detected, analyze if every component is up to date or not, and

148
0:11:55.840 --> 0:12:01.640
then even start the, well, I don't want to say complicated upgrade process, but there's,

149
0:12:01.640 --> 0:12:06.240
especially with something like SAV, there's more than just, ah, let me just restart it.

150
0:12:06.240 --> 0:12:11.840
There's checks before every component is restarted for SAV native ways of like, it's basically

151
0:12:11.840 --> 0:12:17.000
commands that are, well, okay to stop, they're basically called like that.

152
0:12:17.000 --> 0:12:21.080
And that's the whole idea there, that the operator helps you with that and in the end

153
0:12:21.080 --> 0:12:22.520
just fully takes care of it.

154
0:12:22.520 --> 0:12:27.360
So that in the end, for the main part of your work, you can just sit back, change it in

155
0:12:27.360 --> 0:12:36.200
the YAML in a few minutes or it can even be hours depending on how big the cluster is.

156
0:12:36.200 --> 0:12:40.760
The operator will take care of that.

157
0:12:40.760 --> 0:12:44.320
As I mentioned before, like the example of the monitor count, for example, if you want

158
0:12:44.320 --> 0:12:45.720
to change that, change it.

159
0:12:45.720 --> 0:12:50.080
A few seconds later, the operator will pick that up and start making the changes necessary

160
0:12:50.080 --> 0:12:56.160
or even if you would want to scale it down from like five to three or three to one, which

161
0:12:56.160 --> 0:13:00.880
not recommended, we need highly availability there.

162
0:13:00.880 --> 0:13:05.240
Or another option, the operator again watches it, takes care of doing it.

163
0:13:05.240 --> 0:13:11.720
Or even if you want to specifically say on this one node, please use this one device

164
0:13:11.720 --> 0:13:19.200
or even for this then, disk or NVMe, for example, use more than one storage team, OSD team

165
0:13:19.200 --> 0:13:20.440
for that.

166
0:13:20.440 --> 0:13:28.000
These things are possible and quite easily just by writing some lines of YAML in the

167
0:13:28.000 --> 0:13:29.000
end.

168
0:13:29.000 --> 0:13:33.120
According to your workload, you can easily just customize your, according to your workload,

169
0:13:33.120 --> 0:13:34.640
you can easily customize your YAMLs.

170
0:13:34.640 --> 0:13:37.840
That will make your life easier.

171
0:13:37.840 --> 0:13:44.480
And we've mainly talked about having like the cluster running or even setting up the

172
0:13:44.480 --> 0:13:48.640
cluster with the YAML definition of a self-cluster object.

173
0:13:48.640 --> 0:13:57.240
But if you would, for example, want to, well, run some Prometheus in your Kubernetes cluster

174
0:13:57.240 --> 0:13:59.280
and need storage for them.

175
0:13:59.280 --> 0:14:04.160
To be able to use storage in SAV, you need to have a storage pool, for example, RBD storage,

176
0:14:04.160 --> 0:14:06.280
block storage basically.

177
0:14:06.280 --> 0:14:12.320
We also again just go ahead, create a SAV block pool object, which is simply containing

178
0:14:12.320 --> 0:14:20.120
the information of, if we go from here, failure domain, where, well, you basically tell SAV

179
0:14:20.120 --> 0:14:27.120
to only store data on different hosts to keep it simple for now.

180
0:14:27.120 --> 0:14:34.000
The replicated size that there will be free total replica or free copies of data in your

181
0:14:34.000 --> 0:14:35.800
cluster that require a safe replica.

182
0:14:35.800 --> 0:14:36.800
Let's just skip to for now.

183
0:14:36.800 --> 0:14:43.400
There's like a safe replica size, and even that you could technically set the compression

184
0:14:43.400 --> 0:14:44.400
mode for this pool.

185
0:14:44.400 --> 0:14:50.080
The point is, again, we can just write this in YAML, apply it against the Kubernetes API,

186
0:14:50.080 --> 0:14:56.320
and a few seconds later, also for like the other objects, same way you need the SAV file

187
0:14:56.320 --> 0:15:03.480
system, SAV object storage, same way the operator takes care of creating all the components.

188
0:15:03.480 --> 0:15:09.960
For example, the MDS for a file system, we have the standard components, like the manager,

189
0:15:09.960 --> 0:15:17.080
the monitors, operator, the OSDs, and even for the object store, for example, the RGW

190
0:15:17.080 --> 0:15:18.240
components.

191
0:15:18.240 --> 0:15:21.760
The operator simply takes care of that, and again, if you change the SAV version, a few

192
0:15:21.760 --> 0:15:26.400
seconds to maybe a minute or two later, depending on what the state of your cluster is, operator

193
0:15:26.400 --> 0:15:32.120
will take care of doing the update.

194
0:15:32.120 --> 0:15:43.160
We've talked about deploying RookSAV cluster, mainly right now.

195
0:15:43.160 --> 0:15:51.320
We want to highlight in that point as well the crew plugin that RookSAV is building and

196
0:15:51.320 --> 0:16:03.120
providing, it allows you to have certain processes automated, even certain disaster recovery

197
0:16:03.120 --> 0:16:06.640
cases are easier to handle with that.

198
0:16:06.640 --> 0:16:19.760
And GURF will talk a bit about that.

199
0:16:19.760 --> 0:16:21.200
So what's crew, right?

200
0:16:21.200 --> 0:16:26.600
Crew is basically a package manager for kubectl plugins.

201
0:16:26.600 --> 0:16:33.600
It makes the management of Kubernetes easier, and that's how the core developers and maintainers

202
0:16:33.600 --> 0:16:40.880
came together and thought we can definitely write a plugin to make the lives of our developers

203
0:16:40.880 --> 0:16:43.520
and administrators more easier.

204
0:16:43.520 --> 0:16:50.240
Crew was the way to go since it's the de facto package manager for kubectl plugins.

205
0:16:50.240 --> 0:16:58.800
So I mean, you can just do a kubectl install, kubectl crew install RookSAV.

206
0:16:58.800 --> 0:17:01.040
That's how the plugin will be installed.

207
0:17:01.040 --> 0:17:04.880
And just if you can see, we just ran the help command.

208
0:17:04.880 --> 0:17:09.000
It shows a bunch of things that you could do.

209
0:17:09.000 --> 0:17:14.760
You can just run a whole bunch of sef commands, rbd commands right now.

210
0:17:14.760 --> 0:17:16.320
Also check the health of your cluster.

211
0:17:16.320 --> 0:17:21.120
You could just do a bunch of things, like even if you want to remove an OSD.

212
0:17:21.120 --> 0:17:28.160
So the need actually arises because, for example, if you want to use underlying tools like sef

213
0:17:28.160 --> 0:17:33.320
object store or something like that to debug core troubleshooting issues and issues at

214
0:17:33.320 --> 0:17:35.080
core OSD level.

215
0:17:35.080 --> 0:17:41.280
I mean, crew plugin is definitely a great way to go as it provides common management

216
0:17:41.280 --> 0:17:43.520
and troubleshooting tools for sef.

217
0:17:43.520 --> 0:17:47.160
It's currently, I mean, a lot of things work.

218
0:17:47.160 --> 0:17:49.640
We'll show you.

219
0:17:49.640 --> 0:17:53.840
It's just like I mentioned, you just need to run kubectl crew install RookSAV.

220
0:17:53.840 --> 0:17:57.440
It goes ahead quickly, installs the plugin.

221
0:17:57.440 --> 0:18:01.360
It's I mean, way easier that you just don't need to earlier.

222
0:18:01.360 --> 0:18:07.560
I mean, you had to go inside the toolbox pod to debug and troubleshoot every issue with

223
0:18:07.560 --> 0:18:08.560
crew.

224
0:18:08.560 --> 0:18:15.560
It provides such an ease of access that it makes, I mean, lives easier and troubleshooting

225
0:18:15.560 --> 0:18:17.320
makes is definitely easier.

226
0:18:17.320 --> 0:18:23.040
You could just override the cluster configuration just at the runtime and some of the disaster

227
0:18:23.040 --> 0:18:25.600
recovery scenarios are also addressed.

228
0:18:25.600 --> 0:18:29.280
Some of the troubleshooting scenarios that were addressed is mon recovery.

229
0:18:29.280 --> 0:18:33.480
Suppose let's say you were have default three months in the cluster and majority of them

230
0:18:33.480 --> 0:18:35.080
lose code.

231
0:18:35.080 --> 0:18:38.000
I mean, recovering months from one maps.

232
0:18:38.000 --> 0:18:45.560
I mean, just doing a bunch of tasks could be, if not done carefully, it could lead to

233
0:18:45.560 --> 0:18:52.520
more disasters, but certainly with more automation in place when things are definitely working,

234
0:18:52.520 --> 0:18:55.840
this is also made easier with the crew plugin.

235
0:18:55.840 --> 0:19:04.600
And even if you want to troubleshoot CSI issues, it makes it easier for sure.

236
0:19:04.600 --> 0:19:14.320
So yeah, I mean, just like if you want to restore mods with OSDs and even if we just

237
0:19:14.320 --> 0:19:20.040
delete the RookSAV cluster after accidental deletion of custom resources, that could be

238
0:19:20.040 --> 0:19:23.640
also restored.

239
0:19:23.640 --> 0:19:28.520
And one of the common goals in the roadmap is also automating core dump collection,

240
0:19:28.520 --> 0:19:33.640
because let's say if there's an issue that happens with the safe daemon and we want to

241
0:19:33.640 --> 0:19:39.720
collect the core dump of the process for further investigation to share it with the developers

242
0:19:39.720 --> 0:19:44.320
and with the community to understand what issues we are facing.

243
0:19:44.320 --> 0:19:49.680
It can easily do well if you want to just do a performance profiling of a process with

244
0:19:49.680 --> 0:19:52.120
GDB that could be made easier as well.

245
0:19:52.120 --> 0:19:54.000
So these are some of the goals.

246
0:19:54.000 --> 0:20:00.560
The current plugin is written in bash, but there's a work going on to rewrite the whole

247
0:20:00.560 --> 0:20:08.720
plugin in Golang so that it's definitely more scalable and much more easier to manage and

248
0:20:08.720 --> 0:20:11.160
even for contributors.

249
0:20:11.160 --> 0:20:18.120
So yeah, just like that.

250
0:20:18.120 --> 0:20:37.400
So, I guess the point we're more or less just trying to make is that if you have Kubernetes

251
0:20:37.400 --> 0:20:46.840
or even run a distribution of, well, what is the range or open shift obviously as well

252
0:20:46.840 --> 0:20:53.680
on your hardware and I wouldn't even put it to some degree as like you're confident enough

253
0:20:53.680 --> 0:20:57.960
with Kubernetes to run it.

254
0:20:57.960 --> 0:21:04.000
You can have it quite easy running a self cluster as well on top of that.

255
0:21:04.000 --> 0:21:07.520
Obviously to some degree you need some self knowledge, but that's with everything.

256
0:21:07.520 --> 0:21:15.320
If you run it in, if you want to run it in production, it's just that with this abstraction

257
0:21:15.320 --> 0:21:19.280
layer again with Kubernetes, it makes it easier for you.

258
0:21:19.280 --> 0:21:24.360
It's more of like you're kind of stamped in general there to think of more of like, oh,

259
0:21:24.360 --> 0:21:31.360
well I have some nodes and they're simply there to take care of the components that

260
0:21:31.360 --> 0:21:34.200
you need to run for the self cluster.

261
0:21:34.200 --> 0:21:40.240
And especially with the root self operator obviously, it makes the process easier by

262
0:21:40.240 --> 0:21:48.000
well a get ups approach for example, where you can just throw your Yamls into well into

263
0:21:48.000 --> 0:21:55.240
get most of the time and have that automatic mechanism basically take care of this deployment

264
0:21:55.240 --> 0:21:56.240
process.

265
0:21:56.240 --> 0:22:02.120
So that again, the operator just takes this YAML takes care of it and makes the changes

266
0:22:02.120 --> 0:22:09.280
necessary and with the root self crew plugin just to get that summarized real quick again.

267
0:22:09.280 --> 0:22:18.040
It's a way for us to have certain automatic processes in the hand of admins when they

268
0:22:18.040 --> 0:22:23.960
need to and not just as like, hey, here's like a 100 line bash script, please run that

269
0:22:23.960 --> 0:22:26.840
one comment at a time.

270
0:22:26.840 --> 0:22:30.320
And it simply allows it again, because we have this access to communities where we can

271
0:22:30.320 --> 0:22:33.440
just ask Kubernetes, hey, where's the monitor running?

272
0:22:33.440 --> 0:22:38.800
Oh, it's on node A and all that because well, we have an API that can tell us most of this

273
0:22:38.800 --> 0:22:42.880
information and also for recovery scenarios there.

274
0:22:42.880 --> 0:22:49.240
We can just ask Kubernetes to run a new pod or to well, have a new monitor for example,

275
0:22:49.240 --> 0:22:53.600
then running with this old information from the other monitors to have this forum recovered

276
0:22:53.600 --> 0:22:56.120
is required there.

277
0:22:56.120 --> 0:23:05.480
And regarding root self is like a general outlook for the future.

278
0:23:05.480 --> 0:23:09.960
One of the major points we're currently looking at is that we want to improve the cluster

279
0:23:09.960 --> 0:23:13.240
manageability even more than we already have it at.

280
0:23:13.240 --> 0:23:15.080
We want to make it easier.

281
0:23:15.080 --> 0:23:16.320
We're using the root self plugin.

282
0:23:16.320 --> 0:23:21.920
Right now you still need to do quite a lot of manual YAML editing of the objects that

283
0:23:21.920 --> 0:23:29.640
we have in the API, but we would like to have, well, some more crew plugin commands there

284
0:23:29.640 --> 0:23:34.440
again to extend that functionality, make it simply easier.

285
0:23:34.440 --> 0:23:40.160
As well, improved security by having the operator and other components that are running in the

286
0:23:40.160 --> 0:23:46.120
cluster, use separate access credentials to the cluster just to have there a bit more,

287
0:23:46.120 --> 0:23:52.080
well, I guess to some degree, even transparency if you would look at like audit logging of

288
0:23:52.080 --> 0:24:01.480
the self cluster and as well as encryption support for self-affairs and OSDs on partitions.

289
0:24:01.480 --> 0:24:04.760
And as with everything, there's more.

290
0:24:04.760 --> 0:24:10.400
Feel free to check out the roadmap MD file on the GitHub, github.com slash root.

291
0:24:10.400 --> 0:24:13.380
The link will be shown as well.

292
0:24:13.380 --> 0:24:16.600
If you want to get involved, if you want to contribute, if you have questions or anything,

293
0:24:16.600 --> 0:24:20.840
we have, well, obviously to get up, there's even the GitHub discussions open.

294
0:24:20.840 --> 0:24:28.960
If you have any, well, any more, more questions, I guess, then that might not fit on Slack.

295
0:24:28.960 --> 0:24:32.960
Well, we have Twitter account, obviously.

296
0:24:32.960 --> 0:24:40.720
We also have community meetings if you have any more pressing concerns to talk about.

297
0:24:40.720 --> 0:24:46.400
And well, just kind of from that side as well, we're, as Gerv and I mentioned, we're from

298
0:24:46.400 --> 0:24:47.720
Code Technology Sync.

299
0:24:47.720 --> 0:24:52.520
We're building a company that wants to create a product around Rook7 and just in general,

300
0:24:52.520 --> 0:24:54.760
try to help the community out there.

301
0:24:54.760 --> 0:24:58.080
If we do talk with us or contact us as well.

302
0:24:58.080 --> 0:25:04.320
And for now, thank you for listening and we'll gladly take some questions and can simply

303
0:25:04.320 --> 0:25:08.800
take the last, I think you showed 50 minutes for questions or even just talking a bit about

304
0:25:08.800 --> 0:25:11.120
certain scenarios here with everyone, I guess.

305
0:25:11.120 --> 0:25:14.880
I would just like to add one more last thing before we go.

306
0:25:14.880 --> 0:25:16.960
I'll just take it.

307
0:25:16.960 --> 0:25:19.400
It's not a good idea that there's two like.

308
0:25:19.400 --> 0:25:21.360
Yeah, I would just like to add one last thing.

309
0:25:21.360 --> 0:25:27.920
If you want to check a demo and more troubleshooting scenarios, we did a talk at SEF virtual,

310
0:25:27.920 --> 0:25:34.200
Summit 2022, which is already there on YouTube where we demoed a couple troubleshooting scenarios

311
0:25:34.200 --> 0:25:35.200
and crew plugin.

312
0:25:35.200 --> 0:25:39.920
I'll definitely share a reference and add a reference to here, but that'll be good to

313
0:25:39.920 --> 0:25:44.240
check out as well if you want to check out a live demo.

314
0:25:44.240 --> 0:25:45.240
Yeah.

315
0:25:45.240 --> 0:25:46.240
Thanks.

316
0:25:46.240 --> 0:25:47.240
Thanks.

317
0:25:47.240 --> 0:25:48.240
Any questions?

318
0:25:48.240 --> 0:25:49.240
Yeah.

319
0:25:49.240 --> 0:25:50.240
Thank you.

320
0:25:50.240 --> 0:25:51.240
Thank you.

321
0:25:51.240 --> 0:25:52.240
Thank you.

322
0:25:52.240 --> 0:25:53.240
Thank you.

323
0:25:53.240 --> 0:25:54.240
Thank you.

324
0:25:54.240 --> 0:25:55.240
Thank you.

325
0:25:55.240 --> 0:25:56.240
Thank you.

326
0:25:56.240 --> 0:25:57.240
Thank you.

327
0:25:57.240 --> 0:25:58.240
Thank you for this talk.

328
0:25:58.240 --> 0:26:08.520
I was wondering a bit about the downsides of using Rook with SEF because SEF is known

329
0:26:08.520 --> 0:26:16.040
to be really hard in configuring getting the right performance to be some kind of functionality

330
0:26:16.040 --> 0:26:17.040
there.

331
0:26:17.040 --> 0:26:23.320
So if I summarize correctly, the question is what are the downsides?

332
0:26:23.320 --> 0:26:29.080
I would more or less maybe put it at advantages, disadvantages of using Rook to run SEF on

333
0:26:29.080 --> 0:26:31.800
Kubernetes, especially with SEF being quite complex.

334
0:26:31.800 --> 0:26:32.800
Yeah.

335
0:26:32.800 --> 0:26:33.800
Right.

336
0:26:33.800 --> 0:26:41.680
In general, so if you have a decision on the loss of control that may have a concept,

337
0:26:41.680 --> 0:26:46.520
if there is a...

338
0:26:46.520 --> 0:26:48.560
If there's a loss of control on SEF side?

339
0:26:48.560 --> 0:26:49.560
Yeah.

340
0:26:49.560 --> 0:26:53.480
SEF has a lot of nubs and things to configure.

341
0:26:53.480 --> 0:26:56.320
Are there some that we lose and we use Rook?

342
0:26:56.320 --> 0:26:57.320
Oh, I see.

343
0:26:57.320 --> 0:26:58.320
Okay.

344
0:26:58.320 --> 0:27:00.760
And added to that question, if there's anything that's...

345
0:27:00.760 --> 0:27:04.280
Well, you lose when you use Rook SEF.

346
0:27:04.280 --> 0:27:12.720
I guess as a major downside that most people see as well is because you have an additional

347
0:27:12.720 --> 0:27:17.800
layer with Kubernetes being that.

348
0:27:17.800 --> 0:27:24.760
I guess maybe to address that a bit more from what is at least I know the error for some

349
0:27:24.760 --> 0:27:25.760
of SEF ADM.

350
0:27:25.760 --> 0:27:27.400
I think SEF ADM is for Rimmel, correct?

351
0:27:27.400 --> 0:27:30.240
Uses Docker to run containers basically as well, right?

352
0:27:30.240 --> 0:27:31.240
Pudman.

353
0:27:31.240 --> 0:27:32.240
Yeah.

354
0:27:32.240 --> 0:27:33.240
SEF ADM, right?

355
0:27:33.240 --> 0:27:34.240
Yeah.

356
0:27:34.240 --> 0:27:35.440
SEF ADM, for example, at least uses...

357
0:27:35.440 --> 0:27:41.320
Kind of also introduces a scenario, so to say, with Docker slash Pudman.

358
0:27:41.320 --> 0:27:47.120
One that runs container insert here.

359
0:27:47.120 --> 0:27:53.880
It has more or less in regards to installing SEF, for example, in my eyes, but I'm very

360
0:27:53.880 --> 0:27:57.500
biased to containers, obviously.

361
0:27:57.500 --> 0:28:03.120
It has this aspect of here's the SEF image and it should work unless you have something

362
0:28:03.120 --> 0:28:08.120
weird with the host OS going on.

363
0:28:08.120 --> 0:28:14.680
Downside is, again, if Kubernetes just goes completely crazy, the SEF class is probably

364
0:28:14.680 --> 0:28:21.080
also going to have a bad time, but that's kind of then like the weighing of, are you

365
0:28:21.080 --> 0:28:27.960
confident enough, I guess, to run Kubernetes and even running a Kubernetes just for long

366
0:28:27.960 --> 0:28:28.960
term?

367
0:28:28.960 --> 0:28:34.080
Especially with Kubernetes, there's more of this talk about, what's it again, pets versus

368
0:28:34.080 --> 0:28:35.080
cattle.

369
0:28:35.080 --> 0:28:40.040
Instead of just having a cluster for every application or something even and just, oh,

370
0:28:40.040 --> 0:28:46.200
we're done throwing it away versus for, well, obviously something as persistent and important

371
0:28:46.200 --> 0:28:50.960
as a SEF cluster, you can just throw it away then.

372
0:28:50.960 --> 0:28:55.920
From experience so far, I can tell that it is possible to run a Ruke SEF cluster over

373
0:28:55.920 --> 0:28:56.920
multiple years.

374
0:28:56.920 --> 0:28:57.920
I think I...

375
0:28:57.920 --> 0:28:59.920
When did I start mine?

376
0:28:59.920 --> 0:29:05.440
I think I had it running for two years and the only reason I shut it down was because

377
0:29:05.440 --> 0:29:10.880
I had gotten new hardware in another location, I kind of just said it was like, do I migrate

378
0:29:10.880 --> 0:29:11.880
it or do I not mine?

379
0:29:11.880 --> 0:29:16.840
It was just, okay, let's just start from scratch, but it's also because that cluster I'm talking

380
0:29:16.840 --> 0:29:22.360
about there had like 50 other applications running where it's just like, okay, let's

381
0:29:22.360 --> 0:29:25.820
start from scratch anyway, so to say.

382
0:29:25.820 --> 0:29:31.360
In regards to losing control, it's not necessarily.

383
0:29:31.360 --> 0:29:36.120
You don't really have like a...

384
0:29:36.120 --> 0:29:41.920
Like you don't have like a use this disk manual really way besides putting it in the YAML

385
0:29:41.920 --> 0:29:47.880
and fingers crossed the operator takes care of preparing and then applying an OSD to that

386
0:29:47.880 --> 0:29:51.840
disk or even partition.

387
0:29:51.840 --> 0:29:56.500
But it's like, again, I think with most tools there that take away certain aspects at least

388
0:29:56.500 --> 0:30:04.960
in regards to the installation or configuration.

389
0:30:04.960 --> 0:30:09.320
That those points are taking away, but it lies in regards to configuring staff or even

390
0:30:09.320 --> 0:30:11.880
certain aspects.

391
0:30:11.880 --> 0:30:21.560
You can do everything as normal and at least from experience with staff, I guess to put

392
0:30:21.560 --> 0:30:26.920
it like that has gotten a lot better as with the...

393
0:30:26.920 --> 0:30:29.680
Now the config store.

394
0:30:29.680 --> 0:30:36.440
The config store as it basically says, you have a config store in the monitors where

395
0:30:36.440 --> 0:30:40.680
you can just easily set for certain components instead of always having to manually make

396
0:30:40.680 --> 0:30:45.880
changes to any config files on the servers on your storage nodes themselves.

397
0:30:45.880 --> 0:30:48.480
And it has gotten better.

398
0:30:48.480 --> 0:30:49.480
That's awesome.

399
0:30:49.480 --> 0:30:57.520
I would just like to say at a lot of places it gives you a control as well, right?

400
0:30:57.520 --> 0:31:05.880
Because I mean, operator is responsible for reconciliation and taking charge when...

401
0:31:05.880 --> 0:31:10.480
I mean, automated scenarios where we want recovery to happen, right?

402
0:31:10.480 --> 0:31:14.560
And, Seth, the goal is to improve recovery and introductions.

403
0:31:14.560 --> 0:31:18.080
You don't need any unexpected loss of control as well, right?

404
0:31:18.080 --> 0:31:21.680
We would want to give admins a certain level of control.

405
0:31:21.680 --> 0:31:25.520
We don't want them to go ahead and play around with the OSD.

406
0:31:25.520 --> 0:31:34.080
So I think in many of the production scenarios, you need a certain set of control as well,

407
0:31:34.080 --> 0:31:36.640
which Rook actually provides.

408
0:31:36.640 --> 0:31:44.760
So at that point, I would certainly recommend and consider it as an advantage as well.

409
0:31:44.760 --> 0:31:48.480
One, two, three.

410
0:31:48.480 --> 0:31:49.480
Yeah.

411
0:31:49.480 --> 0:31:50.480
Okay.

412
0:31:50.480 --> 0:31:51.480
Yeah.

413
0:31:51.480 --> 0:31:52.480
Yeah.

414
0:31:52.480 --> 0:31:53.480
Yeah.

415
0:31:53.480 --> 0:31:54.480
Please.

416
0:31:54.480 --> 0:31:55.480
Sure.

417
0:31:55.480 --> 0:31:56.480
Please.

418
0:31:56.480 --> 0:31:59.480
We need three guys up here.

419
0:31:59.480 --> 0:32:01.480
Okay, please go ahead.

420
0:32:01.480 --> 0:32:02.480
We need forum.

421
0:32:02.480 --> 0:32:03.480
Yeah, exactly.

422
0:32:03.480 --> 0:32:04.480
Please go ahead.

423
0:32:04.480 --> 0:32:09.480
It's a similar question, but do you expect much more performance here, rather than setting

424
0:32:09.480 --> 0:32:15.080
up Kubernetes versus running some time?

425
0:32:15.080 --> 0:32:26.280
Question is, if there's going to be a performance hit in regards to running, Seth and Kubernetes?

426
0:32:26.280 --> 0:32:28.360
Depends on how you run it.

427
0:32:28.360 --> 0:32:33.800
If you run it, like I'm personally preferring running the myRook, Seth clusters always with

428
0:32:33.800 --> 0:32:38.040
host network, but you kind of can already, depending on how far you're with container

429
0:32:38.040 --> 0:32:41.880
or Kubernetes, it goes over, well, host network.

430
0:32:41.880 --> 0:32:43.880
Some like that, some don't.

431
0:32:43.880 --> 0:32:48.320
I personally do more or less just do it because I don't want the traffic to go over the overlay

432
0:32:48.320 --> 0:32:49.320
network.

433
0:32:49.320 --> 0:32:54.480
I have some plug-in, some CNI, container network interface where anyone who wants to look into

434
0:32:54.480 --> 0:32:57.720
that, that takes care of the network between your nodes.

435
0:32:57.720 --> 0:32:58.840
So it more or less depends.

436
0:32:58.840 --> 0:33:03.800
There's a lot of people using, well, just having Rook, Seth talk over the overlay network

437
0:33:03.800 --> 0:33:04.800
as well.

438
0:33:04.800 --> 0:33:06.320
It works fine as well.

439
0:33:06.320 --> 0:33:11.200
It's just a preference, I would really more or less put it at.

440
0:33:11.200 --> 0:33:15.720
Depending on what your network looks like, if you have 10G or something and your overlay

441
0:33:15.720 --> 0:33:20.120
network in the end maybe brings the, like in the IPERF test at least, brings it down

442
0:33:20.120 --> 0:33:29.040
to like 9.0 something, is it worth exposing that traffic to the host network then versus

443
0:33:29.040 --> 0:33:31.920
just having it go over the overlay network?

444
0:33:31.920 --> 0:33:35.480
But again, if you think about it, just another layout to consider.

445
0:33:35.480 --> 0:33:40.200
If you want that, if you don't, and if you don't want that, there's also options like

446
0:33:40.200 --> 0:33:48.600
maltos to allow you some more fine-grained network connections or config in regards to

447
0:33:48.600 --> 0:33:52.960
the interfaces you want to pass in, like different VLANs or something.

448
0:33:52.960 --> 0:33:55.680
But that's like, again, it depends.

449
0:33:55.680 --> 0:33:57.280
Yeah, yeah.

450
0:33:57.280 --> 0:34:03.320
Can you still manage your Seth cluster via the Seth dashboard or is it another dashboard

451
0:34:03.320 --> 0:34:06.720
or do you need two dashboards?

452
0:34:06.720 --> 0:34:10.840
The question was if you can still use the Seth dashboard, maybe just to expand on that

453
0:34:10.840 --> 0:34:16.880
Seth manager dashboard, just to manage your Seth cluster.

454
0:34:16.880 --> 0:34:26.800
To some degree, there is currently not a functionality to add new OSDs, I think, if I remember correctly.

455
0:34:26.800 --> 0:34:30.880
That's one thing as well with the future roadmap part, with the more manager-ability, where

456
0:34:30.880 --> 0:34:35.760
I also kind of looked at the dashboard and was like, wait, I have a create button.

457
0:34:35.760 --> 0:34:36.760
Why?

458
0:34:36.760 --> 0:34:38.760
Oh, why don't we?

459
0:34:38.760 --> 0:34:42.440
But then it's the typical, oh, there's some roadblocks that we just need to get out of

460
0:34:42.440 --> 0:34:47.120
the way and make sure that we are all, especially with operator and even Sethadium and others

461
0:34:47.120 --> 0:34:51.600
out there, we're all aligned on the same way or if there's a manager interface, because

462
0:34:51.600 --> 0:34:54.080
there is even one.

463
0:34:54.080 --> 0:34:57.480
And I think if I understood you correctly or if I heard from the meetings correctly,

464
0:34:57.480 --> 0:35:02.920
they are even looking into improving that interface further, it will hopefully be easier,

465
0:35:02.920 --> 0:35:08.320
thankfully also faster, to have the dashboard as this point of contact as well.

466
0:35:08.320 --> 0:35:12.680
Yeah, there's a lot of work that is currently going on.

467
0:35:12.680 --> 0:35:15.600
I'll just keep it out.

468
0:35:15.600 --> 0:35:22.440
I'll just say that there's a lot of work going on currently in the usability space from the

469
0:35:22.440 --> 0:35:29.760
recent discussions in upstream that we have had to improve dashboard as well from both

470
0:35:29.760 --> 0:35:35.840
Qubectl, from both Kubernetes and all standalone Seth perspective.

471
0:35:35.840 --> 0:35:43.080
It's to make sure that, I mean, you can easily manage and monitor Seth even in the CNCA fold.

472
0:35:43.080 --> 0:35:51.040
There has been recent discussions that have happened to improve it as well from Rook space.

473
0:35:51.040 --> 0:35:53.880
So a lot of work is going on in the usability space.

474
0:35:53.880 --> 0:35:58.080
But if you have any ideas, it'll be most welcome.

475
0:35:58.080 --> 0:36:03.160
And really, it would be great to have, I mean, usability is one thing that really matters

476
0:36:03.160 --> 0:36:04.160
a lot, right?

477
0:36:04.160 --> 0:36:10.080
I mean, user experience is one thing that, I mean, we would certainly cater to improve

478
0:36:10.080 --> 0:36:11.080
in Rook.

479
0:36:11.080 --> 0:36:14.080
Should we have time for one more question?

480
0:36:14.080 --> 0:36:15.080
Yeah.

481
0:36:15.080 --> 0:36:22.080
Is your main use case to provide storage within a cluster where, for example, other applications

482
0:36:22.080 --> 0:36:31.080
or is the use case more to use Kubernetes as an orchestrator for Seth that I put next

483
0:36:31.080 --> 0:36:35.080
to my cluster where my main applications will have basically just the orchestration the

484
0:36:35.080 --> 0:36:38.680
same way I'm not within the cluster.

485
0:36:38.680 --> 0:36:46.080
So the question is, could I maybe modify it a bit more into the direction of, like, how

486
0:36:46.080 --> 0:36:47.920
can you run Rook, I guess?

487
0:36:47.920 --> 0:36:49.240
I think that plays into that as well.

488
0:36:49.240 --> 0:36:53.960
Like you can run Rook, Seth.

489
0:36:53.960 --> 0:37:00.640
You can run Rook, Seth in a way that you connected to an existing Seth cluster that it doesn't

490
0:37:00.640 --> 0:37:05.600
even matter if it's a Rook, Seth cluster, just a Seth cluster works as well.

491
0:37:05.600 --> 0:37:11.240
It mainly takes care of them just setting up the CSI driver then.

492
0:37:11.240 --> 0:37:13.480
I know people use that to some degree as well.

493
0:37:13.480 --> 0:37:18.040
If they have an existing or even an existing Rook, Seth cluster that they want to share

494
0:37:18.040 --> 0:37:20.880
with the others.

495
0:37:20.880 --> 0:37:26.800
There's also in this external mode the possibility of the Rook, Seth operator to manage certain

496
0:37:26.800 --> 0:37:27.800
components.

497
0:37:27.800 --> 0:37:31.600
So that, for example, if you want a file system, you could run those MDS demons that you need

498
0:37:31.600 --> 0:37:37.540
for the file system in that cluster that your Kubernetes is running on then.

499
0:37:37.540 --> 0:37:38.540
That works as well.

500
0:37:38.540 --> 0:37:40.880
It's kind of like those two main external modes.

501
0:37:40.880 --> 0:37:45.320
And obviously the case of running it in the same cluster.

502
0:37:45.320 --> 0:37:51.240
That's kind of like this either you just share what you have or share and allow, like, Seth

503
0:37:51.240 --> 0:37:57.840
file system or Seth objects or you can just run the demons in the same cluster then.

504
0:37:57.840 --> 0:37:59.600
Both works for all the operators.

505
0:37:59.600 --> 0:38:00.600
Does that answer that?

506
0:38:00.600 --> 0:38:01.600
Yeah.

507
0:38:01.600 --> 0:38:02.600
Thank you.

508
0:38:02.600 --> 0:38:03.600
Done?

509
0:38:03.600 --> 0:38:04.600
Any other questions?

510
0:38:04.600 --> 0:38:05.600
There are no questions.

511
0:38:05.600 --> 0:38:09.600
There are a bunch of stickers here for everyone.

512
0:38:09.600 --> 0:38:10.600
Yeah.

513
0:38:10.600 --> 0:38:11.600
Stickers?

514
0:38:11.600 --> 0:38:12.600
And?

515
0:38:12.600 --> 0:38:13.600
Do we have some more?

516
0:38:13.600 --> 0:38:14.600
If you've asked the question just now, just come see me.

517
0:38:14.600 --> 0:38:15.600
You've got a T-shirt too.

518
0:38:15.600 --> 0:38:33.600
And maybe there's some leftover after that.

