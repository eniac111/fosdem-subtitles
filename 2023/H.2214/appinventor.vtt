WEBVTT

00:00.000 --> 00:08.000
Okay, I think we can start already.

00:08.000 --> 00:10.880
Hi everybody, I'm Diego Barreiro.

00:10.880 --> 00:15.160
I'm one of the open source contributors to the MIT App Inventor project and today I'm

00:15.160 --> 00:19.160
going to be talking about App Inventor and how we can introduce artificial intelligence

00:19.160 --> 00:21.900
to kids using this platform.

00:21.900 --> 00:25.440
But before getting into it, I would like to introduce myself a little bit more.

00:25.440 --> 00:31.720
I started coding with App Inventor when I was 14 years old, it was 2013 at that time,

00:31.720 --> 00:35.840
and I basically wanted to build an app, I didn't know anything about coding, and a high

00:35.840 --> 00:38.260
school teacher showed me this amazing platform.

00:38.260 --> 00:42.440
So I just spent the next couple of years building apps with App Inventor and eventually I switched

00:42.440 --> 00:47.920
it to Java coding and I was able to contribute to the project later on as I'm doing right

00:47.920 --> 00:49.080
now.

00:49.080 --> 00:51.040
So what is MIT App Inventor?

00:51.040 --> 00:56.880
MIT Inventor is an online platform that enables anybody to build any kind of application without

00:56.880 --> 01:01.040
having to learn any programming language like Java or coding that is nowadays the most popular

01:01.040 --> 01:02.040
ones.

01:02.040 --> 01:07.240
This is the interface and it has the mock phone on the center and on the left side we

01:07.240 --> 01:10.880
can have the components, the elements that will make the app like buttons, labels, text

01:10.880 --> 01:16.040
boxes, text areas, like any kind of interaction that the user will have with the app.

01:16.040 --> 01:21.600
And then we can customize the properties like colors, text fonts, text sizes, whatever on

01:21.600 --> 01:26.720
this panel so we can make the app look as we wish for the final user.

01:26.720 --> 01:28.760
And how does the logic work?

01:28.760 --> 01:31.160
Well we, most of you may know about the scratch.

01:31.160 --> 01:35.340
App Inventor works somehow like a scratch using this block language.

01:35.340 --> 01:38.200
So let's say that we want to play a sound when the app opens.

01:38.200 --> 01:43.640
We will be using a block that says when the screen one has opened we want to play a specific

01:43.640 --> 01:45.600
sound later on.

01:45.600 --> 01:48.320
And that's how we can just make any kind of application.

01:48.320 --> 01:55.960
MIT Inventor allows existing Android developers and Android developers to introduce new components

01:55.960 --> 01:57.560
using extensions.

01:57.560 --> 02:01.400
And we will be using today one of those extensions that was developed by a research project

02:01.400 --> 02:09.360
at MIT that enables to classify images on different groups using artificial intelligence.

02:09.360 --> 02:14.640
And to give some numbers of App Inventor it was tested in 2008 as a Google project.

02:14.640 --> 02:18.840
And then a few years later it eventually was transferred to MIT.

02:18.840 --> 02:23.920
Right now it has gathered over 18 million users since it was created, since it was transferred

02:23.920 --> 02:28.560
to MIT with nearly 85 million apps that have been developed.

02:28.560 --> 02:33.080
And on a monthly basis we get roughly around 1 million users.

02:33.080 --> 02:38.240
And in terms of open search contributions we have seen 164 different contributors to

02:38.240 --> 02:43.120
the project on GitHub.

02:43.120 --> 02:46.080
So today I'm not going to be giving the classic talk.

02:46.080 --> 02:49.560
I'm going to be showing a tutorial and people in the audience and at home can just follow

02:49.560 --> 02:53.960
this tutorial by visiting the following link on how to build an app.

02:53.960 --> 02:57.680
And what we will be doing today is building the pickup app.

02:57.680 --> 03:02.280
Pickup is a game that is usually played with babies that when you see the baby that he

03:02.280 --> 03:06.280
loves and when you hide yourself it just cries.

03:06.280 --> 03:10.120
So to show the final result, this will be the final result.

03:10.120 --> 03:12.280
Let me just switch to my phone.

03:12.280 --> 03:14.240
I'm going to be mirroring this phone.

03:14.240 --> 03:19.600
So I open the final app and I'm going to start using the camera.

03:19.600 --> 03:21.240
I can see, yep, here I am.

03:21.240 --> 03:23.600
I can start and I'm looking at the baby that he's happy.

03:23.600 --> 03:27.480
If I hide myself it just cries.

03:27.480 --> 03:30.720
So let's get into it.

03:30.720 --> 03:33.640
So how we can use MIT App Inventor 2?

03:33.640 --> 03:38.480
The standard instance of App Inventor is hosted at ai2.appinventor.mit.edu, but that requires

03:38.480 --> 03:39.480
an account.

03:39.480 --> 03:45.320
MIT has created this specific instance, code.appinventor.mit, that allows you to create these projects

03:45.320 --> 03:48.240
without any existing account.

03:48.240 --> 03:54.280
We will be using an anonymous account so that that can be cleaned up later after finishing.

03:54.280 --> 03:56.920
And we will receive our code like the following one.

03:56.920 --> 04:00.120
Now this is blurred, but you will be able to see our code.

04:00.120 --> 04:03.360
And on the previous screen, if you want to recover the project later on, you can just

04:03.360 --> 04:08.160
paste the code that you previously get right here.

04:08.160 --> 04:11.800
And once that is done, you will be able to access an anonymous account, as you can see

04:11.800 --> 04:15.180
over here, and you can just start creating the app.

04:15.180 --> 04:19.320
So let's get into it.

04:19.320 --> 04:24.680
So we can visit firstm23.appinventor.mit.edu, and we can click on this link.

04:24.680 --> 04:29.360
This link is basically the code App Inventor instance, and we are loading a template project

04:29.360 --> 04:31.560
for the pickable project.

04:31.560 --> 04:35.140
So I'm going to click on this one.

04:35.140 --> 04:40.200
So I click on Continue without an account.

04:40.200 --> 04:44.040
And just wait a few seconds for the project to download from the repository, and here

04:44.040 --> 04:45.040
it is.

04:45.040 --> 04:48.000
That was faster than last night.

04:48.000 --> 04:52.520
I can see the code here, so I can just copy paste the code to access this instance later

04:52.520 --> 04:53.520
on.

04:53.520 --> 04:59.100
And as this is a tutorial, we can see on the left side that we are seeing a description

04:59.100 --> 05:03.080
of what we are trying to build with a detailed step-by-step guide.

05:03.080 --> 05:05.720
And this is the instance of the project.

05:05.720 --> 05:10.040
I can see the happy baby, and then the sad baby is hidden here.

05:10.040 --> 05:15.020
Okay, so let me just continue the presentation.

05:15.020 --> 05:17.000
The next step is turning the classifier.

05:17.000 --> 05:20.760
I'm not going to get too deep into the machine learning and how it works.

05:20.760 --> 05:25.280
I'm just going to be providing a very high-level overview of how this works.

05:25.280 --> 05:30.280
So we will be using an image classification system that consists in creating two groups

05:30.280 --> 05:31.280
of images.

05:31.280 --> 05:37.560
So we are creating one group of images that is the face of myself looking at the baby

05:37.560 --> 05:42.560
and another group of images that is me hiding from the baby so we can show the sad face.

05:42.560 --> 05:44.240
So how does this work?

05:44.240 --> 05:51.520
We can visit this website, classifier.appinventor.mit.edu to train this model.

05:51.520 --> 05:55.400
Just as a side note, this instance only needs internet to load once.

05:55.400 --> 06:00.360
To train the model, that all happens in your browser, so no servers are involved, no images

06:00.360 --> 06:02.960
are transferred outside your desktop.

06:02.960 --> 06:06.000
And this website is also open source, so you can just check.

06:06.000 --> 06:10.260
There is the link on the FOSDAM23 website.

06:10.260 --> 06:14.440
So we visit the website, and we start first creating the images.

06:14.440 --> 06:17.020
So in this case, we will be creating one image group for me.

06:17.020 --> 06:18.880
So I'm looking at the baby and not me.

06:18.880 --> 06:20.560
I'm hiding from the baby.

06:20.560 --> 06:24.480
If for example we are in a biology class and we want to classify trees, we will be creating

06:24.480 --> 06:27.340
one group of images for each kind of tree.

06:27.340 --> 06:30.340
So we can recognize them later on.

06:30.340 --> 06:35.720
Then we will turn on the camera to take a photo of myself, setting the group of images

06:35.720 --> 06:37.640
that I'm going to be saving this.

06:37.640 --> 06:40.880
So if I'm looking at the camera, it's going to be the not me group.

06:40.880 --> 06:44.640
If I'm looking at the camera, it's going to be the me group.

06:44.640 --> 06:46.840
This is the same for the not me.

06:46.840 --> 06:50.200
And once that is done, when we have a reasonable amount of images for each group, which will

06:50.200 --> 06:53.520
be around 5 or 10 images for each group, we can train the model.

06:53.520 --> 07:00.320
As again, this training happens on your computer, so no images transferred outside of your computer.

07:00.320 --> 07:04.600
We can then test the model with new images to make sure that we have properly trained

07:04.600 --> 07:07.560
the model and can identify ourselves.

07:07.560 --> 07:11.920
And once that is done, we can export the model and load it into Appingmentor.

07:11.920 --> 07:14.640
So I'm going to be doing that very quickly.

07:14.640 --> 07:18.720
Go to first and enter tree, Appingmentor.mit.edu.

07:18.720 --> 07:22.080
I open the classifier instance.

07:22.080 --> 07:25.200
It's going to ask for permission to use the webcam for each.

07:25.200 --> 07:26.200
There it is.

07:26.200 --> 07:27.580
I just accepted it for.

07:27.580 --> 07:30.560
So the first step is creating the labels.

07:30.560 --> 07:37.440
First the me label, enter, and next the not me label.

07:37.440 --> 07:39.720
And well, the light is quite hot.

07:39.720 --> 08:03.160
I think it was.

08:03.160 --> 08:11.720
Take a few more images of me looking to different places so I can train better the model.

08:11.720 --> 08:12.720
One more.

08:12.720 --> 08:14.360
One more.

08:14.360 --> 08:18.400
So seven images should be fine for this demo to not take too much time.

08:18.400 --> 08:25.000
And now for the not me, I'm going to take one photo of me not being there, basically.

08:25.000 --> 08:30.320
I'm going to be using the right hand to hide myself so I can put this one in front of my

08:30.320 --> 08:31.320
eyes.

08:31.320 --> 08:37.360
Turn it upside down, diagonal, like more images.

08:37.360 --> 08:39.240
The other hand as well.

08:39.240 --> 08:42.240
Like that.

08:42.240 --> 08:44.520
So that should be enough for now.

08:44.520 --> 08:48.360
And once that is done, we can just hit the train button to train this model.

08:48.360 --> 08:54.960
It will be built on the local machine without sending anywhere.

08:54.960 --> 09:03.400
This was faster last night.

09:03.400 --> 09:06.960
Seems like the time that we saved from the loading the project, we lost it here.

09:06.960 --> 09:08.960
So now it's training.

09:08.960 --> 09:13.040
Yeah, it's my laptop.

09:13.040 --> 09:16.920
So this is our React app that has been open source and the only internet required is to

09:16.920 --> 09:19.240
just get the initial web page later on.

09:19.240 --> 09:21.160
We can just disconnect and it will work perfectly.

09:21.160 --> 09:24.160
It's just offline training.

09:24.160 --> 09:27.960
If you really want to be fully offline, you can just launch the React app locally and

09:27.960 --> 09:29.980
it will work.

09:29.980 --> 09:33.520
So now the model is built and to test it, I'm going to be looking at the camera as I

09:33.520 --> 09:34.840
did before.

09:34.840 --> 09:41.920
I captured the image and I can see that there is a 99.42 confidence that I'm looking at

09:41.920 --> 09:43.040
the camera.

09:43.040 --> 09:50.640
If I take a few images of myself hiding from it, there is a 99.33 confidence that I'm not

09:50.640 --> 09:51.960
looking at the baby.

09:51.960 --> 09:57.960
So once we have validated the model properly, we can export it to the app.

09:57.960 --> 10:04.000
And we will get this model.mdl file for that inventor.

10:04.000 --> 10:08.840
So let's go to the presentation.

10:08.840 --> 10:13.200
And once we have the model, it's time to code the app using blocks.

10:13.200 --> 10:15.080
I'm not going to go through the slides anymore.

10:15.080 --> 10:19.560
For people at home, if you have internet problems or the streaming is down, feel free to follow

10:19.560 --> 10:20.560
the slides.

10:20.560 --> 10:22.120
It's a step-by-step guide.

10:22.120 --> 10:26.800
But for here, I'm going to be showing the tutorial live.

10:26.800 --> 10:30.200
So let's go back to the project that we just loaded.

10:30.200 --> 10:34.520
And right here, we can see a quick description of the project of what we are going to do.

10:34.520 --> 10:39.640
I set up a computer of how to connect to the MIT instance to the app.

10:39.640 --> 10:44.560
I'm going to show that at the end of the presentation.

10:44.560 --> 10:47.800
And we have here the pickup example.

10:47.800 --> 10:49.800
This is the final result.

10:49.800 --> 10:57.640
This is a link one of the MIT curriculum developers that made the original tutorial.

10:57.640 --> 11:01.000
And yeah, basically, it says that we will be using the Personally Much Classified extension

11:01.000 --> 11:05.280
that was developed by that research of MIT.

11:05.280 --> 11:08.520
And the first step is loading, turning the model.

11:08.520 --> 11:10.160
And then we have to upload the model.

11:10.160 --> 11:14.660
To upload the model, we go to this section over here on the media.

11:14.660 --> 11:17.640
We select the Just Uploaded Model file.

11:17.640 --> 11:21.120
It should be here.

11:21.120 --> 11:22.600
And now it's uploaded.

11:22.600 --> 11:25.680
It's in the asset file of the app.

11:25.680 --> 11:35.520
And we can just change the model of the Personally Much Classifier to the now loaded new model.

11:35.520 --> 11:40.480
And we have just loaded the model properly to give an overview of how the app is going

11:40.480 --> 11:41.480
to look.

11:41.480 --> 11:45.680
There is going to be this status label that will tell the user when the app is ready to

11:45.680 --> 11:46.680
work.

11:46.680 --> 11:48.080
And it will be loading because it's the initial state.

11:48.080 --> 11:50.280
It will let them go to the ready state.

11:50.280 --> 11:53.640
And it will just identify the faces.

11:53.640 --> 11:59.400
We have these two bars that will be showing which percentage of confidence that we are

11:59.400 --> 12:01.480
looking at the baby or not.

12:01.480 --> 12:06.640
This is going to be the live image from the camera that I just showed before.

12:06.640 --> 12:10.760
And these are the interaction buttons to start the classification, to toggle the camera from

12:10.760 --> 12:12.280
the front to the back camera.

12:12.280 --> 12:18.560
And we have here the happy baby in this case.

12:18.560 --> 12:21.160
So a pleasant attorney model.

12:21.160 --> 12:24.600
This is the sequence of events that I was just talking about.

12:24.600 --> 12:25.600
First we start the app.

12:25.600 --> 12:29.520
The app will show to ready as soon as the classifier is ready to start working with

12:29.520 --> 12:30.520
the app.

12:30.520 --> 12:32.160
The user will press the start button.

12:32.160 --> 12:37.820
And then the Personally Much Classifier extension will keep classifying the live stream video

12:37.820 --> 12:40.000
from the camera continuously.

12:40.000 --> 12:46.240
And once they have identified the result, if there is a higher confidence that the me

12:46.240 --> 12:49.440
model is working, we will show the smiley face.

12:49.440 --> 12:54.360
Otherwise the baby will just start crying.

12:54.360 --> 12:59.800
So in this template, there is already a set of blocks that are available to speed up the

12:59.800 --> 13:01.680
process.

13:01.680 --> 13:06.000
And we can go here that we see the one Personally Much Classifier error.

13:06.000 --> 13:10.440
So that means that if for any reason the Personally Much Classifier shows an error, maybe because

13:10.440 --> 13:14.420
there are some missing things on the phone or whatever, we will set the status label

13:14.420 --> 13:19.120
text to the actual error that we returned from the image classifier.

13:19.120 --> 13:24.200
Once the image classifier is ready, we will enable the start button as well as the toggle

13:24.200 --> 13:25.560
camera button.

13:25.560 --> 13:30.200
We will set the status label text to be ready so the user knows that they can start using

13:30.200 --> 13:31.240
the app.

13:31.240 --> 13:36.800
And we will set the text boxes of each classification group to the previously defined labels me

13:36.800 --> 13:39.680
and not me in this case.

13:39.680 --> 13:43.560
If the user presses the toggle camera button, we will be changing from the front to the

13:43.560 --> 13:46.680
back camera just every time that they press.

13:46.680 --> 13:52.640
So we can use the front selfie camera or the back normal camera.

13:52.640 --> 13:57.840
And once the user presses the start button, if the Personally Much Classifier is already

13:57.840 --> 14:02.960
classifying an image, we will just stop it and we will show the start button with the

14:02.960 --> 14:03.960
start text.

14:03.960 --> 14:08.880
Otherwise, we have to start a classification, so to do so, we just invoke the start continuous

14:08.880 --> 14:13.760
classification method and we change the text to stop because we will be changing the button

14:13.760 --> 14:16.260
interactions.

14:16.260 --> 14:21.360
And that's the quick overview of the code that is already available in the app.

14:21.360 --> 14:25.560
So how does the image classification work in MIT Prementor?

14:25.560 --> 14:31.880
Well we have this big block that is the when Personally Much Classifier has received a classification

14:31.880 --> 14:35.880
succeeded, we will receive this result variable.

14:35.880 --> 14:41.360
This result variable is a dictionary that just gives a little high level overview of

14:41.360 --> 14:42.640
what is a dictionary.

14:42.640 --> 14:45.680
It's a key value list of elements.

14:45.680 --> 14:51.240
So if we have two different groups, me and not me, we will receive me equals a specific

14:51.240 --> 14:53.480
value, not me equals a specific value.

14:53.480 --> 14:57.680
If we have three groups, we have one, two, and three that they each equal to a specific

14:57.680 --> 14:59.160
values.

14:59.160 --> 15:01.440
This is a little example of how it looks like.

15:01.440 --> 15:06.040
So we have key father equals this value, key mother equals this value, then equals this

15:06.040 --> 15:08.120
value, etc.

15:08.120 --> 15:12.280
For the image classifier, a specific example, we will have something like this.

15:12.280 --> 15:20.600
We have me with this value and not me with this other specific value.

15:20.600 --> 15:26.360
So how we can retrieve a value in the dictionaries area, in the dictionaries block area, we have

15:26.360 --> 15:28.800
get value for a specific key.

15:28.800 --> 15:31.000
And we will be doing something similar to this.

15:31.000 --> 15:35.600
So we have the original dictionary here, we are building it in this area, make a dictionary

15:35.600 --> 15:40.600
me and not me, and we will be getting the value of the group that we want to use right

15:40.600 --> 15:41.600
now.

15:41.600 --> 15:44.920
In this case it's the me example, if we want to take the not me, we just have to change

15:44.920 --> 15:47.140
this label to not me.

15:47.140 --> 15:52.840
And if we are using the wrong model because the groups are not the same, we just return

15:52.840 --> 15:57.360
a zero because we cannot classify that group.

15:57.360 --> 16:00.360
So let's get into it.

16:00.360 --> 16:05.000
By default, the tutorial will provide this block that is some variables, some me confidence

16:05.000 --> 16:08.240
level, and we have to complete them using this block.

16:08.240 --> 16:14.240
So to do so, we will take the get for key in dictionary block, we join it to the me confidence

16:14.240 --> 16:23.320
block, we remove, nice, get value for the key, and we will take from the text blocks

16:23.320 --> 16:28.520
an empty text block to attach it right here, and we can type me.

16:28.520 --> 16:34.440
So we can get the me group into the me confidence variable, the dictionary is the result, we

16:34.440 --> 16:42.320
can just attach it here, and if not found, we will just return an empty zero value.

16:42.320 --> 16:46.840
And for the not me confidence, it's basically the same, so we can copy paste the blocks,

16:46.840 --> 16:53.840
we attach them to the not me confidence area, and we just have to prepend a not in front

16:53.840 --> 16:55.080
of the me.

16:55.080 --> 17:00.200
And now we have just defined that me confidence variable that can be accessed like that will

17:00.200 --> 17:05.800
have the percentage of confidence that we are looking at the baby, and the not me confidence

17:05.800 --> 17:11.720
is the opposite, it's how confident we are that we are not looking at the baby.

17:11.720 --> 17:17.920
The next step, the interaction variables, and now we just have to recap what do we have

17:17.920 --> 17:19.600
to do in the app.

17:19.600 --> 17:25.680
So in the app, we have to first update these labels here with the percentage, and we have

17:25.680 --> 17:32.160
to update these color bars with the correct confidence levels.

17:32.160 --> 17:37.360
We can do that by going to these components, to these two horizontal arrangements, and

17:37.360 --> 17:42.520
we have percentage one, bar graph one, percentage two, and bar graph two.

17:42.520 --> 17:47.240
Percentage one, we can update the text to the percentage that we are showing.

17:47.240 --> 17:50.240
One second, there it is.

17:50.240 --> 17:55.080
We will be, so the value that we return from the dictionary goes from zero to one, but

17:55.080 --> 17:58.520
we want to return a percentage which goes from zero to 100.

17:58.520 --> 18:04.560
So we will take this me confidence value, and we will multiply it by 100, so we can

18:04.560 --> 18:07.720
get the zero to 100 range.

18:07.720 --> 18:17.920
We just join it right here with the math number, and we multiply it by 100.

18:17.920 --> 18:20.320
But we will be missing the percentage sign.

18:20.320 --> 18:24.760
To get the percentage sign, we can use the text blocks with the join block, so we can

18:24.760 --> 18:34.760
join two texts together, and we can just create a new percentage symbol like this, using the

18:34.760 --> 18:36.720
percentage symbol.

18:36.720 --> 18:39.180
And this is for the percentage labels.

18:39.180 --> 18:45.760
For the bar graph labels, we will be pledging with the length of the actual bar graph.

18:45.760 --> 18:52.520
To do so, we have the width percent block that can modify the width according to a percentage,

18:52.520 --> 18:55.780
and we already have defined the percentage right here, so we can just copy paste these

18:55.780 --> 18:59.560
blocks and attach them to the width percent.

18:59.560 --> 19:01.060
And this is for the me group.

19:01.060 --> 19:07.200
For the not me group, we can copy paste the percentage one, which changes to percentage

19:07.200 --> 19:13.120
two, and we change the me confidence value to the not me confidence value.

19:13.120 --> 19:21.160
And for the bar graph, it's going to be bar graph two right here, and me confidence changes

19:21.160 --> 19:23.800
to not me confidence.

19:23.800 --> 19:29.560
And with that, we already have all the sequence of events for the labels updates.

19:29.560 --> 19:37.200
You can just go to the next step and confirm that we have defined it correctly, which is

19:37.200 --> 19:39.640
the same result.

19:39.640 --> 19:47.400
The next step is the fancy image change that if we think that we are looking at the baby,

19:47.400 --> 19:51.560
we will show the happy face, otherwise we just show the crying face.

19:51.560 --> 19:57.440
We will be using the if then logic, so go to the control blocks, and we just take the

19:57.440 --> 19:59.880
if then otherwise block.

19:59.880 --> 20:04.960
We append it here, and what we will do is we will be comparing the me confidence value

20:04.960 --> 20:09.800
to the not me confidence value to know if we are looking at the baby or not.

20:09.800 --> 20:17.160
We go to the math blocks, we pick this comparator block, attach them to the if statement, and

20:17.160 --> 20:22.600
we are going to be changing the comparison to higher or equal because we don't worry

20:22.600 --> 20:26.840
about the equal in this case, we just want the higher or equal.

20:26.840 --> 20:32.280
We take again the me confidence variable, we compare it here, and we take the not me

20:32.280 --> 20:36.720
confidence value, and we compare it right here.

20:36.720 --> 20:42.600
Then we will be updating the background of the app, which is available in the screen

20:42.600 --> 20:43.600
one.

20:43.600 --> 20:49.640
We take the background color block, we attach it here, and the tutorial already provides

20:49.640 --> 20:56.120
the example colors, so I'm just going to be dragging this right below so I can have them

20:56.120 --> 20:58.800
more easily accessible.

20:58.800 --> 20:59.800
Right here.

20:59.800 --> 21:01.440
And I can just join it here.

21:01.440 --> 21:06.320
And for the baby images, we have two images available here, happy baby and sad baby.

21:06.320 --> 21:10.360
So if we think that we are looking at the baby, we show the happy baby, so we use the

21:10.360 --> 21:12.400
visible block.

21:12.400 --> 21:18.120
And if not, we just hide, sorry, if we think that we are looking at the baby, we hide the

21:18.120 --> 21:19.840
sad baby face.

21:19.840 --> 21:25.800
We go to the logic blocks, we take the true, so we can set to true to visible, we can set

21:25.800 --> 21:30.600
visible to true, and we set visible to false for the sad baby, like that.

21:30.600 --> 21:32.420
And we just join it.

21:32.420 --> 21:37.120
For the case of me confidence being higher than not me confidence, for the opposite case,

21:37.120 --> 21:45.800
when we are not looking at the baby, we just change the background color to this pink color.

21:45.800 --> 21:50.840
We hide the happy baby face, like that.

21:50.840 --> 21:57.000
And we show the sad baby face, just like this.

21:57.000 --> 22:01.300
And now the app is finished.

22:01.300 --> 22:05.600
So here we can just check the final code, which is exactly the same as we have right

22:05.600 --> 22:06.600
here.

22:06.600 --> 22:10.920
There are other possibilities, like we can just implement a classifier using a different

22:10.920 --> 22:12.160
person.

22:12.160 --> 22:16.280
But to show how this works, we can use the MIT company app that is available on the Play

22:16.280 --> 22:17.280
Store.

22:17.280 --> 22:19.320
Let me just show my phone again.

22:19.320 --> 22:21.200
Here it is.

22:21.200 --> 22:26.800
So you can just go to the Play Store and go to MIT App Inventor, search MIT App Inventor,

22:26.800 --> 22:28.760
and you have right here the company.

22:28.760 --> 22:32.160
You can open it.

22:32.160 --> 22:33.280
You can just ignore this warning.

22:33.280 --> 22:37.640
It works without Wi-Fi, continue without Wi-Fi.

22:37.640 --> 22:41.840
And over here you can connect the AI company.

22:41.840 --> 22:46.280
And now can scan the QR code like this.

22:46.280 --> 22:52.760
Sorry, it just disappeared.

22:52.760 --> 22:55.040
It takes a few seconds to connect.

22:55.040 --> 22:58.080
Let's see if it was faster than tonight.

22:58.080 --> 23:05.240
Now it's loading the personal classifier extension into my phone.

23:05.240 --> 23:06.240
This works with Wi-Fi.

23:06.240 --> 23:07.960
It doesn't have to be connected.

23:07.960 --> 23:11.640
It's just connected because I'm just mirroring the screen through that cable.

23:11.640 --> 23:14.160
And I see here the layout of the app.

23:14.160 --> 23:17.000
I can see that it shows ready.

23:17.000 --> 23:20.240
So I can just toggle the camera to be the front one.

23:20.240 --> 23:21.720
And I can look at the camera.

23:21.720 --> 23:24.560
And I'm just going to restart.

23:24.560 --> 23:27.560
And there is a higher confidence that I'm looking at the baby if I just put a hand in

23:27.560 --> 23:28.560
front of it.

23:28.560 --> 23:31.440
It's just crying.

23:30.800 --> 23:31.800
Elementor.

23:31.440 --> 23:33.520
And yeah, that's it.

23:31.800 --> 23:55.240
You can just build any kind of classifier, for example, to classify trees, flowers, to

23:33.520 --> 23:40.560
Later, if you want to build any other apps, you can export it to APK files.

23:40.560 --> 23:44.520
So you can start it on your phone or to Android app bundles if you want to distribute it through

23:44.520 --> 23:45.520
Play Store.

23:45.520 --> 23:55.240
But yeah, this is just a very high-level introduction to artificial intelligence in App

23:55.240 --> 23:56.800
classify even people.

23:56.800 --> 24:01.080
For example, for a faculty, if you want to build an app that recognizes people in your

24:01.080 --> 24:07.600
class as a game, you can just use App Inventor and build any kind of app.

24:07.600 --> 24:08.600
Thank you so much.

24:08.600 --> 24:20.960
And I hope that it was useful for everybody.

24:20.960 --> 24:21.960
Any questions?

24:21.960 --> 24:26.960
Do you mentor a tech-novation team?

24:26.960 --> 24:27.960
No.

24:27.960 --> 24:28.960
You have a serious advantage.

24:28.960 --> 24:29.960
Sorry.

24:29.960 --> 24:30.960
I'm a software engineer.

24:30.960 --> 24:32.560
I just contributed to App Inventor.

24:32.560 --> 24:36.640
I started building apps and then I transitioned to open source.

24:36.640 --> 24:39.480
I participated in Google Summer of Code.

24:39.480 --> 24:43.840
Like this option to export the standard app bundles was my project in 2020 for Google Summer

24:43.840 --> 24:44.840
of Code.

24:44.840 --> 24:52.640
But yeah, more technical than actually teaching to kids.

24:52.640 --> 24:53.640
Any other questions?

24:53.640 --> 25:01.000
What's your experience with the relation between the number of pictures you have to submit

25:01.000 --> 25:03.880
to your classifier and your accuracy?

25:03.880 --> 25:05.080
That's a very good question.

25:05.080 --> 25:07.040
So for the linear example...

25:07.040 --> 25:09.000
Maybe you can repeat this for the tempo.

25:09.000 --> 25:10.000
Oh, yeah, sure.

25:10.000 --> 25:14.160
So he was asking, what's the experience with the amount of images that we are going to

25:14.160 --> 25:15.760
be using for the classifier?

25:15.760 --> 25:18.160
So I haven't really tested it right now.

25:18.160 --> 25:21.840
But we have seen that if we go higher than 10 images for each class, for each group of

25:21.840 --> 25:24.160
images, we'll have really good results.

25:24.160 --> 25:28.560
In this case, because I was just running very fast and using just a few number of images,

25:28.560 --> 25:33.040
you can see that the confidence levels were a little bit like 80-20.

25:33.040 --> 25:37.000
But if we provide more than 10 images for each class, we should be able to get around

25:37.000 --> 25:43.440
over 90-95% of confidence for each number.

25:43.440 --> 25:45.720
I'm not sure if there are any questions from the chat.

25:45.720 --> 25:46.720
Let me just check.

25:46.720 --> 25:47.720
Yeah.

25:47.720 --> 25:48.720
What do you capture?

25:48.720 --> 25:49.720
Was that just for your face or did the learning...

25:49.720 --> 25:50.720
What was learned, is it just like eyes and stuff?

25:50.720 --> 26:05.720
So can I go out there and have the same results?

26:05.720 --> 26:06.720
It recognizes...

26:06.720 --> 26:10.840
It depends on what you are training because in this case, we are just providing a very

26:10.840 --> 26:12.680
specific gestures.

26:12.680 --> 26:17.640
That's training my face like any face looking at the camera or a hand in front of the face.

26:17.640 --> 26:27.680
By default, the model that is available, that is here, this model is trained by Salim, is

26:27.680 --> 26:30.160
the example guide that is at the beginning of the tutorial.

26:30.160 --> 26:32.320
And I just tested it last night.

26:32.320 --> 26:36.160
And it worked with me because it recognizes the gestures, not the faces.

26:36.160 --> 26:42.120
If instead we train recognizing people, we will all be looking in the same way as the

26:42.120 --> 26:43.120
camera.

26:43.120 --> 26:54.480
So it will just go for a specific facial features.

26:54.480 --> 27:04.640
In this case, it will work.

27:04.640 --> 27:07.400
You can just try if you want.

27:07.400 --> 27:08.400
We can try.

27:08.400 --> 27:11.400
It should work.

27:11.400 --> 27:12.400
It should work.

27:12.400 --> 27:13.400
It should work.

27:13.400 --> 27:14.400
It should work.

27:14.400 --> 27:15.400
Can you...

27:15.400 --> 27:18.440
It is going to be a little bit tough.

27:18.440 --> 27:23.920
But Mark, can you just try with Mark, for example?

27:23.920 --> 27:24.920
Toggle camera.

27:24.920 --> 27:25.920
You can try it yourself.

27:25.920 --> 27:26.920
You are different as well.

27:26.920 --> 27:27.920
Toggle camera.

27:27.920 --> 27:30.920
And just try it with...

27:30.920 --> 27:31.920
And start.

27:31.920 --> 27:32.920
Press start.

27:32.920 --> 27:35.080
It is a happy face.

27:35.080 --> 27:37.080
So if you put a hand in front, it is a sad face.

27:37.080 --> 27:38.080
It is recognizing the gestures.

27:38.080 --> 27:39.080
Very cool.

27:39.080 --> 27:45.400
So can you also train it to recognize specific people?

27:45.400 --> 27:46.400
Yeah, it can be trained.

27:46.400 --> 27:50.440
But in this case, because the higher difference was the hand, it is just looking for the hand

27:50.440 --> 27:51.440
and model.

27:51.440 --> 27:54.080
But if you do not show the hand, it will look for faces.

27:54.080 --> 28:01.600
Can you fit your own pencil film models into your app?

28:01.600 --> 28:07.160
It can be fit because you can just use this website and fit any kind of models.

28:07.160 --> 28:10.240
The only restriction is that it has to be an MLD file.

28:10.240 --> 28:13.560
But yeah, it can classify any model, basically.

28:13.560 --> 28:14.880
No problem.

28:14.880 --> 28:17.520
Any other questions?

28:17.520 --> 28:21.360
Well, I think we can leave it here.

28:21.360 --> 28:37.720
Thank you so much.
