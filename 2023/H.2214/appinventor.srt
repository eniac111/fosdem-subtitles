1
0:00:00.000 --> 0:00:08.000
Okay, I think we can start already.

2
0:00:08.000 --> 0:00:10.880
Hi everybody, I'm Diego Barreiro.

3
0:00:10.880 --> 0:00:15.160
I'm one of the open source contributors to the MIT App Inventor project and today I'm

4
0:00:15.160 --> 0:00:19.160
going to be talking about App Inventor and how we can introduce artificial intelligence

5
0:00:19.160 --> 0:00:21.900
to kids using this platform.

6
0:00:21.900 --> 0:00:25.440
But before getting into it, I would like to introduce myself a little bit more.

7
0:00:25.440 --> 0:00:31.720
I started coding with App Inventor when I was 14 years old, it was 2013 at that time,

8
0:00:31.720 --> 0:00:35.840
and I basically wanted to build an app, I didn't know anything about coding, and a high

9
0:00:35.840 --> 0:00:38.260
school teacher showed me this amazing platform.

10
0:00:38.260 --> 0:00:42.440
So I just spent the next couple of years building apps with App Inventor and eventually I switched

11
0:00:42.440 --> 0:00:47.920
it to Java coding and I was able to contribute to the project later on as I'm doing right

12
0:00:47.920 --> 0:00:49.080
now.

13
0:00:49.080 --> 0:00:51.040
So what is MIT App Inventor?

14
0:00:51.040 --> 0:00:56.880
MIT Inventor is an online platform that enables anybody to build any kind of application without

15
0:00:56.880 --> 0:01:01.040
having to learn any programming language like Java or coding that is nowadays the most popular

16
0:01:01.040 --> 0:01:02.040
ones.

17
0:01:02.040 --> 0:01:07.240
This is the interface and it has the mock phone on the center and on the left side we

18
0:01:07.240 --> 0:01:10.880
can have the components, the elements that will make the app like buttons, labels, text

19
0:01:10.880 --> 0:01:16.040
boxes, text areas, like any kind of interaction that the user will have with the app.

20
0:01:16.040 --> 0:01:21.600
And then we can customize the properties like colors, text fonts, text sizes, whatever on

21
0:01:21.600 --> 0:01:26.720
this panel so we can make the app look as we wish for the final user.

22
0:01:26.720 --> 0:01:28.760
And how does the logic work?

23
0:01:28.760 --> 0:01:31.160
Well we, most of you may know about the scratch.

24
0:01:31.160 --> 0:01:35.340
App Inventor works somehow like a scratch using this block language.

25
0:01:35.340 --> 0:01:38.200
So let's say that we want to play a sound when the app opens.

26
0:01:38.200 --> 0:01:43.640
We will be using a block that says when the screen one has opened we want to play a specific

27
0:01:43.640 --> 0:01:45.600
sound later on.

28
0:01:45.600 --> 0:01:48.320
And that's how we can just make any kind of application.

29
0:01:48.320 --> 0:01:55.960
MIT Inventor allows existing Android developers and Android developers to introduce new components

30
0:01:55.960 --> 0:01:57.560
using extensions.

31
0:01:57.560 --> 0:02:01.400
And we will be using today one of those extensions that was developed by a research project

32
0:02:01.400 --> 0:02:09.360
at MIT that enables to classify images on different groups using artificial intelligence.

33
0:02:09.360 --> 0:02:14.640
And to give some numbers of App Inventor it was tested in 2008 as a Google project.

34
0:02:14.640 --> 0:02:18.840
And then a few years later it eventually was transferred to MIT.

35
0:02:18.840 --> 0:02:23.920
Right now it has gathered over 18 million users since it was created, since it was transferred

36
0:02:23.920 --> 0:02:28.560
to MIT with nearly 85 million apps that have been developed.

37
0:02:28.560 --> 0:02:33.080
And on a monthly basis we get roughly around 1 million users.

38
0:02:33.080 --> 0:02:38.240
And in terms of open search contributions we have seen 164 different contributors to

39
0:02:38.240 --> 0:02:43.120
the project on GitHub.

40
0:02:43.120 --> 0:02:46.080
So today I'm not going to be giving the classic talk.

41
0:02:46.080 --> 0:02:49.560
I'm going to be showing a tutorial and people in the audience and at home can just follow

42
0:02:49.560 --> 0:02:53.960
this tutorial by visiting the following link on how to build an app.

43
0:02:53.960 --> 0:02:57.680
And what we will be doing today is building the pickup app.

44
0:02:57.680 --> 0:03:02.280
Pickup is a game that is usually played with babies that when you see the baby that he

45
0:03:02.280 --> 0:03:06.280
loves and when you hide yourself it just cries.

46
0:03:06.280 --> 0:03:10.120
So to show the final result, this will be the final result.

47
0:03:10.120 --> 0:03:12.280
Let me just switch to my phone.

48
0:03:12.280 --> 0:03:14.240
I'm going to be mirroring this phone.

49
0:03:14.240 --> 0:03:19.600
So I open the final app and I'm going to start using the camera.

50
0:03:19.600 --> 0:03:21.240
I can see, yep, here I am.

51
0:03:21.240 --> 0:03:23.600
I can start and I'm looking at the baby that he's happy.

52
0:03:23.600 --> 0:03:27.480
If I hide myself it just cries.

53
0:03:27.480 --> 0:03:30.720
So let's get into it.

54
0:03:30.720 --> 0:03:33.640
So how we can use MIT App Inventor 2?

55
0:03:33.640 --> 0:03:38.480
The standard instance of App Inventor is hosted at ai2.appinventor.mit.edu, but that requires

56
0:03:38.480 --> 0:03:39.480
an account.

57
0:03:39.480 --> 0:03:45.320
MIT has created this specific instance, code.appinventor.mit, that allows you to create these projects

58
0:03:45.320 --> 0:03:48.240
without any existing account.

59
0:03:48.240 --> 0:03:54.280
We will be using an anonymous account so that that can be cleaned up later after finishing.

60
0:03:54.280 --> 0:03:56.920
And we will receive our code like the following one.

61
0:03:56.920 --> 0:04:00.120
Now this is blurred, but you will be able to see our code.

62
0:04:00.120 --> 0:04:03.360
And on the previous screen, if you want to recover the project later on, you can just

63
0:04:03.360 --> 0:04:08.160
paste the code that you previously get right here.

64
0:04:08.160 --> 0:04:11.800
And once that is done, you will be able to access an anonymous account, as you can see

65
0:04:11.800 --> 0:04:15.180
over here, and you can just start creating the app.

66
0:04:15.180 --> 0:04:19.320
So let's get into it.

67
0:04:19.320 --> 0:04:24.680
So we can visit firstm23.appinventor.mit.edu, and we can click on this link.

68
0:04:24.680 --> 0:04:29.360
This link is basically the code App Inventor instance, and we are loading a template project

69
0:04:29.360 --> 0:04:31.560
for the pickable project.

70
0:04:31.560 --> 0:04:35.140
So I'm going to click on this one.

71
0:04:35.140 --> 0:04:40.200
So I click on Continue without an account.

72
0:04:40.200 --> 0:04:44.040
And just wait a few seconds for the project to download from the repository, and here

73
0:04:44.040 --> 0:04:45.040
it is.

74
0:04:45.040 --> 0:04:48.000
That was faster than last night.

75
0:04:48.000 --> 0:04:52.520
I can see the code here, so I can just copy paste the code to access this instance later

76
0:04:52.520 --> 0:04:53.520
on.

77
0:04:53.520 --> 0:04:59.100
And as this is a tutorial, we can see on the left side that we are seeing a description

78
0:04:59.100 --> 0:05:03.080
of what we are trying to build with a detailed step-by-step guide.

79
0:05:03.080 --> 0:05:05.720
And this is the instance of the project.

80
0:05:05.720 --> 0:05:10.040
I can see the happy baby, and then the sad baby is hidden here.

81
0:05:10.040 --> 0:05:15.020
Okay, so let me just continue the presentation.

82
0:05:15.020 --> 0:05:17.000
The next step is turning the classifier.

83
0:05:17.000 --> 0:05:20.760
I'm not going to get too deep into the machine learning and how it works.

84
0:05:20.760 --> 0:05:25.280
I'm just going to be providing a very high-level overview of how this works.

85
0:05:25.280 --> 0:05:30.280
So we will be using an image classification system that consists in creating two groups

86
0:05:30.280 --> 0:05:31.280
of images.

87
0:05:31.280 --> 0:05:37.560
So we are creating one group of images that is the face of myself looking at the baby

88
0:05:37.560 --> 0:05:42.560
and another group of images that is me hiding from the baby so we can show the sad face.

89
0:05:42.560 --> 0:05:44.240
So how does this work?

90
0:05:44.240 --> 0:05:51.520
We can visit this website, classifier.appinventor.mit.edu to train this model.

91
0:05:51.520 --> 0:05:55.400
Just as a side note, this instance only needs internet to load once.

92
0:05:55.400 --> 0:06:00.360
To train the model, that all happens in your browser, so no servers are involved, no images

93
0:06:00.360 --> 0:06:02.960
are transferred outside your desktop.

94
0:06:02.960 --> 0:06:06.000
And this website is also open source, so you can just check.

95
0:06:06.000 --> 0:06:10.260
There is the link on the FOSDAM23 website.

96
0:06:10.260 --> 0:06:14.440
So we visit the website, and we start first creating the images.

97
0:06:14.440 --> 0:06:17.020
So in this case, we will be creating one image group for me.

98
0:06:17.020 --> 0:06:18.880
So I'm looking at the baby and not me.

99
0:06:18.880 --> 0:06:20.560
I'm hiding from the baby.

100
0:06:20.560 --> 0:06:24.480
If for example we are in a biology class and we want to classify trees, we will be creating

101
0:06:24.480 --> 0:06:27.340
one group of images for each kind of tree.

102
0:06:27.340 --> 0:06:30.340
So we can recognize them later on.

103
0:06:30.340 --> 0:06:35.720
Then we will turn on the camera to take a photo of myself, setting the group of images

104
0:06:35.720 --> 0:06:37.640
that I'm going to be saving this.

105
0:06:37.640 --> 0:06:40.880
So if I'm looking at the camera, it's going to be the not me group.

106
0:06:40.880 --> 0:06:44.640
If I'm looking at the camera, it's going to be the me group.

107
0:06:44.640 --> 0:06:46.840
This is the same for the not me.

108
0:06:46.840 --> 0:06:50.200
And once that is done, when we have a reasonable amount of images for each group, which will

109
0:06:50.200 --> 0:06:53.520
be around 5 or 10 images for each group, we can train the model.

110
0:06:53.520 --> 0:07:00.320
As again, this training happens on your computer, so no images transferred outside of your computer.

111
0:07:00.320 --> 0:07:04.600
We can then test the model with new images to make sure that we have properly trained

112
0:07:04.600 --> 0:07:07.560
the model and can identify ourselves.

113
0:07:07.560 --> 0:07:11.920
And once that is done, we can export the model and load it into Appingmentor.

114
0:07:11.920 --> 0:07:14.640
So I'm going to be doing that very quickly.

115
0:07:14.640 --> 0:07:18.720
Go to first and enter tree, Appingmentor.mit.edu.

116
0:07:18.720 --> 0:07:22.080
I open the classifier instance.

117
0:07:22.080 --> 0:07:25.200
It's going to ask for permission to use the webcam for each.

118
0:07:25.200 --> 0:07:26.200
There it is.

119
0:07:26.200 --> 0:07:27.580
I just accepted it for.

120
0:07:27.580 --> 0:07:30.560
So the first step is creating the labels.

121
0:07:30.560 --> 0:07:37.440
First the me label, enter, and next the not me label.

122
0:07:37.440 --> 0:07:39.720
And well, the light is quite hot.

123
0:07:39.720 --> 0:08:03.160
I think it was.

124
0:08:03.160 --> 0:08:11.720
Take a few more images of me looking to different places so I can train better the model.

125
0:08:11.720 --> 0:08:12.720
One more.

126
0:08:12.720 --> 0:08:14.360
One more.

127
0:08:14.360 --> 0:08:18.400
So seven images should be fine for this demo to not take too much time.

128
0:08:18.400 --> 0:08:25.000
And now for the not me, I'm going to take one photo of me not being there, basically.

129
0:08:25.000 --> 0:08:30.320
I'm going to be using the right hand to hide myself so I can put this one in front of my

130
0:08:30.320 --> 0:08:31.320
eyes.

131
0:08:31.320 --> 0:08:37.360
Turn it upside down, diagonal, like more images.

132
0:08:37.360 --> 0:08:39.240
The other hand as well.

133
0:08:39.240 --> 0:08:42.240
Like that.

134
0:08:42.240 --> 0:08:44.520
So that should be enough for now.

135
0:08:44.520 --> 0:08:48.360
And once that is done, we can just hit the train button to train this model.

136
0:08:48.360 --> 0:08:54.960
It will be built on the local machine without sending anywhere.

137
0:08:54.960 --> 0:09:03.400
This was faster last night.

138
0:09:03.400 --> 0:09:06.960
Seems like the time that we saved from the loading the project, we lost it here.

139
0:09:06.960 --> 0:09:08.960
So now it's training.

140
0:09:08.960 --> 0:09:13.040
Yeah, it's my laptop.

141
0:09:13.040 --> 0:09:16.920
So this is our React app that has been open source and the only internet required is to

142
0:09:16.920 --> 0:09:19.240
just get the initial web page later on.

143
0:09:19.240 --> 0:09:21.160
We can just disconnect and it will work perfectly.

144
0:09:21.160 --> 0:09:24.160
It's just offline training.

145
0:09:24.160 --> 0:09:27.960
If you really want to be fully offline, you can just launch the React app locally and

146
0:09:27.960 --> 0:09:29.980
it will work.

147
0:09:29.980 --> 0:09:33.520
So now the model is built and to test it, I'm going to be looking at the camera as I

148
0:09:33.520 --> 0:09:34.840
did before.

149
0:09:34.840 --> 0:09:41.920
I captured the image and I can see that there is a 99.42 confidence that I'm looking at

150
0:09:41.920 --> 0:09:43.040
the camera.

151
0:09:43.040 --> 0:09:50.640
If I take a few images of myself hiding from it, there is a 99.33 confidence that I'm not

152
0:09:50.640 --> 0:09:51.960
looking at the baby.

153
0:09:51.960 --> 0:09:57.960
So once we have validated the model properly, we can export it to the app.

154
0:09:57.960 --> 0:10:04.000
And we will get this model.mdl file for that inventor.

155
0:10:04.000 --> 0:10:08.840
So let's go to the presentation.

156
0:10:08.840 --> 0:10:13.200
And once we have the model, it's time to code the app using blocks.

157
0:10:13.200 --> 0:10:15.080
I'm not going to go through the slides anymore.

158
0:10:15.080 --> 0:10:19.560
For people at home, if you have internet problems or the streaming is down, feel free to follow

159
0:10:19.560 --> 0:10:20.560
the slides.

160
0:10:20.560 --> 0:10:22.120
It's a step-by-step guide.

161
0:10:22.120 --> 0:10:26.800
But for here, I'm going to be showing the tutorial live.

162
0:10:26.800 --> 0:10:30.200
So let's go back to the project that we just loaded.

163
0:10:30.200 --> 0:10:34.520
And right here, we can see a quick description of the project of what we are going to do.

164
0:10:34.520 --> 0:10:39.640
I set up a computer of how to connect to the MIT instance to the app.

165
0:10:39.640 --> 0:10:44.560
I'm going to show that at the end of the presentation.

166
0:10:44.560 --> 0:10:47.800
And we have here the pickup example.

167
0:10:47.800 --> 0:10:49.800
This is the final result.

168
0:10:49.800 --> 0:10:57.640
This is a link one of the MIT curriculum developers that made the original tutorial.

169
0:10:57.640 --> 0:11:01.000
And yeah, basically, it says that we will be using the Personally Much Classified extension

170
0:11:01.000 --> 0:11:05.280
that was developed by that research of MIT.

171
0:11:05.280 --> 0:11:08.520
And the first step is loading, turning the model.

172
0:11:08.520 --> 0:11:10.160
And then we have to upload the model.

173
0:11:10.160 --> 0:11:14.660
To upload the model, we go to this section over here on the media.

174
0:11:14.660 --> 0:11:17.640
We select the Just Uploaded Model file.

175
0:11:17.640 --> 0:11:21.120
It should be here.

176
0:11:21.120 --> 0:11:22.600
And now it's uploaded.

177
0:11:22.600 --> 0:11:25.680
It's in the asset file of the app.

178
0:11:25.680 --> 0:11:35.520
And we can just change the model of the Personally Much Classifier to the now loaded new model.

179
0:11:35.520 --> 0:11:40.480
And we have just loaded the model properly to give an overview of how the app is going

180
0:11:40.480 --> 0:11:41.480
to look.

181
0:11:41.480 --> 0:11:45.680
There is going to be this status label that will tell the user when the app is ready to

182
0:11:45.680 --> 0:11:46.680
work.

183
0:11:46.680 --> 0:11:48.080
And it will be loading because it's the initial state.

184
0:11:48.080 --> 0:11:50.280
It will let them go to the ready state.

185
0:11:50.280 --> 0:11:53.640
And it will just identify the faces.

186
0:11:53.640 --> 0:11:59.400
We have these two bars that will be showing which percentage of confidence that we are

187
0:11:59.400 --> 0:12:01.480
looking at the baby or not.

188
0:12:01.480 --> 0:12:06.640
This is going to be the live image from the camera that I just showed before.

189
0:12:06.640 --> 0:12:10.760
And these are the interaction buttons to start the classification, to toggle the camera from

190
0:12:10.760 --> 0:12:12.280
the front to the back camera.

191
0:12:12.280 --> 0:12:18.560
And we have here the happy baby in this case.

192
0:12:18.560 --> 0:12:21.160
So a pleasant attorney model.

193
0:12:21.160 --> 0:12:24.600
This is the sequence of events that I was just talking about.

194
0:12:24.600 --> 0:12:25.600
First we start the app.

195
0:12:25.600 --> 0:12:29.520
The app will show to ready as soon as the classifier is ready to start working with

196
0:12:29.520 --> 0:12:30.520
the app.

197
0:12:30.520 --> 0:12:32.160
The user will press the start button.

198
0:12:32.160 --> 0:12:37.820
And then the Personally Much Classifier extension will keep classifying the live stream video

199
0:12:37.820 --> 0:12:40.000
from the camera continuously.

200
0:12:40.000 --> 0:12:46.240
And once they have identified the result, if there is a higher confidence that the me

201
0:12:46.240 --> 0:12:49.440
model is working, we will show the smiley face.

202
0:12:49.440 --> 0:12:54.360
Otherwise the baby will just start crying.

203
0:12:54.360 --> 0:12:59.800
So in this template, there is already a set of blocks that are available to speed up the

204
0:12:59.800 --> 0:13:01.680
process.

205
0:13:01.680 --> 0:13:06.000
And we can go here that we see the one Personally Much Classifier error.

206
0:13:06.000 --> 0:13:10.440
So that means that if for any reason the Personally Much Classifier shows an error, maybe because

207
0:13:10.440 --> 0:13:14.420
there are some missing things on the phone or whatever, we will set the status label

208
0:13:14.420 --> 0:13:19.120
text to the actual error that we returned from the image classifier.

209
0:13:19.120 --> 0:13:24.200
Once the image classifier is ready, we will enable the start button as well as the toggle

210
0:13:24.200 --> 0:13:25.560
camera button.

211
0:13:25.560 --> 0:13:30.200
We will set the status label text to be ready so the user knows that they can start using

212
0:13:30.200 --> 0:13:31.240
the app.

213
0:13:31.240 --> 0:13:36.800
And we will set the text boxes of each classification group to the previously defined labels me

214
0:13:36.800 --> 0:13:39.680
and not me in this case.

215
0:13:39.680 --> 0:13:43.560
If the user presses the toggle camera button, we will be changing from the front to the

216
0:13:43.560 --> 0:13:46.680
back camera just every time that they press.

217
0:13:46.680 --> 0:13:52.640
So we can use the front selfie camera or the back normal camera.

218
0:13:52.640 --> 0:13:57.840
And once the user presses the start button, if the Personally Much Classifier is already

219
0:13:57.840 --> 0:14:02.960
classifying an image, we will just stop it and we will show the start button with the

220
0:14:02.960 --> 0:14:03.960
start text.

221
0:14:03.960 --> 0:14:08.880
Otherwise, we have to start a classification, so to do so, we just invoke the start continuous

222
0:14:08.880 --> 0:14:13.760
classification method and we change the text to stop because we will be changing the button

223
0:14:13.760 --> 0:14:16.260
interactions.

224
0:14:16.260 --> 0:14:21.360
And that's the quick overview of the code that is already available in the app.

225
0:14:21.360 --> 0:14:25.560
So how does the image classification work in MIT Prementor?

226
0:14:25.560 --> 0:14:31.880
Well we have this big block that is the when Personally Much Classifier has received a classification

227
0:14:31.880 --> 0:14:35.880
succeeded, we will receive this result variable.

228
0:14:35.880 --> 0:14:41.360
This result variable is a dictionary that just gives a little high level overview of

229
0:14:41.360 --> 0:14:42.640
what is a dictionary.

230
0:14:42.640 --> 0:14:45.680
It's a key value list of elements.

231
0:14:45.680 --> 0:14:51.240
So if we have two different groups, me and not me, we will receive me equals a specific

232
0:14:51.240 --> 0:14:53.480
value, not me equals a specific value.

233
0:14:53.480 --> 0:14:57.680
If we have three groups, we have one, two, and three that they each equal to a specific

234
0:14:57.680 --> 0:14:59.160
values.

235
0:14:59.160 --> 0:15:01.440
This is a little example of how it looks like.

236
0:15:01.440 --> 0:15:06.040
So we have key father equals this value, key mother equals this value, then equals this

237
0:15:06.040 --> 0:15:08.120
value, etc.

238
0:15:08.120 --> 0:15:12.280
For the image classifier, a specific example, we will have something like this.

239
0:15:12.280 --> 0:15:20.600
We have me with this value and not me with this other specific value.

240
0:15:20.600 --> 0:15:26.360
So how we can retrieve a value in the dictionaries area, in the dictionaries block area, we have

241
0:15:26.360 --> 0:15:28.800
get value for a specific key.

242
0:15:28.800 --> 0:15:31.000
And we will be doing something similar to this.

243
0:15:31.000 --> 0:15:35.600
So we have the original dictionary here, we are building it in this area, make a dictionary

244
0:15:35.600 --> 0:15:40.600
me and not me, and we will be getting the value of the group that we want to use right

245
0:15:40.600 --> 0:15:41.600
now.

246
0:15:41.600 --> 0:15:44.920
In this case it's the me example, if we want to take the not me, we just have to change

247
0:15:44.920 --> 0:15:47.140
this label to not me.

248
0:15:47.140 --> 0:15:52.840
And if we are using the wrong model because the groups are not the same, we just return

249
0:15:52.840 --> 0:15:57.360
a zero because we cannot classify that group.

250
0:15:57.360 --> 0:16:00.360
So let's get into it.

251
0:16:00.360 --> 0:16:05.000
By default, the tutorial will provide this block that is some variables, some me confidence

252
0:16:05.000 --> 0:16:08.240
level, and we have to complete them using this block.

253
0:16:08.240 --> 0:16:14.240
So to do so, we will take the get for key in dictionary block, we join it to the me confidence

254
0:16:14.240 --> 0:16:23.320
block, we remove, nice, get value for the key, and we will take from the text blocks

255
0:16:23.320 --> 0:16:28.520
an empty text block to attach it right here, and we can type me.

256
0:16:28.520 --> 0:16:34.440
So we can get the me group into the me confidence variable, the dictionary is the result, we

257
0:16:34.440 --> 0:16:42.320
can just attach it here, and if not found, we will just return an empty zero value.

258
0:16:42.320 --> 0:16:46.840
And for the not me confidence, it's basically the same, so we can copy paste the blocks,

259
0:16:46.840 --> 0:16:53.840
we attach them to the not me confidence area, and we just have to prepend a not in front

260
0:16:53.840 --> 0:16:55.080
of the me.

261
0:16:55.080 --> 0:17:00.200
And now we have just defined that me confidence variable that can be accessed like that will

262
0:17:00.200 --> 0:17:05.800
have the percentage of confidence that we are looking at the baby, and the not me confidence

263
0:17:05.800 --> 0:17:11.720
is the opposite, it's how confident we are that we are not looking at the baby.

264
0:17:11.720 --> 0:17:17.920
The next step, the interaction variables, and now we just have to recap what do we have

265
0:17:17.920 --> 0:17:19.600
to do in the app.

266
0:17:19.600 --> 0:17:25.680
So in the app, we have to first update these labels here with the percentage, and we have

267
0:17:25.680 --> 0:17:32.160
to update these color bars with the correct confidence levels.

268
0:17:32.160 --> 0:17:37.360
We can do that by going to these components, to these two horizontal arrangements, and

269
0:17:37.360 --> 0:17:42.520
we have percentage one, bar graph one, percentage two, and bar graph two.

270
0:17:42.520 --> 0:17:47.240
Percentage one, we can update the text to the percentage that we are showing.

271
0:17:47.240 --> 0:17:50.240
One second, there it is.

272
0:17:50.240 --> 0:17:55.080
We will be, so the value that we return from the dictionary goes from zero to one, but

273
0:17:55.080 --> 0:17:58.520
we want to return a percentage which goes from zero to 100.

274
0:17:58.520 --> 0:18:04.560
So we will take this me confidence value, and we will multiply it by 100, so we can

275
0:18:04.560 --> 0:18:07.720
get the zero to 100 range.

276
0:18:07.720 --> 0:18:17.920
We just join it right here with the math number, and we multiply it by 100.

277
0:18:17.920 --> 0:18:20.320
But we will be missing the percentage sign.

278
0:18:20.320 --> 0:18:24.760
To get the percentage sign, we can use the text blocks with the join block, so we can

279
0:18:24.760 --> 0:18:34.760
join two texts together, and we can just create a new percentage symbol like this, using the

280
0:18:34.760 --> 0:18:36.720
percentage symbol.

281
0:18:36.720 --> 0:18:39.180
And this is for the percentage labels.

282
0:18:39.180 --> 0:18:45.760
For the bar graph labels, we will be pledging with the length of the actual bar graph.

283
0:18:45.760 --> 0:18:52.520
To do so, we have the width percent block that can modify the width according to a percentage,

284
0:18:52.520 --> 0:18:55.780
and we already have defined the percentage right here, so we can just copy paste these

285
0:18:55.780 --> 0:18:59.560
blocks and attach them to the width percent.

286
0:18:59.560 --> 0:19:01.060
And this is for the me group.

287
0:19:01.060 --> 0:19:07.200
For the not me group, we can copy paste the percentage one, which changes to percentage

288
0:19:07.200 --> 0:19:13.120
two, and we change the me confidence value to the not me confidence value.

289
0:19:13.120 --> 0:19:21.160
And for the bar graph, it's going to be bar graph two right here, and me confidence changes

290
0:19:21.160 --> 0:19:23.800
to not me confidence.

291
0:19:23.800 --> 0:19:29.560
And with that, we already have all the sequence of events for the labels updates.

292
0:19:29.560 --> 0:19:37.200
You can just go to the next step and confirm that we have defined it correctly, which is

293
0:19:37.200 --> 0:19:39.640
the same result.

294
0:19:39.640 --> 0:19:47.400
The next step is the fancy image change that if we think that we are looking at the baby,

295
0:19:47.400 --> 0:19:51.560
we will show the happy face, otherwise we just show the crying face.

296
0:19:51.560 --> 0:19:57.440
We will be using the if then logic, so go to the control blocks, and we just take the

297
0:19:57.440 --> 0:19:59.880
if then otherwise block.

298
0:19:59.880 --> 0:20:04.960
We append it here, and what we will do is we will be comparing the me confidence value

299
0:20:04.960 --> 0:20:09.800
to the not me confidence value to know if we are looking at the baby or not.

300
0:20:09.800 --> 0:20:17.160
We go to the math blocks, we pick this comparator block, attach them to the if statement, and

301
0:20:17.160 --> 0:20:22.600
we are going to be changing the comparison to higher or equal because we don't worry

302
0:20:22.600 --> 0:20:26.840
about the equal in this case, we just want the higher or equal.

303
0:20:26.840 --> 0:20:32.280
We take again the me confidence variable, we compare it here, and we take the not me

304
0:20:32.280 --> 0:20:36.720
confidence value, and we compare it right here.

305
0:20:36.720 --> 0:20:42.600
Then we will be updating the background of the app, which is available in the screen

306
0:20:42.600 --> 0:20:43.600
one.

307
0:20:43.600 --> 0:20:49.640
We take the background color block, we attach it here, and the tutorial already provides

308
0:20:49.640 --> 0:20:56.120
the example colors, so I'm just going to be dragging this right below so I can have them

309
0:20:56.120 --> 0:20:58.800
more easily accessible.

310
0:20:58.800 --> 0:20:59.800
Right here.

311
0:20:59.800 --> 0:21:01.440
And I can just join it here.

312
0:21:01.440 --> 0:21:06.320
And for the baby images, we have two images available here, happy baby and sad baby.

313
0:21:06.320 --> 0:21:10.360
So if we think that we are looking at the baby, we show the happy baby, so we use the

314
0:21:10.360 --> 0:21:12.400
visible block.

315
0:21:12.400 --> 0:21:18.120
And if not, we just hide, sorry, if we think that we are looking at the baby, we hide the

316
0:21:18.120 --> 0:21:19.840
sad baby face.

317
0:21:19.840 --> 0:21:25.800
We go to the logic blocks, we take the true, so we can set to true to visible, we can set

318
0:21:25.800 --> 0:21:30.600
visible to true, and we set visible to false for the sad baby, like that.

319
0:21:30.600 --> 0:21:32.420
And we just join it.

320
0:21:32.420 --> 0:21:37.120
For the case of me confidence being higher than not me confidence, for the opposite case,

321
0:21:37.120 --> 0:21:45.800
when we are not looking at the baby, we just change the background color to this pink color.

322
0:21:45.800 --> 0:21:50.840
We hide the happy baby face, like that.

323
0:21:50.840 --> 0:21:57.000
And we show the sad baby face, just like this.

324
0:21:57.000 --> 0:22:01.300
And now the app is finished.

325
0:22:01.300 --> 0:22:05.600
So here we can just check the final code, which is exactly the same as we have right

326
0:22:05.600 --> 0:22:06.600
here.

327
0:22:06.600 --> 0:22:10.920
There are other possibilities, like we can just implement a classifier using a different

328
0:22:10.920 --> 0:22:12.160
person.

329
0:22:12.160 --> 0:22:16.280
But to show how this works, we can use the MIT company app that is available on the Play

330
0:22:16.280 --> 0:22:17.280
Store.

331
0:22:17.280 --> 0:22:19.320
Let me just show my phone again.

332
0:22:19.320 --> 0:22:21.200
Here it is.

333
0:22:21.200 --> 0:22:26.800
So you can just go to the Play Store and go to MIT App Inventor, search MIT App Inventor,

334
0:22:26.800 --> 0:22:28.760
and you have right here the company.

335
0:22:28.760 --> 0:22:32.160
You can open it.

336
0:22:32.160 --> 0:22:33.280
You can just ignore this warning.

337
0:22:33.280 --> 0:22:37.640
It works without Wi-Fi, continue without Wi-Fi.

338
0:22:37.640 --> 0:22:41.840
And over here you can connect the AI company.

339
0:22:41.840 --> 0:22:46.280
And now can scan the QR code like this.

340
0:22:46.280 --> 0:22:52.760
Sorry, it just disappeared.

341
0:22:52.760 --> 0:22:55.040
It takes a few seconds to connect.

342
0:22:55.040 --> 0:22:58.080
Let's see if it was faster than tonight.

343
0:22:58.080 --> 0:23:05.240
Now it's loading the personal classifier extension into my phone.

344
0:23:05.240 --> 0:23:06.240
This works with Wi-Fi.

345
0:23:06.240 --> 0:23:07.960
It doesn't have to be connected.

346
0:23:07.960 --> 0:23:11.640
It's just connected because I'm just mirroring the screen through that cable.

347
0:23:11.640 --> 0:23:14.160
And I see here the layout of the app.

348
0:23:14.160 --> 0:23:17.000
I can see that it shows ready.

349
0:23:17.000 --> 0:23:20.240
So I can just toggle the camera to be the front one.

350
0:23:20.240 --> 0:23:21.720
And I can look at the camera.

351
0:23:21.720 --> 0:23:24.560
And I'm just going to restart.

352
0:23:24.560 --> 0:23:27.560
And there is a higher confidence that I'm looking at the baby if I just put a hand in

353
0:23:27.560 --> 0:23:28.560
front of it.

354
0:23:28.560 --> 0:23:31.440
It's just crying.

355
0:23:31.440 --> 0:23:33.520
And yeah, that's it.

356
0:23:33.520 --> 0:23:40.560
Later, if you want to build any other apps, you can export it to APK files.

357
0:23:40.560 --> 0:23:44.520
So you can start it on your phone or to Android app bundles if you want to distribute it through

358
0:23:44.520 --> 0:23:45.520
Play Store.

359
0:23:45.520 --> 0:23:30.800
But yeah, this is just a very high-level introduction to artificial intelligence in App

360
0:23:30.800 --> 0:23:31.800
Elementor.

361
0:23:31.800 --> 0:23:55.240
You can just build any kind of classifier, for example, to classify trees, flowers, to

362
0:23:55.240 --> 0:23:56.800
classify even people.

363
0:23:56.800 --> 0:24:01.080
For example, for a faculty, if you want to build an app that recognizes people in your

364
0:24:01.080 --> 0:24:07.600
class as a game, you can just use App Inventor and build any kind of app.

365
0:24:07.600 --> 0:24:08.600
Thank you so much.

366
0:24:08.600 --> 0:24:20.960
And I hope that it was useful for everybody.

367
0:24:20.960 --> 0:24:21.960
Any questions?

368
0:24:21.960 --> 0:24:26.960
Do you mentor a tech-novation team?

369
0:24:26.960 --> 0:24:27.960
No.

370
0:24:27.960 --> 0:24:28.960
You have a serious advantage.

371
0:24:28.960 --> 0:24:29.960
Sorry.

372
0:24:29.960 --> 0:24:30.960
I'm a software engineer.

373
0:24:30.960 --> 0:24:32.560
I just contributed to App Inventor.

374
0:24:32.560 --> 0:24:36.640
I started building apps and then I transitioned to open source.

375
0:24:36.640 --> 0:24:39.480
I participated in Google Summer of Code.

376
0:24:39.480 --> 0:24:43.840
Like this option to export the standard app bundles was my project in 2020 for Google Summer

377
0:24:43.840 --> 0:24:44.840
of Code.

378
0:24:44.840 --> 0:24:52.640
But yeah, more technical than actually teaching to kids.

379
0:24:52.640 --> 0:24:53.640
Any other questions?

380
0:24:53.640 --> 0:25:01.000
What's your experience with the relation between the number of pictures you have to submit

381
0:25:01.000 --> 0:25:03.880
to your classifier and your accuracy?

382
0:25:03.880 --> 0:25:05.080
That's a very good question.

383
0:25:05.080 --> 0:25:07.040
So for the linear example...

384
0:25:07.040 --> 0:25:09.000
Maybe you can repeat this for the tempo.

385
0:25:09.000 --> 0:25:10.000
Oh, yeah, sure.

386
0:25:10.000 --> 0:25:14.160
So he was asking, what's the experience with the amount of images that we are going to

387
0:25:14.160 --> 0:25:15.760
be using for the classifier?

388
0:25:15.760 --> 0:25:18.160
So I haven't really tested it right now.

389
0:25:18.160 --> 0:25:21.840
But we have seen that if we go higher than 10 images for each class, for each group of

390
0:25:21.840 --> 0:25:24.160
images, we'll have really good results.

391
0:25:24.160 --> 0:25:28.560
In this case, because I was just running very fast and using just a few number of images,

392
0:25:28.560 --> 0:25:33.040
you can see that the confidence levels were a little bit like 80-20.

393
0:25:33.040 --> 0:25:37.000
But if we provide more than 10 images for each class, we should be able to get around

394
0:25:37.000 --> 0:25:43.440
over 90-95% of confidence for each number.

395
0:25:43.440 --> 0:25:45.720
I'm not sure if there are any questions from the chat.

396
0:25:45.720 --> 0:25:46.720
Let me just check.

397
0:25:46.720 --> 0:25:47.720
Yeah.

398
0:25:47.720 --> 0:25:48.720
What do you capture?

399
0:25:48.720 --> 0:25:49.720
Was that just for your face or did the learning...

400
0:25:49.720 --> 0:25:50.720
What was learned, is it just like eyes and stuff?

401
0:25:50.720 --> 0:26:05.720
So can I go out there and have the same results?

402
0:26:05.720 --> 0:26:06.720
It recognizes...

403
0:26:06.720 --> 0:26:10.840
It depends on what you are training because in this case, we are just providing a very

404
0:26:10.840 --> 0:26:12.680
specific gestures.

405
0:26:12.680 --> 0:26:17.640
That's training my face like any face looking at the camera or a hand in front of the face.

406
0:26:17.640 --> 0:26:27.680
By default, the model that is available, that is here, this model is trained by Salim, is

407
0:26:27.680 --> 0:26:30.160
the example guide that is at the beginning of the tutorial.

408
0:26:30.160 --> 0:26:32.320
And I just tested it last night.

409
0:26:32.320 --> 0:26:36.160
And it worked with me because it recognizes the gestures, not the faces.

410
0:26:36.160 --> 0:26:42.120
If instead we train recognizing people, we will all be looking in the same way as the

411
0:26:42.120 --> 0:26:43.120
camera.

412
0:26:43.120 --> 0:26:54.480
So it will just go for a specific facial features.

413
0:26:54.480 --> 0:27:04.640
In this case, it will work.

414
0:27:04.640 --> 0:27:07.400
You can just try if you want.

415
0:27:07.400 --> 0:27:08.400
We can try.

416
0:27:08.400 --> 0:27:11.400
It should work.

417
0:27:11.400 --> 0:27:12.400
It should work.

418
0:27:12.400 --> 0:27:13.400
It should work.

419
0:27:13.400 --> 0:27:14.400
It should work.

420
0:27:14.400 --> 0:27:15.400
Can you...

421
0:27:15.400 --> 0:27:18.440
It is going to be a little bit tough.

422
0:27:18.440 --> 0:27:23.920
But Mark, can you just try with Mark, for example?

423
0:27:23.920 --> 0:27:24.920
Toggle camera.

424
0:27:24.920 --> 0:27:25.920
You can try it yourself.

425
0:27:25.920 --> 0:27:26.920
You are different as well.

426
0:27:26.920 --> 0:27:27.920
Toggle camera.

427
0:27:27.920 --> 0:27:30.920
And just try it with...

428
0:27:30.920 --> 0:27:31.920
And start.

429
0:27:31.920 --> 0:27:32.920
Press start.

430
0:27:32.920 --> 0:27:35.080
It is a happy face.

431
0:27:35.080 --> 0:27:37.080
So if you put a hand in front, it is a sad face.

432
0:27:37.080 --> 0:27:38.080
It is recognizing the gestures.

433
0:27:38.080 --> 0:27:39.080
Very cool.

434
0:27:39.080 --> 0:27:45.400
So can you also train it to recognize specific people?

435
0:27:45.400 --> 0:27:46.400
Yeah, it can be trained.

436
0:27:46.400 --> 0:27:50.440
But in this case, because the higher difference was the hand, it is just looking for the hand

437
0:27:50.440 --> 0:27:51.440
and model.

438
0:27:51.440 --> 0:27:54.080
But if you do not show the hand, it will look for faces.

439
0:27:54.080 --> 0:28:01.600
Can you fit your own pencil film models into your app?

440
0:28:01.600 --> 0:28:07.160
It can be fit because you can just use this website and fit any kind of models.

441
0:28:07.160 --> 0:28:10.240
The only restriction is that it has to be an MLD file.

442
0:28:10.240 --> 0:28:13.560
But yeah, it can classify any model, basically.

443
0:28:13.560 --> 0:28:14.880
No problem.

444
0:28:14.880 --> 0:28:17.520
Any other questions?

445
0:28:17.520 --> 0:28:21.360
Well, I think we can leave it here.

446
0:28:21.360 --> 0:28:37.720
Thank you so much.

