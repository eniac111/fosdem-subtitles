WEBVTT

00:00.000 --> 00:12.240
Okay, I am Marco Mancini, I am a consumption architect in Open Nebula system.

00:12.240 --> 00:21.400
Open Nebula system is the company behind Open Nebula, the open source software.

00:21.400 --> 00:28.640
So today I will talk about how you can easily deploy Kubernetes clusters on a hybrid, the

00:28.640 --> 00:34.360
multi-cloud environments by using our open source solution.

00:34.360 --> 00:36.480
So let me introduce Open Nebula.

00:36.480 --> 00:43.840
Open Nebula was born around 14 years ago as a solution for private cloud computing, now

00:43.840 --> 00:46.000
for on-premise.

00:46.000 --> 00:54.200
And evolved during the last years and now we provide a solution that allow you to manage

00:54.200 --> 00:59.760
different types of workloads, so going from virtual machines to application containers

00:59.760 --> 01:05.720
to Kubernetes clusters along with what we call today the data center cloud edge continuum.

01:05.720 --> 01:12.480
So you can have resources on on-premise, you can have resources on public or on the far

01:12.480 --> 01:14.360
edge and so on.

01:14.360 --> 01:20.800
So what we would like to do with this open source solution, Open Nebula is to provide

01:20.800 --> 01:27.920
you with a simple way in order to manage different type of workloads along the, let's say this

01:27.920 --> 01:30.080
cloud edge continuum.

01:30.080 --> 01:36.600
And so you can minimize the complexity, you know, to manage these workloads, you reduce

01:36.600 --> 01:42.440
no consumption of resources because you can manage different types across different kind

01:42.440 --> 01:45.600
of resources and so on.

01:45.600 --> 01:53.600
So mainly what we, at the core of Open Nebula, we use different virtualization technologies.

01:53.600 --> 02:03.360
So we go from supporting using VMware KVM for virtual machine workloads to LXE for system

02:03.360 --> 02:11.440
of containers to Firecracker where you can manage micro VMs and deploy container-based

02:11.440 --> 02:14.720
applications.

02:14.720 --> 02:19.920
And we manage these technologies by using clearly advanced features.

02:19.920 --> 02:26.640
Now you can have multi-tenancy, self-service, so you can provide resources on these different

02:26.640 --> 02:30.600
virtualization technologies and so on.

02:30.600 --> 02:38.240
We have a graphical user interface where you can manage all your resources across, as I

02:38.240 --> 02:45.240
said, this continuum and also we have integrated different third-party tools, you know, going

02:45.240 --> 02:50.200
from Terraform to Ansible to Kubernetes and so on.

02:50.200 --> 02:59.600
So our vision about the multi-cloud is that we can, you know, we would like to provide

02:59.600 --> 03:06.280
an easy way for automatic provisioning of resources, you know, across multiple cloud

03:06.280 --> 03:07.280
providers.

03:07.280 --> 03:12.680
So at the moment, so what we have built is a tool that is called the One Provision.

03:12.680 --> 03:17.360
So you can see in the bottom, so we have also a graphical user interface but it's also a

03:17.360 --> 03:19.040
command line interface.

03:19.040 --> 03:22.120
So you can create resources on different providers.

03:22.120 --> 03:31.440
At the moment, we support providers that as bare metal servers like AWS and Equinix.

03:31.440 --> 03:35.520
But yeah, we can support other providers.

03:35.520 --> 03:42.040
We just need to write some drivers, you know, that allow us to provide resources also across

03:42.040 --> 03:44.360
different providers.

03:44.360 --> 03:49.760
Behind One Provision tool, we use open source tools like Terraform and Ansible.

03:49.760 --> 03:56.880
So with this tool, with this One Provision tool, we can build so different what we call

03:56.880 --> 03:58.320
edge clusters.

03:58.320 --> 04:04.280
So an edge cluster for OpenEble is an abstraction where you have computing, you have storage

04:04.280 --> 04:06.160
and networking, okay?

04:06.160 --> 04:13.120
So once you provide this edge cluster, every cluster, whenever it's provisioning, can be

04:13.120 --> 04:22.000
managed by our uniform, just from one management place, that is our Sunstone graphical user

04:22.000 --> 04:26.680
interface or with our command line interface.

04:26.680 --> 04:33.760
And so from one just panel, you can manage all your clusters across different, for example,

04:33.760 --> 04:38.080
your servers or your premise resources.

04:38.080 --> 04:41.920
And then at the end, what we have is the concept of marketplace.

04:41.920 --> 04:48.520
So where you can have appliances or you can have, we have also integrated Docker apps.

04:48.520 --> 04:54.080
So you can have also Docker images that you can deploy.

04:54.080 --> 05:00.800
So you can deploy virtual machine, multi-virtual machine, containers, and Kubernetes clusters

05:00.800 --> 05:09.120
across these different resources that we have provisioned.

05:09.120 --> 05:14.040
So this is how we manage, let's say, multi-cloud environment.

05:14.040 --> 05:21.120
So by using this One Provision tool and then our graphical user interface and the marketplace.

05:21.120 --> 05:29.360
So let me introduce also how we have built, you know, how Kubernetes is integrated in

05:29.360 --> 05:31.320
Open Nebula.

05:31.320 --> 05:35.520
So for us, Kubernetes is just a service.

05:35.520 --> 05:37.080
So we have built an appliance.

05:37.080 --> 05:41.640
I will talk soon about how we have built this appliance.

05:41.640 --> 05:51.240
So as I said, you can manage Kubernetes by using our tool for managing any application,

05:51.240 --> 05:52.240
right?

05:52.240 --> 05:56.780
So then you can deploy on different edge clusters, right?

05:56.780 --> 06:00.840
So you can exploit all the features that we have.

06:00.840 --> 06:06.000
So if we have, since we have a multi-tenant environment, you can deploy Kubernetes clusters

06:06.000 --> 06:09.640
for all your tenants within Open Nebula.

06:09.640 --> 06:15.960
So you will not deploy Kubernetes clusters on the same physical resources that are shared.

06:15.960 --> 06:23.880
They will be not deployed in a secure way because you can deploy by using our visualization

06:23.880 --> 06:26.520
technologies.

06:26.520 --> 06:28.000
And so on.

06:28.000 --> 06:32.640
And also you are not looking to an event or because you can just deploy your Kubernetes

06:32.640 --> 06:41.240
clusters on any, let's say, cloud edge or premise or far edge provider that you would

06:41.240 --> 06:45.760
like to integrate within your infrastructure, enterprise infrastructure.

06:45.760 --> 06:52.240
So how we have built Kubernetes, integrated Kubernetes in Open Nebula is we have defined

06:52.240 --> 06:53.240
an appliance.

06:53.240 --> 06:54.640
It's called the one key.

06:54.640 --> 07:00.520
This is just a complete Kubernetes deployment.

07:00.520 --> 07:03.040
So it's based on the Airkey 2.

07:03.040 --> 07:08.160
And we use the version 1.24 of Kubernetes.

07:08.160 --> 07:09.980
So we provide all the features.

07:09.980 --> 07:15.120
So when you deploy this appliance, you have all the features included.

07:15.120 --> 07:22.840
So you don't have to deal with managing deployment of a storage solution or ingress controllers

07:22.840 --> 07:24.800
or load balancing.

07:24.800 --> 07:30.240
At the moment, we have used these technologies on our roadmap.

07:30.240 --> 07:34.920
There are some features that we would like to include, especially a better integration

07:34.920 --> 07:38.760
with some of the features that is Open Nebula.

07:38.760 --> 07:44.520
But at the moment, yeah, we have this kind of solution that is based on, as I said, on

07:44.520 --> 07:47.600
the Airkey 2.

07:47.600 --> 07:53.040
The one key appliance, these are the components, is based on one flow.

07:53.040 --> 08:00.080
One flow is a component in Open Nebula that allow you to define multi-VM's application.

08:00.080 --> 08:05.320
So in a one flow service, you can have different roles.

08:05.320 --> 08:10.400
And each role, for example, in this case, for the one, the Kubernetes appliance, we have

08:10.400 --> 08:14.480
defined different roles according, for example, we use the VNF role.

08:14.480 --> 08:21.160
This is the load balancer for the control plane, but it also does not enrooting because

08:21.160 --> 08:24.760
we have two networks within our appliance.

08:24.760 --> 08:30.480
One is the public network and another is the private network between the different components.

08:30.480 --> 08:39.160
So this VNF also allows to go for the different VMs within the private network to communicate

08:39.160 --> 08:42.440
outside to the public.

08:42.440 --> 08:46.600
Then we have the master role.

08:46.600 --> 08:54.440
His role is to manage the control plane, the etc database, the API, and so on.

08:54.440 --> 09:02.440
Then we have the worker nodes that you can use for any workloads that you want to deploy

09:02.440 --> 09:03.920
on your Kubernetes cluster.

09:03.920 --> 09:05.960
And then finally, we have the storage nodes.

09:05.960 --> 09:10.560
These are dedicated, so they will not be used for when you have to deploy some workloads,

09:10.560 --> 09:14.000
but they are used just for your storage needs.

09:14.000 --> 09:22.240
And we use long-core for the persistent volumes within other Kubernetes, within the one case

09:22.240 --> 09:23.240
service.

09:23.240 --> 09:24.240
Okay.

09:24.240 --> 09:31.320
As I said, the VNF, this virtual network function service provides a load balancer.

09:31.320 --> 09:36.680
So you can have multiple VNFs, so in an high availability mode.

09:36.680 --> 09:44.080
Back into account, the Open Nebula offers you the abstraction of virtual machine groups.

09:44.080 --> 09:51.320
So usually for having an availability solution, if you have a virtual machine, you would like

09:51.320 --> 09:56.960
to deploy your virtual machine on different host in order to have an available solution.

09:56.960 --> 10:03.400
So you can use Open Nebula VM groups and then using some affinity or unaffinity rules, your

10:03.400 --> 10:06.560
VMs will be deployed, for example, on different hosts.

10:06.560 --> 10:08.900
So you can have an high available solution.

10:08.900 --> 10:13.340
And this is valid for any role that you have seen before.

10:13.340 --> 10:19.240
So for any role, you can use these VM groups in order to have also an available solution.

10:19.240 --> 10:27.640
So one key by default, just create one VM for each role, but you can modify and scale

10:27.640 --> 10:28.640
the solution.

10:28.640 --> 10:34.280
So having multiple VMs for each role.

10:34.280 --> 10:42.960
So this is the VM, as I said, for the persistent volumes, we have this storage nodes where

10:42.960 --> 10:48.000
we use a long call.

10:48.000 --> 10:56.840
So you can have replicas of your volumes on different VM related to the storage nodes.

10:56.840 --> 11:06.040
Then we have in order to access your services that you deploy within your Kubernetes cluster,

11:06.040 --> 11:09.160
you can have the ingress controller you can use.

11:09.160 --> 11:12.940
We deploy an ingress controller based on traffic.

11:12.940 --> 11:18.200
So this can be used for HTTP and HTTPS protocols.

11:18.200 --> 11:24.560
And then you can access the service by just defining an ingress for your service.

11:24.560 --> 11:30.240
And then we have integrated also Metal-LB instead for the load balancer service.

11:30.240 --> 11:40.720
So in this case, you can use this for other kind of protocols that are HTTP or HTTP based.

11:40.720 --> 11:48.720
Yeah I would like to go because it's almost I have five minutes now, more or less.

11:48.720 --> 11:55.280
I will prepare just a demo to show you how you can use Open Nebula.

11:55.280 --> 12:04.480
So I will show you how to use one provision in order to provide resources on AWS and Equinix.

12:04.480 --> 12:11.000
And then we can deploy Kubernetes clusters on both edge clusters on this two public cloud

12:11.000 --> 12:12.200
provider.

12:12.200 --> 12:21.360
And then we can just access one of the Kubernetes clusters and just deploy an application.

12:21.360 --> 12:25.240
Let me go on the demo.

12:25.240 --> 12:30.440
Okay, so this is the Sunstone graphical user interface.

12:30.440 --> 12:33.880
You can see here if we go to clusters, we have just the default cluster.

12:33.880 --> 12:37.160
But there are no host.

12:37.160 --> 12:39.040
There are only data stores, but there are no hosts.

12:39.040 --> 12:43.600
So in this moment, we have just our front end without any resources.

12:43.600 --> 12:46.720
Now what's a goal is to go to the one provision.

12:46.720 --> 12:53.280
We have defined already two providers, one for Equinix and one for AWS.

12:53.280 --> 13:00.440
And once you define these providers, you can create clusters on the two providers.

13:00.440 --> 13:03.720
So we are going to create a cluster, for example, in AWS.

13:03.720 --> 13:09.640
In this case, we have defined a provider for AWS in London, not the zone.

13:09.640 --> 13:14.160
And this will now create an edge cluster on AWS.

13:14.160 --> 13:18.800
As I said, we use Terraform and Dansible to create resources and to configure in such

13:18.800 --> 13:22.880
a way that you create an edge cluster for OpenEbola.

13:22.880 --> 13:27.720
And then here I'm going to create another cluster instead of on Equinix.

13:27.720 --> 13:29.960
Clearly, you have some parameters.

13:29.960 --> 13:34.720
You can define the number of hosts, you can define the number of public IP that you would

13:34.720 --> 13:38.640
like to access, and so on.

13:38.640 --> 13:42.040
By the way, you can define two types of clusters with one provision.

13:42.040 --> 13:48.200
One is an edge cluster, it's based, or you can also create a safe cluster, an hyperconverged

13:48.200 --> 13:49.280
cluster.

13:49.280 --> 13:53.560
As you can see here, once you use one provision in a soundstorm, graphical user interface,

13:53.560 --> 13:57.040
you will see the hosts that are going to be provisioning.

13:57.040 --> 14:03.080
And it will take around 5-10 minutes, this depends clearly on the cloud provider how

14:03.080 --> 14:07.080
much time it needs to create resources.

14:07.080 --> 14:12.120
But once you have created the resources, you can see here the two clusters.

14:12.120 --> 14:18.360
What you have to do is to instantiate a couple of, to use Kubernetes appliance, we have to

14:18.360 --> 14:27.480
define a couple of private networks, one for Equinix and one for the AWS clusters.

14:27.480 --> 14:33.520
And in order to do this, it's simplified because we create a template, then you just instantiate

14:33.520 --> 14:39.760
the template, and then you can create also the private networks, both for AWS and Equinix.

14:39.760 --> 14:47.160
Because we need the private for the internal VMs, the roles like the master, the storage,

14:47.160 --> 14:52.480
and the worker nodes, and then we need the public network instead for the VNF, that is

14:52.480 --> 15:00.760
our main endpoint where to access the Kubernetes clusters.

15:00.760 --> 15:06.920
Now what we are going to do is to import the one key appliance for our marketplace within

15:06.920 --> 15:09.240
our Open Nebula.

15:09.240 --> 15:10.920
You can do that just once.

15:10.920 --> 15:14.520
So we are going to import the appliance.

15:14.520 --> 15:20.760
And once you import the appliance, what will be imported are templates for the VMs that

15:20.760 --> 15:25.600
are for each role, and the template for the service.

15:25.600 --> 15:32.000
And this service is based on one flow, and also the images that are related to the different

15:32.000 --> 15:33.280
roles.

15:33.280 --> 15:38.120
So in order to create a new Kubernetes cluster, what we have to do is to just instantiate

15:38.120 --> 15:42.880
a service by selecting the appropriate networks, for example.

15:42.880 --> 15:47.200
So in this case, you can see now I'm creating a cluster on AWS.

15:47.200 --> 15:54.080
So I select for the public network, the AWS cluster public for the private, the AWS private.

15:54.080 --> 15:57.600
And then I just have to put a couple of IPs, internal.

15:57.600 --> 16:05.440
These are for the internal networks for the virtual IP for the VNF and for the gateway.

16:05.440 --> 16:08.160
And we can do the same for Equinix.

16:08.160 --> 16:16.040
So by just selecting the public networks of Equinix and then the private networks that

16:16.040 --> 16:17.040
we have defined.

16:17.040 --> 16:27.080
Also in this case, I've used the same network for both clusters.

16:27.080 --> 16:33.560
And here you see that now we are deploying the two Kubernetes clusters on the two different

16:33.560 --> 16:39.120
edge clusters that are on AWS London and Equinix.

16:39.120 --> 16:43.240
As you see, the first role that is deployed is VNF.

16:43.240 --> 16:49.200
Once the VNF is ready or running in one flow, you can define dependencies.

16:49.200 --> 16:59.040
And once the VNF is ready, one flow is going to deploy the other roles, master, the worker,

16:59.040 --> 17:00.700
and the storage node.

17:00.700 --> 17:09.720
In order to access the Kubernetes clusters, you have to use the public IP of the VNF.

17:09.720 --> 17:19.160
And you can use SSH agent for wording by using first connecting to the VNF and then connecting

17:19.160 --> 17:23.080
to the master by using the private IP.

17:23.080 --> 17:24.240
Here we can see the nodes.

17:24.240 --> 17:28.560
So we can have, as I said, by default, you have one node for each master.

17:28.560 --> 17:33.440
This is not for production environment.

17:33.440 --> 17:40.160
If you want to have a production environment, you need to scale each node, for example.

17:40.160 --> 17:42.160
So here I just create an image.

17:42.160 --> 17:48.640
And I prepared also a YAML file, a manifest file, for exposing the service through the

17:48.640 --> 17:50.500
ingress controller.

17:50.500 --> 17:57.040
And then you can use the public IP of the VNF to access the service.

17:57.040 --> 18:07.560
Clearly, Open Nebula doesn't have any tools for managing the deployment of application

18:07.560 --> 18:08.560
on Google needs.

18:08.560 --> 18:13.020
So we manage the infrastructure and the deployment of the Kubernetes cluster.

18:13.020 --> 18:19.840
Then you can use kubectl, you can use ranger, you can use other open source tooling that

18:19.840 --> 18:23.440
maybe in the future we can add also.

18:23.440 --> 18:29.520
As you can see here by using the public IP of the VNF, I have access to the engineics.

18:29.520 --> 18:33.880
Another thing, you can scale the roles once you deploy, for example.

18:33.880 --> 18:37.880
In this case, I can scale, for example, the worker.

18:37.880 --> 18:39.120
You just put the number here.

18:39.120 --> 18:46.880
We use the one flow allow us to scale the cluster for each role.

18:46.880 --> 18:52.680
And now you can see another worker is going to be deployed.

18:52.680 --> 18:55.120
This was the demo.

18:55.120 --> 18:58.480
I think that's the conclusion.

18:58.480 --> 18:59.480
Okay.

18:59.480 --> 19:00.480
Thank you.

19:00.480 --> 19:01.480
Thank you.

19:01.480 --> 19:23.260
Ok Parts fans, thank you.
