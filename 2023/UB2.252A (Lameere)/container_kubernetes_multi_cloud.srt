1
0:00:00.000 --> 0:00:12.240
Okay, I am Marco Mancini, I am a consumption architect in Open Nebula system.

2
0:00:12.240 --> 0:00:21.400
Open Nebula system is the company behind Open Nebula, the open source software.

3
0:00:21.400 --> 0:00:28.640
So today I will talk about how you can easily deploy Kubernetes clusters on a hybrid, the

4
0:00:28.640 --> 0:00:34.360
multi-cloud environments by using our open source solution.

5
0:00:34.360 --> 0:00:36.480
So let me introduce Open Nebula.

6
0:00:36.480 --> 0:00:43.840
Open Nebula was born around 14 years ago as a solution for private cloud computing, now

7
0:00:43.840 --> 0:00:46.000
for on-premise.

8
0:00:46.000 --> 0:00:54.200
And evolved during the last years and now we provide a solution that allow you to manage

9
0:00:54.200 --> 0:00:59.760
different types of workloads, so going from virtual machines to application containers

10
0:00:59.760 --> 0:01:05.720
to Kubernetes clusters along with what we call today the data center cloud edge continuum.

11
0:01:05.720 --> 0:01:12.480
So you can have resources on on-premise, you can have resources on public or on the far

12
0:01:12.480 --> 0:01:14.360
edge and so on.

13
0:01:14.360 --> 0:01:20.800
So what we would like to do with this open source solution, Open Nebula is to provide

14
0:01:20.800 --> 0:01:27.920
you with a simple way in order to manage different type of workloads along the, let's say this

15
0:01:27.920 --> 0:01:30.080
cloud edge continuum.

16
0:01:30.080 --> 0:01:36.600
And so you can minimize the complexity, you know, to manage these workloads, you reduce

17
0:01:36.600 --> 0:01:42.440
no consumption of resources because you can manage different types across different kind

18
0:01:42.440 --> 0:01:45.600
of resources and so on.

19
0:01:45.600 --> 0:01:53.600
So mainly what we, at the core of Open Nebula, we use different virtualization technologies.

20
0:01:53.600 --> 0:02:03.360
So we go from supporting using VMware KVM for virtual machine workloads to LXE for system

21
0:02:03.360 --> 0:02:11.440
of containers to Firecracker where you can manage micro VMs and deploy container-based

22
0:02:11.440 --> 0:02:14.720
applications.

23
0:02:14.720 --> 0:02:19.920
And we manage these technologies by using clearly advanced features.

24
0:02:19.920 --> 0:02:26.640
Now you can have multi-tenancy, self-service, so you can provide resources on these different

25
0:02:26.640 --> 0:02:30.600
virtualization technologies and so on.

26
0:02:30.600 --> 0:02:38.240
We have a graphical user interface where you can manage all your resources across, as I

27
0:02:38.240 --> 0:02:45.240
said, this continuum and also we have integrated different third-party tools, you know, going

28
0:02:45.240 --> 0:02:50.200
from Terraform to Ansible to Kubernetes and so on.

29
0:02:50.200 --> 0:02:59.600
So our vision about the multi-cloud is that we can, you know, we would like to provide

30
0:02:59.600 --> 0:03:06.280
an easy way for automatic provisioning of resources, you know, across multiple cloud

31
0:03:06.280 --> 0:03:07.280
providers.

32
0:03:07.280 --> 0:03:12.680
So at the moment, so what we have built is a tool that is called the One Provision.

33
0:03:12.680 --> 0:03:17.360
So you can see in the bottom, so we have also a graphical user interface but it's also a

34
0:03:17.360 --> 0:03:19.040
command line interface.

35
0:03:19.040 --> 0:03:22.120
So you can create resources on different providers.

36
0:03:22.120 --> 0:03:31.440
At the moment, we support providers that as bare metal servers like AWS and Equinix.

37
0:03:31.440 --> 0:03:35.520
But yeah, we can support other providers.

38
0:03:35.520 --> 0:03:42.040
We just need to write some drivers, you know, that allow us to provide resources also across

39
0:03:42.040 --> 0:03:44.360
different providers.

40
0:03:44.360 --> 0:03:49.760
Behind One Provision tool, we use open source tools like Terraform and Ansible.

41
0:03:49.760 --> 0:03:56.880
So with this tool, with this One Provision tool, we can build so different what we call

42
0:03:56.880 --> 0:03:58.320
edge clusters.

43
0:03:58.320 --> 0:04:04.280
So an edge cluster for OpenEble is an abstraction where you have computing, you have storage

44
0:04:04.280 --> 0:04:06.160
and networking, okay?

45
0:04:06.160 --> 0:04:13.120
So once you provide this edge cluster, every cluster, whenever it's provisioning, can be

46
0:04:13.120 --> 0:04:22.000
managed by our uniform, just from one management place, that is our Sunstone graphical user

47
0:04:22.000 --> 0:04:26.680
interface or with our command line interface.

48
0:04:26.680 --> 0:04:33.760
And so from one just panel, you can manage all your clusters across different, for example,

49
0:04:33.760 --> 0:04:38.080
your servers or your premise resources.

50
0:04:38.080 --> 0:04:41.920
And then at the end, what we have is the concept of marketplace.

51
0:04:41.920 --> 0:04:48.520
So where you can have appliances or you can have, we have also integrated Docker apps.

52
0:04:48.520 --> 0:04:54.080
So you can have also Docker images that you can deploy.

53
0:04:54.080 --> 0:05:00.800
So you can deploy virtual machine, multi-virtual machine, containers, and Kubernetes clusters

54
0:05:00.800 --> 0:05:09.120
across these different resources that we have provisioned.

55
0:05:09.120 --> 0:05:14.040
So this is how we manage, let's say, multi-cloud environment.

56
0:05:14.040 --> 0:05:21.120
So by using this One Provision tool and then our graphical user interface and the marketplace.

57
0:05:21.120 --> 0:05:29.360
So let me introduce also how we have built, you know, how Kubernetes is integrated in

58
0:05:29.360 --> 0:05:31.320
Open Nebula.

59
0:05:31.320 --> 0:05:35.520
So for us, Kubernetes is just a service.

60
0:05:35.520 --> 0:05:37.080
So we have built an appliance.

61
0:05:37.080 --> 0:05:41.640
I will talk soon about how we have built this appliance.

62
0:05:41.640 --> 0:05:51.240
So as I said, you can manage Kubernetes by using our tool for managing any application,

63
0:05:51.240 --> 0:05:52.240
right?

64
0:05:52.240 --> 0:05:56.780
So then you can deploy on different edge clusters, right?

65
0:05:56.780 --> 0:06:00.840
So you can exploit all the features that we have.

66
0:06:00.840 --> 0:06:06.000
So if we have, since we have a multi-tenant environment, you can deploy Kubernetes clusters

67
0:06:06.000 --> 0:06:09.640
for all your tenants within Open Nebula.

68
0:06:09.640 --> 0:06:15.960
So you will not deploy Kubernetes clusters on the same physical resources that are shared.

69
0:06:15.960 --> 0:06:23.880
They will be not deployed in a secure way because you can deploy by using our visualization

70
0:06:23.880 --> 0:06:26.520
technologies.

71
0:06:26.520 --> 0:06:28.000
And so on.

72
0:06:28.000 --> 0:06:32.640
And also you are not looking to an event or because you can just deploy your Kubernetes

73
0:06:32.640 --> 0:06:41.240
clusters on any, let's say, cloud edge or premise or far edge provider that you would

74
0:06:41.240 --> 0:06:45.760
like to integrate within your infrastructure, enterprise infrastructure.

75
0:06:45.760 --> 0:06:52.240
So how we have built Kubernetes, integrated Kubernetes in Open Nebula is we have defined

76
0:06:52.240 --> 0:06:53.240
an appliance.

77
0:06:53.240 --> 0:06:54.640
It's called the one key.

78
0:06:54.640 --> 0:07:00.520
This is just a complete Kubernetes deployment.

79
0:07:00.520 --> 0:07:03.040
So it's based on the Airkey 2.

80
0:07:03.040 --> 0:07:08.160
And we use the version 1.24 of Kubernetes.

81
0:07:08.160 --> 0:07:09.980
So we provide all the features.

82
0:07:09.980 --> 0:07:15.120
So when you deploy this appliance, you have all the features included.

83
0:07:15.120 --> 0:07:22.840
So you don't have to deal with managing deployment of a storage solution or ingress controllers

84
0:07:22.840 --> 0:07:24.800
or load balancing.

85
0:07:24.800 --> 0:07:30.240
At the moment, we have used these technologies on our roadmap.

86
0:07:30.240 --> 0:07:34.920
There are some features that we would like to include, especially a better integration

87
0:07:34.920 --> 0:07:38.760
with some of the features that is Open Nebula.

88
0:07:38.760 --> 0:07:44.520
But at the moment, yeah, we have this kind of solution that is based on, as I said, on

89
0:07:44.520 --> 0:07:47.600
the Airkey 2.

90
0:07:47.600 --> 0:07:53.040
The one key appliance, these are the components, is based on one flow.

91
0:07:53.040 --> 0:08:00.080
One flow is a component in Open Nebula that allow you to define multi-VM's application.

92
0:08:00.080 --> 0:08:05.320
So in a one flow service, you can have different roles.

93
0:08:05.320 --> 0:08:10.400
And each role, for example, in this case, for the one, the Kubernetes appliance, we have

94
0:08:10.400 --> 0:08:14.480
defined different roles according, for example, we use the VNF role.

95
0:08:14.480 --> 0:08:21.160
This is the load balancer for the control plane, but it also does not enrooting because

96
0:08:21.160 --> 0:08:24.760
we have two networks within our appliance.

97
0:08:24.760 --> 0:08:30.480
One is the public network and another is the private network between the different components.

98
0:08:30.480 --> 0:08:39.160
So this VNF also allows to go for the different VMs within the private network to communicate

99
0:08:39.160 --> 0:08:42.440
outside to the public.

100
0:08:42.440 --> 0:08:46.600
Then we have the master role.

101
0:08:46.600 --> 0:08:54.440
His role is to manage the control plane, the etc database, the API, and so on.

102
0:08:54.440 --> 0:09:02.440
Then we have the worker nodes that you can use for any workloads that you want to deploy

103
0:09:02.440 --> 0:09:03.920
on your Kubernetes cluster.

104
0:09:03.920 --> 0:09:05.960
And then finally, we have the storage nodes.

105
0:09:05.960 --> 0:09:10.560
These are dedicated, so they will not be used for when you have to deploy some workloads,

106
0:09:10.560 --> 0:09:14.000
but they are used just for your storage needs.

107
0:09:14.000 --> 0:09:22.240
And we use long-core for the persistent volumes within other Kubernetes, within the one case

108
0:09:22.240 --> 0:09:23.240
service.

109
0:09:23.240 --> 0:09:24.240
Okay.

110
0:09:24.240 --> 0:09:31.320
As I said, the VNF, this virtual network function service provides a load balancer.

111
0:09:31.320 --> 0:09:36.680
So you can have multiple VNFs, so in an high availability mode.

112
0:09:36.680 --> 0:09:44.080
Back into account, the Open Nebula offers you the abstraction of virtual machine groups.

113
0:09:44.080 --> 0:09:51.320
So usually for having an availability solution, if you have a virtual machine, you would like

114
0:09:51.320 --> 0:09:56.960
to deploy your virtual machine on different host in order to have an available solution.

115
0:09:56.960 --> 0:10:03.400
So you can use Open Nebula VM groups and then using some affinity or unaffinity rules, your

116
0:10:03.400 --> 0:10:06.560
VMs will be deployed, for example, on different hosts.

117
0:10:06.560 --> 0:10:08.900
So you can have an high available solution.

118
0:10:08.900 --> 0:10:13.340
And this is valid for any role that you have seen before.

119
0:10:13.340 --> 0:10:19.240
So for any role, you can use these VM groups in order to have also an available solution.

120
0:10:19.240 --> 0:10:27.640
So one key by default, just create one VM for each role, but you can modify and scale

121
0:10:27.640 --> 0:10:28.640
the solution.

122
0:10:28.640 --> 0:10:34.280
So having multiple VMs for each role.

123
0:10:34.280 --> 0:10:42.960
So this is the VM, as I said, for the persistent volumes, we have this storage nodes where

124
0:10:42.960 --> 0:10:48.000
we use a long call.

125
0:10:48.000 --> 0:10:56.840
So you can have replicas of your volumes on different VM related to the storage nodes.

126
0:10:56.840 --> 0:11:06.040
Then we have in order to access your services that you deploy within your Kubernetes cluster,

127
0:11:06.040 --> 0:11:09.160
you can have the ingress controller you can use.

128
0:11:09.160 --> 0:11:12.940
We deploy an ingress controller based on traffic.

129
0:11:12.940 --> 0:11:18.200
So this can be used for HTTP and HTTPS protocols.

130
0:11:18.200 --> 0:11:24.560
And then you can access the service by just defining an ingress for your service.

131
0:11:24.560 --> 0:11:30.240
And then we have integrated also Metal-LB instead for the load balancer service.

132
0:11:30.240 --> 0:11:40.720
So in this case, you can use this for other kind of protocols that are HTTP or HTTP based.

133
0:11:40.720 --> 0:11:48.720
Yeah I would like to go because it's almost I have five minutes now, more or less.

134
0:11:48.720 --> 0:11:55.280
I will prepare just a demo to show you how you can use Open Nebula.

135
0:11:55.280 --> 0:12:04.480
So I will show you how to use one provision in order to provide resources on AWS and Equinix.

136
0:12:04.480 --> 0:12:11.000
And then we can deploy Kubernetes clusters on both edge clusters on this two public cloud

137
0:12:11.000 --> 0:12:12.200
provider.

138
0:12:12.200 --> 0:12:21.360
And then we can just access one of the Kubernetes clusters and just deploy an application.

139
0:12:21.360 --> 0:12:25.240
Let me go on the demo.

140
0:12:25.240 --> 0:12:30.440
Okay, so this is the Sunstone graphical user interface.

141
0:12:30.440 --> 0:12:33.880
You can see here if we go to clusters, we have just the default cluster.

142
0:12:33.880 --> 0:12:37.160
But there are no host.

143
0:12:37.160 --> 0:12:39.040
There are only data stores, but there are no hosts.

144
0:12:39.040 --> 0:12:43.600
So in this moment, we have just our front end without any resources.

145
0:12:43.600 --> 0:12:46.720
Now what's a goal is to go to the one provision.

146
0:12:46.720 --> 0:12:53.280
We have defined already two providers, one for Equinix and one for AWS.

147
0:12:53.280 --> 0:13:00.440
And once you define these providers, you can create clusters on the two providers.

148
0:13:00.440 --> 0:13:03.720
So we are going to create a cluster, for example, in AWS.

149
0:13:03.720 --> 0:13:09.640
In this case, we have defined a provider for AWS in London, not the zone.

150
0:13:09.640 --> 0:13:14.160
And this will now create an edge cluster on AWS.

151
0:13:14.160 --> 0:13:18.800
As I said, we use Terraform and Dansible to create resources and to configure in such

152
0:13:18.800 --> 0:13:22.880
a way that you create an edge cluster for OpenEbola.

153
0:13:22.880 --> 0:13:27.720
And then here I'm going to create another cluster instead of on Equinix.

154
0:13:27.720 --> 0:13:29.960
Clearly, you have some parameters.

155
0:13:29.960 --> 0:13:34.720
You can define the number of hosts, you can define the number of public IP that you would

156
0:13:34.720 --> 0:13:38.640
like to access, and so on.

157
0:13:38.640 --> 0:13:42.040
By the way, you can define two types of clusters with one provision.

158
0:13:42.040 --> 0:13:48.200
One is an edge cluster, it's based, or you can also create a safe cluster, an hyperconverged

159
0:13:48.200 --> 0:13:49.280
cluster.

160
0:13:49.280 --> 0:13:53.560
As you can see here, once you use one provision in a soundstorm, graphical user interface,

161
0:13:53.560 --> 0:13:57.040
you will see the hosts that are going to be provisioning.

162
0:13:57.040 --> 0:14:03.080
And it will take around 5-10 minutes, this depends clearly on the cloud provider how

163
0:14:03.080 --> 0:14:07.080
much time it needs to create resources.

164
0:14:07.080 --> 0:14:12.120
But once you have created the resources, you can see here the two clusters.

165
0:14:12.120 --> 0:14:18.360
What you have to do is to instantiate a couple of, to use Kubernetes appliance, we have to

166
0:14:18.360 --> 0:14:27.480
define a couple of private networks, one for Equinix and one for the AWS clusters.

167
0:14:27.480 --> 0:14:33.520
And in order to do this, it's simplified because we create a template, then you just instantiate

168
0:14:33.520 --> 0:14:39.760
the template, and then you can create also the private networks, both for AWS and Equinix.

169
0:14:39.760 --> 0:14:47.160
Because we need the private for the internal VMs, the roles like the master, the storage,

170
0:14:47.160 --> 0:14:52.480
and the worker nodes, and then we need the public network instead for the VNF, that is

171
0:14:52.480 --> 0:15:00.760
our main endpoint where to access the Kubernetes clusters.

172
0:15:00.760 --> 0:15:06.920
Now what we are going to do is to import the one key appliance for our marketplace within

173
0:15:06.920 --> 0:15:09.240
our Open Nebula.

174
0:15:09.240 --> 0:15:10.920
You can do that just once.

175
0:15:10.920 --> 0:15:14.520
So we are going to import the appliance.

176
0:15:14.520 --> 0:15:20.760
And once you import the appliance, what will be imported are templates for the VMs that

177
0:15:20.760 --> 0:15:25.600
are for each role, and the template for the service.

178
0:15:25.600 --> 0:15:32.000
And this service is based on one flow, and also the images that are related to the different

179
0:15:32.000 --> 0:15:33.280
roles.

180
0:15:33.280 --> 0:15:38.120
So in order to create a new Kubernetes cluster, what we have to do is to just instantiate

181
0:15:38.120 --> 0:15:42.880
a service by selecting the appropriate networks, for example.

182
0:15:42.880 --> 0:15:47.200
So in this case, you can see now I'm creating a cluster on AWS.

183
0:15:47.200 --> 0:15:54.080
So I select for the public network, the AWS cluster public for the private, the AWS private.

184
0:15:54.080 --> 0:15:57.600
And then I just have to put a couple of IPs, internal.

185
0:15:57.600 --> 0:16:05.440
These are for the internal networks for the virtual IP for the VNF and for the gateway.

186
0:16:05.440 --> 0:16:08.160
And we can do the same for Equinix.

187
0:16:08.160 --> 0:16:16.040
So by just selecting the public networks of Equinix and then the private networks that

188
0:16:16.040 --> 0:16:17.040
we have defined.

189
0:16:17.040 --> 0:16:27.080
Also in this case, I've used the same network for both clusters.

190
0:16:27.080 --> 0:16:33.560
And here you see that now we are deploying the two Kubernetes clusters on the two different

191
0:16:33.560 --> 0:16:39.120
edge clusters that are on AWS London and Equinix.

192
0:16:39.120 --> 0:16:43.240
As you see, the first role that is deployed is VNF.

193
0:16:43.240 --> 0:16:49.200
Once the VNF is ready or running in one flow, you can define dependencies.

194
0:16:49.200 --> 0:16:59.040
And once the VNF is ready, one flow is going to deploy the other roles, master, the worker,

195
0:16:59.040 --> 0:17:00.700
and the storage node.

196
0:17:00.700 --> 0:17:09.720
In order to access the Kubernetes clusters, you have to use the public IP of the VNF.

197
0:17:09.720 --> 0:17:19.160
And you can use SSH agent for wording by using first connecting to the VNF and then connecting

198
0:17:19.160 --> 0:17:23.080
to the master by using the private IP.

199
0:17:23.080 --> 0:17:24.240
Here we can see the nodes.

200
0:17:24.240 --> 0:17:28.560
So we can have, as I said, by default, you have one node for each master.

201
0:17:28.560 --> 0:17:33.440
This is not for production environment.

202
0:17:33.440 --> 0:17:40.160
If you want to have a production environment, you need to scale each node, for example.

203
0:17:40.160 --> 0:17:42.160
So here I just create an image.

204
0:17:42.160 --> 0:17:48.640
And I prepared also a YAML file, a manifest file, for exposing the service through the

205
0:17:48.640 --> 0:17:50.500
ingress controller.

206
0:17:50.500 --> 0:17:57.040
And then you can use the public IP of the VNF to access the service.

207
0:17:57.040 --> 0:18:07.560
Clearly, Open Nebula doesn't have any tools for managing the deployment of application

208
0:18:07.560 --> 0:18:08.560
on Google needs.

209
0:18:08.560 --> 0:18:13.020
So we manage the infrastructure and the deployment of the Kubernetes cluster.

210
0:18:13.020 --> 0:18:19.840
Then you can use kubectl, you can use ranger, you can use other open source tooling that

211
0:18:19.840 --> 0:18:23.440
maybe in the future we can add also.

212
0:18:23.440 --> 0:18:29.520
As you can see here by using the public IP of the VNF, I have access to the engineics.

213
0:18:29.520 --> 0:18:33.880
Another thing, you can scale the roles once you deploy, for example.

214
0:18:33.880 --> 0:18:37.880
In this case, I can scale, for example, the worker.

215
0:18:37.880 --> 0:18:39.120
You just put the number here.

216
0:18:39.120 --> 0:18:46.880
We use the one flow allow us to scale the cluster for each role.

217
0:18:46.880 --> 0:18:52.680
And now you can see another worker is going to be deployed.

218
0:18:52.680 --> 0:18:55.120
This was the demo.

219
0:18:55.120 --> 0:18:58.480
I think that's the conclusion.

220
0:18:58.480 --> 0:18:59.480
Okay.

221
0:18:59.480 --> 0:19:00.480
Thank you.

222
0:19:00.480 --> 0:19:01.480
Thank you.

223
0:19:01.480 --> 0:19:23.260
Ok Parts fans, thank you.

