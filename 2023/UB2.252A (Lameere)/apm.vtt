WEBVTT

00:00.000 --> 00:01.000
Okay.

00:01.000 --> 00:02.000
Hello, everyone.

00:02.000 --> 00:14.920
Welcome to the talk on open telemetry with Grafana.

00:14.920 --> 00:17.320
Microphone broke, so I need to do it with this microphone now.

00:17.320 --> 00:21.440
Let's see how it goes with typing and live demo.

00:21.440 --> 00:22.440
Few words about me.

00:22.440 --> 00:23.960
So who am I?

00:23.960 --> 00:27.600
Why am I here talking about Grafana and open telemetry?

00:27.600 --> 00:30.320
So I work at Grafana Labs.

00:30.320 --> 00:35.300
I'm an engineering manager and also manager for our open telemetry squad.

00:35.300 --> 00:39.640
And I'm also active in open source, so I'm a member of the Prometheus team where I maintain

00:39.640 --> 00:42.520
the Java metrics library.

00:42.520 --> 00:46.880
So what are we going to do in this talk in the next 25 minutes or so?

00:46.880 --> 00:50.320
So it will almost exclusively be a live demo.

00:50.320 --> 00:55.540
So basically the idea is I have a little example application running on my laptop, and it is

00:55.540 --> 00:57.440
instrumented with open telemetry.

00:57.440 --> 01:01.480
I will show you in a minute what it does and how I instrumented it.

01:01.480 --> 01:05.400
And I also have an open source monitoring backend running.

01:05.400 --> 01:08.840
It consists of three databases.

01:08.840 --> 01:12.760
One is Loki, which is an open source logs database.

01:12.760 --> 01:16.220
One is Temple, which is an open source trace database.

01:16.220 --> 01:19.540
And one is Mimir, which is an open source metrics database.

01:19.540 --> 01:25.660
So Mimir is compatible with Prometheus, so it could have shown the exact same demo using

01:25.660 --> 01:30.080
Prometheus instead of Mimir, so it doesn't really matter for now.

01:30.080 --> 01:31.840
And of course I also have Grafana.

01:31.840 --> 01:35.180
I have those databases configured as data sources.

01:35.180 --> 01:40.000
And what we are going to do, we are going to start up Grafana, have a look at metrics,

01:40.000 --> 01:42.200
have a look at traces, have a look at logs.

01:42.200 --> 01:46.600
And basically the idea is that at the end of the talk you kind of have seen all the

01:46.600 --> 01:51.920
signals that come out of open telemetry, explored a bit what you can do with this type of data.

01:51.920 --> 01:56.680
And so you should have a good overview how open source monitoring with open telemetry

01:56.680 --> 01:59.360
looks like.

01:59.360 --> 02:02.800
So last slide before we jump into the live demo.

02:02.800 --> 02:08.000
So this is just a quick overview of what the example application does so that you know

02:08.000 --> 02:09.800
what we are going to look at.

02:09.800 --> 02:16.200
It's a simple hello world REST service written in Java using Spring Boot.

02:16.200 --> 02:22.120
And so basically you can send a request to port 8080 and it will respond with hello world.

02:22.120 --> 02:26.880
And in order to make it a bit more interesting, I made it a distributed hello world service.

02:26.880 --> 02:31.680
So it doesn't respond directly, but when it receives a request, it reaches out to a greeting

02:31.680 --> 02:34.280
service running on port 8081.

02:34.280 --> 02:38.360
The greeting service responds with the greeting, which is hello world, and then the response

02:38.360 --> 02:40.880
is forwarded to the client.

02:40.880 --> 02:44.400
And there are random errors to have some error rates as well.

02:44.400 --> 02:50.320
So basically a hello world microservice architecture or whatever.

02:50.320 --> 02:55.740
And in order to instrument this with open telemetry, I used the Java instrumentation

02:55.740 --> 02:58.760
agent that's provided by the open telemetry community.

02:58.760 --> 03:00.900
That's something you can download on GitHub.

03:00.900 --> 03:06.240
And the thing is this thing, you basically attach it to the Java virtual machine at start-up

03:06.240 --> 03:08.880
time with a special command line parameter.

03:08.880 --> 03:10.920
So I didn't modify any source code.

03:10.920 --> 03:14.600
I didn't use any SDK or introduce any custom stuff.

03:14.600 --> 03:20.680
All we are going to look at in this demo is just data produced by attaching the open telemetry

03:20.680 --> 03:24.640
instrumentation to a standard Spring Boot application.

03:24.640 --> 03:25.640
Right?

03:25.640 --> 03:26.640
Cool.

03:26.640 --> 03:29.200
So let's get started.

03:29.200 --> 03:31.720
As said, I have my data sources configured here.

03:31.720 --> 03:38.720
So Prometheus and MIMIR are compatible, so it doesn't really matter which one we choose.

03:38.720 --> 03:42.080
There are a lot of, so I want to start with metrics.

03:42.080 --> 03:43.960
And yeah, Sarah.

03:43.960 --> 03:54.200
Can we turn the lights down a bit?

03:54.200 --> 03:58.920
I don't know.

03:58.920 --> 04:02.080
Okay.

04:02.080 --> 04:09.240
Maybe the other way around.

04:09.240 --> 04:12.000
Okay.

04:12.000 --> 04:13.000
I will just continue.

04:13.000 --> 04:14.000
Come on.

04:14.000 --> 04:19.200
So there are lots of metrics that you get from the open telemetry instrumentation.

04:19.200 --> 04:24.800
So kind of JVM related stuff like garbage collection activity and so forth.

04:24.800 --> 04:29.040
But the one I want to look at, oh, no.

04:29.040 --> 04:33.480
It's getting brighter and brighter.

04:33.480 --> 04:37.160
Yay.

04:37.160 --> 04:40.200
Okay.

04:40.200 --> 04:44.720
Great.

04:44.720 --> 04:47.680
I think there is also a light mode in Grafana.

04:47.680 --> 04:51.720
Maybe that would have been a better choice.

04:51.720 --> 04:56.520
But no, I'm not going to use light mode.

04:56.520 --> 05:01.160
So let's figure out how to do the demo while I have a microphone that I should hold in

05:01.160 --> 05:02.160
my hands.

05:02.160 --> 05:04.400
Let's just put it here.

05:04.400 --> 05:05.400
So okay.

05:05.400 --> 05:06.400
Thank you.

05:06.400 --> 05:07.400
Cool.

05:07.400 --> 05:19.960
So the metric we are going to look at for the demo, it's a metric named HTTP server duration.

05:19.960 --> 05:22.480
This is a metric of type histogram.

05:22.480 --> 05:25.640
So histograms have a couple of different numbers attached to them.

05:25.640 --> 05:30.360
So there are histogram buckets with the distribution data and so forth.

05:30.360 --> 05:31.880
And there's also a count.

05:31.880 --> 05:33.360
The count is the most simple one.

05:33.360 --> 05:37.800
So we are going to use this in our example.

05:37.800 --> 05:39.240
I actually got it two times.

05:39.240 --> 05:43.580
I got it once for my greeting service here and once for the hello world application.

05:43.580 --> 05:51.200
And if we are just, you know, running this query, maybe take a little bit of a shorter

05:51.200 --> 05:56.040
time window here, then we basically see two request counters, right?

05:56.040 --> 06:01.400
One is the green line which is counting the requests residing in HTTP status 200.

06:01.400 --> 06:03.320
So the successful requests.

06:03.320 --> 06:07.640
And basically we see that since I started the application on my laptop, I got about

06:07.640 --> 06:10.680
a little more than 400 successful requests.

06:10.680 --> 06:15.800
And the yellow line is, you know, requests residing in HTTP status 500 and we got around

06:15.800 --> 06:18.360
50 of them, right?

06:18.360 --> 06:23.520
And obviously raw counter values are not very useful, right?

06:23.520 --> 06:28.840
Nobody's interested in how often was my service called since I started the application.

06:28.840 --> 06:33.960
And the way, you know, metric monitoring works with Prometheus, as probably most of you know,

06:33.960 --> 06:39.000
is that you use the Prometheus query language to get some useful information out of that

06:39.000 --> 06:41.080
kind of data, right?

06:41.080 --> 06:46.760
And I guess most of you have run some Prometheus queries, but are still going to show maybe

06:46.760 --> 06:48.000
a couple of examples.

06:48.000 --> 06:54.080
So for those of you who are not very familiar with that, does this one work again?

06:54.080 --> 06:55.080
Hey, nice.

06:55.080 --> 06:56.080
It's even better.

06:56.080 --> 06:58.760
The lights work, the microphone works.

06:58.760 --> 06:59.760
Wow.

06:59.760 --> 07:03.400
Now let's hope the demo works.

07:03.400 --> 07:08.520
So I'm going to run just a couple of quick, you know, Prometheus queries so that for those

07:08.520 --> 07:12.360
of you who are not very familiar with it so that you get an idea what it is, right?

07:12.360 --> 07:19.000
And the most important function in the Prometheus query language is called the rate function.

07:19.000 --> 07:23.200
And what the rate function does, it takes a counter like this and a time interval like

07:23.200 --> 07:27.120
five minutes, and then it calculates a per second rate, right?

07:27.120 --> 07:33.000
So based on a five minute time interval, we now see that we have about 0.6 requests per

07:33.000 --> 07:39.720
second residing in HTTP status 200, and we have about 0.1 requests per second residing

07:39.720 --> 07:42.120
in HTTP status 500.

07:42.120 --> 07:45.760
And this is already quite some useful information, right?

07:45.760 --> 07:51.480
So typically you want to know the total load on your system, not by status code or something.

07:51.480 --> 07:54.440
So you basically want to sum these two values up.

07:54.440 --> 07:58.560
And obviously there's also a sum function to sum values up.

07:58.560 --> 08:03.400
And if you call that, you get the total load on your system, which is just one line now,

08:03.400 --> 08:08.480
and it's just, you know, about around 0.7 requests per second, right?

08:08.480 --> 08:12.400
And this is, yeah, this is basically how Prometheus queries work.

08:12.400 --> 08:16.680
If you're not familiar with the syntax, there's also kind of a graphical query builder where

08:16.680 --> 08:21.800
you can, you know, use a bit drag and drop and get a bit more help and so forth, right?

08:21.800 --> 08:25.440
And so eventually, you know, when you've got your queries and get your metrics, so what

08:25.440 --> 08:28.480
you want to do is you create a metrics dashboard.

08:28.480 --> 08:34.880
And for monitoring HTTP services, there are a couple of best practices, what type of data

08:34.880 --> 08:39.760
you want to visualize on a dashboard for monitoring HTTP services.

08:39.760 --> 08:44.840
And the most simple and straightforward thing is to visualize three things.

08:44.840 --> 08:51.000
One is the request rate, so for the current load on the system, which is exactly the query

08:51.000 --> 08:52.560
that we are seeing here.

08:52.560 --> 08:57.000
The next thing you want to see is the error rate, so the percentage of calls that fail.

08:57.000 --> 09:00.720
And the third thing is duration, how long does it take, right?

09:00.720 --> 09:05.840
And I created a simple example dashboard just to show you how this looks like.

09:05.840 --> 09:11.280
So I put the name of the service as a parameter up here so we can reuse the same dashboard

09:11.280 --> 09:14.560
for both services.

09:14.560 --> 09:18.500
Maybe let's use a 15-minute time window, so here I started the application.

09:18.500 --> 09:22.840
The first is the request rate, that's the exact same query that we just saw.

09:22.840 --> 09:27.360
Second thing here is the error rate, so we have about, I don't know, around 10 percent

09:27.360 --> 09:30.440
errors in my example application.

09:30.440 --> 09:35.160
And then for duration, there are a couple of different ways how to visualize that.

09:35.160 --> 09:40.460
So what we see here is basically the raw histogram, right, the histogram buckets.

09:40.460 --> 09:45.000
And this representation is actually quite useful because it shows you the shape of the

09:45.000 --> 09:46.380
distribution.

09:46.380 --> 09:54.160
So what we see here is two spikes, one around 600 milliseconds and one around 1.8 seconds.

09:54.160 --> 09:59.760
And this is a typical shape that you would see if your application uses a cache, right,

09:59.760 --> 10:03.360
because then you have a couple of requests that are responded quite quickly, those are

10:03.360 --> 10:08.640
the cache hits, couple of requests are slow that are the cache misses.

10:08.640 --> 10:13.320
And visualizing the shape of the histogram helps you understand kind of the latency behavior

10:13.320 --> 10:15.840
of your application, right.

10:15.840 --> 10:20.840
The other and most popular way to visualize durations is this one here.

10:20.840 --> 10:27.440
These are percentiles, so the green line is the 95th percentile, so it tells us 95 percent

10:27.440 --> 10:32.520
of the calls have been faster than 1.7 seconds and 5 percent slower than that.

10:32.520 --> 10:36.920
The yellow line is the 50th, so half of the calls faster than that, half of the calls

10:36.920 --> 10:38.160
slower than that.

10:38.160 --> 10:42.560
And this doesn't really tell you the shape of the distribution, but it shows you a development

10:42.560 --> 10:44.880
over time, which is useful as well.

10:44.880 --> 10:50.280
So if your service becomes slower, those lines will go up, right, and it's also a good indicator

10:50.280 --> 10:54.400
if you want to do, you know, alerting and so forth, you can define a threshold and say

10:54.400 --> 11:00.000
it's above, if it's above a certain threshold, I want to be notified and stuff like that.

11:00.000 --> 11:05.440
And there are other more, you know, experimental things like this, heat maps showing basically

11:05.440 --> 11:08.280
development of histograms over time and stuff like that.

11:08.280 --> 11:14.040
So it's pretty cool to play with all the different visualizations in Grafana and, you know, see

11:14.040 --> 11:16.120
what you can get.

11:16.120 --> 11:22.120
So this is a, you know, quick example of a so-called red dashboard, request rates, error

11:22.120 --> 11:25.720
rates, duration based on open telemetry data.

11:25.720 --> 11:32.760
And the cool thing about it is that it actually, all that we are seeing here is just based

11:32.760 --> 11:37.120
on that single histogram metric HTTP server duration.

11:37.120 --> 11:41.720
And the fact that this metric is there is not a coincidence.

11:41.720 --> 11:46.760
The metric HTTP server duration is actually defined in the open telemetry standard as

11:46.760 --> 11:51.300
part of the semantic conventions for HTTP services.

11:51.300 --> 11:58.400
So whenever you monitor an HTTP server with open telemetry, then you will find a histogram

11:58.400 --> 12:00.680
named HTTP server duration.

12:00.680 --> 12:03.800
It will have the HTTP status as an attribute.

12:03.800 --> 12:07.560
It will contain the latencies in milliseconds.

12:07.560 --> 12:08.840
That's all part of the standard.

12:08.840 --> 12:15.500
So it doesn't matter what programming language your services uses, what framework, whatever.

12:15.500 --> 12:20.060
If it's being monitored with open telemetry and it's compatible, you will find that metric

12:20.060 --> 12:22.720
and you can create a similar dashboard like that.

12:22.720 --> 12:28.200
And this is kind of one of the things that make application monitoring with open telemetry

12:28.200 --> 12:32.160
a lot easier than it used to be before these standardization.

12:32.160 --> 12:33.160
Right?

12:33.160 --> 12:34.460
Good.

12:34.460 --> 12:37.900
So that was a quick look at metrics.

12:37.900 --> 12:41.200
But of course we want to look at the other signals as well.

12:41.200 --> 12:46.360
So let's switch data sources for now and have a look at traces.

12:46.360 --> 12:53.800
So tracing, again, there's a kind of search, like graphical search where you can create

12:53.800 --> 12:56.120
your search criteria with drag and drop.

12:56.120 --> 13:00.080
There's a relatively new feature, which is a query language for traces.

13:00.080 --> 13:02.400
So I'm going to use that for now.

13:02.400 --> 13:07.040
And one thing you can do is to just search by labels.

13:07.040 --> 13:13.880
So I can, for example, say I'm interested in the service name greeting service.

13:13.880 --> 13:18.140
And then I could basically just open a random trace here.

13:18.140 --> 13:20.760
Let's take this as an example.

13:20.760 --> 13:28.160
Can I need to zoom out a little bit to be able to close the search window here?

13:28.160 --> 13:29.160
Okay.

13:29.160 --> 13:33.440
So this is how a distributed trace looks like.

13:33.440 --> 13:38.160
And if you see it for the first time, it might be a bit hard to understand, but it's actually

13:38.160 --> 13:39.160
fairly easy.

13:39.160 --> 13:43.840
So you just need like two minutes of introduction and then you will understand traces forever.

13:43.840 --> 13:47.780
And to give you that introduction, I actually have one more slide.

13:47.780 --> 13:51.600
So just to help you understand what we are seeing here.

13:51.600 --> 13:56.560
And the thing is distributed traces consist of spans.

13:56.560 --> 13:58.200
And spans are time spans.

13:58.200 --> 14:02.200
So a span is something that has a point in time where it starts and a point in time where

14:02.200 --> 14:04.080
it ends.

14:04.080 --> 14:08.280
And in open telemetry, there are three different kinds of spans.

14:08.280 --> 14:09.880
One are server spans.

14:09.880 --> 14:14.720
The second is internal spans and the third is client spans.

14:14.720 --> 14:18.960
So what happens when my Hello World application receives a request?

14:18.960 --> 14:23.920
So the first thing that happens if a server receives a request, a server span is created.

14:23.920 --> 14:25.960
So that's the first slide here.

14:25.960 --> 14:28.840
It's started as soon as the request is received.

14:28.840 --> 14:33.400
It remains open until the request is responded.

14:33.400 --> 14:40.640
Then I said in the introduction that I used Spring Boot for implementing the example application.

14:40.640 --> 14:45.280
And the way Spring Boot works is that it takes the request and passes it to the corresponding

14:45.280 --> 14:48.520
Spring Controller that would handle the request.

14:48.520 --> 14:54.280
And open telemetry's Java instrumentation agent is nice for Java developers because

14:54.280 --> 14:58.560
it just creates internal spans for each Spring Controller that is involved.

14:58.560 --> 15:01.000
And that is the second line that we are seeing here.

15:01.000 --> 15:06.360
It's basically opened as soon as the Spring Controller takes over and remains open until

15:06.360 --> 15:12.000
the Spring Controller is done by handling the request, which might seem not too useful

15:12.000 --> 15:14.960
if I have just a single Spring Controller anyway.

15:14.960 --> 15:20.120
But if you have kind of a larger application and you have multiple controllers involved,

15:20.120 --> 15:24.480
it gives you quite some interesting insights into what's happening inside your application.

15:24.480 --> 15:30.440
You would see immediately which controller do I spend most time in and so forth.

15:30.440 --> 15:36.240
And then eventually my Hello World application reaches out to the greeting service and outgoing

15:36.240 --> 15:38.640
requests are represented by client spans.

15:38.640 --> 15:43.920
So the client span is basically opened as soon as my HTTP request goes out and remains

15:43.920 --> 15:46.120
open until the response is received.

15:46.120 --> 15:50.280
And then in the greeting service the same thing starts again.

15:50.280 --> 15:53.800
Request is received which creates a server span and then I have a Spring Controller as

15:53.800 --> 15:58.720
well which is an internal span and that's the end of my distributed application here.

15:58.720 --> 16:01.520
And this is exactly what we are seeing here.

16:01.520 --> 16:06.000
And each of those span types has a corresponding metadata attached to it.

16:06.000 --> 16:11.200
So if you look at one of the internal spans here we see the name of the Spring Controller

16:11.200 --> 16:17.360
and the name of the controller method and a couple of JVM related attributes, whatever.

16:17.360 --> 16:21.760
And if you look at an HTTP span for example we see of course HTTP attributes like the

16:21.760 --> 16:25.400
status code method and so forth.

16:25.400 --> 16:32.280
So of course you do not want to just look at random spans so usually you are looking

16:32.280 --> 16:33.280
for something.

16:33.280 --> 16:37.560
There are standard attributes in OpenTelemetry that you can use for searching.

16:37.560 --> 16:43.160
So we already had the service name, greeting service for example.

16:43.160 --> 16:49.840
But that's the most important or one of the most important attributes is HTTP.status

16:49.840 --> 16:53.400
code, this one here.

16:53.400 --> 16:59.320
And if we for example search for spans with HTTP status code 500 then we should find an

16:59.320 --> 17:01.880
example of a request that failed.

17:01.880 --> 17:04.280
So let's close the search window again.

17:04.280 --> 17:06.760
Yes, that's an example of a failed request.

17:06.760 --> 17:12.360
You see it with the indicated by those red exclamation marks at the bottom here.

17:12.360 --> 17:15.160
So this is where the thing failed, right?

17:15.160 --> 17:20.080
So the root cause of the error is the internal span, something in my Spring Controller in

17:20.080 --> 17:21.520
the greeting service.

17:21.520 --> 17:27.120
If I look at the metadata attached to that I actually see that the instrumentation attached

17:27.120 --> 17:32.060
the event that caused the error and this even includes the stack trace.

17:32.060 --> 17:37.120
So you can basically immediately navigate to the exact line of code that is the root

17:37.120 --> 17:39.920
cause of this error, right?

17:39.920 --> 17:41.240
And this is quite cool.

17:41.240 --> 17:47.240
So if you have a distributed application and you get an unexpected response from your Hello

17:47.240 --> 17:53.640
World application without distributed tracing it's pretty hard to find that actually there's

17:53.640 --> 17:58.520
an exception in the greeting service that propagated through your distributed landscape

17:58.520 --> 18:01.840
and then eventually caused the unexpected response.

18:01.840 --> 18:06.640
And with distributed tracing finding these kind of things becomes pretty easy because

18:06.640 --> 18:12.400
you get all the related calls grouped together, you get the failed ones marked with an exclamation

18:12.400 --> 18:17.960
mark and it can pretty easily navigate to what's the root cause of your error.

18:17.960 --> 18:18.960
Okay?

18:18.960 --> 18:19.960
Cool.

18:19.960 --> 18:23.000
So that was a quick look at traces.

18:23.000 --> 18:26.000
There are a lot of interesting things about tracing.

18:26.000 --> 18:30.240
Maybe one thing I would like to show you because I find it particularly cool.

18:30.240 --> 18:36.320
So if you have all your services instrumented with tracing in your back end then basically

18:36.320 --> 18:41.840
those traces give you metadata about all the network calls happening in your system.

18:41.840 --> 18:44.920
And you can do something with that type of data, right?

18:44.920 --> 18:50.040
So for example, you can calculate something that we call the service graph.

18:50.040 --> 18:51.480
So it looks like this.

18:51.480 --> 18:56.800
It's maybe not too impressive if you just have two services calling each other, right?

18:56.800 --> 19:02.200
But if you imagine more larger, dozens or hundreds of services, so it will generate

19:02.200 --> 19:07.000
a map of all the services and indicate which service calls which other service.

19:07.000 --> 19:12.640
And this is quite useful, for example, if you intend to deploy a breaking change in

19:12.640 --> 19:16.520
your greeting service and you want to know who's using the greeting service, what would

19:16.520 --> 19:17.520
I break?

19:17.520 --> 19:22.280
Then looking at the service graph you basically get this information right away.

19:22.280 --> 19:26.280
Traditionally if you don't have that, you basically have a PDF with your architecture

19:26.280 --> 19:28.600
diagram and then you look it up there.

19:28.600 --> 19:33.520
And also traditionally there's at least one team that deployed something and forgot to

19:33.520 --> 19:36.280
update the diagram and then you missed that.

19:36.280 --> 19:38.280
And it's a service graph that won't happen, right?

19:38.280 --> 19:39.760
This is the actual truth.

19:39.760 --> 19:44.040
This is based on what's actually happening in your backend and this is pretty useful

19:44.040 --> 19:46.460
in these situations, right?

19:46.460 --> 19:51.600
And you can do other things as well like, you know, have some statistics like the most

19:51.600 --> 19:56.480
frequently called endpoint or the endpoint with the most errors and stuff like that.

19:56.480 --> 20:00.600
So that was a quick look at traces.

20:00.600 --> 20:04.240
So we covered metrics, we covered traces.

20:04.240 --> 20:08.960
One thing I want to show you is that metrics and traces are actually related to each other,

20:08.960 --> 20:09.960
right?

20:09.960 --> 20:15.640
And so in order to show that I'm going to go back to our dashboard because if you, let's

20:15.640 --> 20:19.360
take a 15 minute window, then we get a bit more examples.

20:19.360 --> 20:24.560
So if you look at the latency data here, you notice these little green dots.

20:24.560 --> 20:29.780
These are called exemplars and this is something that's provided by the auto instrumentation

20:29.780 --> 20:31.240
of OpenTelemetry.

20:31.240 --> 20:38.640
So whenever it generates latency data, it basically attaches trace IDs of example traces

20:38.640 --> 20:40.460
to the latency data.

20:40.460 --> 20:43.480
And this is visualized by these little green dots, right?

20:43.480 --> 20:47.960
And so you see some examples of particularly fast calls, some examples of particularly

20:47.960 --> 20:49.480
slow calls and so forth.

20:49.480 --> 20:54.440
And if you, for example, take this dot up here, which is kind of slower than anything

20:54.440 --> 20:57.120
else, it's almost two seconds, right?

20:57.120 --> 21:02.120
And you have the trace ID here and you can navigate to tempo and have a look at the trace

21:02.120 --> 21:08.360
and start figuring out why did I have an example of such a slow call in my system, right?

21:08.360 --> 21:12.960
And in that case, you would immediately see that most of the time spent in the greeting

21:12.960 --> 21:13.960
service.

21:13.960 --> 21:18.040
So if you're looking for the performance bottleneck, then this is the most likely thing.

21:18.040 --> 21:20.800
Yeah, four minutes, that's fine.

21:20.800 --> 21:21.800
Good.

21:21.800 --> 21:27.920
So if I have four minutes, it's high time to jump to logs, the third signal that we

21:27.920 --> 21:29.000
didn't look at yet.

21:29.000 --> 21:36.520
So let's select Loki or open source logs database as a data source.

21:36.520 --> 21:41.200
So again, there's a query language, there's a graphical query builder and so forth.

21:41.200 --> 21:46.880
So let's just open random logs coming from the greeting service looks a bit like this.

21:46.880 --> 21:51.400
So it's even I don't know, I didn't even log anything explicitly.

21:51.400 --> 21:56.040
I just turned on some whatever spring request logging so that I get some log data.

21:56.040 --> 22:00.400
And from time to time, I throw an exception, which is an IO exception to simulate these

22:00.400 --> 22:01.400
errors.

22:01.400 --> 22:06.840
It looks a bit broken, but that's just because of the resolution that I have here.

22:06.840 --> 22:12.200
Yeah, so what you can do, of course, you can do some full text search, for example, can

22:12.200 --> 22:23.360
say, I'm interested in these IO exception, and then you would basically get what if you

22:23.360 --> 22:30.880
spell it correctly, like that, then you would get the list of all IO exceptions, which in

22:30.880 --> 22:34.040
my case are just the random errors I'm throwing here.

22:34.040 --> 22:36.120
And this query language is actually quite powerful.

22:36.120 --> 22:40.520
So you can, this is kind of filtering by a label and filtering by full text search, but

22:40.520 --> 22:42.680
you can do totally different things as well.

22:42.680 --> 22:47.720
For example, you can have queries that, you know, derive metrics based from on log data.

22:47.720 --> 22:52.360
There's a function pretty similar to what we have seen in the in the metrics demo, which

22:52.360 --> 22:53.960
is called the rate function.

22:53.960 --> 23:00.840
So the rate function again, takes a time interval, and then calculates the per second increase

23:00.840 --> 23:01.840
rate.

23:01.840 --> 23:08.560
So it basically tells you that we have almost 0.1 of these IO exceptions per second in our

23:08.560 --> 23:14.320
log data, which is also kind of useful for information to have.

23:14.320 --> 23:21.880
And the last thing to show you, because that's particularly interesting, is that these logs

23:21.880 --> 23:25.960
and traces and metrics are again, you know, not independent of each other.

23:25.960 --> 23:28.080
They are related to each other.

23:28.080 --> 23:33.080
And so if you look at an example here, just let's open a random log line.

23:33.080 --> 23:36.280
So what we see here, there's a trace ID.

23:36.280 --> 23:37.280
And this is interesting.

23:37.280 --> 23:40.080
So how does a trace ID end up in my log line?

23:40.080 --> 23:46.560
So this is actually also a feature of the Java instrumentation that's provided by the

23:46.560 --> 23:48.320
OpenTelemetry community.

23:48.320 --> 23:54.360
So the way logging in general works in Java is that there's a global thing with key value

23:54.360 --> 23:56.720
pairs called the log context.

23:56.720 --> 24:00.520
And applications can put arbitrary key value pairs into that context.

24:00.520 --> 24:04.640
And when you configure your log format, you can define which of those values you want

24:04.640 --> 24:06.640
to include in your log data.

24:06.640 --> 24:12.600
And if you have this OpenTelemetry agent attached, then as soon as a log line is written in the

24:12.600 --> 24:17.840
context of serving an HTTP request, then the corresponding trace ID is put into that log

24:17.840 --> 24:18.880
context.

24:18.880 --> 24:23.240
And you can configure your log format to include the trace ID in your log data.

24:23.240 --> 24:24.240
And that's what I did.

24:24.240 --> 24:27.280
And so each of my log lines actually has a trace ID.

24:27.280 --> 24:31.960
And so if I see something fancy and I want to know maybe somewhere down my distributed

24:31.960 --> 24:36.600
stack something went wrong, I can just query that in tempo, navigate to the corresponding

24:36.600 --> 24:37.600
trace.

24:37.600 --> 24:40.480
And I close that here.

24:40.480 --> 24:43.800
And then basically maybe get some information what happened.

24:43.800 --> 24:46.960
And then same navigation works the other way around as well.

24:46.960 --> 24:50.000
So of course there's a little log button here.

24:50.000 --> 24:55.160
So if I see something fancy going on in my greeting service thing here and maybe the

24:55.160 --> 25:00.000
logs have more information, I can click on that, navigate to the logs.

25:00.000 --> 25:02.280
And then it basically just generates a query.

25:02.280 --> 25:06.120
I click on the greeting service with that trace ID.

25:06.120 --> 25:09.600
So it's basically just a full text search for that trace ID.

25:09.600 --> 25:12.600
And so I will find all my corresponding log lines.

25:12.600 --> 25:13.920
In that case just one line.

25:13.920 --> 25:17.960
But if you have a bit better logging, then maybe it would give you some indication what

25:17.960 --> 25:19.400
happened there.

25:19.400 --> 25:20.360
OK.

25:20.360 --> 25:28.120
So that was a very quick 25 minutes overview of looking a bit into metrics, looking a bit

25:28.120 --> 25:30.600
into tracing, looking a bit into logs.

25:30.600 --> 25:36.000
I hope it gave you some impression, what's the type of data that you get out of open

25:36.000 --> 25:38.680
telemetry looks like.

25:38.680 --> 25:42.560
All of what we did is really without even modifying the application.

25:42.560 --> 25:47.240
I didn't even start with custom metrics, custom traces, and so forth.

25:47.240 --> 25:51.420
So but it's already quite some useful data that we get out of that.

25:51.420 --> 25:57.360
If you like the demo, if you want to explore it a bit more, want to try it at home, I pushed

25:57.360 --> 26:01.480
it on my GitHub and there's a readme telling you how to run it.

26:01.480 --> 26:03.480
So you can do that.

26:03.480 --> 26:10.400
And yeah, next up we have a talk that goes a bit more in detail into the tracing part

26:10.400 --> 26:11.400
of this.

26:11.400 --> 26:15.920
And then after that we have a talk that goes a bit more into detail how to run open telemetry

26:15.920 --> 26:16.920
in Kubernetes.

26:16.920 --> 26:25.080
So stay here and thanks for listening.

26:25.080 --> 26:27.360
Please remain seated during Q&A.

26:27.360 --> 26:29.200
Otherwise we can't do a real Q&A.

26:29.200 --> 26:31.320
So please remain seated.

26:31.320 --> 26:32.800
Order any questions.

26:32.800 --> 26:33.800
Yes.

26:33.800 --> 26:34.800
Hi.

26:34.800 --> 26:45.760
Thank you for this.

26:45.760 --> 26:46.760
One quick question.

26:46.760 --> 26:52.920
You mentioned you just need to add some parameters to the Java virtual machine to run the telemetry.

26:52.920 --> 26:59.360
What happens to my application if, for example, the back end of the telemetry is down?

26:59.360 --> 27:03.040
Is my application failing or impacted in any way?

27:03.040 --> 27:04.920
If the monitoring back end is down.

27:04.920 --> 27:05.920
Yes.

27:05.920 --> 27:09.740
Say the monitoring is down but I started my application with these parameters.

27:09.740 --> 27:11.800
Is it impacting the application?

27:11.800 --> 27:12.800
No.

27:12.800 --> 27:17.680
I mean, you won't see metrics, of course, if you're monitoring back end is down but

27:17.680 --> 27:20.160
the application would just continue running.

27:20.160 --> 27:27.560
So typically in production setups the applications wouldn't send telemetry data directly to the

27:27.560 --> 27:29.000
monitoring back end.

27:29.000 --> 27:32.280
But what you usually have is something in the middle.

27:32.280 --> 27:33.800
There's alternatives.

27:33.800 --> 27:36.480
There's the Grafana agent that you can use for that.

27:36.480 --> 27:39.340
There's the open telemetry collector that you can use for that.

27:39.340 --> 27:45.520
And it's basically a thing that runs close to the application, takes the telemetry data

27:45.520 --> 27:52.040
off the application very quickly and then can buffer stuff and process stuff and send

27:52.040 --> 27:53.880
it over to the monitoring back end.

27:53.880 --> 27:56.940
That's used for decoupling that a little bit.

27:56.940 --> 28:01.600
If you have such an architect for the application, shouldn't be affected at all by that.

28:01.600 --> 28:03.640
Two more questions.

28:03.640 --> 28:04.640
Yeah.

28:04.640 --> 28:05.640
Two more.

28:05.640 --> 28:13.180
So I really like being able to link from your metrics to traces.

28:13.180 --> 28:19.400
But what I'm actually really curious to be able to do, and as far as I know doesn't exist

28:19.400 --> 28:23.760
or I guess that's my question is like is there any thought towards doing this, is being able

28:23.760 --> 28:31.320
to go the other direction where what I'd like to be able to answer is here's all my trace

28:31.320 --> 28:36.320
data and this node of the trace incremented these counters by this much.

28:36.320 --> 28:43.640
So I could ask things like how much network IO or disk IOPS did this complete request

28:43.640 --> 28:47.480
do and where in the tree would that occur?

28:47.480 --> 28:48.480
Yeah.

28:48.480 --> 28:49.480
That's a good question.

28:49.480 --> 29:00.200
So thinking from traces to metrics is not so straightforward because I think the things

29:00.200 --> 29:03.220
you can do to relate this is to use the service name.

29:03.220 --> 29:08.880
So if you have the service name part of your resource attributes of the metrics and consistently

29:08.880 --> 29:13.840
you have the same service name in your trace data, then you can at least navigate to all

29:13.840 --> 29:18.580
traces coming to all metrics coming from the same service.

29:18.580 --> 29:24.240
Maybe you have some more related attributes like whatever instance ID and so forth.

29:24.240 --> 29:27.000
But it's not really a one to one relationship.

29:27.000 --> 29:28.000
Yeah.

29:28.000 --> 29:37.000
No, I don't think that's possible.

29:37.000 --> 29:44.760
So in this example you've shown that Grafana world works great with server side applications.

29:44.760 --> 29:52.920
Have you had examples of client side applications, mobile, desktop applications that use the

29:52.920 --> 30:01.720
me field metrics and then ship their metrics and traces to the metric backend?

30:01.720 --> 30:04.400
So did I hear it correctly?

30:04.400 --> 30:09.320
You're asking about starting your traces on the client side in the web browser and stuff?

30:09.320 --> 30:10.320
Yeah.

30:10.320 --> 30:14.880
So what do you have tracing on the server side, but what about having traces and metrics

30:14.880 --> 30:19.040
on the client side in, for example, for an embedded or mobile application so that you

30:19.040 --> 30:26.160
could actually see the trace from when the customer clicked a thing and see the full

30:26.160 --> 30:27.240
customer journey?

30:27.240 --> 30:29.120
That's a great question.

30:29.120 --> 30:33.800
That's actually an area where there's currently a lot of research and new projects and so

30:33.800 --> 30:34.800
forth.

30:34.800 --> 30:42.160
There's a group called real user monitoring, that deal with client side applications.

30:42.160 --> 30:50.560
There's also a project by Grafana, it's kind of JavaScript that you can include in your

30:50.560 --> 30:58.240
front end, in your HTML page and it gives you traces and metrics coming from the web

30:58.240 --> 30:59.440
browser.

30:59.440 --> 31:01.720
And this is currently a pretty active area.

31:01.720 --> 31:05.720
So lots of movement there.

31:05.720 --> 31:07.920
So there are things to explore.

31:07.920 --> 31:14.360
If you like, check out Faro, that's a nice new project and standardization is also currently

31:14.360 --> 31:21.160
being discussed, but it's newer than the rest of what I showed you, right?

31:21.160 --> 31:25.400
So there's no clear standard yet or nothing decided yet.

31:25.400 --> 31:26.400
Cool.

31:26.400 --> 31:27.400
Okay.

31:27.400 --> 31:28.400
Thanks everyone again.

31:28.400 --> 31:31.940
Thank you.

31:31.940 --> 31:35.740
Thank you.
