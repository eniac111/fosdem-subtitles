1
0:00:00.000 --> 0:00:01.000
Okay.

2
0:00:01.000 --> 0:00:02.000
Hello, everyone.

3
0:00:02.000 --> 0:00:14.920
Welcome to the talk on open telemetry with Grafana.

4
0:00:14.920 --> 0:00:17.320
Microphone broke, so I need to do it with this microphone now.

5
0:00:17.320 --> 0:00:21.440
Let's see how it goes with typing and live demo.

6
0:00:21.440 --> 0:00:22.440
Few words about me.

7
0:00:22.440 --> 0:00:23.960
So who am I?

8
0:00:23.960 --> 0:00:27.600
Why am I here talking about Grafana and open telemetry?

9
0:00:27.600 --> 0:00:30.320
So I work at Grafana Labs.

10
0:00:30.320 --> 0:00:35.300
I'm an engineering manager and also manager for our open telemetry squad.

11
0:00:35.300 --> 0:00:39.640
And I'm also active in open source, so I'm a member of the Prometheus team where I maintain

12
0:00:39.640 --> 0:00:42.520
the Java metrics library.

13
0:00:42.520 --> 0:00:46.880
So what are we going to do in this talk in the next 25 minutes or so?

14
0:00:46.880 --> 0:00:50.320
So it will almost exclusively be a live demo.

15
0:00:50.320 --> 0:00:55.540
So basically the idea is I have a little example application running on my laptop, and it is

16
0:00:55.540 --> 0:00:57.440
instrumented with open telemetry.

17
0:00:57.440 --> 0:01:01.480
I will show you in a minute what it does and how I instrumented it.

18
0:01:01.480 --> 0:01:05.400
And I also have an open source monitoring backend running.

19
0:01:05.400 --> 0:01:08.840
It consists of three databases.

20
0:01:08.840 --> 0:01:12.760
One is Loki, which is an open source logs database.

21
0:01:12.760 --> 0:01:16.220
One is Temple, which is an open source trace database.

22
0:01:16.220 --> 0:01:19.540
And one is Mimir, which is an open source metrics database.

23
0:01:19.540 --> 0:01:25.660
So Mimir is compatible with Prometheus, so it could have shown the exact same demo using

24
0:01:25.660 --> 0:01:30.080
Prometheus instead of Mimir, so it doesn't really matter for now.

25
0:01:30.080 --> 0:01:31.840
And of course I also have Grafana.

26
0:01:31.840 --> 0:01:35.180
I have those databases configured as data sources.

27
0:01:35.180 --> 0:01:40.000
And what we are going to do, we are going to start up Grafana, have a look at metrics,

28
0:01:40.000 --> 0:01:42.200
have a look at traces, have a look at logs.

29
0:01:42.200 --> 0:01:46.600
And basically the idea is that at the end of the talk you kind of have seen all the

30
0:01:46.600 --> 0:01:51.920
signals that come out of open telemetry, explored a bit what you can do with this type of data.

31
0:01:51.920 --> 0:01:56.680
And so you should have a good overview how open source monitoring with open telemetry

32
0:01:56.680 --> 0:01:59.360
looks like.

33
0:01:59.360 --> 0:02:02.800
So last slide before we jump into the live demo.

34
0:02:02.800 --> 0:02:08.000
So this is just a quick overview of what the example application does so that you know

35
0:02:08.000 --> 0:02:09.800
what we are going to look at.

36
0:02:09.800 --> 0:02:16.200
It's a simple hello world REST service written in Java using Spring Boot.

37
0:02:16.200 --> 0:02:22.120
And so basically you can send a request to port 8080 and it will respond with hello world.

38
0:02:22.120 --> 0:02:26.880
And in order to make it a bit more interesting, I made it a distributed hello world service.

39
0:02:26.880 --> 0:02:31.680
So it doesn't respond directly, but when it receives a request, it reaches out to a greeting

40
0:02:31.680 --> 0:02:34.280
service running on port 8081.

41
0:02:34.280 --> 0:02:38.360
The greeting service responds with the greeting, which is hello world, and then the response

42
0:02:38.360 --> 0:02:40.880
is forwarded to the client.

43
0:02:40.880 --> 0:02:44.400
And there are random errors to have some error rates as well.

44
0:02:44.400 --> 0:02:50.320
So basically a hello world microservice architecture or whatever.

45
0:02:50.320 --> 0:02:55.740
And in order to instrument this with open telemetry, I used the Java instrumentation

46
0:02:55.740 --> 0:02:58.760
agent that's provided by the open telemetry community.

47
0:02:58.760 --> 0:03:00.900
That's something you can download on GitHub.

48
0:03:00.900 --> 0:03:06.240
And the thing is this thing, you basically attach it to the Java virtual machine at start-up

49
0:03:06.240 --> 0:03:08.880
time with a special command line parameter.

50
0:03:08.880 --> 0:03:10.920
So I didn't modify any source code.

51
0:03:10.920 --> 0:03:14.600
I didn't use any SDK or introduce any custom stuff.

52
0:03:14.600 --> 0:03:20.680
All we are going to look at in this demo is just data produced by attaching the open telemetry

53
0:03:20.680 --> 0:03:24.640
instrumentation to a standard Spring Boot application.

54
0:03:24.640 --> 0:03:25.640
Right?

55
0:03:25.640 --> 0:03:26.640
Cool.

56
0:03:26.640 --> 0:03:29.200
So let's get started.

57
0:03:29.200 --> 0:03:31.720
As said, I have my data sources configured here.

58
0:03:31.720 --> 0:03:38.720
So Prometheus and MIMIR are compatible, so it doesn't really matter which one we choose.

59
0:03:38.720 --> 0:03:42.080
There are a lot of, so I want to start with metrics.

60
0:03:42.080 --> 0:03:43.960
And yeah, Sarah.

61
0:03:43.960 --> 0:03:54.200
Can we turn the lights down a bit?

62
0:03:54.200 --> 0:03:58.920
I don't know.

63
0:03:58.920 --> 0:04:02.080
Okay.

64
0:04:02.080 --> 0:04:09.240
Maybe the other way around.

65
0:04:09.240 --> 0:04:12.000
Okay.

66
0:04:12.000 --> 0:04:13.000
I will just continue.

67
0:04:13.000 --> 0:04:14.000
Come on.

68
0:04:14.000 --> 0:04:19.200
So there are lots of metrics that you get from the open telemetry instrumentation.

69
0:04:19.200 --> 0:04:24.800
So kind of JVM related stuff like garbage collection activity and so forth.

70
0:04:24.800 --> 0:04:29.040
But the one I want to look at, oh, no.

71
0:04:29.040 --> 0:04:33.480
It's getting brighter and brighter.

72
0:04:33.480 --> 0:04:37.160
Yay.

73
0:04:37.160 --> 0:04:40.200
Okay.

74
0:04:40.200 --> 0:04:44.720
Great.

75
0:04:44.720 --> 0:04:47.680
I think there is also a light mode in Grafana.

76
0:04:47.680 --> 0:04:51.720
Maybe that would have been a better choice.

77
0:04:51.720 --> 0:04:56.520
But no, I'm not going to use light mode.

78
0:04:56.520 --> 0:05:01.160
So let's figure out how to do the demo while I have a microphone that I should hold in

79
0:05:01.160 --> 0:05:02.160
my hands.

80
0:05:02.160 --> 0:05:04.400
Let's just put it here.

81
0:05:04.400 --> 0:05:05.400
So okay.

82
0:05:05.400 --> 0:05:06.400
Thank you.

83
0:05:06.400 --> 0:05:07.400
Cool.

84
0:05:07.400 --> 0:05:19.960
So the metric we are going to look at for the demo, it's a metric named HTTP server duration.

85
0:05:19.960 --> 0:05:22.480
This is a metric of type histogram.

86
0:05:22.480 --> 0:05:25.640
So histograms have a couple of different numbers attached to them.

87
0:05:25.640 --> 0:05:30.360
So there are histogram buckets with the distribution data and so forth.

88
0:05:30.360 --> 0:05:31.880
And there's also a count.

89
0:05:31.880 --> 0:05:33.360
The count is the most simple one.

90
0:05:33.360 --> 0:05:37.800
So we are going to use this in our example.

91
0:05:37.800 --> 0:05:39.240
I actually got it two times.

92
0:05:39.240 --> 0:05:43.580
I got it once for my greeting service here and once for the hello world application.

93
0:05:43.580 --> 0:05:51.200
And if we are just, you know, running this query, maybe take a little bit of a shorter

94
0:05:51.200 --> 0:05:56.040
time window here, then we basically see two request counters, right?

95
0:05:56.040 --> 0:06:01.400
One is the green line which is counting the requests residing in HTTP status 200.

96
0:06:01.400 --> 0:06:03.320
So the successful requests.

97
0:06:03.320 --> 0:06:07.640
And basically we see that since I started the application on my laptop, I got about

98
0:06:07.640 --> 0:06:10.680
a little more than 400 successful requests.

99
0:06:10.680 --> 0:06:15.800
And the yellow line is, you know, requests residing in HTTP status 500 and we got around

100
0:06:15.800 --> 0:06:18.360
50 of them, right?

101
0:06:18.360 --> 0:06:23.520
And obviously raw counter values are not very useful, right?

102
0:06:23.520 --> 0:06:28.840
Nobody's interested in how often was my service called since I started the application.

103
0:06:28.840 --> 0:06:33.960
And the way, you know, metric monitoring works with Prometheus, as probably most of you know,

104
0:06:33.960 --> 0:06:39.000
is that you use the Prometheus query language to get some useful information out of that

105
0:06:39.000 --> 0:06:41.080
kind of data, right?

106
0:06:41.080 --> 0:06:46.760
And I guess most of you have run some Prometheus queries, but are still going to show maybe

107
0:06:46.760 --> 0:06:48.000
a couple of examples.

108
0:06:48.000 --> 0:06:54.080
So for those of you who are not very familiar with that, does this one work again?

109
0:06:54.080 --> 0:06:55.080
Hey, nice.

110
0:06:55.080 --> 0:06:56.080
It's even better.

111
0:06:56.080 --> 0:06:58.760
The lights work, the microphone works.

112
0:06:58.760 --> 0:06:59.760
Wow.

113
0:06:59.760 --> 0:07:03.400
Now let's hope the demo works.

114
0:07:03.400 --> 0:07:08.520
So I'm going to run just a couple of quick, you know, Prometheus queries so that for those

115
0:07:08.520 --> 0:07:12.360
of you who are not very familiar with it so that you get an idea what it is, right?

116
0:07:12.360 --> 0:07:19.000
And the most important function in the Prometheus query language is called the rate function.

117
0:07:19.000 --> 0:07:23.200
And what the rate function does, it takes a counter like this and a time interval like

118
0:07:23.200 --> 0:07:27.120
five minutes, and then it calculates a per second rate, right?

119
0:07:27.120 --> 0:07:33.000
So based on a five minute time interval, we now see that we have about 0.6 requests per

120
0:07:33.000 --> 0:07:39.720
second residing in HTTP status 200, and we have about 0.1 requests per second residing

121
0:07:39.720 --> 0:07:42.120
in HTTP status 500.

122
0:07:42.120 --> 0:07:45.760
And this is already quite some useful information, right?

123
0:07:45.760 --> 0:07:51.480
So typically you want to know the total load on your system, not by status code or something.

124
0:07:51.480 --> 0:07:54.440
So you basically want to sum these two values up.

125
0:07:54.440 --> 0:07:58.560
And obviously there's also a sum function to sum values up.

126
0:07:58.560 --> 0:08:03.400
And if you call that, you get the total load on your system, which is just one line now,

127
0:08:03.400 --> 0:08:08.480
and it's just, you know, about around 0.7 requests per second, right?

128
0:08:08.480 --> 0:08:12.400
And this is, yeah, this is basically how Prometheus queries work.

129
0:08:12.400 --> 0:08:16.680
If you're not familiar with the syntax, there's also kind of a graphical query builder where

130
0:08:16.680 --> 0:08:21.800
you can, you know, use a bit drag and drop and get a bit more help and so forth, right?

131
0:08:21.800 --> 0:08:25.440
And so eventually, you know, when you've got your queries and get your metrics, so what

132
0:08:25.440 --> 0:08:28.480
you want to do is you create a metrics dashboard.

133
0:08:28.480 --> 0:08:34.880
And for monitoring HTTP services, there are a couple of best practices, what type of data

134
0:08:34.880 --> 0:08:39.760
you want to visualize on a dashboard for monitoring HTTP services.

135
0:08:39.760 --> 0:08:44.840
And the most simple and straightforward thing is to visualize three things.

136
0:08:44.840 --> 0:08:51.000
One is the request rate, so for the current load on the system, which is exactly the query

137
0:08:51.000 --> 0:08:52.560
that we are seeing here.

138
0:08:52.560 --> 0:08:57.000
The next thing you want to see is the error rate, so the percentage of calls that fail.

139
0:08:57.000 --> 0:09:00.720
And the third thing is duration, how long does it take, right?

140
0:09:00.720 --> 0:09:05.840
And I created a simple example dashboard just to show you how this looks like.

141
0:09:05.840 --> 0:09:11.280
So I put the name of the service as a parameter up here so we can reuse the same dashboard

142
0:09:11.280 --> 0:09:14.560
for both services.

143
0:09:14.560 --> 0:09:18.500
Maybe let's use a 15-minute time window, so here I started the application.

144
0:09:18.500 --> 0:09:22.840
The first is the request rate, that's the exact same query that we just saw.

145
0:09:22.840 --> 0:09:27.360
Second thing here is the error rate, so we have about, I don't know, around 10 percent

146
0:09:27.360 --> 0:09:30.440
errors in my example application.

147
0:09:30.440 --> 0:09:35.160
And then for duration, there are a couple of different ways how to visualize that.

148
0:09:35.160 --> 0:09:40.460
So what we see here is basically the raw histogram, right, the histogram buckets.

149
0:09:40.460 --> 0:09:45.000
And this representation is actually quite useful because it shows you the shape of the

150
0:09:45.000 --> 0:09:46.380
distribution.

151
0:09:46.380 --> 0:09:54.160
So what we see here is two spikes, one around 600 milliseconds and one around 1.8 seconds.

152
0:09:54.160 --> 0:09:59.760
And this is a typical shape that you would see if your application uses a cache, right,

153
0:09:59.760 --> 0:10:03.360
because then you have a couple of requests that are responded quite quickly, those are

154
0:10:03.360 --> 0:10:08.640
the cache hits, couple of requests are slow that are the cache misses.

155
0:10:08.640 --> 0:10:13.320
And visualizing the shape of the histogram helps you understand kind of the latency behavior

156
0:10:13.320 --> 0:10:15.840
of your application, right.

157
0:10:15.840 --> 0:10:20.840
The other and most popular way to visualize durations is this one here.

158
0:10:20.840 --> 0:10:27.440
These are percentiles, so the green line is the 95th percentile, so it tells us 95 percent

159
0:10:27.440 --> 0:10:32.520
of the calls have been faster than 1.7 seconds and 5 percent slower than that.

160
0:10:32.520 --> 0:10:36.920
The yellow line is the 50th, so half of the calls faster than that, half of the calls

161
0:10:36.920 --> 0:10:38.160
slower than that.

162
0:10:38.160 --> 0:10:42.560
And this doesn't really tell you the shape of the distribution, but it shows you a development

163
0:10:42.560 --> 0:10:44.880
over time, which is useful as well.

164
0:10:44.880 --> 0:10:50.280
So if your service becomes slower, those lines will go up, right, and it's also a good indicator

165
0:10:50.280 --> 0:10:54.400
if you want to do, you know, alerting and so forth, you can define a threshold and say

166
0:10:54.400 --> 0:11:00.000
it's above, if it's above a certain threshold, I want to be notified and stuff like that.

167
0:11:00.000 --> 0:11:05.440
And there are other more, you know, experimental things like this, heat maps showing basically

168
0:11:05.440 --> 0:11:08.280
development of histograms over time and stuff like that.

169
0:11:08.280 --> 0:11:14.040
So it's pretty cool to play with all the different visualizations in Grafana and, you know, see

170
0:11:14.040 --> 0:11:16.120
what you can get.

171
0:11:16.120 --> 0:11:22.120
So this is a, you know, quick example of a so-called red dashboard, request rates, error

172
0:11:22.120 --> 0:11:25.720
rates, duration based on open telemetry data.

173
0:11:25.720 --> 0:11:32.760
And the cool thing about it is that it actually, all that we are seeing here is just based

174
0:11:32.760 --> 0:11:37.120
on that single histogram metric HTTP server duration.

175
0:11:37.120 --> 0:11:41.720
And the fact that this metric is there is not a coincidence.

176
0:11:41.720 --> 0:11:46.760
The metric HTTP server duration is actually defined in the open telemetry standard as

177
0:11:46.760 --> 0:11:51.300
part of the semantic conventions for HTTP services.

178
0:11:51.300 --> 0:11:58.400
So whenever you monitor an HTTP server with open telemetry, then you will find a histogram

179
0:11:58.400 --> 0:12:00.680
named HTTP server duration.

180
0:12:00.680 --> 0:12:03.800
It will have the HTTP status as an attribute.

181
0:12:03.800 --> 0:12:07.560
It will contain the latencies in milliseconds.

182
0:12:07.560 --> 0:12:08.840
That's all part of the standard.

183
0:12:08.840 --> 0:12:15.500
So it doesn't matter what programming language your services uses, what framework, whatever.

184
0:12:15.500 --> 0:12:20.060
If it's being monitored with open telemetry and it's compatible, you will find that metric

185
0:12:20.060 --> 0:12:22.720
and you can create a similar dashboard like that.

186
0:12:22.720 --> 0:12:28.200
And this is kind of one of the things that make application monitoring with open telemetry

187
0:12:28.200 --> 0:12:32.160
a lot easier than it used to be before these standardization.

188
0:12:32.160 --> 0:12:33.160
Right?

189
0:12:33.160 --> 0:12:34.460
Good.

190
0:12:34.460 --> 0:12:37.900
So that was a quick look at metrics.

191
0:12:37.900 --> 0:12:41.200
But of course we want to look at the other signals as well.

192
0:12:41.200 --> 0:12:46.360
So let's switch data sources for now and have a look at traces.

193
0:12:46.360 --> 0:12:53.800
So tracing, again, there's a kind of search, like graphical search where you can create

194
0:12:53.800 --> 0:12:56.120
your search criteria with drag and drop.

195
0:12:56.120 --> 0:13:00.080
There's a relatively new feature, which is a query language for traces.

196
0:13:00.080 --> 0:13:02.400
So I'm going to use that for now.

197
0:13:02.400 --> 0:13:07.040
And one thing you can do is to just search by labels.

198
0:13:07.040 --> 0:13:13.880
So I can, for example, say I'm interested in the service name greeting service.

199
0:13:13.880 --> 0:13:18.140
And then I could basically just open a random trace here.

200
0:13:18.140 --> 0:13:20.760
Let's take this as an example.

201
0:13:20.760 --> 0:13:28.160
Can I need to zoom out a little bit to be able to close the search window here?

202
0:13:28.160 --> 0:13:29.160
Okay.

203
0:13:29.160 --> 0:13:33.440
So this is how a distributed trace looks like.

204
0:13:33.440 --> 0:13:38.160
And if you see it for the first time, it might be a bit hard to understand, but it's actually

205
0:13:38.160 --> 0:13:39.160
fairly easy.

206
0:13:39.160 --> 0:13:43.840
So you just need like two minutes of introduction and then you will understand traces forever.

207
0:13:43.840 --> 0:13:47.780
And to give you that introduction, I actually have one more slide.

208
0:13:47.780 --> 0:13:51.600
So just to help you understand what we are seeing here.

209
0:13:51.600 --> 0:13:56.560
And the thing is distributed traces consist of spans.

210
0:13:56.560 --> 0:13:58.200
And spans are time spans.

211
0:13:58.200 --> 0:14:02.200
So a span is something that has a point in time where it starts and a point in time where

212
0:14:02.200 --> 0:14:04.080
it ends.

213
0:14:04.080 --> 0:14:08.280
And in open telemetry, there are three different kinds of spans.

214
0:14:08.280 --> 0:14:09.880
One are server spans.

215
0:14:09.880 --> 0:14:14.720
The second is internal spans and the third is client spans.

216
0:14:14.720 --> 0:14:18.960
So what happens when my Hello World application receives a request?

217
0:14:18.960 --> 0:14:23.920
So the first thing that happens if a server receives a request, a server span is created.

218
0:14:23.920 --> 0:14:25.960
So that's the first slide here.

219
0:14:25.960 --> 0:14:28.840
It's started as soon as the request is received.

220
0:14:28.840 --> 0:14:33.400
It remains open until the request is responded.

221
0:14:33.400 --> 0:14:40.640
Then I said in the introduction that I used Spring Boot for implementing the example application.

222
0:14:40.640 --> 0:14:45.280
And the way Spring Boot works is that it takes the request and passes it to the corresponding

223
0:14:45.280 --> 0:14:48.520
Spring Controller that would handle the request.

224
0:14:48.520 --> 0:14:54.280
And open telemetry's Java instrumentation agent is nice for Java developers because

225
0:14:54.280 --> 0:14:58.560
it just creates internal spans for each Spring Controller that is involved.

226
0:14:58.560 --> 0:15:01.000
And that is the second line that we are seeing here.

227
0:15:01.000 --> 0:15:06.360
It's basically opened as soon as the Spring Controller takes over and remains open until

228
0:15:06.360 --> 0:15:12.000
the Spring Controller is done by handling the request, which might seem not too useful

229
0:15:12.000 --> 0:15:14.960
if I have just a single Spring Controller anyway.

230
0:15:14.960 --> 0:15:20.120
But if you have kind of a larger application and you have multiple controllers involved,

231
0:15:20.120 --> 0:15:24.480
it gives you quite some interesting insights into what's happening inside your application.

232
0:15:24.480 --> 0:15:30.440
You would see immediately which controller do I spend most time in and so forth.

233
0:15:30.440 --> 0:15:36.240
And then eventually my Hello World application reaches out to the greeting service and outgoing

234
0:15:36.240 --> 0:15:38.640
requests are represented by client spans.

235
0:15:38.640 --> 0:15:43.920
So the client span is basically opened as soon as my HTTP request goes out and remains

236
0:15:43.920 --> 0:15:46.120
open until the response is received.

237
0:15:46.120 --> 0:15:50.280
And then in the greeting service the same thing starts again.

238
0:15:50.280 --> 0:15:53.800
Request is received which creates a server span and then I have a Spring Controller as

239
0:15:53.800 --> 0:15:58.720
well which is an internal span and that's the end of my distributed application here.

240
0:15:58.720 --> 0:16:01.520
And this is exactly what we are seeing here.

241
0:16:01.520 --> 0:16:06.000
And each of those span types has a corresponding metadata attached to it.

242
0:16:06.000 --> 0:16:11.200
So if you look at one of the internal spans here we see the name of the Spring Controller

243
0:16:11.200 --> 0:16:17.360
and the name of the controller method and a couple of JVM related attributes, whatever.

244
0:16:17.360 --> 0:16:21.760
And if you look at an HTTP span for example we see of course HTTP attributes like the

245
0:16:21.760 --> 0:16:25.400
status code method and so forth.

246
0:16:25.400 --> 0:16:32.280
So of course you do not want to just look at random spans so usually you are looking

247
0:16:32.280 --> 0:16:33.280
for something.

248
0:16:33.280 --> 0:16:37.560
There are standard attributes in OpenTelemetry that you can use for searching.

249
0:16:37.560 --> 0:16:43.160
So we already had the service name, greeting service for example.

250
0:16:43.160 --> 0:16:49.840
But that's the most important or one of the most important attributes is HTTP.status

251
0:16:49.840 --> 0:16:53.400
code, this one here.

252
0:16:53.400 --> 0:16:59.320
And if we for example search for spans with HTTP status code 500 then we should find an

253
0:16:59.320 --> 0:17:01.880
example of a request that failed.

254
0:17:01.880 --> 0:17:04.280
So let's close the search window again.

255
0:17:04.280 --> 0:17:06.760
Yes, that's an example of a failed request.

256
0:17:06.760 --> 0:17:12.360
You see it with the indicated by those red exclamation marks at the bottom here.

257
0:17:12.360 --> 0:17:15.160
So this is where the thing failed, right?

258
0:17:15.160 --> 0:17:20.080
So the root cause of the error is the internal span, something in my Spring Controller in

259
0:17:20.080 --> 0:17:21.520
the greeting service.

260
0:17:21.520 --> 0:17:27.120
If I look at the metadata attached to that I actually see that the instrumentation attached

261
0:17:27.120 --> 0:17:32.060
the event that caused the error and this even includes the stack trace.

262
0:17:32.060 --> 0:17:37.120
So you can basically immediately navigate to the exact line of code that is the root

263
0:17:37.120 --> 0:17:39.920
cause of this error, right?

264
0:17:39.920 --> 0:17:41.240
And this is quite cool.

265
0:17:41.240 --> 0:17:47.240
So if you have a distributed application and you get an unexpected response from your Hello

266
0:17:47.240 --> 0:17:53.640
World application without distributed tracing it's pretty hard to find that actually there's

267
0:17:53.640 --> 0:17:58.520
an exception in the greeting service that propagated through your distributed landscape

268
0:17:58.520 --> 0:18:01.840
and then eventually caused the unexpected response.

269
0:18:01.840 --> 0:18:06.640
And with distributed tracing finding these kind of things becomes pretty easy because

270
0:18:06.640 --> 0:18:12.400
you get all the related calls grouped together, you get the failed ones marked with an exclamation

271
0:18:12.400 --> 0:18:17.960
mark and it can pretty easily navigate to what's the root cause of your error.

272
0:18:17.960 --> 0:18:18.960
Okay?

273
0:18:18.960 --> 0:18:19.960
Cool.

274
0:18:19.960 --> 0:18:23.000
So that was a quick look at traces.

275
0:18:23.000 --> 0:18:26.000
There are a lot of interesting things about tracing.

276
0:18:26.000 --> 0:18:30.240
Maybe one thing I would like to show you because I find it particularly cool.

277
0:18:30.240 --> 0:18:36.320
So if you have all your services instrumented with tracing in your back end then basically

278
0:18:36.320 --> 0:18:41.840
those traces give you metadata about all the network calls happening in your system.

279
0:18:41.840 --> 0:18:44.920
And you can do something with that type of data, right?

280
0:18:44.920 --> 0:18:50.040
So for example, you can calculate something that we call the service graph.

281
0:18:50.040 --> 0:18:51.480
So it looks like this.

282
0:18:51.480 --> 0:18:56.800
It's maybe not too impressive if you just have two services calling each other, right?

283
0:18:56.800 --> 0:19:02.200
But if you imagine more larger, dozens or hundreds of services, so it will generate

284
0:19:02.200 --> 0:19:07.000
a map of all the services and indicate which service calls which other service.

285
0:19:07.000 --> 0:19:12.640
And this is quite useful, for example, if you intend to deploy a breaking change in

286
0:19:12.640 --> 0:19:16.520
your greeting service and you want to know who's using the greeting service, what would

287
0:19:16.520 --> 0:19:17.520
I break?

288
0:19:17.520 --> 0:19:22.280
Then looking at the service graph you basically get this information right away.

289
0:19:22.280 --> 0:19:26.280
Traditionally if you don't have that, you basically have a PDF with your architecture

290
0:19:26.280 --> 0:19:28.600
diagram and then you look it up there.

291
0:19:28.600 --> 0:19:33.520
And also traditionally there's at least one team that deployed something and forgot to

292
0:19:33.520 --> 0:19:36.280
update the diagram and then you missed that.

293
0:19:36.280 --> 0:19:38.280
And it's a service graph that won't happen, right?

294
0:19:38.280 --> 0:19:39.760
This is the actual truth.

295
0:19:39.760 --> 0:19:44.040
This is based on what's actually happening in your backend and this is pretty useful

296
0:19:44.040 --> 0:19:46.460
in these situations, right?

297
0:19:46.460 --> 0:19:51.600
And you can do other things as well like, you know, have some statistics like the most

298
0:19:51.600 --> 0:19:56.480
frequently called endpoint or the endpoint with the most errors and stuff like that.

299
0:19:56.480 --> 0:20:00.600
So that was a quick look at traces.

300
0:20:00.600 --> 0:20:04.240
So we covered metrics, we covered traces.

301
0:20:04.240 --> 0:20:08.960
One thing I want to show you is that metrics and traces are actually related to each other,

302
0:20:08.960 --> 0:20:09.960
right?

303
0:20:09.960 --> 0:20:15.640
And so in order to show that I'm going to go back to our dashboard because if you, let's

304
0:20:15.640 --> 0:20:19.360
take a 15 minute window, then we get a bit more examples.

305
0:20:19.360 --> 0:20:24.560
So if you look at the latency data here, you notice these little green dots.

306
0:20:24.560 --> 0:20:29.780
These are called exemplars and this is something that's provided by the auto instrumentation

307
0:20:29.780 --> 0:20:31.240
of OpenTelemetry.

308
0:20:31.240 --> 0:20:38.640
So whenever it generates latency data, it basically attaches trace IDs of example traces

309
0:20:38.640 --> 0:20:40.460
to the latency data.

310
0:20:40.460 --> 0:20:43.480
And this is visualized by these little green dots, right?

311
0:20:43.480 --> 0:20:47.960
And so you see some examples of particularly fast calls, some examples of particularly

312
0:20:47.960 --> 0:20:49.480
slow calls and so forth.

313
0:20:49.480 --> 0:20:54.440
And if you, for example, take this dot up here, which is kind of slower than anything

314
0:20:54.440 --> 0:20:57.120
else, it's almost two seconds, right?

315
0:20:57.120 --> 0:21:02.120
And you have the trace ID here and you can navigate to tempo and have a look at the trace

316
0:21:02.120 --> 0:21:08.360
and start figuring out why did I have an example of such a slow call in my system, right?

317
0:21:08.360 --> 0:21:12.960
And in that case, you would immediately see that most of the time spent in the greeting

318
0:21:12.960 --> 0:21:13.960
service.

319
0:21:13.960 --> 0:21:18.040
So if you're looking for the performance bottleneck, then this is the most likely thing.

320
0:21:18.040 --> 0:21:20.800
Yeah, four minutes, that's fine.

321
0:21:20.800 --> 0:21:21.800
Good.

322
0:21:21.800 --> 0:21:27.920
So if I have four minutes, it's high time to jump to logs, the third signal that we

323
0:21:27.920 --> 0:21:29.000
didn't look at yet.

324
0:21:29.000 --> 0:21:36.520
So let's select Loki or open source logs database as a data source.

325
0:21:36.520 --> 0:21:41.200
So again, there's a query language, there's a graphical query builder and so forth.

326
0:21:41.200 --> 0:21:46.880
So let's just open random logs coming from the greeting service looks a bit like this.

327
0:21:46.880 --> 0:21:51.400
So it's even I don't know, I didn't even log anything explicitly.

328
0:21:51.400 --> 0:21:56.040
I just turned on some whatever spring request logging so that I get some log data.

329
0:21:56.040 --> 0:22:00.400
And from time to time, I throw an exception, which is an IO exception to simulate these

330
0:22:00.400 --> 0:22:01.400
errors.

331
0:22:01.400 --> 0:22:06.840
It looks a bit broken, but that's just because of the resolution that I have here.

332
0:22:06.840 --> 0:22:12.200
Yeah, so what you can do, of course, you can do some full text search, for example, can

333
0:22:12.200 --> 0:22:23.360
say, I'm interested in these IO exception, and then you would basically get what if you

334
0:22:23.360 --> 0:22:30.880
spell it correctly, like that, then you would get the list of all IO exceptions, which in

335
0:22:30.880 --> 0:22:34.040
my case are just the random errors I'm throwing here.

336
0:22:34.040 --> 0:22:36.120
And this query language is actually quite powerful.

337
0:22:36.120 --> 0:22:40.520
So you can, this is kind of filtering by a label and filtering by full text search, but

338
0:22:40.520 --> 0:22:42.680
you can do totally different things as well.

339
0:22:42.680 --> 0:22:47.720
For example, you can have queries that, you know, derive metrics based from on log data.

340
0:22:47.720 --> 0:22:52.360
There's a function pretty similar to what we have seen in the in the metrics demo, which

341
0:22:52.360 --> 0:22:53.960
is called the rate function.

342
0:22:53.960 --> 0:23:00.840
So the rate function again, takes a time interval, and then calculates the per second increase

343
0:23:00.840 --> 0:23:01.840
rate.

344
0:23:01.840 --> 0:23:08.560
So it basically tells you that we have almost 0.1 of these IO exceptions per second in our

345
0:23:08.560 --> 0:23:14.320
log data, which is also kind of useful for information to have.

346
0:23:14.320 --> 0:23:21.880
And the last thing to show you, because that's particularly interesting, is that these logs

347
0:23:21.880 --> 0:23:25.960
and traces and metrics are again, you know, not independent of each other.

348
0:23:25.960 --> 0:23:28.080
They are related to each other.

349
0:23:28.080 --> 0:23:33.080
And so if you look at an example here, just let's open a random log line.

350
0:23:33.080 --> 0:23:36.280
So what we see here, there's a trace ID.

351
0:23:36.280 --> 0:23:37.280
And this is interesting.

352
0:23:37.280 --> 0:23:40.080
So how does a trace ID end up in my log line?

353
0:23:40.080 --> 0:23:46.560
So this is actually also a feature of the Java instrumentation that's provided by the

354
0:23:46.560 --> 0:23:48.320
OpenTelemetry community.

355
0:23:48.320 --> 0:23:54.360
So the way logging in general works in Java is that there's a global thing with key value

356
0:23:54.360 --> 0:23:56.720
pairs called the log context.

357
0:23:56.720 --> 0:24:00.520
And applications can put arbitrary key value pairs into that context.

358
0:24:00.520 --> 0:24:04.640
And when you configure your log format, you can define which of those values you want

359
0:24:04.640 --> 0:24:06.640
to include in your log data.

360
0:24:06.640 --> 0:24:12.600
And if you have this OpenTelemetry agent attached, then as soon as a log line is written in the

361
0:24:12.600 --> 0:24:17.840
context of serving an HTTP request, then the corresponding trace ID is put into that log

362
0:24:17.840 --> 0:24:18.880
context.

363
0:24:18.880 --> 0:24:23.240
And you can configure your log format to include the trace ID in your log data.

364
0:24:23.240 --> 0:24:24.240
And that's what I did.

365
0:24:24.240 --> 0:24:27.280
And so each of my log lines actually has a trace ID.

366
0:24:27.280 --> 0:24:31.960
And so if I see something fancy and I want to know maybe somewhere down my distributed

367
0:24:31.960 --> 0:24:36.600
stack something went wrong, I can just query that in tempo, navigate to the corresponding

368
0:24:36.600 --> 0:24:37.600
trace.

369
0:24:37.600 --> 0:24:40.480
And I close that here.

370
0:24:40.480 --> 0:24:43.800
And then basically maybe get some information what happened.

371
0:24:43.800 --> 0:24:46.960
And then same navigation works the other way around as well.

372
0:24:46.960 --> 0:24:50.000
So of course there's a little log button here.

373
0:24:50.000 --> 0:24:55.160
So if I see something fancy going on in my greeting service thing here and maybe the

374
0:24:55.160 --> 0:25:00.000
logs have more information, I can click on that, navigate to the logs.

375
0:25:00.000 --> 0:25:02.280
And then it basically just generates a query.

376
0:25:02.280 --> 0:25:06.120
I click on the greeting service with that trace ID.

377
0:25:06.120 --> 0:25:09.600
So it's basically just a full text search for that trace ID.

378
0:25:09.600 --> 0:25:12.600
And so I will find all my corresponding log lines.

379
0:25:12.600 --> 0:25:13.920
In that case just one line.

380
0:25:13.920 --> 0:25:17.960
But if you have a bit better logging, then maybe it would give you some indication what

381
0:25:17.960 --> 0:25:19.400
happened there.

382
0:25:19.400 --> 0:25:20.360
OK.

383
0:25:20.360 --> 0:25:28.120
So that was a very quick 25 minutes overview of looking a bit into metrics, looking a bit

384
0:25:28.120 --> 0:25:30.600
into tracing, looking a bit into logs.

385
0:25:30.600 --> 0:25:36.000
I hope it gave you some impression, what's the type of data that you get out of open

386
0:25:36.000 --> 0:25:38.680
telemetry looks like.

387
0:25:38.680 --> 0:25:42.560
All of what we did is really without even modifying the application.

388
0:25:42.560 --> 0:25:47.240
I didn't even start with custom metrics, custom traces, and so forth.

389
0:25:47.240 --> 0:25:51.420
So but it's already quite some useful data that we get out of that.

390
0:25:51.420 --> 0:25:57.360
If you like the demo, if you want to explore it a bit more, want to try it at home, I pushed

391
0:25:57.360 --> 0:26:01.480
it on my GitHub and there's a readme telling you how to run it.

392
0:26:01.480 --> 0:26:03.480
So you can do that.

393
0:26:03.480 --> 0:26:10.400
And yeah, next up we have a talk that goes a bit more in detail into the tracing part

394
0:26:10.400 --> 0:26:11.400
of this.

395
0:26:11.400 --> 0:26:15.920
And then after that we have a talk that goes a bit more into detail how to run open telemetry

396
0:26:15.920 --> 0:26:16.920
in Kubernetes.

397
0:26:16.920 --> 0:26:25.080
So stay here and thanks for listening.

398
0:26:25.080 --> 0:26:27.360
Please remain seated during Q&A.

399
0:26:27.360 --> 0:26:29.200
Otherwise we can't do a real Q&A.

400
0:26:29.200 --> 0:26:31.320
So please remain seated.

401
0:26:31.320 --> 0:26:32.800
Order any questions.

402
0:26:32.800 --> 0:26:33.800
Yes.

403
0:26:33.800 --> 0:26:34.800
Hi.

404
0:26:34.800 --> 0:26:45.760
Thank you for this.

405
0:26:45.760 --> 0:26:46.760
One quick question.

406
0:26:46.760 --> 0:26:52.920
You mentioned you just need to add some parameters to the Java virtual machine to run the telemetry.

407
0:26:52.920 --> 0:26:59.360
What happens to my application if, for example, the back end of the telemetry is down?

408
0:26:59.360 --> 0:27:03.040
Is my application failing or impacted in any way?

409
0:27:03.040 --> 0:27:04.920
If the monitoring back end is down.

410
0:27:04.920 --> 0:27:05.920
Yes.

411
0:27:05.920 --> 0:27:09.740
Say the monitoring is down but I started my application with these parameters.

412
0:27:09.740 --> 0:27:11.800
Is it impacting the application?

413
0:27:11.800 --> 0:27:12.800
No.

414
0:27:12.800 --> 0:27:17.680
I mean, you won't see metrics, of course, if you're monitoring back end is down but

415
0:27:17.680 --> 0:27:20.160
the application would just continue running.

416
0:27:20.160 --> 0:27:27.560
So typically in production setups the applications wouldn't send telemetry data directly to the

417
0:27:27.560 --> 0:27:29.000
monitoring back end.

418
0:27:29.000 --> 0:27:32.280
But what you usually have is something in the middle.

419
0:27:32.280 --> 0:27:33.800
There's alternatives.

420
0:27:33.800 --> 0:27:36.480
There's the Grafana agent that you can use for that.

421
0:27:36.480 --> 0:27:39.340
There's the open telemetry collector that you can use for that.

422
0:27:39.340 --> 0:27:45.520
And it's basically a thing that runs close to the application, takes the telemetry data

423
0:27:45.520 --> 0:27:52.040
off the application very quickly and then can buffer stuff and process stuff and send

424
0:27:52.040 --> 0:27:53.880
it over to the monitoring back end.

425
0:27:53.880 --> 0:27:56.940
That's used for decoupling that a little bit.

426
0:27:56.940 --> 0:28:01.600
If you have such an architect for the application, shouldn't be affected at all by that.

427
0:28:01.600 --> 0:28:03.640
Two more questions.

428
0:28:03.640 --> 0:28:04.640
Yeah.

429
0:28:04.640 --> 0:28:05.640
Two more.

430
0:28:05.640 --> 0:28:13.180
So I really like being able to link from your metrics to traces.

431
0:28:13.180 --> 0:28:19.400
But what I'm actually really curious to be able to do, and as far as I know doesn't exist

432
0:28:19.400 --> 0:28:23.760
or I guess that's my question is like is there any thought towards doing this, is being able

433
0:28:23.760 --> 0:28:31.320
to go the other direction where what I'd like to be able to answer is here's all my trace

434
0:28:31.320 --> 0:28:36.320
data and this node of the trace incremented these counters by this much.

435
0:28:36.320 --> 0:28:43.640
So I could ask things like how much network IO or disk IOPS did this complete request

436
0:28:43.640 --> 0:28:47.480
do and where in the tree would that occur?

437
0:28:47.480 --> 0:28:48.480
Yeah.

438
0:28:48.480 --> 0:28:49.480
That's a good question.

439
0:28:49.480 --> 0:29:00.200
So thinking from traces to metrics is not so straightforward because I think the things

440
0:29:00.200 --> 0:29:03.220
you can do to relate this is to use the service name.

441
0:29:03.220 --> 0:29:08.880
So if you have the service name part of your resource attributes of the metrics and consistently

442
0:29:08.880 --> 0:29:13.840
you have the same service name in your trace data, then you can at least navigate to all

443
0:29:13.840 --> 0:29:18.580
traces coming to all metrics coming from the same service.

444
0:29:18.580 --> 0:29:24.240
Maybe you have some more related attributes like whatever instance ID and so forth.

445
0:29:24.240 --> 0:29:27.000
But it's not really a one to one relationship.

446
0:29:27.000 --> 0:29:28.000
Yeah.

447
0:29:28.000 --> 0:29:37.000
No, I don't think that's possible.

448
0:29:37.000 --> 0:29:44.760
So in this example you've shown that Grafana world works great with server side applications.

449
0:29:44.760 --> 0:29:52.920
Have you had examples of client side applications, mobile, desktop applications that use the

450
0:29:52.920 --> 0:30:01.720
me field metrics and then ship their metrics and traces to the metric backend?

451
0:30:01.720 --> 0:30:04.400
So did I hear it correctly?

452
0:30:04.400 --> 0:30:09.320
You're asking about starting your traces on the client side in the web browser and stuff?

453
0:30:09.320 --> 0:30:10.320
Yeah.

454
0:30:10.320 --> 0:30:14.880
So what do you have tracing on the server side, but what about having traces and metrics

455
0:30:14.880 --> 0:30:19.040
on the client side in, for example, for an embedded or mobile application so that you

456
0:30:19.040 --> 0:30:26.160
could actually see the trace from when the customer clicked a thing and see the full

457
0:30:26.160 --> 0:30:27.240
customer journey?

458
0:30:27.240 --> 0:30:29.120
That's a great question.

459
0:30:29.120 --> 0:30:33.800
That's actually an area where there's currently a lot of research and new projects and so

460
0:30:33.800 --> 0:30:34.800
forth.

461
0:30:34.800 --> 0:30:42.160
There's a group called real user monitoring, that deal with client side applications.

462
0:30:42.160 --> 0:30:50.560
There's also a project by Grafana, it's kind of JavaScript that you can include in your

463
0:30:50.560 --> 0:30:58.240
front end, in your HTML page and it gives you traces and metrics coming from the web

464
0:30:58.240 --> 0:30:59.440
browser.

465
0:30:59.440 --> 0:31:01.720
And this is currently a pretty active area.

466
0:31:01.720 --> 0:31:05.720
So lots of movement there.

467
0:31:05.720 --> 0:31:07.920
So there are things to explore.

468
0:31:07.920 --> 0:31:14.360
If you like, check out Faro, that's a nice new project and standardization is also currently

469
0:31:14.360 --> 0:31:21.160
being discussed, but it's newer than the rest of what I showed you, right?

470
0:31:21.160 --> 0:31:25.400
So there's no clear standard yet or nothing decided yet.

471
0:31:25.400 --> 0:31:26.400
Cool.

472
0:31:26.400 --> 0:31:27.400
Okay.

473
0:31:27.400 --> 0:31:28.400
Thanks everyone again.

474
0:31:28.400 --> 0:31:31.940
Thank you.

475
0:31:31.940 --> 0:31:35.740
Thank you.

