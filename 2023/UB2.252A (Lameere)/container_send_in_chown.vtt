WEBVTT

00:00.000 --> 00:10.560
Okay, then our next talk is by Fraser.

00:10.560 --> 00:11.560
Over to you.

00:11.560 --> 00:12.860
Okay, thank you.

00:12.860 --> 00:21.800
So I'm going to talk about user namespaces and delegation of control of C groups in container

00:21.800 --> 00:24.800
orchestration systems.

00:24.800 --> 00:28.280
So yeah, container and container standards.

00:28.280 --> 00:33.440
This is the container's dev room, I think hopefully most people should have some idea.

00:33.440 --> 00:39.120
Talk about Kubernetes and OpenShift, user namespaces and C groups, and then a demo of

00:39.120 --> 00:45.840
a system B-based workload on Kubernetes and more specifically OpenShift.

00:45.840 --> 00:51.880
So a container is a process isolation and confinement abstraction.

00:51.880 --> 00:57.840
Most commonly it uses operating system level virtualisation where the process is running

00:57.840 --> 01:03.280
in the container using the same kernel as the host system.

01:03.280 --> 01:09.840
Examples of this include FreeBSD jails and Solaris zones as well as Linux containers.

01:09.840 --> 01:14.440
And the container image is not a container, the container image just defines the file

01:14.440 --> 01:21.240
system contents for a container and some metadata suggesting what process should be run, what

01:21.240 --> 01:25.500
user ID it should run underneath and so on.

01:25.500 --> 01:32.880
On Linux containers use a combination of the following technologies, so namespaces, process

01:32.880 --> 01:39.840
ID namespaces, mount namespaces, network namespaces, et cetera, to restrict what the process running

01:39.840 --> 01:43.080
in the container can see.

01:43.080 --> 01:48.000
The container may have restricted capabilities or a sec comp profile that limits which system

01:48.000 --> 01:51.520
calls or operating system features it can use.

01:51.520 --> 01:59.760
There may be SE Linux or App Armor confinement and it can use C groups for resource limits.

01:59.760 --> 02:08.160
Not necessarily all of these are used at the same time, it depends on the implementation.

02:08.160 --> 02:15.940
The Open Container Initiative defines standards around containers in the free software ecosystem

02:15.940 --> 02:21.460
and its runtime specification in particular defines a low-level runtime interface for

02:21.460 --> 02:29.000
containers that is not just for Linux containers but it defines the runtime semantics for Linux,

02:29.000 --> 02:34.040
Solaris containers, Windows containers, virtual machines.

02:34.040 --> 02:38.360
And so there are a bunch of implementations of the runtime spec including RunC, the reference

02:38.360 --> 02:46.160
implementation, C run and CART containers which is a virtual machine-based container

02:46.160 --> 02:48.240
runtime.

02:48.240 --> 02:54.120
The OCI runtime spec has a JSON configuration, there's a link to an example.

02:54.120 --> 03:00.520
It defines or lets you define the mounts, what process to be executed, its environment,

03:00.520 --> 03:05.880
life cycle hooks, so extra code to run when the container is created, destroyed, started,

03:05.880 --> 03:07.120
stopped.

03:07.120 --> 03:14.440
The Linux specific capabilities that can be controlled via the OCI runtime spec include

03:14.440 --> 03:23.200
the capabilities, that's the kernel feature capabilities, namespaces, the C group, the

03:23.200 --> 03:29.560
system controls that should be set for that container, the seccomp profile and so on.

03:29.560 --> 03:39.760
This is what an example runtime specification looks like, so it has a process structure

03:39.760 --> 03:44.960
which includes a field for the user ID and the group ID.

03:44.960 --> 03:51.240
It has this Linux structure which defines the Linux specific attributes for this container

03:51.240 --> 03:58.360
and one of those attributes is the namespaces list which defines a list of the different

03:58.360 --> 04:07.160
namespaces that should be used or should be newly created for that container.

04:07.160 --> 04:13.960
Kubernetes is a container orchestration platform, has a declarative configuration system and

04:13.960 --> 04:19.720
it integrates with many different cloud providers.

04:19.720 --> 04:25.800
Anyone not know what Kubernetes is in the room?

04:25.800 --> 04:33.600
The container is an isolated or confined process or process tree, a pod is one or more related

04:33.600 --> 04:42.560
containers that together constitute an application of some sort, so it might be an HTTP server

04:42.560 --> 04:48.480
and a database to encapsulate the entire web application.

04:48.480 --> 04:55.480
A namespace in Kubernetes terminology is not a namespace in Linux kernel terminology,

04:55.480 --> 05:01.760
this is just an object scope and authorization scope for a bunch of objects in the Kubernetes

05:01.760 --> 05:07.280
data store, so if you have a particular team or project in your organisation you might

05:07.280 --> 05:13.760
deploy all of the Kubernetes applications in a single namespace.

05:13.760 --> 05:17.800
And a node is a machine in the cluster where pods are executed, there are different kinds

05:17.800 --> 05:22.620
of nodes, there are control plane nodes, there are worker nodes where the actual business

05:22.620 --> 05:28.280
applications run.

05:28.280 --> 05:34.720
Kuberlet is the agent that executes pods on nodes, so there's a scheduling system, the

05:34.720 --> 05:42.620
scheduler will, when a pod is created, decide what node that pod should run on and Kuberlet

05:42.620 --> 05:48.520
is the agent on the node that takes the pod specification and turns it into a container

05:48.520 --> 05:51.600
running on that node.

05:51.600 --> 05:58.800
The Kubernetes terminology uses the term sandbox, that's an isolation or confinement mechanism

05:58.800 --> 06:04.120
and there's one sandbox per pod, so there could be multiple containers running in the

06:04.120 --> 06:05.960
sandbox.

06:05.960 --> 06:14.200
And the container runtime interface defines how Kuberlet actually starts and stops the

06:14.200 --> 06:17.600
containers or the sandboxes.

06:17.600 --> 06:24.040
So cryo is one implementation of the container runtime interface, that's what's used in OpenShift,

06:24.040 --> 06:31.080
there's also containerd, that's used in some other distributions of Kubernetes.

06:31.080 --> 06:38.480
So visualising this, the whole box is one Kubernetes node, the Kuberlet has the gRPC

06:38.480 --> 06:46.260
client to talk to a CRI runtime, the CRI runtime does something and containers appear.

06:46.260 --> 06:54.320
So we could instantiate the container runtime interface implementation as cryo and then

06:54.320 --> 07:01.400
we can see that cryo talks to an OCI runtime, it uses exec to use the OCI runtime and we

07:01.400 --> 07:08.720
can go one step further and say that the OCI runtime implementation will be runc and this

07:08.720 --> 07:13.840
is the setup that we use on OpenShift.

07:13.840 --> 07:21.760
This is a pod spec in YAML format, so we have kind pod, the specification has a list of

07:21.760 --> 07:26.000
containers, in this case there's one, the container has a name, defines the image to

07:26.000 --> 07:34.000
use, the command to execute environment variables that should be set and so on.

07:34.000 --> 07:40.360
OpenShift or OpenShift container platform is an enterprise ready Kubernetes container platform,

07:40.360 --> 07:46.040
it's commercially supported by Red Hat, there's a community upstream distribution called OKD,

07:46.040 --> 07:53.640
as I mentioned before, it uses cryo and runc and the latest stable release is 4.12, I think

07:53.640 --> 07:58.160
that came out just a week ago or so.

07:58.160 --> 08:06.440
And its default, the default way that it creates containers is it uses se Linux and namespaces

08:06.440 --> 08:08.400
to confine the processes.

08:08.400 --> 08:17.480
Each namespace gets assigned a unique user ID range and the processes for the pods in

08:17.480 --> 08:22.080
that namespace have to run in those host UIDs.

08:22.080 --> 08:28.840
You can circumvent this using the run as user security context constraint, but that is not

08:28.840 --> 08:35.640
a good idea, you don't want your containers running as root on the container host because

08:35.640 --> 08:41.360
if they escape the sandbox then your cluster got owned.

08:41.360 --> 08:48.480
So this is the why of user namespaces, user namespaces, we're talking now Linux kernel,

08:48.480 --> 08:54.160
user namespaces can be used to improve the workload isolation and the confinement of

08:54.160 --> 08:57.240
the pods in your cluster.

08:57.240 --> 09:01.920
They can also be used to run applications that require or assume that they're running

09:01.920 --> 09:08.160
as specific user IDs or to phrase this a different way, you can drag your legacy applications

09:08.160 --> 09:16.040
kicking and screaming into the cloud and get the benefits of all of the orchestration and

09:16.040 --> 09:22.880
networking support that these platforms like Kubernetes and OpenShift can give you while

09:22.880 --> 09:24.920
still running that workload securely.

09:24.920 --> 09:31.360
In other words, trick it to believe it is running as root.

09:31.360 --> 09:39.720
And there have been a bunch of CVS in Kubernetes and the broader container orchestration ecosystem

09:39.720 --> 09:47.000
arising from sandbox escapes where user namespaces would have prevented the vulnerability or

09:47.000 --> 09:53.040
severely limited or curtailed its impact.

09:53.040 --> 09:58.600
So visualising a user namespace, we can have two separate containers with a user namespace

09:58.600 --> 10:08.320
mapping of UID range 0 to 65535 inside the container's user namespace to a range of unprivileged

10:08.320 --> 10:12.240
user IDs in the host's user ID namespace.

10:12.240 --> 10:16.920
So the processes running in the container believe that they are running as, for example,

10:16.920 --> 10:25.080
root UID0 or some other privileged user ID when in fact it's running as UID200,000 on

10:25.080 --> 10:28.400
the host and unprivileged user ID.

10:28.400 --> 10:35.460
Escape the sandbox and you're still an unprivileged user on the host.

10:35.460 --> 10:40.640
So in Linux there's some references to some man pages about user namespaces and how to

10:40.640 --> 10:41.640
use them.

10:41.640 --> 10:50.360
The critical thing is the unshare system call which is how you create and use the user namespace.

10:50.360 --> 10:56.720
In the OCI runtime specification there are some fields and again this is Linux specific

10:56.720 --> 11:02.360
so it's inside the Linux specific part of that configuration that you can specify that

11:02.360 --> 11:07.600
a user namespace should be created or used for that container and you can specify the

11:07.600 --> 11:16.840
mappings of how do we map the container's user ID range to the host user ID range.

11:16.840 --> 11:22.040
These namespaces were implemented, before they were implemented in Kubernetes upstream,

11:22.040 --> 11:24.360
we did it in Cryo.

11:24.360 --> 11:32.840
It first shipped in OpenShift 4.7 but it required a considerable amount of additional configuration

11:32.840 --> 11:37.760
of the cluster to use it and since OpenShift 4.10 you've been able to use it out of the

11:37.760 --> 11:38.760
box.

11:38.760 --> 11:46.680
You do still have to opt in using annotations on a per pod basis.

11:46.680 --> 11:53.240
It requires the NEU ID security context constraint or an equivalent privileged security context

11:53.240 --> 11:58.880
constraint in order to admit the pod because the admission machinery does not yet understand

11:58.880 --> 12:01.520
about user namespaces.

12:01.520 --> 12:07.800
So the pod spec says I want to run as user ID zero and the admission machinery says no

12:07.800 --> 12:08.800
way.

12:08.800 --> 12:15.400
We need to circumvent that for the time being but the workload itself will run in the user

12:15.400 --> 12:18.760
namespace.

12:18.760 --> 12:25.120
And depending on the workload it may still require some additional cluster configuration.

12:25.120 --> 12:31.240
So the annotations to opt in, you can say I o.OpenShift.builder is true, that activates

12:31.240 --> 12:37.840
a particular Cryo what we call a workload, basically an alternative bunch of runtime

12:37.840 --> 12:46.400
settings and then we use the user NS mode annotation to specify that we want an arbitrary

12:46.400 --> 12:49.920
user namespace of size 6.536.

12:49.920 --> 12:55.760
So that will allocate you a contiguous host UID range for that container and map it to

12:55.760 --> 13:00.480
unprivileged host user IDs.

13:00.480 --> 13:04.920
In the Kubernetes upstream it took a bit longer to get user namespace support and it's still

13:04.920 --> 13:11.280
a work in progress but the initial support was delivered in Kubernetes 1.25 and that

13:11.280 --> 13:19.440
version is what we've moved to in OpenShift 4.12 so you can now use the first class user

13:19.440 --> 13:21.920
namespace support in OpenShift.

13:21.920 --> 13:26.200
It is an alpha feature so it's not enabled by default, you have to turn it on with a

13:26.200 --> 13:33.400
feature gate and at the moment it only supports ephemeral volume types so empty config maps,

13:33.400 --> 13:37.880
it's no persistent volume support yet.

13:37.880 --> 13:46.880
You opt in by putting host user's false in your pod spec and currently it gives you a

13:46.880 --> 13:54.720
fixed mapping size of 6.536 that will be unique to that pod.

13:54.720 --> 14:01.200
It is hoped that a later phase will deliver support for the additional volume types.

14:01.200 --> 14:10.760
The reason we didn't have them is the complexity around ID mapped mounts and how to adapt the

14:10.760 --> 14:16.400
volume mounts to understand how to map the user IDs between the host UID namespace and

14:16.400 --> 14:19.520
the container's user namespace.

14:19.520 --> 14:23.280
There's also very simple heuristics around the ID range assignment, as I mentioned it's

14:23.280 --> 14:30.480
a fixed size of 6.536 that limits the number of pods that you could run in user namespaces

14:30.480 --> 14:33.800
on a given node.

14:33.800 --> 14:37.840
There are still some other mount point and file ownership issues, for example with the

14:37.840 --> 14:40.040
C group FS.

14:40.040 --> 14:42.720
That takes us to the C group's topic.

14:42.720 --> 14:50.320
OpenShift creates a unique C group for each container and it also creates a C group namespace

14:50.320 --> 14:58.640
so that the container in the CFS C group mount only sees its namespace.

14:58.640 --> 15:05.880
Inside the container CFS C group points to CFS C group slash a whole bunch of stuff specific

15:05.880 --> 15:11.580
to that container in the host's file system.

15:11.580 --> 15:17.480
If you want to run a system D based workload, system D needs right access to the C group

15:17.480 --> 15:18.680
FS.

15:18.680 --> 15:24.800
But by default the C group FS will be mounted read only.

15:24.800 --> 15:32.000
So the solution, we modify the container runtime to the C group to the container's process

15:32.000 --> 15:33.080
UID.

15:33.080 --> 15:43.520
So that is we chain it to the host UID corresponding to UID zero in the container's user namespace.

15:43.520 --> 15:50.960
But first before we did this in an ad hoc basis we engaged with the open container initiative

15:50.960 --> 15:57.280
to define the semantics for C group ownership in a container.

15:57.280 --> 16:03.880
Those proposals were workshopped and accepted and after that we were able to implement those

16:03.880 --> 16:07.080
semantics in run C.

16:07.080 --> 16:08.480
So what are the semantics?

16:08.480 --> 16:16.520
The container's C group will be choned to the host UID matching the process UID in the

16:16.520 --> 16:24.200
container's user namespace if and only if the node is using C group speed two and the

16:24.200 --> 16:34.200
container has its own C group namespace and the C group FS is mounted read right.

16:34.200 --> 16:41.320
So only the C group directory itself and the files mentioned in Cyskernel C group delegate

16:41.320 --> 16:42.560
are choned.

16:42.560 --> 16:51.560
These are the ones that are safe to delegate to a subprocess.

16:51.560 --> 16:58.560
And the container runtime, if that file Cyskernel C group delegate is defined then it will read

16:58.560 --> 17:03.660
that file and only chone the files mentioned there.

17:03.660 --> 17:13.120
So it can respond to the evolution of the kernel where new C group nodes may come and

17:13.120 --> 17:14.120
go.

17:14.120 --> 17:16.320
Some of them may be safe to delegate.

17:16.320 --> 17:19.420
Some of them may not.

17:19.420 --> 17:24.060
In OpenShift C group speed two is not yet the default when you deploy a cluster but it does

17:24.060 --> 17:26.820
work and it is supported.

17:26.820 --> 17:36.440
And to activate the C group choned semantics that I just explained we still require an

17:36.440 --> 17:41.160
annotation in the pod spec.

17:41.160 --> 17:44.320
So let's do a demo.

17:44.320 --> 17:49.760
Here's a cluster I prepared earlier.

17:49.760 --> 17:59.760
And we can see new project test, okay, OC create user test.

17:59.760 --> 18:01.440
Maybe test already exists, okay.

18:01.440 --> 18:03.400
We'll just use test.

18:03.400 --> 18:09.600
So we can now, oh, I'll show you the pod I'm going to create.

18:09.600 --> 18:14.340
Cat pod nginx host users false.

18:14.340 --> 18:16.520
So this is a pod spec.

18:16.520 --> 18:22.840
Let's get some syntax.

18:22.840 --> 18:25.400
This is going to run nginx.

18:25.400 --> 18:28.760
It's a system D based workload.

18:28.760 --> 18:38.200
So it's a Fedora system that will come up and system D will run and it will start nginx.

18:38.200 --> 18:42.240
We setting host users false so that it will run in the user name space.

18:42.240 --> 18:49.720
I have already enabled the feature flag on this cluster.

18:49.720 --> 18:55.240
There's that annotation for the C group choned.

18:55.240 --> 18:57.480
And the name of the pod will be nginx.

18:57.480 --> 18:59.200
So let's create that.

18:59.200 --> 19:07.680
So OC as test create dash F. Okay.

19:07.680 --> 19:09.680
Fingers crossed.

19:09.680 --> 19:15.120
Okay.

19:15.120 --> 19:23.040
So we'll say OC admin policy add role to user edit.

19:23.040 --> 19:24.040
Okay.

19:24.040 --> 19:29.360
Let's try that again.

19:29.360 --> 19:33.440
So the pod has been created.

19:33.440 --> 19:38.520
And we'll just check its status to see which node it is running on.

19:38.520 --> 19:40.640
And it hasn't yet started.

19:40.640 --> 19:45.760
So we don't have a container ID for it yet.

19:45.760 --> 19:54.840
But in the upper pane, I'll get a shell on that worker node.

19:54.840 --> 20:00.400
Okay.

20:00.400 --> 20:03.120
Pod is now running.

20:03.120 --> 20:09.800
So we can run cry control, inspect, container ID.

20:09.800 --> 20:12.320
And we'll just pull out the PID.

20:12.320 --> 20:15.840
So this is our PID.

20:15.840 --> 20:20.960
Now if we have a look at the user ID map for this process.

20:20.960 --> 20:40.640
Okay, so here we see that we have a user name space with a user ID mapping of size 65536.

20:40.640 --> 20:50.000
Which is mapping user ID zero in the container's user name space to user ID 131072 in the host

20:50.000 --> 20:53.960
user name space.

20:53.960 --> 20:58.640
And now we can look at the processes that are actually running there.

20:58.640 --> 21:08.120
So we'll do p grep, double dash n s says show me everything with the same set of name spaces

21:08.120 --> 21:12.600
as this process ID.

21:12.600 --> 21:16.960
And then I'll just pipe that to PS.

21:16.960 --> 21:24.760
Let's print the user, the PID and the command.

21:24.760 --> 21:26.800
Sorting by PID.

21:26.800 --> 21:28.340
Okay.

21:28.340 --> 21:36.160
So we can see that this container is running, well, in it.

21:36.160 --> 21:40.480
And then a bunch of system D processes.

21:40.480 --> 21:43.600
And then eventually, nginx.

21:43.600 --> 21:48.480
And these are running under 131072.

21:48.480 --> 21:50.480
Yeah.

21:50.480 --> 21:52.480
132071.

21:52.480 --> 21:54.480
Yeah.

21:54.480 --> 22:03.840
So these are running as various user IDs in the container's user name space.

22:03.840 --> 22:11.840
And those are being mapped to the host user ID name space as these PIDs.

22:11.840 --> 22:21.160
If we look at the logs, we can see that it looks like a regular system D system has come

22:21.160 --> 22:22.160
up.

22:22.160 --> 22:26.320
Indeed, it has.

22:26.320 --> 22:27.320
And let's see.

22:27.320 --> 22:28.320
I see.

22:28.320 --> 22:37.480
Maybe we'll get a shell on the pod.

22:37.480 --> 22:48.840
And, yeah, if we have a look at what is the container's view of the processes that are

22:48.840 --> 22:57.520
running, it sees that system D is running as root or other system D related users inside

22:57.520 --> 23:01.280
the container's user name space.

23:01.280 --> 23:07.120
nginx is running as the nginx user in that container's user name space.

23:07.120 --> 23:16.800
But as we saw, these are all mapped to unprivileged host user IDs in the host user name space.

23:16.800 --> 23:19.320
So that concludes the demo.

23:19.320 --> 23:22.360
Here's links to various resources.

23:22.360 --> 23:26.400
I have a lot of blog posts and this and related topics.

23:26.400 --> 23:31.440
So you can hit my blog and just look at the container's tag.

23:31.440 --> 23:34.480
A recording of a demo or a similar demo.

23:34.480 --> 23:36.560
Slightly earlier version.

23:36.560 --> 23:42.880
And to KEP 127, which is where all of the discussion around how to do the upstream support

23:42.880 --> 23:46.640
for user name spaces in Kubernetes.

23:46.640 --> 23:48.320
All of that discussion happened there.

23:48.320 --> 23:52.800
The OCI runtime spec is referenced there.

23:52.800 --> 23:53.800
And that's it.

23:53.800 --> 23:58.840
So I think there is time for some questions.

23:58.840 --> 24:07.560
So please stay seated until 25 so we can ask questions.

24:07.560 --> 24:08.560
Okay.

24:08.560 --> 24:09.560
There is one in the back.

24:09.560 --> 24:12.080
Do you want to read the one from the chat first?

24:12.080 --> 24:14.280
There's a question in chat.

24:14.280 --> 24:17.240
I don't see it anymore.

24:17.240 --> 24:21.040
It says, why would I want to run system D in a container?

24:21.040 --> 24:26.640
It's cool that it's possible with user name spaces, but I lack an idea for a use case.

24:26.640 --> 24:34.400
So the use case is you have a complicated legacy workload that runs under system D or

24:34.400 --> 24:40.680
makes assumptions about the environment it runs in, the user IDs that the different components

24:40.680 --> 24:42.900
are running as.

24:42.900 --> 24:44.040
You've got two choices.

24:44.040 --> 24:51.280
One is to spend a whole lot of upfront engineering effort to break up that application and containerize

24:51.280 --> 24:57.880
it and make it quote unquote a cloud native application which is expensive and typically

24:57.880 --> 25:03.120
has a long lead time or you can just wrap that whole application up in a container and

25:03.120 --> 25:08.840
run it securely, hopefully, in a container orchestration platform and get the benefit

25:08.840 --> 25:16.080
of all of the scaling, networking, observability features that the orchestration platform gives

25:16.080 --> 25:23.560
you without having to spend that effort to bust your application into 100 pieces.

25:23.560 --> 25:26.240
So I would say that that is the use case.

25:26.240 --> 25:28.340
I think it's a compelling one.

25:28.340 --> 25:32.080
If you were building applications today, you certainly wouldn't do that, but there are

25:32.080 --> 25:36.720
100 million legacy applications out there and people don't want to break them up and

25:36.720 --> 25:38.960
change them.

25:38.960 --> 25:42.520
Hi.

25:42.520 --> 25:43.520
Thanks for the talk.

25:43.520 --> 25:50.120
I'm actually doing this right now at my company, but basically the container is running as

25:50.120 --> 25:51.120
privileged.

25:51.120 --> 25:55.280
So that's why it can access a C group.

25:55.280 --> 25:59.040
Doesn't use user namespaces, just runs as privileged.

25:59.040 --> 26:06.440
So I was wondering if using this method you could set a memory max, memory high or other

26:06.440 --> 26:13.800
values for some processes running in the container, I mean?

26:13.800 --> 26:18.480
I'm sorry, I couldn't hear the question because it's rather echoey.

26:18.480 --> 26:19.480
Sorry.

26:19.480 --> 26:20.480
Yeah.

26:20.480 --> 26:27.000
So can you set memory high, memory max, values, CPU affinities, like all these kinds of things

26:27.000 --> 26:29.960
you would set in the C group usually?

26:29.960 --> 26:34.600
Can you set them from this particular use case of C groups in a container?

26:34.600 --> 26:41.160
Yes, absolutely, because the container still has its own C group namespace, so all of the

26:41.160 --> 26:48.040
standard C group confinement and resource limit capabilities can be used.

26:48.040 --> 26:49.040
Okay.

26:49.040 --> 26:56.080
I guess I got confused by the list of values that were allowed to do a list in the previous

26:56.080 --> 26:57.160
slide.

26:57.160 --> 27:00.080
There was a restricted list of values.

27:00.080 --> 27:09.200
Yeah, yeah, so those were just the particular files that need to be chined in order for

27:09.200 --> 27:17.560
safe delegation of control of that branch of the C group hierarchy to another process.

27:17.560 --> 27:28.480
So you can still set on the C group directory all of the limits and the container won't

27:28.480 --> 27:36.000
be able to change those because those will not be chined to the container's process UID.

27:36.000 --> 27:41.920
Okay, thank you.

27:41.920 --> 28:09.560
So I have a question regarding the CSFSC group CSH own.

28:09.560 --> 28:18.260
So you mentioned it's going to be changed to the UID of the process container.

28:18.260 --> 28:24.640
Can you do that if you want to have several ports to run their own system D?

28:24.640 --> 28:32.240
So, because it seems, is it in the sub-tree of the C group hierarchy that you do that

28:32.240 --> 28:35.320
or at the top level?

28:35.320 --> 28:38.040
Is your question around can you do it in a nested way?

28:38.040 --> 28:40.720
No, not in a nested way.

28:40.720 --> 28:46.520
You have three different ports which each port needs their own system.

28:46.520 --> 28:48.520
Yes, yes, absolutely.

28:48.520 --> 28:56.360
So if I created more pods in my demo, you would see that they would then be mapped to

28:56.360 --> 28:58.200
different host UID ranges.

28:58.200 --> 29:08.720
So the limit is only how many of the range allocations can you fit into the host UID

29:08.720 --> 29:09.720
range.

29:09.720 --> 29:21.280
So the limit will be a little under 65536 because the size of the host UID range on

29:21.280 --> 29:28.440
Linux by default is 2 to the 32.

29:28.440 --> 29:31.520
I think we have time for one more question.

29:31.520 --> 29:34.680
Thank you all for your patience.

29:34.680 --> 29:35.680
Thank you, Fraser.

29:35.680 --> 29:41.960
I just wanted to know if v2, v2 by default in OpenShift is on the roadmap yet and whether

29:41.960 --> 29:45.000
or not there's any sort of estimated time scale for that?

29:45.000 --> 29:46.680
Yes, it is.

29:46.680 --> 29:50.800
So there is a plan to eventually move to secret v2 as the default.

29:50.800 --> 29:56.280
I don't know the exact time frame.

29:56.280 --> 29:59.120
Thank you so much for your talk.

29:59.120 --> 30:01.880
Thank you all for your patience.

30:01.880 --> 30:22.520
I know this sounds weird, but your patience is on me.
