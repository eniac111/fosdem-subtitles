1
0:00:00.000 --> 0:00:02.000
So, I'm going to give you a round of applause.

2
0:00:07.000 --> 0:00:09.000
Welcome our next speakers and give them a round of applause.

3
0:00:10.000 --> 0:00:12.000
APPLAUSE

4
0:00:18.000 --> 0:00:21.000
. Can you hear me? I guess you can.

5
0:00:22.000 --> 0:00:26.000
Hello, everyone, and welcome to the session about how we can use

6
0:00:26.000 --> 0:00:28.000
open telemetry on Kubernetes to collect traces, metrics, and

7
0:00:28.000 --> 0:00:30.000
how we can use open telemetry on Kubernetes to collect traces.

8
0:00:30.000 --> 0:00:32.000
So, my name is Pavel. I'm a software engineer at Red Hat.

9
0:00:34.000 --> 0:00:36.000
I'm a contributor and maintainer of open telemetry

10
0:00:36.000 --> 0:00:38.000
operator and the EAGLE project.

11
0:00:40.000 --> 0:00:42.000
My name is Ben and I'm also working on the open telemetry

12
0:00:42.000 --> 0:00:46.000
operator and spent most of the time on open telemetry.

13
0:00:48.000 --> 0:00:50.000
And so, as I mentioned on today's agenda, there is the open

14
0:00:50.000 --> 0:00:53.000
telemetry operator. We will show how you can use it to

15
0:00:53.000 --> 0:00:56.000
deploy the collector, how you can as well use it to instrument

16
0:00:56.000 --> 0:00:58.000
your workloads on Kubernetes.

17
0:00:59.000 --> 0:01:01.000
And after this brief introduction, we will walk you

18
0:01:01.000 --> 0:01:03.000
through use cases, how you can use it to collect traces,

19
0:01:03.000 --> 0:01:08.000
metrics, and logs. However, I will start with the history of

20
0:01:08.000 --> 0:01:12.000
open source observability. I'm doing this because I believe

21
0:01:12.000 --> 0:01:14.000
that if we understand the history, maybe we will better

22
0:01:14.000 --> 0:01:18.000
understand where we as industry are going.

23
0:01:21.000 --> 0:01:23.000
So, on this slide, you essentially see a timeline with

24
0:01:23.000 --> 0:01:28.000
the open source project. And it's divided into the upper

25
0:01:28.000 --> 0:01:32.000
and bottom parts. In the bottom, you see the open source

26
0:01:32.000 --> 0:01:35.000
projects or platforms that you can deploy, and they provide

27
0:01:35.000 --> 0:01:41.000
you with the storage and visualization capabilities for

28
0:01:41.000 --> 0:01:44.000
the observability data. Most of them work with distributed

29
0:01:44.000 --> 0:01:48.000
traces, however, some of them, like the Apache sky walking,

30
0:01:48.000 --> 0:01:53.000
hyper trace, and signals, those are more like end-to-end

31
0:01:53.000 --> 0:01:57.000
platforms that can show traces, metrics, and logs. I would like

32
0:01:57.000 --> 0:02:01.000
to focus on the upper part that shows you the open source data

33
0:02:01.000 --> 0:02:05.000
collection kind of frameworks. And what we see there,

34
0:02:05.000 --> 0:02:09.000
especially with open sensors and open telemetry, is that it's

35
0:02:09.000 --> 0:02:12.000
becoming more important that these frameworks kind of work

36
0:02:12.000 --> 0:02:20.000
with all the signals. For me, the data collection, especially

37
0:02:20.000 --> 0:02:24.000
for tracing started with Zipkin project. It gave us a stable

38
0:02:24.000 --> 0:02:27.000
data model that we as developers could use to export traces

39
0:02:27.000 --> 0:02:31.000
into Zipkin, but as well to many other kind of platforms that

40
0:02:31.000 --> 0:02:35.000
adopted Zipkin project. As a developer, when we wanted to use

41
0:02:35.000 --> 0:02:39.000
Zipkin clients, because the ecosystem hosted client

42
0:02:39.000 --> 0:02:42.000
libraries as well, it was a bit problematic in polyglot

43
0:02:42.000 --> 0:02:46.000
environments because those clients were using inconsistent

44
0:02:46.000 --> 0:02:51.000
APIs. There was no standardization. And so this

45
0:02:51.000 --> 0:02:55.000
problem then was partially solved with open tracing. The

46
0:02:55.000 --> 0:02:58.000
scope of the project was a bit wider. There was a

47
0:02:58.000 --> 0:03:02.000
specification, there was a document that defines which

48
0:03:02.000 --> 0:03:06.000
data should be collected, and as well how the API in those

49
0:03:06.000 --> 0:03:11.000
languages should look like. This enabled us to build

50
0:03:11.000 --> 0:03:15.000
reusable instrumentation libraries. And then you later

51
0:03:15.000 --> 0:03:19.000
open sensors project started with slightly different

52
0:03:19.000 --> 0:03:22.000
approach. There was no specification, there was no API,

53
0:03:22.000 --> 0:03:27.000
but there was SDK that everybody could use and a collector.

54
0:03:27.000 --> 0:03:31.000
So with open tracing, the approach was that developers

55
0:03:31.000 --> 0:03:36.000
would use the API and then at the build time provide the

56
0:03:36.000 --> 0:03:40.000
SDK from a vendor. With open sensors, everybody would use

57
0:03:40.000 --> 0:03:44.000
the SDK, and then the collector decides where the data should

58
0:03:44.000 --> 0:03:47.000
be sent. Those two projects were kind of competing, and then

59
0:03:47.000 --> 0:03:52.000
finally they merged into open telemetry in 2019. So the

60
0:03:52.000 --> 0:03:56.000
hotel, it adopted all the pieces from open tracing and open

61
0:03:56.000 --> 0:04:01.000
sensors, but kind of the biggest innovation in hotel is the

62
0:04:01.000 --> 0:04:07.000
auto instrumentation libraries or the agents. Those agents

63
0:04:07.000 --> 0:04:10.000
are production ready, most of them, because they were

64
0:04:10.000 --> 0:04:14.000
donated by one of the observability vendors. So they

65
0:04:14.000 --> 0:04:19.000
are production tested. So when we kind of summarize what

66
0:04:19.000 --> 0:04:22.000
happened is that we started with some instrumentation

67
0:04:22.000 --> 0:04:24.000
libraries, we started with a

68
0:04:26.000 --> 0:04:29.000
project, and since we have some kind of standardization, we

69
0:04:29.000 --> 0:04:32.000
could build reusable instrumentation libraries and kind

70
0:04:32.000 --> 0:04:36.000
of create more sophisticated instrumentations for run times.

71
0:04:36.000 --> 0:04:40.000
And now we are in an age that we have available in open source

72
0:04:40.000 --> 0:04:43.000
agents or auto instrumentation libraries that we can just

73
0:04:43.000 --> 0:04:47.000
grab, put into our platforms, and we will get telemetry data

74
0:04:47.000 --> 0:04:52.000
from our platform. And I think, you know, so where are we

75
0:04:52.000 --> 0:04:56.000
going? I think we are going into an era where we as developers

76
0:04:56.000 --> 0:05:00.000
we won't have to care about how the telemetry is created for us.

77
0:05:00.000 --> 0:05:05.000
It will be the instrumentation will become maybe the feature

78
0:05:05.000 --> 0:05:09.000
of the platform where we deploy the application. So this is one

79
0:05:09.000 --> 0:05:12.000
way to look at it. The other way might be that the

80
0:05:12.000 --> 0:05:15.000
observability will shift left, and since we have this data, we

81
0:05:15.000 --> 0:05:17.000
can do a lot of things. So we are going to look at the

82
0:05:17.000 --> 0:05:17.000
observability and the

83
0:05:17.000 --> 0:05:21.000
observability of the platform. And we will be utilizing it for

84
0:05:21.000 --> 0:05:26.000
other use cases, probably like testing and security.

85
0:05:26.000 --> 0:05:30.000
So with that, I would like to move to the open telemetry.

86
0:05:30.000 --> 0:05:35.000
And it's obviously open source project hosted in the

87
0:05:35.000 --> 0:05:38.000
community computing foundation, and its main goal is to provide

88
0:05:38.000 --> 0:05:42.000
vendor on neutral telemetry data collection. It's the second

89
0:05:42.000 --> 0:05:46.000
step. And it's a very large and there is several independent

90
0:05:46.000 --> 0:05:50.000
components that we can use. There is a specification that

91
0:05:50.000 --> 0:05:54.000
defines what data should be collected and how the API should

92
0:05:54.000 --> 0:05:58.000
look like. And obviously then there is the implementation of

93
0:05:58.000 --> 0:06:03.000
the API, the SDK and the standard data model called

94
0:06:03.000 --> 0:06:06.000
OTLP, or open telemetry protocol. These four pieces are

95
0:06:06.000 --> 0:06:09.000
meant to be used primarily by instrumentation authors or, you

96
0:06:09.000 --> 0:06:11.000
know, the user-based

97
0:06:13.000 --> 0:06:17.000
system. And the last two components, the auto

98
0:06:17.000 --> 0:06:20.000
instrumentation, or agent, and collector are meant to be used

99
0:06:20.000 --> 0:06:23.000
by end users to kind of roll out observability in their

100
0:06:23.000 --> 0:06:27.000
organization. To facilitate open telemetry deployment on

101
0:06:27.000 --> 0:06:31.000
Kubernetes, there is a helm chart and Kubernetes operator.

102
0:06:31.000 --> 0:06:35.000
What I would like to stress is that open telemetry is only

103
0:06:35.000 --> 0:06:37.000
available in the open telemetry

104
0:06:39.000 --> 0:06:41.000
collection. It's not the platform that you can deploy.

105
0:06:41.000 --> 0:06:45.000
It doesn't provide any storage or query APIs.

106
0:06:45.000 --> 0:06:49.000
So now let's go to the main part, the Kubernetes operator.

107
0:06:49.000 --> 0:06:53.000
The operator itself is a go-long application. It uses

108
0:06:53.000 --> 0:06:58.000
kubebuilder and it has three primary use cases. It can

109
0:06:58.000 --> 0:07:03.000
deploy the open telemetry collector as a deployment,

110
0:07:03.000 --> 0:07:05.000
for instance, it can also install your own

111
0:07:06.000 --> 0:07:09.000
workload. It can also install your own workload,

112
0:07:09.000 --> 0:07:13.000
or instead of a stateful set, it can as well inject the

113
0:07:13.000 --> 0:07:16.000
collector as a sitecard to your workload. The second use case is

114
0:07:16.000 --> 0:07:20.000
that it can instrument your workloads running on Kubernetes

115
0:07:20.000 --> 0:07:24.000
by using those instrumentation libraries or agents from open

116
0:07:24.000 --> 0:07:28.000
telemetry. And last but not least, it integrates with

117
0:07:28.000 --> 0:07:30.500
and split them across the collector instances

118
0:07:30.500 --> 0:07:33.240
that you have deployed.

119
0:07:33.240 --> 0:07:35.320
To enable this functionality, the operator

120
0:07:35.320 --> 0:07:38.480
provides two CRDs, one for the collector that

121
0:07:38.480 --> 0:07:42.080
is used to deploy the collector and integrate the primitives.

122
0:07:42.080 --> 0:07:45.200
And the second one is the instrumentation CRD,

123
0:07:45.200 --> 0:07:47.480
where you define how the applications should

124
0:07:47.480 --> 0:07:49.840
be instrumented.

125
0:07:49.840 --> 0:07:52.000
The operator itself then can be deployed

126
0:07:52.000 --> 0:07:58.880
through manifest files, home chart, or OLM.

127
0:07:58.880 --> 0:08:01.800
So what we see here is the Kubernetes cluster.

128
0:08:01.800 --> 0:08:06.120
There are three workloads, pod1, pod2, and pod3.

129
0:08:06.120 --> 0:08:09.800
The first workload is instrumented with the auto-SDK

130
0:08:09.800 --> 0:08:10.520
directly.

131
0:08:10.520 --> 0:08:12.440
So when we were building this application,

132
0:08:12.440 --> 0:08:15.800
we pulled in the auto dependency,

133
0:08:15.800 --> 0:08:19.520
and we compiled it against it, and used those APIs directly

134
0:08:19.520 --> 0:08:24.240
in our business code and in the middleware that we are using.

135
0:08:24.240 --> 0:08:27.560
The second pod is using the auto-instrumentation libraries

136
0:08:27.560 --> 0:08:30.640
that were injected by the operator

137
0:08:30.640 --> 0:08:34.080
through the admission webhook.

138
0:08:34.080 --> 0:08:38.360
And the third pod is using Zipkin instrumentation and permit

139
0:08:38.360 --> 0:08:39.840
use instrumentation libraries.

140
0:08:39.840 --> 0:08:44.880
And it has the collector sidecar as well injected

141
0:08:44.880 --> 0:08:47.400
by the operator.

142
0:08:47.400 --> 0:08:48.920
So essentially, the operator there,

143
0:08:48.920 --> 0:08:55.800
it reconciles three open telemetry CRs, two 4D collector,

144
0:08:55.800 --> 0:08:58.040
and one instrumentation.

145
0:08:58.040 --> 0:08:59.680
And then all these workloads, they

146
0:08:59.680 --> 0:09:02.840
send data to the collector deployed probably as a daemon

147
0:09:02.840 --> 0:09:04.400
set.

148
0:09:04.400 --> 0:09:07.400
And then this collector then does some data normalization

149
0:09:07.400 --> 0:09:11.120
and sends finally data into the platform of your choice, which

150
0:09:11.120 --> 0:09:14.560
can be permit use for metrics and the algorithm for traces.

151
0:09:14.560 --> 0:09:17.840
With that, I would like to move to the second part,

152
0:09:17.840 --> 0:09:20.160
explaining the CRDs in more detail.

153
0:09:20.160 --> 0:09:20.640
Yep.

154
0:09:20.640 --> 0:09:23.720
I think the microphone should work.

155
0:09:23.720 --> 0:09:26.040
Yeah, so with the CRDs for today,

156
0:09:26.040 --> 0:09:28.840
we wanted to show both of them.

157
0:09:28.840 --> 0:09:30.840
And we start with the collector one.

158
0:09:30.840 --> 0:09:33.640
The collector CID is a bit loaded.

159
0:09:33.640 --> 0:09:35.840
So therefore, we picked a few things here,

160
0:09:35.840 --> 0:09:41.240
which I would say are the most used or important.

161
0:09:41.240 --> 0:09:44.680
So as Pavel mentioned, there are different deployment modes,

162
0:09:44.680 --> 0:09:47.840
different use cases for the open telemetry collector.

163
0:09:47.840 --> 0:09:51.000
And in this specification, we can go to the mode

164
0:09:51.000 --> 0:09:53.560
and just specify it there.

165
0:09:53.560 --> 0:09:56.120
There's a handy thing, which is the sidecar.

166
0:09:56.120 --> 0:09:58.160
We would see it afterwards.

167
0:09:58.160 --> 0:09:59.880
But if we want to use it, we only

168
0:09:59.880 --> 0:10:02.520
go to the part definition of our deployment

169
0:10:02.520 --> 0:10:06.760
and inject the annotation we see on the top right.

170
0:10:06.760 --> 0:10:10.720
And if we go to the part definition of our deployment,

171
0:10:10.720 --> 0:10:13.440
if we go with the deployment mode or something like this

172
0:10:13.440 --> 0:10:16.800
and we want to expose it for collecting matrix

173
0:10:16.800 --> 0:10:19.520
locks and traces from a different system,

174
0:10:19.520 --> 0:10:21.520
for example, we can use the ingress type.

175
0:10:21.520 --> 0:10:24.880
We can set there a lot of more.

176
0:10:24.880 --> 0:10:27.400
We configure there a lot of more, like also the annotations,

177
0:10:27.400 --> 0:10:29.880
your ingress class.

178
0:10:29.880 --> 0:10:32.960
But yeah, mainly the operator takes care of everything,

179
0:10:32.960 --> 0:10:34.240
creating services.

180
0:10:34.240 --> 0:10:37.280
Also is able to balance your load there.

181
0:10:37.280 --> 0:10:41.440
And the last thing here is then the image section,

182
0:10:41.440 --> 0:10:43.880
which is also important.

183
0:10:43.880 --> 0:10:45.800
With the open telemetry operator,

184
0:10:45.800 --> 0:10:49.080
it usually shapes the core distribution of open telemetry

185
0:10:49.080 --> 0:10:51.840
by default. So in open telemetry,

186
0:10:51.840 --> 0:10:55.480
the collector is split into two repositories when you go up

187
0:10:55.480 --> 0:10:57.560
and look at that GitHub.

188
0:10:57.560 --> 0:11:02.080
So in core, you will find OTLP, a logging exporter,

189
0:11:02.080 --> 0:11:04.040
so some basic stuff.

190
0:11:04.040 --> 0:11:07.160
And in contrib, you find basically everything.

191
0:11:07.160 --> 0:11:12.000
So if you want to send your traces to some proprietary

192
0:11:12.000 --> 0:11:16.560
vendor or to Jager, you probably need to look there.

193
0:11:16.560 --> 0:11:19.480
OK, the next thing is then the configuration.

194
0:11:19.480 --> 0:11:21.720
The configuration for the open telemetry collector

195
0:11:21.720 --> 0:11:24.240
is here provided like it's usually

196
0:11:24.240 --> 0:11:25.960
done for the collector itself.

197
0:11:25.960 --> 0:11:28.360
So it's passed directly forward.

198
0:11:28.360 --> 0:11:30.520
It's split into three parts here.

199
0:11:30.520 --> 0:11:31.960
We see the receiving part there.

200
0:11:31.960 --> 0:11:34.640
We specify our OTLP receiver.

201
0:11:34.640 --> 0:11:37.560
Here it accepts GRPC on a specific port.

202
0:11:37.560 --> 0:11:40.240
It could also be there that we specify a parameters

203
0:11:40.240 --> 0:11:43.280
receiver, which is then scraping something.

204
0:11:43.280 --> 0:11:47.720
Then the optional part is basically the processing part.

205
0:11:47.720 --> 0:11:49.400
We might want to save some resources,

206
0:11:49.400 --> 0:11:52.960
and we batch them our telemetry data.

207
0:11:52.960 --> 0:11:56.680
And yeah, there are other useful things.

208
0:11:56.680 --> 0:11:58.440
And on the exporter section here,

209
0:11:58.440 --> 0:12:00.080
we use the logging exporter, which

210
0:12:00.080 --> 0:12:01.440
is part of the core distribution.

211
0:12:01.440 --> 0:12:04.280
But you can configure whatever you like.

212
0:12:04.280 --> 0:12:08.600
You can also have multiple exporters for one resource.

213
0:12:08.600 --> 0:12:09.680
There is one thing.

214
0:12:09.680 --> 0:12:11.400
On the right side, we see the extensions.

215
0:12:11.400 --> 0:12:15.360
It didn't fit on the slide, so it's there in this box.

216
0:12:15.360 --> 0:12:18.120
This is then used if you have, for example, an exporter,

217
0:12:18.120 --> 0:12:22.040
which needs some additional headers.

218
0:12:22.040 --> 0:12:24.280
You want to set a barrier token or something else,

219
0:12:24.280 --> 0:12:25.920
you can do it there.

220
0:12:25.920 --> 0:12:28.440
And then finally, we go to the service section

221
0:12:28.440 --> 0:12:31.400
where we have different pipelines for each signal.

222
0:12:31.400 --> 0:12:34.240
And then we can then configure a processor and receiver

223
0:12:34.240 --> 0:12:37.960
and exporter in the way we want it.

224
0:12:41.200 --> 0:12:43.320
So then there is another CD, which

225
0:12:43.320 --> 0:12:46.360
is used for the auto instrumentation.

226
0:12:46.360 --> 0:12:49.240
And it looks slightly different.

227
0:12:49.240 --> 0:12:50.680
So here we have also the exporter.

228
0:12:50.680 --> 0:12:52.880
In the specification, we have the exporter.

229
0:12:52.880 --> 0:12:57.040
And the exporter only exports OTLP,

230
0:12:57.040 --> 0:13:01.560
so which means if we want to export it to some backend

231
0:13:01.560 --> 0:13:04.880
of our choice, we usually instrument our application

232
0:13:04.880 --> 0:13:08.440
directly then for what this traces to a collector instance

233
0:13:08.440 --> 0:13:10.320
which is running next to it.

234
0:13:10.320 --> 0:13:16.680
And we can use the power of these processors.

235
0:13:16.680 --> 0:13:18.920
Then we can configure some other useful things,

236
0:13:18.920 --> 0:13:22.560
like how the context is propagated and the sample rate.

237
0:13:22.560 --> 0:13:25.120
And to use it, it's also quite easy.

238
0:13:25.120 --> 0:13:26.280
So we have our deployment.

239
0:13:26.280 --> 0:13:29.640
In this case, we can choose from this list

240
0:13:29.640 --> 0:13:31.200
of supported languages.

241
0:13:31.200 --> 0:13:33.040
We might use Java.

242
0:13:33.040 --> 0:13:35.480
And we only set this annotation on the pod level.

243
0:13:35.480 --> 0:13:41.440
And it will take care of adding the SDK and also

244
0:13:41.440 --> 0:13:44.200
setting and configuring the environment variables.

245
0:13:44.200 --> 0:13:47.080
If we use something like Rust, we

246
0:13:47.080 --> 0:13:51.000
can also use the inject SDK path annotation

247
0:13:51.000 --> 0:13:54.840
to configure then just the destination

248
0:13:54.840 --> 0:13:57.760
because then the SDK should be there.

249
0:13:57.760 --> 0:14:00.560
And if we have a setup where there is,

250
0:14:00.560 --> 0:14:03.320
let's say, some proxy in front like Envoy,

251
0:14:03.320 --> 0:14:10.680
we can then just skip the adding the auto instrumentation

252
0:14:10.680 --> 0:14:13.240
there by only configuring the container names

253
0:14:13.240 --> 0:14:16.200
we want to instrument.

254
0:14:16.200 --> 0:14:20.400
And we will see this in a minute a bit more in detail.

255
0:14:20.400 --> 0:14:23.120
So this is then basically what we would need to do.

256
0:14:23.120 --> 0:14:26.080
So we create this instrumentation CRD.

257
0:14:26.080 --> 0:14:27.160
We add this annotation.

258
0:14:27.160 --> 0:14:30.960
On the left, we see the pod, there is our application.

259
0:14:30.960 --> 0:14:36.320
And in this gray box, you see what automatically is added.

260
0:14:36.320 --> 0:14:39.160
And this is then forwarded in this example to a collector.

261
0:14:41.760 --> 0:14:43.680
And how does this work?

262
0:14:43.680 --> 0:14:46.240
So the operator in the admission webhook,

263
0:14:46.240 --> 0:14:50.240
he will add this init container.

264
0:14:50.240 --> 0:14:52.640
On the top left, we see how the container looks before.

265
0:14:52.640 --> 0:14:54.320
So there are no environment variables.

266
0:14:54.320 --> 0:14:58.200
It's just a plain application.

267
0:14:58.200 --> 0:14:59.760
And in the command section, there

268
0:14:59.760 --> 0:15:02.160
is then the copy, which copies the Java agent

269
0:15:02.160 --> 0:15:04.480
to our original container.

270
0:15:04.480 --> 0:15:06.960
And on the right side, we see the final result.

271
0:15:06.960 --> 0:15:09.720
We see the Java tool options where the container is loaded.

272
0:15:09.720 --> 0:15:14.400
And then we see all these environment variables

273
0:15:14.400 --> 0:15:15.360
to configure our SDK.

274
0:15:18.400 --> 0:15:21.520
And finally, what we have seen also in the presentation

275
0:15:21.520 --> 0:15:28.080
from Nicholas previously, we have here the Jaeger output.

276
0:15:28.080 --> 0:15:30.480
So we can see the resource attributes

277
0:15:30.480 --> 0:15:35.320
and all the beautiful stuff that comes with it.

278
0:15:35.320 --> 0:15:39.440
So next, we have a look at metrics.

279
0:15:39.440 --> 0:15:43.240
So there is the open telemetry SDK.

280
0:15:43.240 --> 0:15:46.000
So if you want to go with open telemetry metrics,

281
0:15:46.000 --> 0:15:48.920
but I assume a lot of people have already

282
0:15:48.920 --> 0:15:51.160
some parameter stuff in place.

283
0:15:51.160 --> 0:15:56.520
And the open telemetry operator also helps us with this.

284
0:15:56.520 --> 0:16:02.200
So we can make we look first on the receiver part on the bottom.

285
0:16:02.200 --> 0:16:05.320
We see there we configure the parameters receiver, which

286
0:16:05.320 --> 0:16:06.840
has a scrape configuration.

287
0:16:06.840 --> 0:16:10.240
And there we can, for example, add some static targets.

288
0:16:10.240 --> 0:16:13.600
So we assume we add three different scrape endpoints.

289
0:16:13.600 --> 0:16:18.320
Then afterwards, if the target allocator is enabled,

290
0:16:18.320 --> 0:16:20.480
this will then take these scrape targets

291
0:16:20.480 --> 0:16:26.440
and spread these targets across our replicas, which

292
0:16:26.440 --> 0:16:29.080
are then responsible for getting the metrics.

293
0:16:29.080 --> 0:16:32.680
And yeah, that's basically how it works.

294
0:16:32.680 --> 0:16:36.240
There is also an option to enable parameter CRs.

295
0:16:36.240 --> 0:16:38.680
So we can then forward to this one.

296
0:16:38.680 --> 0:16:41.360
And the target allocator, which is an extra instance created

297
0:16:41.360 --> 0:16:47.560
by the open telemetry operator, will then

298
0:16:47.560 --> 0:16:49.160
get the targets from there.

299
0:16:49.160 --> 0:16:51.280
So we see this here in this graphic.

300
0:16:51.280 --> 0:16:52.320
Quite good.

301
0:16:52.320 --> 0:16:53.800
On the left side, we see pod one,

302
0:16:53.800 --> 0:16:56.680
which is using open telemetry.

303
0:16:56.680 --> 0:16:58.760
And it's pushing the information telemetry data.

304
0:16:58.760 --> 0:17:01.560
It gets directly to a collector.

305
0:17:01.560 --> 0:17:03.880
And in this gray box, we see there

306
0:17:03.880 --> 0:17:10.080
we have two instances running instrumented with permittoys.

307
0:17:10.080 --> 0:17:12.400
And the collector one and collector two

308
0:17:12.400 --> 0:17:14.520
are pulling the information from there.

309
0:17:14.520 --> 0:17:17.080
So this is all managed then by the operator.

310
0:17:17.080 --> 0:17:18.120
We have seen the replicas.

311
0:17:18.120 --> 0:17:20.840
This is basically collector one and collector two.

312
0:17:20.840 --> 0:17:23.040
And since we enable the target allocator,

313
0:17:23.040 --> 0:17:25.400
we get the targets from there, so which is then

314
0:17:25.400 --> 0:17:27.640
coming from the pod monitor.

315
0:17:27.640 --> 0:17:32.280
And finally, we send the information somewhere.

316
0:17:32.280 --> 0:17:36.560
So the last thing here, the last signal are then locks.

317
0:17:36.560 --> 0:17:38.640
So for locks, there are different options.

318
0:17:38.640 --> 0:17:43.240
So the first one would be to use the open telemetry SDK,

319
0:17:43.240 --> 0:17:47.080
what we might don't want right now,

320
0:17:47.080 --> 0:17:48.400
because we need to do some work.

321
0:17:48.400 --> 0:17:50.360
But if we directly want to go ahead,

322
0:17:50.360 --> 0:17:51.840
there is the file lock receiver.

323
0:17:51.840 --> 0:17:55.840
We can configure it to get the information from disk.

324
0:17:55.840 --> 0:17:59.720
And yeah, it's available in the Confrib repository.

325
0:17:59.720 --> 0:18:01.440
And we have different parsers there,

326
0:18:01.440 --> 0:18:08.080
which help us to move the locks into the OTLP format.

327
0:18:08.080 --> 0:18:10.920
We will see in a minute how this looks like.

328
0:18:10.920 --> 0:18:12.960
And there are other options if you

329
0:18:12.960 --> 0:18:16.560
want to integrate with Fluent bit.

330
0:18:16.560 --> 0:18:19.040
So there is a forwarder, so you can use it

331
0:18:19.040 --> 0:18:21.880
as a kind of a gateway then.

332
0:18:21.880 --> 0:18:24.760
And yeah, the only thing we need to do then

333
0:18:24.760 --> 0:18:29.040
is we can configure it as a daemon set.

334
0:18:29.040 --> 0:18:32.600
We need to pass our information there.

335
0:18:32.600 --> 0:18:34.520
And the file lock receiver, for example,

336
0:18:34.520 --> 0:18:38.200
can then get all the locks.

337
0:18:38.200 --> 0:18:40.600
And how does this look like at the end?

338
0:18:40.600 --> 0:18:46.040
So this is when we exported the locks to the logging output,

339
0:18:46.040 --> 0:18:47.560
so it's done it out.

340
0:18:47.560 --> 0:18:49.800
We see that we have the resource attributes, which

341
0:18:49.800 --> 0:18:52.240
are added automatically.

342
0:18:52.240 --> 0:18:55.640
And yeah, we see then the lock information.

343
0:18:55.640 --> 0:18:58.280
And on the bottom, the trace ID and span ID,

344
0:18:58.280 --> 0:19:00.600
which are not given if we read it just from disk.

345
0:19:00.600 --> 0:19:01.760
But that's it.

346
0:19:04.360 --> 0:19:05.840
Then we are almost at the end.

347
0:19:05.840 --> 0:19:07.840
Thank you.

348
0:19:07.840 --> 0:19:08.840
Thank you.

349
0:19:08.840 --> 0:19:19.240
Thanks a lot for the interesting talk.

350
0:19:19.240 --> 0:19:22.920
Does anyone have questions?

351
0:19:22.920 --> 0:19:23.960
Any questions?

352
0:19:23.960 --> 0:19:24.800
Raise your hand.

353
0:19:27.800 --> 0:19:28.400
Question?

354
0:19:28.400 --> 0:19:35.960
Yeah, there we go.

355
0:19:35.960 --> 0:19:39.000
For the logging part, would you suggest

356
0:19:39.000 --> 0:19:44.000
to replace any kind of cluster logging with fluent bit,

357
0:19:44.000 --> 0:19:47.760
or that's sending it off to a low key or something

358
0:19:47.760 --> 0:19:53.960
with an open telemetry log scraping?

359
0:19:53.960 --> 0:19:58.680
Or is that complementary?

360
0:19:58.680 --> 0:20:00.760
So I'm not sure if I fully got it.

361
0:20:00.760 --> 0:20:02.760
So you want to?

362
0:20:02.760 --> 0:20:04.640
Well, we already used like file being

363
0:20:04.640 --> 0:20:08.600
to replace it for the logging part of the cluster.

364
0:20:08.600 --> 0:20:10.440
And then we see that this is just

365
0:20:10.440 --> 0:20:12.880
one of the ways to do it.

366
0:20:12.880 --> 0:20:14.760
Yeah, in this case, it's just another way.

367
0:20:14.760 --> 0:20:20.120
But the useful thing is if you have the open telemetry SDK,

368
0:20:20.120 --> 0:20:24.600
it will automatically add then the trace ID to it.

369
0:20:24.600 --> 0:20:26.520
And then you can correlate your signals.

370
0:20:34.160 --> 0:20:35.120
Sorry.

371
0:20:35.120 --> 0:20:38.720
So I'm super newbie to this.

372
0:20:38.720 --> 0:20:44.280
So I fail to understand how the open telemetry is

373
0:20:44.280 --> 0:20:47.320
trying to replace, for example, the log

374
0:20:47.320 --> 0:20:50.920
parsers like the telegraph, for example,

375
0:20:50.920 --> 0:20:53.560
which is able to generate promethyose metrics

376
0:20:53.560 --> 0:20:55.560
by log scraping.

377
0:20:55.560 --> 0:20:59.880
Also how Zipkin, which is the tracing thing,

378
0:20:59.880 --> 0:21:03.280
fits in the metric collection of all this picture.

379
0:21:03.280 --> 0:21:06.800
So I'm not trying to understand how you couple together

380
0:21:06.800 --> 0:21:12.520
all these sources and how open telemetry either replaces

381
0:21:12.520 --> 0:21:16.200
or either makes it easier to use all these technologies

382
0:21:16.200 --> 0:21:16.840
together.

383
0:21:16.840 --> 0:21:17.340
Thank you.

384
0:21:21.560 --> 0:21:23.120
Yeah, so maybe on this slide, you

385
0:21:23.120 --> 0:21:29.080
see that the third part is using the Zipkin and promethyose.

386
0:21:29.080 --> 0:21:34.240
And the collector can receive data in Zipkin format.

387
0:21:34.240 --> 0:21:36.680
It can scrape promethyose metrics,

388
0:21:36.680 --> 0:21:42.400
then transform this data into OTLT or Zipkin as well,

389
0:21:42.400 --> 0:21:45.560
and then send it to the other collector.

390
0:21:45.560 --> 0:21:47.720
So the collector essentially can receive data

391
0:21:47.720 --> 0:21:49.800
in multiple formats, transform them

392
0:21:49.800 --> 0:21:53.560
to the format of your choice, and then use that format

393
0:21:53.560 --> 0:21:54.640
to send it to other systems.

394
0:22:06.000 --> 0:22:08.600
Hello, and thanks for the talk.

395
0:22:08.600 --> 0:22:13.640
I'm just wondering what's your strategy of filtering health

396
0:22:13.640 --> 0:22:15.520
check requests, for example?

397
0:22:15.520 --> 0:22:20.400
Or the life probes requests that you get on the pod?

398
0:22:20.400 --> 0:22:23.640
Health checks like to avoid generating traces

399
0:22:23.640 --> 0:22:24.680
for health checks?

400
0:22:24.680 --> 0:22:25.520
Sorry?

401
0:22:25.520 --> 0:22:29.040
To avoid generating traces for health check endpoints?

402
0:22:29.040 --> 0:22:31.120
Yeah.

403
0:22:31.120 --> 0:22:34.000
That's a very good question.

404
0:22:34.000 --> 0:22:41.200
So you could maybe configure the collector to drop the data.

405
0:22:41.200 --> 0:22:43.200
But I think the best way would be

406
0:22:43.200 --> 0:22:46.840
to tell the instrumentation to keep

407
0:22:46.840 --> 0:22:49.040
instrumenting those endpoints.

408
0:22:49.040 --> 0:22:51.960
To be honest, I'm not sure if this is implemented

409
0:22:51.960 --> 0:22:54.200
in OTLT agents.

410
0:22:54.200 --> 0:22:57.600
But I saw a lot of discussions around this problem.

411
0:22:57.600 --> 0:22:59.400
So probably there will be some solution.

412
0:23:05.280 --> 0:23:09.880
We have time for one last question, if there is any.

413
0:23:09.880 --> 0:23:10.400
No?

414
0:23:10.400 --> 0:23:10.880
OK.

415
0:23:10.880 --> 0:23:12.160
Oh, no.

416
0:23:12.160 --> 0:23:13.320
Then thanks a lot.

417
0:23:13.320 --> 0:23:42.320
And I'll see you next time.

