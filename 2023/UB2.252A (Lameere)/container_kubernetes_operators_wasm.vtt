WEBVTT

00:00.000 --> 00:11.240
So, hi everyone. I am Merlin and we're going to talk about lightweight Kubernetes operators

00:11.240 --> 00:19.520
with WebAssembly. So, basically, it's an attempt to lower the memory and CPU footprint of the

00:19.520 --> 00:28.200
Kubernetes control plane. So, I am Merlin. You can also say it in Dutch Merlin. And I

00:28.200 --> 00:35.000
am a researcher at iMac and I teach at Gent University. I'm also part of the Ubuntu Community

00:35.000 --> 00:41.680
Council. But right now, I'm here to talk about my research, which is service orchestration in the

00:41.680 --> 00:49.440
cloud and in the edge. And so, it's specifically the edge part of this research. Edge computing is

00:49.440 --> 00:55.080
becoming more and more popular. More and more people want to run their applications closer to

00:55.080 --> 01:02.920
end users on devices inside of users' homes, for example. And as a result, you have a lot of

01:02.920 --> 01:08.120
these people who are coming from a background of developing cloud applications and who now

01:08.120 --> 01:14.920
suddenly want to develop applications that run on devices, which are very low-powered. And they

01:14.920 --> 01:19.840
really like the development experience of the cloud. They like all the tools. They like the

01:19.840 --> 01:26.320
cloud-native experience with tools like Kubernetes, for example. But as most of you might know,

01:26.320 --> 01:35.440
Kubernetes isn't really a great fit for the edge. Kubernetes is incredibly resource-hungry. It

01:35.440 --> 01:42.480
really likes to gobble up RAM. It really likes to block all your CPUs. And there's a lot of

01:42.480 --> 01:49.760
components inside of the Kubernetes control plane that do this. Part of it is the Cubelet that runs

01:49.760 --> 01:57.240
on every worker machine. Part of it is the container runtimes themselves or the API server. But what

01:57.240 --> 02:11.560
I'm going to talk about in this session... I think I have no idea why. I still have batteries. So,

02:11.560 --> 02:20.320
I'm going to talk about operators specifically. Operators tend to take a lot of resources, eat up

02:20.320 --> 02:27.880
a lot of resources from your Kubernetes cluster. So first of all, operators, these are basically

02:27.880 --> 02:34.800
plugins to the Kubernetes control plane, which add additional functionality to the Kubernetes API.

02:34.800 --> 02:41.960
For example, it could add a resource to deploy and manage a MySQL cluster, or it could add a resource

02:41.960 --> 02:51.960
to deploy and manage a SEF cluster, for example. And these operators, they are also really resource

02:51.960 --> 02:58.800
hungry. And this is part of it is because they are long-running processes. So, these processes,

02:58.800 --> 03:03.080
they see something change in your Kubernetes cluster. They want to do something with it. And

03:03.080 --> 03:08.520
then write those changes back to the API server in order to manage the applications. But after

03:08.520 --> 03:14.320
that writing is done, these processes, they keep running because they keep listening for events

03:14.320 --> 03:21.600
from the Kubernetes API or even sometimes manually watching if some resource has changed. And so,

03:21.600 --> 03:26.640
even if they're doing nothing, they're still running. A lot of them are written in Golang,

03:26.640 --> 03:33.160
and Golang really likes memory. They are running inside of containers. Most of them are running

03:33.160 --> 03:39.280
inside of separate containers. And they're basically sitting in RAM doing nothing, eating up

03:39.280 --> 03:45.360
that RAM. And so, this is an issue if you want to run Kubernetes in the edge on devices which

03:45.360 --> 03:55.440
have like 512 megabytes of RAM. These operators are basically unusable in situations like that.

03:55.440 --> 04:03.400
So, how could we solve this? One of the ways that you could solve this is that we think we can

04:03.400 --> 04:09.040
solve this is by using WebAssembly and the WebAssembly system interface. And so, yes, really,

04:09.040 --> 04:15.880
we're trying to lower the footprint of Kubernetes by taking a web technology and putting it inside

04:15.880 --> 04:23.280
of Kubernetes. If you don't believe me, this is a tweet from one of the co-founders of Docker who

04:23.280 --> 04:28.280
basically said like if WebAssembly and the WebAssembly system interface would have existed in

04:28.280 --> 04:37.600
2008, they wouldn't have needed to create Docker. It's a very interesting technology which we think

04:37.600 --> 04:44.640
is a very good fit to solve this issue in Kubernetes. So, what is WebAssembly created

04:44.640 --> 04:51.960
originally for the browser? It's basically a binary code format. You compile your applications to

04:51.960 --> 04:59.480
WebAssembly instead of compiling them to x86 or to ARM. And then this code runs inside of a

04:59.480 --> 05:05.800
runtime. You could call it a very lightweight virtual machine. It runs in your browser, it runs in

05:05.800 --> 05:11.800
the Node.js runtime, but there's also a whole bunch of new purpose built, very lightweight

05:11.800 --> 05:19.280
runtimes such as wasm time, the one that we're using right now. And the WebAssembly system interface

05:19.280 --> 05:25.440
is basically a syscall interface. So, WebAssembly is your binary, but it doesn't have access to

05:25.440 --> 05:31.520
anything. And then the system interface is a syscall interface. So, that's an interface that it uses

05:31.520 --> 05:37.720
to open files, open sockets, start new threads and stuff like that. And so, if you combine these two,

05:37.720 --> 05:46.440
you basically have a very lightweight super fast sandbox. And so, the result of running these

05:46.440 --> 05:54.960
operators inside of WebAssembly containers is that they use a lot less RAM. So, here on this slide,

05:54.960 --> 06:03.440
at the top, you see 100 operators running as Docker containers. Then you have 100 operators

06:03.440 --> 06:09.720
running as WebAssembly containers, and then 100 running just on bare metal. So, we're not reaching

06:09.720 --> 06:17.640
the performance of bare metal. There's still some overhead. However, we're compared to the Docker

06:17.640 --> 06:26.680
containers, like we're getting a lot closer than that. As an advantage that we didn't see coming

06:26.680 --> 06:33.480
initially, but they also have a lot less latency. They run a lot quicker. This also shows the

06:33.480 --> 06:39.360
difference between Golang operators and Rust operators. So, obviously, Rust will have a lot

06:39.360 --> 06:45.240
less latency and a lot less latency distribution because it's not a garbage collected language.

06:45.240 --> 06:50.720
However, we were surprised to see that running them inside of WebAssembly gave them even better,

06:50.720 --> 07:00.320
even more consistent latency. So, how did we do this? We basically work with a client server

07:00.320 --> 07:07.360
model or like a parent operator and a child operator. The parent operator, it is a WebAssembly

07:07.360 --> 07:13.040
runtime with a bunch of additions to it in order to support running operators inside of that

07:13.040 --> 07:22.080
runtime. And it watches the Kubernetes resources in the name of the operators running inside of it.

07:22.080 --> 07:27.520
So, the operators don't have to keep running to watch it. They can just shut down when there's

07:27.520 --> 07:35.080
nothing to do. And the parent operator will call them once there is a change to process. The child

07:35.080 --> 07:41.720
operators, those are where the actual operators run inside. And the interesting part is that they

07:41.720 --> 07:49.880
are just regular operators compiled to WebAssembly using a patched version of the Kubernetes SDK.

07:49.880 --> 07:57.840
So, in the future, this will probably make it possible to just take a regular Kubernetes

07:57.840 --> 08:05.960
operator, compile it to WebAssembly and then use it in this system. Right now, we only support Rust

08:05.960 --> 08:14.320
because Rust support for WebAssembly is very good. Go-Lang support for WebAssembly is iffy. And we

08:14.320 --> 08:25.040
have a patched version of Kube RS, Kubernetes SDK, to then contact the parent operator instead of

08:25.040 --> 08:34.760
contacting the Kubernetes API itself. So, how does this loading and unloading work? This is the

08:34.760 --> 08:42.440
WebAssembly engine. This is basically just wasn't time, the WebAssembly runtime. And in here is your

08:42.440 --> 08:48.600
client operator, your child operator is running. Once the child operator wants to contact the

08:48.600 --> 08:54.880
Kubernetes API server, it does a sys call. We extended the WebAssembly system interface to

08:54.880 --> 09:02.320
add a few sys calls to support the scenario. And this sys call goes through to the parent

09:02.320 --> 09:08.840
operator and then the parent operator is the one who actually contacts the Kubernetes API. Once

09:08.840 --> 09:15.440
these calls are finished, the parent operator, it contacts the child operator back again in order

09:15.440 --> 09:22.400
to give it the result of these calls. And if the child operator is not doing anything, the parent

09:22.400 --> 09:28.000
operator shuts down the child operator. And once there changes to process, it starts it up again.

09:28.000 --> 09:35.640
And so the results I showed you on the first slides, those results are just not unloading

09:35.640 --> 09:42.640
anything. Just running Kubernetes operators inside of WebAssembly. So these results are what you

09:42.640 --> 09:51.760
get when you have the worst case scenario for unloading operators when they're not doing anything.

09:51.760 --> 09:58.840
And so we see that in a worst case scenario, they still use 50% less RAM because they're constantly

09:58.840 --> 10:06.760
being unloaded and then reloaded again once there's changes to process. However, this is obviously

10:06.760 --> 10:17.760
at the cost of latency. Even though WebAssembly, it starts incredibly fast. It has latency that just

10:17.760 --> 10:23.760
can't be compared to Docker containers for starting applications. There is still some latency to start

10:23.760 --> 10:30.240
a WebAssembly application. And so this compounds in the worst case scenario of like 100 operators

10:30.240 --> 10:41.360
chaining themselves up to 12 seconds, which is an issue. So what are we doing now? So we have this

10:41.360 --> 10:48.080
basic proof of concept to show that this seems to be a very good approach to lower the footprint of

10:48.080 --> 10:55.120
the Kubernetes control plane. And we want to do more with this. Currently, we're improving the build

10:55.120 --> 11:00.800
tools and we're making more realistic tests. All the tests we did right now were a worst case

11:00.800 --> 11:06.880
scenario of operators constantly doing stuff. However, in the real world, most operators don't

11:06.880 --> 11:13.360
do anything most of the time. So we're creating more realistic tests to see what these operators,

11:13.360 --> 11:20.240
what the performance benefits are for real workloads. We're also working on predictive

11:20.240 --> 11:25.600
unloading so that if we know that an operator is going to have to run again in a few milliseconds,

11:25.600 --> 11:32.320
we don't unload it because it's better to just keep it running. In the future, we want to work on

11:32.320 --> 11:39.520
better support for controllers that wake periodically. So right now, we see that a lot of production

11:39.520 --> 11:45.440
controllers actually wake periodically every five seconds or every 20 seconds in order to

11:45.440 --> 11:51.440
manually check resources in the Kubernetes API because some of those resources, they can't

11:52.240 --> 11:57.840
work with callbacks. So we are trying to figure out a way to actually put that functionality

11:57.840 --> 12:03.120
into the host operator itself so that even when you're watching resources that don't support

12:03.840 --> 12:10.640
event-based APIs, the operator is still sleeping as long as there's nothing to process.

12:11.600 --> 12:16.400
And we're also really interested in upstreaming and standardizing this. We have patches for

12:16.400 --> 12:21.760
QBAR-S. We have an extension for the WebAssembly system interface. It would be very interesting to

12:21.760 --> 12:28.960
see if there's people in the ecosystem who are interested in this and support for Golang, although

12:28.960 --> 12:35.760
this will probably not be work that we're doing. We'll just wait until Golang is better supported

12:35.760 --> 12:44.240
in WebAssembly. So I have to thank the developers. Francesco is somewhere here in the audience.

12:44.240 --> 12:51.840
We started from a prototype created by Francesco and Marcus, which runs Kubernetes controllers

12:51.840 --> 12:59.120
inside of WebAssembly. And we refactored it to use WasmTime and we added the unloading mechanism.

12:59.120 --> 13:06.240
This was done by Tim as part of his master's thesis. And right now, student Kevin is working on it

13:06.240 --> 13:14.240
also as part of his master's thesis to improve the build system so that it's much easier to get started

13:14.240 --> 13:20.720
with it and to add predictive unloading and more realistic benchmarks to have a better idea of what

13:20.720 --> 13:29.200
is the performance for actual production controllers. So the main reason I am here today is to say like,

13:29.200 --> 13:35.360
hey, we have a really cool proof of concept, which solves an issue that we have been having.

13:35.360 --> 13:43.360
Is this solving an issue for other people in the community? And are you interested in working together on this?

13:43.360 --> 13:49.360
If you're interested in working together on this, please get in touch. If you're a student yourself

13:49.360 --> 13:55.360
and you want to do like an internship or a master's thesis working on this, we have a lot of opportunities,

13:55.360 --> 14:09.360
same for a PhD. So please contact us, send me an email to see what we can do for you and how we could collaborate.

14:09.360 --> 14:19.360
So this is the end of my presentation and there's now room for questions. I also put the link to part of our code here.

14:19.360 --> 14:25.360
I think this GitHub repo also links to the other repositories that you need.

14:25.360 --> 14:49.360
Okay, we can take a couple of questions.

14:49.360 --> 14:59.360
So why was is so fast and why it is not possible to do something similar with JVM?

14:59.360 --> 15:09.360
So definitely, like JVM and WebAssembly are very similar in that regard and a lot of people,

15:09.360 --> 15:18.360
they position WebAssembly as being like a more cross-platform and a more cross-language version of the JVM.

15:18.360 --> 15:31.360
But if you're only interested in Java and Java-based languages, then the Java runtime itself is a very good alternative to this.

15:31.360 --> 15:40.360
Okay, there was another one over here, right?

15:40.360 --> 15:50.360
So if I understood correctly, you are deploying your operators outside containers and that makes them much more efficient.

15:50.360 --> 15:59.360
But besides the security aspects, when you deploy in containers in Kubernetes,

15:59.360 --> 16:07.360
you have many other things that you can set, like resource limits, but also things like post-topology spread constraints

16:07.360 --> 16:13.360
and notations to make sure that some processes are running on specific nodes and so on.

16:13.360 --> 16:15.360
How can you address that with WebAssembly?

16:15.360 --> 16:22.360
Because you cannot package then your operator like any other workload that you deploy in Kubernetes.

16:22.360 --> 16:26.360
Yeah, so it's a very good question.

16:26.360 --> 16:32.360
So one of the benchmarks was just running the operators on bare metal, but that's not actually what I'm proposing.

16:32.360 --> 16:39.360
It was just to see what is the absolute maximum amount of performance we could get out of this.

16:39.360 --> 16:44.360
Our plan is to run each operator inside of its own container.

16:44.360 --> 16:51.360
It's just a WebAssembly plus WebAssembly system interface container instead of a Docker container.

16:51.360 --> 17:02.360
And so most of the security profile and stuff like that that you have with Docker containers is very similar with WebAssembly.

17:02.360 --> 17:10.360
Some would even argue that it's more secure in WebAssembly because it has a much smaller API footprint

17:10.360 --> 17:16.360
and it has some of the best teams working on it to make sure it's secure for the browser.

17:16.360 --> 17:28.360
Moreover, the code that is running in these WebAssembly containers in my proof of concept, this is control plane code.

17:28.360 --> 17:32.360
So this is code that the system administrator selected.

17:32.360 --> 17:39.360
Like, okay, yeah, I want this specific system administration code to manage my applications.

17:39.360 --> 17:55.360
And so in that sense, there is also like a higher level of trust put into the code, which means that things like attacks and stuff like that,

17:55.360 --> 17:58.360
there is less of a risk to it.

17:58.360 --> 18:11.360
But even then, like it's still running inside of containers.

18:11.360 --> 18:20.360
So one of the most important scalability aspects of Kubernetes controllers is the watch based cache, right?

18:20.360 --> 18:26.360
So without it, the API server wouldn't be able to handle all the long pulling and so on.

18:26.360 --> 18:33.360
And it's also one of the most memory intensive aspects of Kubernetes controllers.

18:33.360 --> 18:40.360
I was wondering in your memory benchmarks if you were cutting down on this watch based aspect,

18:40.360 --> 18:44.360
or if it is still included in the parent operator.

18:44.360 --> 18:50.360
So for example, is the parent operator caching as a proxy for the child operators?

18:50.360 --> 18:52.360
Is that the case?

18:52.360 --> 18:57.360
Yeah, that's what's happening basically. The parent operator is where the caches are.
