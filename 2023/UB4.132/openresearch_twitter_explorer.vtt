WEBVTT

00:00.000 --> 00:14.760
Okay. Hi, everyone. It's a big pleasure to be here. My name is Armin Purnaki, and I'm

00:14.760 --> 00:22.680
a PhD candidate in applied mathematics, and I work on building tools for discourse analysis.

00:22.680 --> 00:28.640
And we build tools for discourse analysis based on methods from graph theory, network

00:28.640 --> 00:34.120
science and natural language processing. And today I want to present a tool called the

00:34.120 --> 00:40.520
Twitter Explorer that is already a bit older, but that was built in the institute in the

00:40.520 --> 00:45.520
Max Planck Institute for Mathematics and the Sciences in my previous group. And the idea

00:45.520 --> 00:51.080
was to build a tool that allows researchers who don't necessarily have programming skills

00:51.080 --> 00:59.360
to collect Twitter data, visualize them using graphs, and explore the data and maybe generate

00:59.360 --> 01:06.280
hypotheses in their pipelines. So this kind of tool building and this research happens

01:06.280 --> 01:13.120
in the field called computational social science. So when I was preparing my slides two days

01:13.120 --> 01:17.120
ago, I thought it would be good to maybe give a little overview of computational social

01:17.120 --> 01:21.840
science, then say why we built the Twitter Explorer and where we saw somehow the need

01:21.840 --> 01:27.480
for a new tool. Of course, introduce the features of the tool because it's kind of a talk on

01:27.480 --> 01:34.040
programming, the architecture, and maybe give some insights on the usage. But when I was

01:34.040 --> 01:43.280
sitting down to make the slides two days ago, I was confronted with this. And of course,

01:43.280 --> 01:51.240
since the tool is essentially an entry point into the free API, there's also a part of

01:51.240 --> 01:58.200
it that uses the research API, which of course led us directly to this question, what happens

01:58.200 --> 02:06.680
to the research API? It's also not entirely clear, right? So I want to maybe instead of

02:06.680 --> 02:12.440
giving this talk the way I was planning to do it, I will do it, but maybe I wanted to

02:12.440 --> 02:17.440
ask a few questions first that we might then discuss maybe in the discussion also. And

02:17.440 --> 02:22.560
I think there is even something planned later, right? So some kind of panel discussion. So

02:22.560 --> 02:26.560
I'm just going to throw some questions out there that I think are really pressing now,

02:26.560 --> 02:32.480
especially in the research field. How serious is this? By this, I don't mean the implications

02:32.480 --> 02:37.400
of it because I know a few people whose thesis is now in jeopardy because they can't collect

02:37.400 --> 02:46.080
data in a way. But how serious is it in the sense? Will it actually happen? Or is it some

02:46.080 --> 02:53.280
scare tactic? So I think this is something that is hard to predict. And then these are

02:53.280 --> 02:58.960
questions also I think that we can discuss here is how, or is there a way for us as users

02:58.960 --> 03:04.320
and not necessarily only as researchers to claim our data or our digital traces that

03:04.320 --> 03:09.520
we use and that we leave on these platforms? And how can things like the Digital Services

03:09.520 --> 03:17.920
Act play a role in this? And the last question is very broad, but how do we move on in the

03:17.920 --> 03:25.440
sense of how can we see this as some kind of wake up call maybe? And how can we use

03:25.440 --> 03:30.720
this new development to maybe on one hand move to different platforms, but on the other

03:30.720 --> 03:37.200
hand also to think about how we do computational social science in the future? So with these

03:37.200 --> 03:43.080
questions that we're going to discuss later, I'm still going to give my original talk.

03:43.080 --> 03:50.280
So in computational social science, a typical pipeline for a project is you have a research

03:50.280 --> 03:56.040
question, then you collect data related to it. And in this case, it may be data from

03:56.040 --> 04:02.520
online social platforms. And then you analyze it and ideally you generated some more insights

04:02.520 --> 04:06.680
on the research question you had in the beginning. And sometimes the exploration and the analysis

04:06.680 --> 04:11.400
of the data can help you maybe refine also the questions you had in the beginning. So

04:11.400 --> 04:17.080
it's some kind of loop that you can see in this way. And the tool that I'm going to present

04:17.080 --> 04:23.240
the Twitter Explorer is precisely made for this second part, for both facilitating the

04:23.240 --> 04:30.840
collection and also the exploration of such data. And this pipeline is that we start with

04:30.840 --> 04:37.880
text. So in our case, it's tweets that are annotated with some kind of metadata. We have

04:37.880 --> 04:42.160
on Twitter different types of interactions. So you can mention someone, you can reply

04:42.160 --> 04:50.240
to someone or retweet. And we choose one type of metadata and cast it into an interaction

04:50.240 --> 04:58.280
network. And then we want to find the most significant, for instance, clusters or the

04:58.280 --> 05:06.040
significant correlations in this data by using 2D spatializations. And typically, these are

05:06.040 --> 05:10.640
done using forced layouts. But today, for instance, in the graph room, there were also

05:10.640 --> 05:14.960
some talks about new methods of node embedding. And so I think this is also something that

05:14.960 --> 05:21.840
we can discuss maybe in the questions section. But one reason why I think forced layouts

05:21.840 --> 05:27.200
are good is that especially if you use them in a context where you work with social science

05:27.200 --> 05:33.640
researchers who don't necessarily have a lot of knowledge about the latest machine learning

05:33.640 --> 05:38.600
algorithms, they are quite straightforward to explain in the sense that you have a spring

05:38.600 --> 05:45.520
system and nodes that are strongly connected tend to attract each other. And especially

05:45.520 --> 05:54.200
if you look at interaction networks on Twitter, since retweeting can be considered endorsement,

05:54.200 --> 05:59.760
clusters in such 2D spatializations can then correspond to something like opinion clusters.

05:59.760 --> 06:04.400
And there's a lot of research being done in that way. But one question that we always

06:04.400 --> 06:09.960
had when we look at these networks is how do we actually go back to the data that generated

06:09.960 --> 06:16.720
them? And this is something that we tried to tackle with building these tools. So why

06:16.720 --> 06:21.560
we built it is firstly to provide an interface for researchers without programming skills

06:21.560 --> 06:26.440
also to collect and visualize the data, because we were working a lot with social scientists

06:26.440 --> 06:32.360
that did not have these programming skills, but had a lot of hypotheses about the data

06:32.360 --> 06:38.600
that they could not test. Then, of course, to facilitate the exploration of controversial

06:38.600 --> 06:45.960
issues on social media. And this is the point that I was making before, is add some layer

06:45.960 --> 06:52.120
of interpretability to these 2D spatializations by providing an access from within the interface

06:52.120 --> 07:00.320
to the actual data that created these node positions. And finally, we see it in the context

07:00.320 --> 07:07.560
of a larger scientific scope of using the network paradigm as something like a sampling

07:07.560 --> 07:13.320
mechanism for the data. Because if you're confronted with a large number of tweets,

07:13.320 --> 07:17.240
for instance, of course, everyone knows that you can't read all of them manually. So you

07:17.240 --> 07:23.480
need some kind of way to get to the tweets that are relevant for you to read. And this

07:23.480 --> 07:29.440
is what we use the network for, essentially. So we can look, when we look at retweet networks,

07:29.440 --> 07:33.960
immediately identify, for instance, the most influential actors in the debate, and then

07:33.960 --> 07:39.800
read precisely those tweets that they made to maybe influence other actors. And we call

07:39.800 --> 07:45.440
this guided close reading, because if you do only close reading, then you have to read

07:45.440 --> 07:50.600
all the text. If you have distant reading, you kind of look only at the network on a

07:50.600 --> 08:01.920
structural level, and this is something in between. So what can the tool do? It collects

08:01.920 --> 08:11.880
tweets. I mean, I think we have like one week left for the V2 and the V1. So far, the V2

08:11.880 --> 08:16.440
academic is safe, but we don't know that. So you can search for it from the past seven

08:16.440 --> 08:22.720
days using the API. In the second part, in the visualizer, you can do, display just a

08:22.720 --> 08:28.400
simple time series of the tweets to see maybe if there's some kind of special activity during

08:28.400 --> 08:36.160
one day. You can build these interaction networks, build co-hashtag networks. So we divide it

08:36.160 --> 08:42.160
into some kind of two types of networks, which we call semantic networks and interaction

08:42.160 --> 08:49.480
networks. And then you can compute the typical measures people compute on networks, and especially

08:49.480 --> 08:58.120
compute clusters like using modularity-based algorithms. And all this happens in some kind

08:58.120 --> 09:05.320
of interactive interface using JavaScript and D3.js. And this is essentially the part

09:05.320 --> 09:08.640
where it gets interesting, because so far, all the other things you can do it with a

09:08.640 --> 09:14.400
lot of other tools, especially like Gefi. I think you can even collect tweets with some

09:14.400 --> 09:20.200
plugins. So I think all of this is not new, and this is kind of where it gets interesting.

09:20.200 --> 09:25.280
And I think this is time for a quick demo. I don't know how much. Okay, I have plenty

09:25.280 --> 09:43.200
of time. I think I talked too fast. Okay, so I've prepared some Python environments

09:43.200 --> 09:53.040
that already have the Twitter Explorer installed, but usually you would do it like this. And

09:53.040 --> 10:02.480
then all you need to do to fire up this interactive interface is type Twitter Explorer collector.

10:02.480 --> 10:10.960
This will open a browser window from which you can choose your API access, choose the

10:10.960 --> 10:17.800
path to which the tweets will be downloaded, and insert your search query, maybe adding

10:17.800 --> 10:23.920
some advanced settings and saving options. So I don't know. This is a question to the

10:23.920 --> 10:32.440
audience now, what we should search for. This is easy. And you're looking into the future.

10:32.440 --> 10:37.400
I already have this network prepared for the last slide. Sorry. Can you look at the reaction

10:37.400 --> 10:43.800
to the Twitter API? We could, but what would we look for then? Is there maybe a hashtag

10:43.800 --> 10:57.920
like API shutdown? Maybe we need to go to Twitter itself. API something like this. Ideally,

10:57.920 --> 11:08.680
we would find some kind of hashtag. Okay, let's just use maybe this as a search query.

11:08.680 --> 11:25.000
Okay. Now it's collecting in the background. Then we can open another browser window here.

11:25.000 --> 11:33.480
Fire up the visualizer. Now we see that while this is still collecting, we can already access

11:33.480 --> 11:49.080
this. There were only 400 tweets, so there seems to be. So we can look at a time series

11:49.080 --> 11:57.320
of tweets, and then we can choose different types of networks to create. We can filter

11:57.320 --> 12:02.800
them by language if we want. And this is the language that the Twitter API returns. So

12:02.800 --> 12:11.800
there's no language detection going on here. We can do some network reduction methods like

12:11.800 --> 12:19.320
taking only the largest connected component of the graph. Then we have this option here

12:19.320 --> 12:24.060
to remove the metadata of nodes that are not what we call public figures. So if you want

12:24.060 --> 12:32.240
to publish some explorable networks, it is advisable to do so. There is not, as far as

12:32.240 --> 12:38.600
I know, not a very distinctive or clear rule after which point one is considered such a

12:38.600 --> 12:44.000
public figure, but within our consortium, we decided that it's 5,000 followers. This

12:44.000 --> 12:51.120
is also something we could discuss. But since Twitter is public by default, in a way, anything

12:51.120 --> 12:58.480
you post is somehow potentially to be used and displayed somewhere. Then you can export

12:58.480 --> 13:05.560
the graph to all sorts of formats. Then you can aggregate nodes. This means that, for

13:05.560 --> 13:10.720
instance, removing them based on how many retweets they have or how many retweets they

13:10.720 --> 13:17.120
did themselves, and remove, for instance, nodes that only retweeted one person. So is

13:17.120 --> 13:36.480
there a chalk? Maybe somewhere? If you have a graph, and then there are some nodes that

13:36.480 --> 13:44.340
only retweet this person. I don't know if everyone can see that, but they tend to clutter

13:44.340 --> 13:50.200
the force-directed algorithm. Structurally, they do not necessarily add anything to the

13:50.200 --> 13:54.480
network. So if you have very, very large graphs, it makes sense to remove these and somehow

13:54.480 --> 14:08.440
englope them into this super node. Then you can do traditional community detection. Then

14:08.440 --> 14:24.440
it will be saved as an HTML that you can then open. So we see here that this is, again,

14:24.440 --> 14:29.600
in a retweet network, every node is a user, and the link is drawn from A to B if A retweets

14:29.600 --> 14:41.240
B. Now we can look at this user, T Chambers, and look at the actual tweets that were made

14:41.240 --> 14:54.240
for them to end up at this part of the visualization. Okay. So the data we collect is kind of sparse,

14:54.240 --> 15:05.800
so this network doesn't look that interesting, but I have prepared some fallback options.

15:05.800 --> 15:13.080
So what we did in a case study a few months ago was to look at the repercussion of some

15:13.080 --> 15:21.040
discussions in the US about red flag laws. And red flag laws are specific kinds of laws

15:21.040 --> 15:28.720
for gun control that allow state level judges to confiscate temporarily guns from people

15:28.720 --> 15:36.000
that are deemed to be a threat to themselves or to the public. And these laws created very

15:36.000 --> 15:42.200
big repercussions, especially on social media and especially in the conservative camps.

15:42.200 --> 15:48.480
And this is one typical example where people then can analyze on Twitter if there's something

15:48.480 --> 15:53.800
like echo chambers or if people then maybe retweet each other only from the similar camps,

15:53.800 --> 15:59.160
and then people draw very quick conclusions very fast. And what we want to do with this

15:59.160 --> 16:06.080
tool is to show that maybe things are not that simple as they seem. So I have prepared

16:06.080 --> 16:23.360
these networks, but I think I will make it a bit smaller. So this is now a bit bigger

16:23.360 --> 16:34.600
than what we had before. We have roughly 25,000 nodes and 90,000 links. And this is already

16:34.600 --> 16:38.600
one limitation of the tool that I think I would also like to discuss in the end is that

16:38.600 --> 16:43.760
you can't display mentally huge graphs. So 100,000 links approximately is kind of the

16:43.760 --> 16:50.080
limit. And I think this is also where integrating it with other tools such as Sigma or Gayfri

16:50.080 --> 16:58.880
might actually make a lot of sense. And so now I can color the nodes by the Luvan community.

16:58.880 --> 17:09.360
We can turn off the light also. And now we can wonder what are these two communities.

17:09.360 --> 17:15.520
And right now the node size is proportional to the indegree, meaning how often a given

17:15.520 --> 17:21.360
node was retweeted. But if we want to, so these may then be considered as something

17:21.360 --> 17:27.280
like the opinion leaders of the given camps. And so if we go here, we see for instance

17:27.280 --> 17:36.800
on this side Donald Trump Jr. And we can then look exactly at the tweets that led the visualization

17:36.800 --> 17:43.600
to put him where he was. So okay, we don't need to go into the details of what he said,

17:43.600 --> 17:50.760
but you see the point. We can also change the node size to the number of followers.

17:50.760 --> 17:56.980
And then we get an immediate view at who the main actors are that in general are also influential

17:56.980 --> 18:10.960
on Twitter. So we have the New York Times here. And Wall Street Journal. So we can see

18:10.960 --> 18:17.720
that we have something like of a more liberal versus a more conservative camp. But if we

18:17.720 --> 18:23.240
look only at the retweet behavior, we might think that okay, these are separated echo

18:23.240 --> 18:27.800
chambers and people do not talk to each other. But what is interesting is if we look at other

18:27.800 --> 18:33.800
types of networks in these examples. So we can look at the replies. I think I will make

18:33.800 --> 18:42.960
it a bit smaller. And all of the sudden we don't see this very strong segregated clustering

18:42.960 --> 18:52.920
anymore that we saw here. Maybe it's easier if I put it in. But we see something more

18:52.920 --> 19:04.120
of a hairball layout. And when we look at the nodes, we see that indeed the path of

19:04.120 --> 19:09.360
going for instance from Donald Trump to Hillary Clinton or New York Times of those people

19:09.360 --> 19:14.800
that were very far apart of the network is maybe not that long in the reply network.

19:14.800 --> 19:19.080
These opposing camps actually maybe do talk to each other. And it might be more interesting

19:19.080 --> 19:23.080
to see how they talk to each other and what they say. And this is something that you can

19:23.080 --> 19:32.320
do when you use this interface and look at the tweets and that actually replies. So it

19:32.320 --> 19:39.600
allows you to then actually go to the parts of the platform that generate this data and

19:39.600 --> 19:50.160
that then generate these networks. And finally as a small example of the semantic networks,

19:50.160 --> 20:04.200
you can look at the hashtags that I used. Again, I'll make it smaller. And you see that

20:04.200 --> 20:10.720
for instance there is one kind of hateful conservative hashtag cluster which and again

20:10.720 --> 20:15.600
maybe I should have said that in the hashtag networks every node is a hashtag and they

20:15.600 --> 20:23.120
are connected if they appear together in the same tweet. So this is a very low level way

20:23.120 --> 20:26.800
of seeing what is going on in the data in a way. You don't need to do some kind of topic

20:26.800 --> 20:31.800
modeling or complicated techniques. You can literally just by looking at the hashtags

20:31.800 --> 20:38.160
already get a hint at how the different camps speak about the same topic. So if you go here

20:38.160 --> 20:48.560
in this area, this is about confiscation laws. So Marxism in this case is also a good example.

20:48.560 --> 20:53.880
Right now we don't really know how it is used, right? And it can be used either by conservatives

20:53.880 --> 20:59.920
or by liberals and it's important to look at it in the context of the data. So then

20:59.920 --> 21:14.640
we would have to, okay, five minutes left. Good. I will go back to the slides. Okay.

21:14.640 --> 21:21.720
So under the hood, this whole backend of the collector and the visualizer is written in

21:21.720 --> 21:31.280
Python and it's using the streamlit Python library to serve it on a local front end.

21:31.280 --> 21:37.080
So this is actually a very convenient library and I guess a lot of people also know it,

21:37.080 --> 21:41.680
but you can write your code in Python and then it essentially serves it in interfaces

21:41.680 --> 21:54.560
that look like this. And the explorer is written in HTML and JavaScript and it uses D3 and

21:54.560 --> 22:03.240
prints the graph on canvas, which is also why it's probably not as fast as Sigma is,

22:03.240 --> 22:09.080
but it has some nice other features that are especially due to this force graph library.

22:09.080 --> 22:15.360
So I think if anyone has questions, I'm going to go into the details and the questions.

22:15.360 --> 22:20.360
So this is how we install it. It's fairly simple. If you have a running Python bigger

22:20.360 --> 22:26.620
than 3.7 and there's also an API. So of course, especially here, probably people will not

22:26.620 --> 22:31.980
be so interested in using the streamlit interface, but you may want to include it into some kind

22:31.980 --> 22:40.440
of existing code pipeline that you have. And this is essentially the API for semantic networks

22:40.440 --> 22:48.720
and interaction networks. So I invite you to try it out yourself while you still can.

22:48.720 --> 22:57.320
You have five days. Of course, if you have the research API, you might be able to use

22:57.320 --> 23:06.000
it for a bit longer, but otherwise go on these websites fast. And I will stop the talk with

23:06.000 --> 23:10.040
some questions. Actually, I came here with more questions than answers and I'm really

23:10.040 --> 23:15.280
hoping for a lively discussion now because I'm not originally a developer, so I kind

23:15.280 --> 23:21.160
of wrote this a bit on my own. And I wonder if this integration of Python and JavaScript

23:21.160 --> 23:24.880
is actually a good idea because in theory it would also be possible to probably do everything

23:24.880 --> 23:29.840
in JavaScript and maybe do it on the client side so you wouldn't have to install all these

23:29.840 --> 23:35.600
libraries. Then, okay, maybe one thing that I would like to show is that I experimented

23:35.600 --> 23:41.440
with temporal networks. So of course, doing temporal force layouts is kind of a non-trivial

23:41.440 --> 23:49.320
task, but we can kind of look a little bit at the temporality of these networks by at

23:49.320 --> 23:56.960
least displaying only the links that are active during a given day. So this is also kind of

23:56.960 --> 24:01.400
nice, I think, but I would like to discuss maybe other visualization paradigms for this

24:01.400 --> 24:07.160
kind of network. Then one thing that would be really interesting, I think, is to dig

24:07.160 --> 24:12.600
deeper into a visualization paradigm for hierarchical structure of communities, meaning

24:12.600 --> 24:18.360
that okay, in theory I can either run stochastic block models or luvain community detections

24:18.360 --> 24:22.840
and stop them at a certain level and then have some kind of hierarchical node structure.

24:22.840 --> 24:26.120
But how to visualize that is another question, but I think it would be very interesting, especially

24:26.120 --> 24:32.240
for very large graphs. Then another question is force layouts. Should we still use them

24:32.240 --> 24:37.080
now that everyone is doing node2vec and all these other things? I think yes, but maybe

24:37.080 --> 24:44.760
there's good arguments against it. And on a more deeper conceptual level is, and this

24:44.760 --> 24:48.680
is a question, the first one is a question for people who already have much more experience

24:48.680 --> 24:53.520
in building tools for the social sciences, is how do you kind of further integrate these

24:53.520 --> 25:00.920
kinds of methods into existing maybe also more qualitative social science pipelines?

25:00.920 --> 25:05.640
So yeah, it's kind of an open question. And how can we devise something like a research

25:05.640 --> 25:10.640
protocol for these kinds of interactive network visualizations? Because as you saw in my demo,

25:10.640 --> 25:16.040
we kind of look at the big nodes, we look at the tweets they made, and it gives us some

25:16.040 --> 25:20.920
kind of intuition of what's going on in the debate. But how can we formalize such kinds

25:20.920 --> 25:25.320
of visual network analysis? And I think, I mean, there's people in the audience who actually

25:25.320 --> 25:30.520
work on this, so it would be very interesting for me to talk about this. And finally, to

25:30.520 --> 25:39.440
end on actually maybe a bit nicer note is that there is the network of firsts as we

25:39.440 --> 25:44.440
had already said in the beginning on this website. So it is updated every 15 minutes

25:44.440 --> 25:50.800
thanks to a data collection done by my colleague, Beatrice. Thank you very much. And so if you

25:50.800 --> 25:59.040
go on this website, you can see the retweet network of firsts. And if you tweet, then

25:59.040 --> 26:07.400
you can find yourself in the network also. So yeah, what do we have here in the middle?

26:07.400 --> 26:20.600
OK, first them itself. Then there was Ubuntu, Debian somewhere. OK, time's up. Thank you.

26:20.600 --> 26:38.200
Do we have questions?

26:38.200 --> 26:59.400
Yes. So the question is, I mentioned that you can only collect tweets from the last

26:59.400 --> 27:06.800
seven days. With the free API, this is a limitation. But the tool itself just writes into existing

27:06.800 --> 27:11.400
CSV. It depends, basically. So if you do the same keyword search multiple times, then it

27:11.400 --> 27:20.120
will just append to a CSV. Yeah, so this is.

27:20.120 --> 27:25.840
Yes, I mean, this is the question right now. It depends because the question is what happens

27:25.840 --> 27:32.760
on Mastodon. I don't know. All these, like if you want to look at political controversies

27:32.760 --> 27:39.960
and such discussions, I don't know if Mastodon is mature enough yet to or adopted enough

27:39.960 --> 27:44.320
yet. I think if you want to look at the first income, it's great. So for this, yes. But

27:44.320 --> 27:45.320
yeah.

27:45.320 --> 28:03.520
I know computer science for more than 40 years. And I am very skeptical of this kind of thing,

28:03.520 --> 28:27.600
I'm one. You don't buy paper, right? Really?

28:27.600 --> 28:30.120
Yes.

28:30.120 --> 28:33.280
You know, dancing.

28:33.280 --> 28:36.800
computers, computers, computers, or many people

28:36.800 --> 28:38.360
visits which we have so often.

28:38.360 --> 28:42.400
So I'm very reactive about this kind of thing.

28:42.400 --> 28:44.520
I don't know what you should think about that.

28:44.520 --> 28:49.560
Well, I don't know which point exactly should I address,

28:49.560 --> 28:51.400
because you raised a lot of.

28:51.400 --> 28:54.160
So are you OK?

28:54.160 --> 28:56.360
If I can rephrase, so you are concerned

28:56.360 --> 28:59.640
about this kind of research also?

28:59.640 --> 29:02.680
Yes, because it can be used to track users

29:02.680 --> 29:03.800
across political camps.

29:03.800 --> 29:08.880
We have problems with computers of a popular section

29:08.880 --> 29:09.800
of population.

29:09.800 --> 29:14.880
There are no computers taking them in account.

29:14.880 --> 29:15.380
I see.

29:15.380 --> 29:17.960
So I think it's more about the representativity of Twitter

29:17.960 --> 29:21.280
data for the wider population, which of course,

29:21.280 --> 29:23.040
you're totally right.

29:23.040 --> 29:26.960
It is kind of a subset of highly politicized, maybe also

29:26.960 --> 29:29.360
a bit more educated than average people.

29:29.360 --> 29:30.200
So you cannot.

29:30.200 --> 29:32.160
But this is not what we're trying to do also.

29:32.160 --> 29:34.640
You're not trying to infer, I don't know,

29:34.640 --> 29:39.000
actual election results based on Twitter data.

29:39.000 --> 29:43.040
So yeah, I don't know if I addressed the point.

29:43.040 --> 29:44.480
Maybe we can take more about it.

29:44.480 --> 29:46.200
To be concerned about this kind of thing.

29:46.200 --> 29:47.320
Right.

29:47.320 --> 29:48.320
Thank you.

29:48.320 --> 30:17.600
Next time.
