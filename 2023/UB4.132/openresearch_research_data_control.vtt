WEBVTT

00:00.000 --> 00:16.380
All right, so I'll start a little bit earlier of the, actually, the 2PM.

00:16.380 --> 00:20.240
Just by saying that, so I'm wearing this t-shirt because I'm one of the Dev Room Manager here

00:20.240 --> 00:25.660
in this room, and I'm taking over, actually, a talk slot that has been cancelled.

00:25.660 --> 00:33.800
So we were supposed to hear a talk by Maya Arias de Reina, who couldn't make it today,

00:33.800 --> 00:34.960
unfortunately.

00:34.960 --> 00:42.120
She was supposed to talk about data flowing in the right way, which is a talk about a

00:42.120 --> 00:49.200
tool called Kauto, which implements data workflows with a low-code, no-code approach.

00:49.200 --> 00:53.320
This is what it looks like, so of course I can't talk about this tool because I don't

00:53.320 --> 00:54.320
know it.

00:54.320 --> 00:57.520
It really looks pretty cool.

00:57.520 --> 01:06.400
So we are very sorry for Maya not being here, and we hope we can host her next time.

01:06.400 --> 01:19.600
So I will speak about a project, a research project, called Ricardo in Digital Maintenance,

01:19.600 --> 01:25.200
which I've done with, oops, sorry, are we going for me?

01:25.200 --> 01:32.840
Yes, I worked with Bertrice de Dangere from Sciences Po, she is a historian, and I am

01:32.840 --> 01:33.840
Paul Girard.

01:33.840 --> 01:40.800
I am from a company called Westware, a small agency, specialized into developing data-oriented

01:40.800 --> 01:45.040
web applications, and we do work a lot with researchers.

01:45.040 --> 01:50.600
Today I'm here to talk about how actually a collaboration between a researcher, Bertrice

01:50.600 --> 01:56.640
and Dangere, myself, can be fostered by using data control systems.

01:56.640 --> 02:01.880
By data control systems, I mean making sure we care about the data we are using in the

02:01.880 --> 02:09.600
research by documentation, version control, and also quality control.

02:09.600 --> 02:14.920
So the research is about the history of trade.

02:14.920 --> 02:22.600
So together with Bertrice, we build a database of trade flows between nations, well, between

02:22.600 --> 02:29.760
geopolitical entities in the world in the 19th century, which means that we have the

02:29.760 --> 02:38.640
main data is we know how much amount of money in different currencies has been exchanged

02:38.640 --> 02:42.480
between different geopolitical entities in the 19th century.

02:42.480 --> 02:47.360
We know important exports, and we know this with a bilateral view, which means that we

02:47.360 --> 02:54.880
know the trade from France to UK, for instance, and the reverse two from two different sources,

02:54.880 --> 02:58.880
which makes quite a nightmare to deal with, but still.

02:58.880 --> 03:04.120
So this is basically the main publication we already achieved.

03:04.120 --> 03:13.200
So we started by releasing in 2016-17 a web exploration tool, I'll show you, and also

03:13.200 --> 03:16.440
a paper about how we build this database.

03:16.440 --> 03:24.720
And then in 2021, we released a new database called Geopol East, which is basically a dataset

03:24.720 --> 03:30.000
that explained trying to track which geopolitical entity.

03:30.000 --> 03:35.040
So I'm using this weird word because we have countries, of course, but also entities that

03:35.040 --> 03:40.440
are part of countries, but we also have trade with entities that were colonies at that time

03:40.440 --> 03:43.720
and all the kind of weird political statuses.

03:43.720 --> 03:49.600
And because of that, we built this Geopoly database where we tried to track which geopolitical

03:49.600 --> 03:55.320
entities were controlled by which other one in time.

03:55.320 --> 04:04.520
And recently, we released a new version of the database, adding 230,000 new trade flows.

04:04.520 --> 04:11.040
And this releasing of new data, because actually Beatriz discovered new archives, new archival

04:11.040 --> 04:19.360
book about trade, this massive updates needed a tool to make sure we can actually release

04:19.360 --> 04:25.160
the data which are clean the structure the way we want it without having to deal with

04:25.160 --> 04:28.120
all those kind of issues manually.

04:28.120 --> 04:30.000
I will speak about that a little bit later.

04:30.000 --> 04:33.480
So this is what the websites, the main website looks like.

04:33.480 --> 04:40.040
It's a web application you can go where you can explore basically the trade of the world

04:40.040 --> 04:42.040
in the 19th century.

04:42.040 --> 04:43.920
So we have different kinds of visualizations.

04:43.920 --> 04:49.480
I will not go through all of them because I don't want to focus too much on this today.

04:49.480 --> 04:54.840
Well if you have questions about this, we can go back to that later.

04:54.840 --> 05:01.120
We also have these websites, Geopoly, that allows you to actually explore the political

05:01.120 --> 05:10.960
evolutions of the links, sovereignty, links between the different entities.

05:10.960 --> 05:17.480
I'll show you a little bit what it's like just afterwards I think.

05:17.480 --> 05:23.120
So just as we totally announced, this slide deck is actually something I already presented

05:23.120 --> 05:25.600
in another conference.

05:25.600 --> 05:32.320
So I wanted to speak about the visual data exploration tools we made, the frictionless

05:32.320 --> 05:33.680
data integration.

05:33.680 --> 05:37.280
So this is the main point I want to speak about today, point two.

05:37.280 --> 05:44.600
And also the third point was how we can actually analyze heterogeneous data in the long run,

05:44.600 --> 05:47.200
like one century of data.

05:47.200 --> 05:55.400
My main, I will try to convince you that actually using data integration is a very nice and

05:55.400 --> 06:00.520
important tool to foster this long lasting collaboration we had between Bertrice, historian,

06:00.520 --> 06:03.560
and I, a data engineer.

06:03.560 --> 06:07.360
So that collaboration I just put a link to a conference I gave a few years ago about

06:07.360 --> 06:10.760
this specific subject.

06:10.760 --> 06:15.600
So visual data exploration, so I will really go quickly on this part to focus more on the

06:15.600 --> 06:18.400
second part.

06:18.400 --> 06:26.480
Our main objective here is to propose basically a tool, a set of interactive data visualizations

06:26.480 --> 06:34.520
on the web that all those researchers are basically people exploring this to change

06:34.520 --> 06:41.240
points of view on the data, looking at, for instance, the total trade of the world, then

06:41.240 --> 06:49.600
focusing on one specific country, then on one specific currency, and to be able to add

06:49.600 --> 06:54.080
all the different ways to look at the data in the same tool.

06:54.080 --> 06:58.480
We also like to offer visual documentation, like visualization is a really nice important

06:58.480 --> 07:08.080
tool to spot issues or surprises or errors in the data and to unfold the complexity of

07:08.080 --> 07:09.680
the whole phenomenon.

07:09.680 --> 07:16.680
So this is, for instance, the world view, so we are able to retrace the world trade

07:16.680 --> 07:20.460
in a century, but as you can see there is more than one curve, so we have different

07:20.460 --> 07:23.320
ways we can calculate that actually from the data.

07:23.320 --> 07:31.560
We can, for instance, take some researchers that did re-estimations of this total trade

07:31.560 --> 07:36.080
by correcting sources and all this kind of stuff, so that's one way to do it.

07:36.080 --> 07:42.480
That's the green curve in this visualization, but we also can actually sum all the totals

07:42.480 --> 07:43.480
we have in our data.

07:43.480 --> 07:50.480
This is the yellow one, and we also have the, so here we are summing all the flows we have,

07:50.480 --> 07:55.520
the yellow, and the red is more, we are summing only the total that were in the archive books,

07:55.520 --> 07:57.160
and it's not the same thing.

07:57.160 --> 08:01.760
If you sum what we have or if you take the sum that's done at the time, you don't have

08:01.760 --> 08:03.760
the same results.

08:03.760 --> 08:09.080
Welcome to the nightmare of dealing with archive data.

08:09.080 --> 08:13.800
In this visualization, for instance, we are focusing on one country, Denmark, and then

08:13.800 --> 08:22.800
we can actually spot the trade on the long run of this specific country, and we can also

08:22.800 --> 08:31.880
visualize, so here is Germany's on the right, we can also depict actually not only the total

08:31.880 --> 08:38.120
trade, but also the difference trade, bilateral trade between Germany and its trade partner

08:38.120 --> 08:39.520
a long time.

08:39.520 --> 08:45.560
Okay, so this is an objective, so geopolysts here, for instance, we see like when we talk

08:45.560 --> 08:49.440
about Germany's overline, what are we talking about?

08:49.440 --> 08:56.120
And we are talking about geopolitical entity that had a different statisticism on time,

08:56.120 --> 09:02.200
you can see here, and then you can see on the bottom line which other parts, all other

09:02.200 --> 09:08.840
geopolitical entities were actually part of Germany through time.

09:08.840 --> 09:17.480
Because sometimes we have trade with only Saxony or Waldeck, and we want to know eventually

09:17.480 --> 09:21.640
if those entities are part of another one.

09:21.640 --> 09:26.000
So frictionless data integration.

09:26.000 --> 09:32.300
So we are using a data package from frictionless data here from the Open Knowledge Foundation.

09:32.300 --> 09:39.920
So actually there is a talk from Evgeny from frictionless team later today in the online

09:39.920 --> 09:46.080
part of our room, we'll talk about the new tool fostering data package, and actually

09:46.080 --> 09:50.620
I'm very interested into that, but I will talk about what I've done myself.

09:50.620 --> 09:59.180
So about this project, the main, yeah sorry, the first thing we do is versioning the data.

09:59.180 --> 10:05.320
So we put data as CSVs into a version control system like a Git, simply.

10:05.320 --> 10:10.360
Here it's on GitHub, and you can see that you can track actually just the same way we

10:10.360 --> 10:14.400
do with code, who changed which data, when and why.

10:14.400 --> 10:20.640
So here for instance is Beatrice who actually corrected a typo in the flow number, adding

10:20.640 --> 10:24.760
the comma at the right place, and we have the commentary here.

10:24.760 --> 10:30.520
This is very important to keep track of what's going on with the, because we have like hundreds

10:30.520 --> 10:33.360
and even thousands of files like that.

10:33.360 --> 10:40.000
So it's very important to have that also to know if we have issues, if that happens, where

10:40.000 --> 10:42.000
it comes from.

10:42.000 --> 10:43.000
So data package.

10:43.000 --> 10:49.040
Data package is a formalism where it's basically a JSON file where you will actually describe

10:49.040 --> 10:52.240
the data you have, adding a documentation.

10:52.240 --> 10:56.680
So the first interest of using data package is to actually document your data set to make

10:56.680 --> 11:02.040
it easier for other people to actually understand what you want it to do.

11:02.040 --> 11:05.840
And it's very important for publication at the end, open science.

11:05.840 --> 11:12.200
So here we have the title of the project, we have the license, the contributors, that's

11:12.200 --> 11:14.440
also very important to have.

11:14.440 --> 11:17.400
And then we describe resources.

11:17.400 --> 11:20.040
Resources can be seen as a data table.

11:20.040 --> 11:25.640
So here from some of the RIC entities, and for each resources, which is a CSV here, we

11:25.640 --> 11:27.720
describe the fields we have in the table.

11:27.720 --> 11:33.300
So we know that the RIC name table is a string, it's unique, and it is required.

11:33.300 --> 11:37.040
So it's really like a relational database schema.

11:37.040 --> 11:41.600
It's really kind of the same spirit, but an images and format.

11:41.600 --> 11:44.520
The reason why you can do that, as I said, is documenting.

11:44.520 --> 11:51.840
The second reason is actually to control your data, so doing right, driving data validation.

11:51.840 --> 11:59.400
So if you have a data package, describe like that, you can then use a Python library, frictionless,

11:59.400 --> 12:05.120
it's called frictionless, which will actually check all your data line if each data line

12:05.120 --> 12:08.360
you have respects the schema you wrote.

12:08.360 --> 12:14.920
And if it's not the case, it will actually provide you with a report with errors like,

12:14.920 --> 12:20.840
for instance, here I have a foreign key error because the modified currency here is not

12:20.840 --> 12:24.480
known in the table that's supposed to have this data.

12:24.480 --> 12:29.560
So it's a very nice way to actually, we get new data, and then we check, okay, where do

12:29.560 --> 12:35.440
we stand about what we want to achieve at the end, which is to respect the data package

12:35.440 --> 12:37.800
formalism we wrote.

12:37.800 --> 12:40.160
So that's very nice.

12:40.160 --> 12:49.200
It's very cool for data engineers, but as I said, our goal with Beatrice is that we

12:49.200 --> 12:55.920
work together and she, because the thing is like when she enters new data in the system,

12:55.920 --> 13:01.720
she has an historian to take decisions on how to interpret the data that were in the

13:01.720 --> 13:02.720
archive.

13:02.720 --> 13:04.480
I count, that's not my job.

13:04.480 --> 13:07.160
I thought about responsibility, I don't have the skills to do it.

13:07.160 --> 13:13.720
So we need something to allow her to actually correct the data, update the data that comes

13:13.720 --> 13:16.840
in into the data package format.

13:16.840 --> 13:21.400
And she can't use a common line tools in Python script and that kind of stuff.

13:21.400 --> 13:27.560
So we need something, we missed a tool here to let actually humanist researchers, in this

13:27.560 --> 13:33.600
case, but people in general, to be able to interact with the data flow with something

13:33.600 --> 13:36.780
else than actually a two-technical interface.

13:36.780 --> 13:43.840
So we built, we developed a specific web application that actually helps Beatrice to integrate

13:43.840 --> 13:49.180
new data by using the data package as a validation system.

13:49.180 --> 13:51.000
So all of this is done in JavaScript.

13:51.000 --> 13:55.220
You also have a JavaScript library for the data package.

13:55.220 --> 13:56.780
So basically, this is the steps.

13:56.780 --> 14:03.460
So the idea is Beatrice will upload a data spreadsheet, so a new data file, transcription

14:03.460 --> 14:09.840
of one new archival book she found, the tool which first checks the spreadsheet format,

14:09.840 --> 14:12.720
saying like, do we have all the columns we want and everything?

14:12.720 --> 14:20.920
If it's correct, then it will go through all the data points, checking all the errors and

14:20.920 --> 14:30.680
grouping them to propose a creation interface for her to correct all source issues through

14:30.680 --> 14:32.040
a form.

14:32.040 --> 14:39.640
And we tried to do, to develop something that makes this process, which is tedious and a

14:39.640 --> 14:46.200
long process, as easy and as fast as we could for her, Beatrice, to actually go through

14:46.200 --> 14:47.200
this.

14:47.200 --> 14:54.800
At the very end, this tool will actually commit to and push, commit it into a Git repository

14:54.800 --> 14:57.080
and push it into the GitHub repository.

14:57.080 --> 15:02.200
It follows that down into a serverless web application, which means that I didn't have

15:02.200 --> 15:07.340
had to introduce Git command line to Beatrice neither.

15:07.340 --> 15:10.240
The tool does that for her.

15:10.240 --> 15:11.240
So this is what it looks like.

15:11.240 --> 15:14.280
So it's a React web application.

15:14.280 --> 15:20.760
Here we have the schema validation summary where we see for the fields, so the different

15:20.760 --> 15:24.860
fields we have for which we have errors, the kind of the error we have.

15:24.860 --> 15:30.840
And at the end, we have the error overview which says how many rows that has an issue.

15:30.840 --> 15:36.840
For instance, here in the source column, we have two different invalid values that impact

15:36.840 --> 15:40.200
169 rows.

15:40.200 --> 15:47.580
The idea here is to correct all this group of 169 rows with only one edit.

15:47.580 --> 15:55.040
So once we have all sorts of errors, basically the process of workflow using this tool will

15:55.040 --> 15:59.240
be to go through the error groups one by one.

15:59.240 --> 16:06.480
The web application will actually generate a form with inputs to actually help Beatrice

16:06.480 --> 16:07.480
to decide.

16:07.480 --> 16:10.520
So in this example, we have a value for a partner.

16:10.520 --> 16:12.320
So partner is a trade partner.

16:12.320 --> 16:14.440
So it's geopolitical entity.

16:14.440 --> 16:15.440
Here it's in French.

16:15.440 --> 16:17.500
It's il de célon.

16:17.500 --> 16:22.280
So we use English-based vocabularies to translate the partner.

16:22.280 --> 16:27.520
So we need to decide what is il de célon in our vocabulary in the rest of the data.

16:27.520 --> 16:32.960
And this is where we have search input where actually Beatrice can actually search for

16:32.960 --> 16:39.040
célon which is called in our vocabulary, scélonca, célon, in parenthesis.

16:39.040 --> 16:45.600
And once she should chose that, actually the tool we correct this column and put the data

16:45.600 --> 16:54.240
at the right place to make sure we will translate il de célon to the scélonca célon.

16:54.240 --> 17:01.200
At the end, once she went through all the process, we have summary here explaining all

17:01.200 --> 17:03.560
the corrections she made.

17:03.560 --> 17:10.240
So in the first line, for instance, a year was misspelled.

17:10.240 --> 17:11.600
All that kind of thing.

17:11.600 --> 17:15.480
We change the source name and everything is fixed.

17:15.480 --> 17:21.720
So once all the errors have been corrected through the form, the data form I just showed

17:21.720 --> 17:32.200
you, then she can move on with the last step which is actually to publish this new data

17:32.200 --> 17:37.000
that is now valid because we know it's valid because we can control it with the data package

17:37.000 --> 17:41.040
into the GitHub repository.

17:41.040 --> 17:49.240
And this is how basically the RAC-to-web application will really prepare the data.

17:49.240 --> 17:58.120
So I could go into details into what does that mean later and make it possible for Beatrice

17:58.120 --> 18:06.720
to actually take the right decisions to adapt the raw data into the data package we worked

18:06.720 --> 18:09.440
with.

18:09.440 --> 18:12.880
So I have a little bit more time.

18:12.880 --> 18:14.400
So this is the analysis.

18:14.400 --> 18:24.880
Maybe I can try to demo a little bit the tool live.

18:24.880 --> 18:29.040
So the very important thing is it's a several or less web application.

18:29.040 --> 18:35.520
So here it's on my local host on my laptop, but actually it's hosted on GitHub directly.

18:35.520 --> 18:43.160
So what is the media lab, actually a lot of this work has been done at the media lab,

18:43.160 --> 18:45.520
my previous employer.

18:45.520 --> 18:51.040
So congrats to them too because they contributed to that work too a lot.

18:51.040 --> 18:59.280
So this is the tool.

18:59.280 --> 19:02.640
It's hosted on GitHub.io because it doesn't need any server.

19:02.640 --> 19:07.720
All the logging process with GitHub is done through a personal token, which is a very

19:07.720 --> 19:12.800
specific long key that you have to produce in your GitHub account for once.

19:12.800 --> 19:16.600
Then you use that as a login mechanism.

19:16.600 --> 19:19.680
So this is what it looks like.

19:19.680 --> 19:27.960
Once I am logged in, I can fetch the data from GitHub to make sure I have the last version

19:27.960 --> 19:31.200
of the data before adding new things.

19:31.200 --> 19:37.480
In here I can prepare the little file here, which normally should have some errors.

19:37.480 --> 19:42.280
So the first thing here you see like this green message here on the bottom says that

19:42.280 --> 19:47.600
actually this CSV file is valid structure wise.

19:47.600 --> 19:53.720
And the errors of the columns are good, which is a good first step.

19:53.720 --> 20:00.200
And then this is the all the errors I have in my file.

20:00.200 --> 20:04.520
This is a nice step because you want to review what kind of mess you are going to on top

20:04.520 --> 20:06.920
of where you are before starting the process.

20:06.920 --> 20:11.320
Because if you have too many, maybe you want to do that later or asking for help.

20:11.320 --> 20:15.280
So once you've seen that, you can start.

20:15.280 --> 20:19.640
So this is basically all the things we have to do.

20:19.640 --> 20:21.280
So this is the first one.

20:21.280 --> 20:27.840
I can move to the next error or go back, even though I haven't corrected it yet.

20:27.840 --> 20:36.000
And here I say like, okay, so the value is parallel to, commercial A, I don't know, whatever.

20:36.000 --> 20:39.600
This character is not actually a unit because the unit should be an integer.

20:39.600 --> 20:42.320
Yeah, it's true.

20:42.320 --> 20:44.520
So it's better with a, oops, sorry.

20:44.520 --> 20:46.360
So better with a one.

20:46.360 --> 20:48.800
And I can confirm this fix.

20:48.800 --> 20:50.080
And now we're good.

20:50.080 --> 20:51.080
Unit is one.

20:51.080 --> 20:52.680
Now I move to the next one.

20:52.680 --> 20:55.160
You see here I am in two on nine.

20:55.160 --> 20:58.840
So all the processors are trying to make that as smooth as we can.

20:58.840 --> 21:04.880
So as soon as I fix it, so here I have, it's written in French, it's Milne-Lafsant-Trent-Witz.

21:04.880 --> 21:07.880
But actually we want that to be an integer again.

21:07.880 --> 21:10.880
I don't want the later version of the year.

21:10.880 --> 21:14.840
So Milne-Lafsant-Trent-Witz has a number.

21:14.840 --> 21:18.080
As soon as I confirm the fix, I move to the next page.

21:18.080 --> 21:22.680
So that's, we can try to make that process as seamless as possible.

21:22.680 --> 21:25.320
So here I have a source, so this is a foreign key.

21:25.320 --> 21:31.200
So in the source, in the data table, the source is actually, it's a key that is referring,

21:31.200 --> 21:33.720
which is referring to the source table.

21:33.720 --> 21:36.560
And say like, so here basically foreign key source violation.

21:36.560 --> 21:40.100
So it means that this sort doesn't exist in our system.

21:40.100 --> 21:42.560
So here I have two choice.

21:42.560 --> 21:47.720
Or I was supposed to, okay, normally I should, oh yeah, sorry.

21:47.720 --> 21:49.560
Can you dad?

21:49.560 --> 21:51.220
No.

21:51.220 --> 21:54.560
So normally, okay, so whatever.

21:54.560 --> 22:01.880
So I can search for it and find it, or, and in which case it will, the edit will be only

22:01.880 --> 22:06.180
replacing the key, or I can create a new item.

22:06.180 --> 22:09.360
And here you can see that, here I'm creating a new source because sometimes the source

22:09.360 --> 22:10.360
doesn't exist yet.

22:10.360 --> 22:14.280
So I have to go through all the, you see this form is much, much longer because here I'm

22:14.280 --> 22:18.320
creating a new line into the source table.

22:18.320 --> 22:23.120
I will not do that because it's too long, I will just give me something please.

22:23.120 --> 22:25.440
And that will make it, okay.

22:25.440 --> 22:26.440
And so on and so forth.

22:26.440 --> 22:33.440
Again, we have an issue with the, sorry, this example is a little bit up.

22:33.440 --> 22:35.960
Okay, here it's a Trini-Dad and Tobago.

22:35.960 --> 22:43.400
It's a duopatical line time, I don't know because it's a Trini-Dad and Tobago, not a.

22:43.400 --> 22:44.400
Okay.

22:44.400 --> 22:45.400
Up.

22:45.400 --> 22:49.520
And we are good.

22:49.520 --> 22:54.720
Australia with a lot of E at the end is not correct.

22:54.720 --> 22:55.720
It's Australia.

22:55.720 --> 22:56.720
Yep.

22:56.720 --> 23:00.720
Sorry, it's very long.

23:00.720 --> 23:03.720
Yeah, whatever.

23:03.720 --> 23:05.720
Dollar.

23:05.720 --> 23:12.600
Let's say it's this, it's this crap.

23:12.600 --> 23:14.600
Don't do that, right?

23:14.600 --> 23:15.600
Okay.

23:15.600 --> 23:16.600
Ah.

23:16.600 --> 23:28.800
Okay, so you see, that's important point is like, so we are based on the data package,

23:28.800 --> 23:29.960
in the data package.

23:29.960 --> 23:36.000
So we are using foreign keys, the specification and so on, but actually we had to add specific

23:36.000 --> 23:37.800
forms for our case.

23:37.800 --> 23:39.600
So the application here is not generic.

23:39.600 --> 23:43.680
You can't just put a new data package of yourself with your data and it will not work because

23:43.680 --> 23:51.600
we had actually for UX and UI reasons to make specific cases like that where the forms are

23:51.600 --> 23:55.200
not exactly as the data package described it.

23:55.200 --> 24:00.320
It was too complex to make it very generic, but actually with more work that could be

24:00.320 --> 24:02.160
achieved maybe at some point.

24:02.160 --> 24:05.480
And actually the talk from the evergreen is this afternoon, we'll talk a little bit about

24:05.480 --> 24:07.840
that kind of stuff.

24:07.840 --> 24:10.560
So here we are.

24:10.560 --> 24:14.600
I'll stop the demo here.

24:14.600 --> 24:19.560
Just to finish, why do we do all kinds of stuffs?

24:19.560 --> 24:21.960
Because we want to analyze trade in the long run.

24:21.960 --> 24:23.880
We have lots of trade values as you can see.

24:23.880 --> 24:27.040
A lot of trading entities, very too much.

24:27.040 --> 24:32.760
And then at the end we try to, this is a visual documentation where we depict the kind of

24:32.760 --> 24:37.480
different source we use in the database.

24:37.480 --> 24:45.920
And at the end we try to do something like that where actually here we have the trade

24:45.920 --> 24:50.040
of the world in 1890.

24:50.040 --> 24:56.080
So each node here, circles, are geopolitical entities.

24:56.080 --> 24:59.040
And the links between them are the trade of that year.

24:59.040 --> 25:01.240
So it's total trade.

25:01.240 --> 25:02.960
We could choose import or export here.

25:02.960 --> 25:05.320
I just summed it up.

25:05.320 --> 25:10.720
The important part here is the color here is based on the type of geopolitical entity

25:10.720 --> 25:11.720
we have.

25:11.720 --> 25:19.240
So in this orange or kind of yellow thing, it's what we call the GPH entity.

25:19.240 --> 25:25.080
It's entities that geopolitical entity we know, mainly countries but not always.

25:25.080 --> 25:27.480
In green, so are colonial areas.

25:27.480 --> 25:28.480
So it's not a colony.

25:28.480 --> 25:30.560
It's not a country which is a colony.

25:30.560 --> 25:34.000
It's like French Africa.

25:34.000 --> 25:36.920
It's like, we know it's a colony.

25:36.920 --> 25:39.120
We don't know which one exactly.

25:39.120 --> 25:42.880
Like here, for instance, we have European Russia, which is a city part of.

25:42.880 --> 25:46.000
It's from Russia, but it's the European part of it.

25:46.000 --> 25:48.160
And this is what we find in the archive books.

25:48.160 --> 25:51.000
So we can't really decide what that means exactly.

25:51.000 --> 25:57.600
So we try to analyze this kind of, so we have this, there is this gap between very heterogeneous

25:57.600 --> 26:03.960
data, very difficult to interpret, but still try to do quantification and analyze this

26:03.960 --> 26:11.680
like this networks on top of this very complex and rich dataset.

26:11.680 --> 26:16.520
I think I'm good with what I wanted to share with you today.

26:16.520 --> 26:23.200
We can move to question if you have.

26:23.200 --> 26:28.200
Yes, please.

26:28.200 --> 26:33.200
There was a slide.

26:33.200 --> 26:39.200
The slide was had a title, which was development of a specific web application to integrate

26:39.200 --> 26:40.200
new data.

26:40.200 --> 26:41.200
Yes.

26:41.200 --> 26:48.720
You might not have time to tell me, but please tell me if you have time at some point, what

26:48.720 --> 26:51.680
was the conversation with your historian?

26:51.680 --> 26:53.680
How did this happen?

26:53.680 --> 26:56.520
What did you actually do to plan?

26:56.520 --> 27:04.000
So the question is like how Beatrice and I ended up, decided to do that.

27:04.000 --> 27:08.040
So basically the whole point is very like a collaboration because we worked without that

27:08.040 --> 27:09.640
for a very long time.

27:09.640 --> 27:13.760
And the process was we had to meet in the same room.

27:13.760 --> 27:18.680
I was doing the script, checking the data, editing the data, because editing the data

27:18.680 --> 27:23.960
in a spreadsheet data wise that doesn't mess up your numbers and everything is not easy

27:23.960 --> 27:24.960
actually.

27:24.960 --> 27:27.920
And we were working together on that.

27:27.920 --> 27:31.760
It was necessary and actually very nice to do because we had to exchange.

27:31.760 --> 27:36.480
So she was explaining to me why she was taking this decision or not.

27:36.480 --> 27:40.040
And she was taking this and I was just putting data.

27:40.040 --> 27:46.380
But at some point we ended up with the fact that we had so much more to add that this

27:46.380 --> 27:48.840
process couldn't scale basically.

27:48.840 --> 27:54.600
So we had to find something else to make sure she could do this process on her own.

27:54.600 --> 28:00.760
And I would intervene once the data is in the GitHub repository, checking myself with

28:00.760 --> 28:06.440
quantification and scripts and everything again because you always need to check everything

28:06.440 --> 28:08.660
many times.

28:08.660 --> 28:14.120
And then it makes the whole process much more smoother.

28:14.120 --> 28:16.840
Yes?

28:16.840 --> 28:46.200
Sorry, I don't get it.

28:46.200 --> 28:54.080
So, the question is like, would it be beneficial or possible to actually commit the data before

28:54.080 --> 28:57.200
checking it and put it in GitHub?

28:57.200 --> 28:59.680
So yes and no.

28:59.680 --> 29:04.280
The reason why we don't do that is because, the first one, because I need Bethz to take

29:04.280 --> 29:11.520
the decision of documenting the raw data to make it compatible with all the nice visualization

29:11.520 --> 29:13.080
I showed you.

29:13.080 --> 29:14.360
And she needs to take the decision.

29:14.360 --> 29:16.480
She needs to do it.

29:16.480 --> 29:23.960
So that's why we put this data into the GitHub after she has done this work of data creation.

29:23.960 --> 29:31.240
We could actually host the data as a raw file first and then do that later on that kind

29:31.240 --> 29:32.240
of stuff.

29:32.240 --> 29:40.360
Still, we still need a web interface that lets Bertris, the historian, take the decision.

29:40.360 --> 29:44.880
So no.

29:44.880 --> 29:45.880
Any more?

29:45.880 --> 29:46.880
Yeah?

29:46.880 --> 29:47.880
Just a few things.

29:47.880 --> 29:54.200
I saw on the last slide that the board gave me the idea of organizing.

29:54.200 --> 29:55.200
Yes.

29:55.200 --> 30:00.000
So, this tool I'm using here is actually brand new.

30:00.000 --> 30:01.880
It's a Giphy but on the web.

30:01.880 --> 30:05.080
We are working on this with my company, Westware.

30:05.080 --> 30:07.720
And we are very close to release it.

30:07.720 --> 30:14.080
It's basically the same thing as the Giphy but lighter version and the web version.

30:14.080 --> 30:19.720
It's already there, but you shouldn't go because it's not live yet.

30:19.720 --> 30:21.720
Thank you.

30:21.720 --> 30:38.080
Thank you.
