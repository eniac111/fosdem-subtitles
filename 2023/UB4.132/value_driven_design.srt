1
0:00:00.000 --> 0:00:11.160
So, hello, welcome everybody. I'm Announcer Zwinfritilanes, but I should also announce

2
0:00:11.160 --> 0:00:21.420
Emily Trump. We did this project together, and unfortunately she couldn't make it today

3
0:00:21.420 --> 0:00:28.060
because of problems with public transportation. So, she's now at home watching this presentation,

4
0:00:28.060 --> 0:00:37.200
and I hope I can make her proud. I'm Zwinfritilanes. I'm just a privacy expert, and usually making

5
0:00:37.200 --> 0:00:47.840
totally dual, bad design presentations. Emily is a designer, really working with design

6
0:00:47.840 --> 0:00:56.560
thinking and really likes to reframe things. And she's also the one who is responsible

7
0:00:56.560 --> 0:01:10.200
for this really great presentation. I'm totally happy with it. So, failure-driven design.

8
0:01:10.200 --> 0:01:16.440
This is about a case study, and it has been really successful privately by design projects.

9
0:01:16.440 --> 0:01:23.480
But we did a lot wrong, and we make a lot of failures. Unfortunately, we learned from

10
0:01:23.480 --> 0:01:33.680
it, and we went wrong. And so, this presentation will be the story about that one. But before

11
0:01:33.680 --> 0:01:41.440
I really dive deep into it, I want to ask you a question. When you think about privacy,

12
0:01:41.440 --> 0:01:48.240
this concept, do you think it's modulated? Is it about the requirements the technicians

13
0:01:48.240 --> 0:01:55.600
to have to build this system in a private way? Is it about the trust of the end users?

14
0:01:55.600 --> 0:02:03.840
Or is it about agency, about their ability of end users to make their own choices? So,

15
0:02:03.840 --> 0:02:12.080
what do you think? Let's make a little poll of it. I hope it works. This is the first

16
0:02:12.080 --> 0:02:28.240
time I use Nextcloud for a poll. It goes to a domain totally owned by me, so you have

17
0:02:28.240 --> 0:02:55.600
to trust me. So, you already answered the question.

18
0:02:55.600 --> 0:03:15.440
It's also an IPv6 that might be a problem. Are you connected to the dual stack?

19
0:03:15.440 --> 0:03:44.960
Yeah, let's... Wait. Yeah. So, who thinks it's about requirements?

20
0:03:44.960 --> 0:03:56.720
To one hand. Who thinks it's about trust? A little bit more than half of it. Who thinks

21
0:03:56.720 --> 0:04:08.080
it's about agency? Quite close, we said. But requirements are down the list, I believe.

22
0:04:08.080 --> 0:04:18.400
All right. This is what I will be talking about. The design challenge, the thing we

23
0:04:18.400 --> 0:04:25.040
had to design. Then three perspectives on privacy. The approach we did in the design

24
0:04:25.040 --> 0:04:37.200
project and what we learned from it. So, the design challenge was promoting agency with

25
0:04:37.200 --> 0:04:46.800
an open source ecosystem for e-hals. There are some things that are really important

26
0:04:46.800 --> 0:05:00.160
here. First of all, it had to be about a digital open source exchange standard. The idea that

27
0:05:00.160 --> 0:05:06.280
was envisioned for all different kind of e-hals apps, all kind of health professionals, all

28
0:05:06.280 --> 0:05:11.200
working together in unpredictable ways, in unpredictable combinations, and that had to

29
0:05:11.200 --> 0:05:25.440
cooperate. So, we had to find a way to make standards to have them communicate. And another

30
0:05:25.440 --> 0:05:31.760
starting point of the project was user agency. The idea was when people have to control about

31
0:05:31.760 --> 0:05:37.160
their own path in the healthcare system or in the care system, then they make same choices.

32
0:05:37.160 --> 0:05:42.880
They choose what's good for them. And that makes the healthcare system better and better

33
0:05:42.880 --> 0:05:52.560
affordable. So, agency was not only just because we like to be ourselves and to have the possibility

34
0:05:52.560 --> 0:05:59.520
to be ourselves. No, it was really at the heart of the project the question. Another

35
0:05:59.520 --> 0:06:16.000
part of it is, I'm already told a bit, it's really about the ecosystem of care applications

36
0:06:16.000 --> 0:06:23.360
and blending with health applications. So, more informal care, more social work, comparing

37
0:06:23.360 --> 0:06:30.760
with really hard medical care. And these have different security levels. And they need different

38
0:06:30.760 --> 0:06:36.520
levels of trust. And trusting your neighbor is something else than trusting your doctor.

39
0:06:36.520 --> 0:06:49.360
So, we all had to design for this. Well, then we started. And we discovered that there was

40
0:06:49.360 --> 0:06:56.920
three perspectives on privacy that were really causing havoc in the project. Confusion,

41
0:06:56.920 --> 0:07:05.600
misunderstandings, everything. So, the three perspectives was really a theoretical perspective.

42
0:07:05.600 --> 0:07:10.800
I told I'm the privacy expert. We like to make the presentations with lots of text.

43
0:07:10.800 --> 0:07:18.200
And I really like to think about what means privacy, how does it relate to agency. And

44
0:07:18.200 --> 0:07:25.600
I came up with talking with peers on relational view on privacy. And that relational view

45
0:07:25.600 --> 0:07:33.040
is really focusing on the choices we make. I'm standing here in certain clothes, are

46
0:07:33.040 --> 0:07:37.920
you feeling certain things about myself? Not feeling other things about myself. Those

47
0:07:37.920 --> 0:07:46.960
choices about how we feel create how I relate to you and how I appear to you. And also that

48
0:07:46.960 --> 0:07:52.960
creates a relation. And we all make these choices. Just look around and see how we are.

49
0:07:52.960 --> 0:08:00.160
There's no process all this time. And by these choices, we create agency. By these choices,

50
0:08:00.160 --> 0:08:06.560
we can determine who we are and who we are in certain contexts. So, that's really the

51
0:08:06.560 --> 0:08:15.440
theoretical thought we had about it. And then we had to end you with a perspective. And

52
0:08:15.440 --> 0:08:24.440
we really went into neighborhood centers, talked with users, gave them an app, asked

53
0:08:24.440 --> 0:08:32.640
how do you experience this. And they were totally somewhere different. What's this?

54
0:08:32.640 --> 0:08:39.280
This is complicated. Don't know how to handle it. And I have to enter this data. What's

55
0:08:39.280 --> 0:08:48.240
why? And where does it go? And I don't trust this thing. And then we were sitting at the

56
0:08:48.240 --> 0:08:56.960
table with developers from the companies making the ELs and the care apps. And they were asking,

57
0:08:56.960 --> 0:09:06.160
well, tell us what to do. What are the requirements? So, all talking different languages.

58
0:09:06.160 --> 0:09:19.160
So, we had to find a way to get on. To get this big misunderstanding solved and to get

59
0:09:19.160 --> 0:09:27.920
something that got everything together. And we had to test it with users. Which might

60
0:09:27.920 --> 0:09:32.720
be a problem because we were creating an abstract exchange standard and not an app or something

61
0:09:32.720 --> 0:09:42.560
like that. So, we had to bridge a gap. And we took a step back. We're thinking, what

62
0:09:42.560 --> 0:09:46.360
are the common failures? What are they talking about? What are the things that they share

63
0:09:46.360 --> 0:09:53.040
and what they all want to have in the system? And that's where the trust in the agency came

64
0:09:53.040 --> 0:10:02.320
in. That's where when we noticed users really wanted to see experience that they could trust

65
0:10:02.320 --> 0:10:09.200
the system and experience that they could make changes. And the developers, well, they're

66
0:10:09.200 --> 0:10:14.840
not bad guys. They don't want to violate everybody's privacy. They're working in health apps so

67
0:10:14.840 --> 0:10:22.080
they know that this is important. So, we talked with them about what's needed in such an ecosystem.

68
0:10:22.080 --> 0:10:27.880
They very well understand this also trust in agency. It's really important to give the

69
0:10:27.880 --> 0:10:36.800
choices and to make sure that you are trusted. So, we translated these principles of these

70
0:10:36.800 --> 0:10:44.880
common failures into design principles. Tell more about it later. And, of course, I was

71
0:10:44.880 --> 0:10:56.200
there also. And I helped to trick them, give my own input to the design principles. So,

72
0:10:56.200 --> 0:11:05.200
that's how we came to a set of design principles that were bridging this gap. And then we came

73
0:11:05.200 --> 0:11:17.000
to the next problem. I already mentioned it. How to use a test. Yeah. How to use a test

74
0:11:17.000 --> 0:11:26.600
as a standard abstract thing that can have totally different traces and all different

75
0:11:26.600 --> 0:11:31.040
kind of apps. And, you know, when we literally went to neighborhoods and talked about trust

76
0:11:31.040 --> 0:11:40.480
in agency and everybody was, what are you talking about? So, we came up with a privacy

77
0:11:40.480 --> 0:11:52.600
game. And it was a small game, small simulation. And we used that to envision abstract scenarios.

78
0:11:52.600 --> 0:11:58.880
And we made a scenario about you're in this situation and you want to share this with

79
0:11:58.880 --> 0:12:05.080
somebody with who you want to share it or you're in this situation, you're talking to

80
0:12:05.080 --> 0:12:12.200
this person, what do you share? Then we changed in the twist of the game. Turned the page

81
0:12:12.200 --> 0:12:19.520
around and then there was a new part of it. For example, somebody else or some changing

82
0:12:19.520 --> 0:12:27.320
your information, what you maybe made want to share. And based on that, we got into a

83
0:12:27.320 --> 0:12:33.480
discussion, talks with the users about how they made the choices. And if we could validate

84
0:12:33.480 --> 0:12:40.280
the principles, we formulated. So, by playing this game, we made it, creating this game,

85
0:12:40.280 --> 0:12:51.800
we really made it possible to see visible how the abstract principles, design principles

86
0:12:51.800 --> 0:13:03.440
worked out. So, what did we learn? What are the things we got out of this? First of all,

87
0:13:03.440 --> 0:13:14.040
trust is iterative process. You don't even when you go in at the doctor, it would be

88
0:13:14.040 --> 0:13:19.760
totally strange to have a shield at the door of the doctor. Please undress yourself in

89
0:13:19.760 --> 0:13:28.000
the waiting room. You know, you first want to make contact, want to see, well, is this

90
0:13:28.000 --> 0:13:35.280
a doctor I like to see? And then slowly you start to trust the doctor and then slowly

91
0:13:35.280 --> 0:13:42.840
you say, well, now it's appropriate to undress. And the same happens between a human and a

92
0:13:42.840 --> 0:13:50.480
machine. Or when you interact with an app. It's not just, well, there are many apps where

93
0:13:50.480 --> 0:13:57.960
you have to first page you see, create an account, and you have to tell everything about

94
0:13:57.960 --> 0:14:02.360
yourself. And you're looking at it, well, why should I trust this app with all this

95
0:14:02.360 --> 0:14:07.680
information? Maybe the apps you show that it's useful and there's some nice interaction

96
0:14:07.680 --> 0:14:20.120
with you. So, next thing is, reciprocity of the information or what happens. I share,

97
0:14:20.120 --> 0:14:25.880
you share. It's not one way around. Everyone wants to know everything about you. It doesn't

98
0:14:25.880 --> 0:14:32.120
give you anything back. It doesn't show what it is. Maybe you say I want to donate information

99
0:14:32.120 --> 0:14:39.120
about myself because I can help somebody else. But it has to be made visible. It has to be

100
0:14:39.120 --> 0:14:50.920
two-way thing. So, really, I share, you share. People are willing to trust if there's balance.

101
0:14:50.920 --> 0:14:58.520
And then the last one, and that one was really important for the designers. We tell them

102
0:14:58.520 --> 0:15:05.920
these principles, they must be backed with your technical solution. The first thing they

103
0:15:05.920 --> 0:15:12.160
said, well, identify in the system. Well, let's use the email address because it's easy

104
0:15:12.160 --> 0:15:17.560
and everybody has it and then you can use it everywhere. But then people lose control

105
0:15:17.560 --> 0:15:23.520
about where the data goes. Because somebody can link all the email addresses across all

106
0:15:23.520 --> 0:15:31.000
applications. So, if you want to give people really agency, really the possibility to make

107
0:15:31.000 --> 0:15:38.200
choices on a technical level also, then you have to say to the developers, well, for each

108
0:15:38.200 --> 0:15:45.560
link between two apps, you use a unique identifier. And if you want to share a piece of information

109
0:15:45.560 --> 0:15:52.400
across several apps, you have to create a star and make a deliberate decision to share

110
0:15:52.400 --> 0:15:59.840
it and really back that up in your technical architecture. So, we really could tell that

111
0:15:59.840 --> 0:16:05.320
the developers, well, this is what you should do. And it was really nice because we had

112
0:16:05.320 --> 0:16:09.720
many meetings with the developers, different developers, they were really, oh, we don't

113
0:16:09.720 --> 0:16:13.800
understand this. And then we came with these principles. Oh, and then we said, oh, then

114
0:16:13.800 --> 0:16:21.120
can do this and this. And then it all piece together for them. And then they really drafted

115
0:16:21.120 --> 0:16:36.560
in several days and first version of a standard. So, there was these are the principles we

116
0:16:36.560 --> 0:16:47.240
found. Well, I have another poll, but let's do it manual. I have five minutes left in

117
0:16:47.240 --> 0:16:54.120
my talk and five minutes for questions and I want to lengthen this. I'm a bit wondering,

118
0:16:54.120 --> 0:17:00.840
curious, and let's just shout out the way it ends, whatever. What do you take away from

119
0:17:00.840 --> 0:17:07.960
this presentation? What did you learn? Because I told about what we learned, but I'm wondering

120
0:17:07.960 --> 0:17:08.960
what you did learn.

121
0:17:08.960 --> 0:17:34.800
So, the interactive, I repeat it a bit because the device is not on the video. So, it's the

122
0:17:34.800 --> 0:17:39.240
interactive building of trust. Yeah.

123
0:17:39.240 --> 0:17:47.240
I might be projecting here a little bit, but some of the struggles you mentioned remind

124
0:17:47.240 --> 0:17:53.640
me of when I worked in the aid sector where many people would try to apply design principles

125
0:17:53.640 --> 0:18:00.200
like human centered design to develop a solution for certain people. But it's very difficult

126
0:18:00.200 --> 0:18:07.480
to ignore these larger economic contexts. So, when you talk about privacy and people

127
0:18:07.480 --> 0:18:11.320
not being engaged with it, maybe they aren't politically engaged with the topic, educating

128
0:18:11.320 --> 0:18:15.640
them and figuring out how to mediate that, it reminded me of looking in the aid sector

129
0:18:15.640 --> 0:18:17.640
where funding is.

130
0:18:17.640 --> 0:18:26.240
You've experienced in a similar sector and you run into really the commercial barriers

131
0:18:26.240 --> 0:18:36.880
of parties who didn't think that way, didn't want to implement it. So, it's interesting

132
0:18:36.880 --> 0:18:45.520
because we were with lots of commercial organizations around the table and they were willing to

133
0:18:45.520 --> 0:18:51.040
cooperate and they were willing to change something. They had some idealism there too.

134
0:18:51.040 --> 0:18:55.640
But it is a problem in the care sector because it is a huge economic sector with lots of

135
0:18:55.640 --> 0:19:02.520
money and lots of people who try to get a bit of the money. So, I feel your problem.

136
0:19:02.520 --> 0:19:03.520
Any other takeaways?

137
0:19:03.520 --> 0:19:16.520
I have a quick abstract question. How did you come up with the user testing as a game?

138
0:19:16.520 --> 0:19:29.480
It was not a game you can win, but it might be played a bit as a party game and it might

139
0:19:29.480 --> 0:19:32.800
be nice. We have lots of games, card games from this.

140
0:19:32.800 --> 0:19:39.800
Was it an issue that the users were anonymous or was it the case that you managed to get

141
0:19:39.800 --> 0:19:42.800
the users? You knew what the user was.

142
0:19:42.800 --> 0:19:52.120
It was a paper game. We literally went to neighborhood centers and sat with people visiting

143
0:19:52.120 --> 0:19:59.280
there around the table and played the game with them and interviewed them afterwards

144
0:19:59.280 --> 0:20:05.280
about even before and after about how they even influenced.

145
0:20:05.280 --> 0:20:15.640
I was curious, I find it really interesting how open source projects make decisions about

146
0:20:15.640 --> 0:20:21.640
which methodology to use or which tool to use. How did you guys make the decision to

147
0:20:21.640 --> 0:20:26.640
use the game? It is a tool I have never heard of before.

148
0:20:26.640 --> 0:20:35.960
I am very open to learning. The question is how did you make the choice for the game for

149
0:20:35.960 --> 0:20:43.880
testing? Well, we didn't make a choice. We tried lots of things. We had many sessions

150
0:20:43.880 --> 0:20:52.440
in neighborhood centers that were totally filled. Then we had a feeling we need a way

151
0:20:52.440 --> 0:20:59.520
to discuss more. Then we came up with the game. We noticed this works.

152
0:20:59.520 --> 0:21:02.640
Now when we are looking back, we can make a nice story about it.

153
0:21:02.640 --> 0:21:13.520
We had a game and it worked. I left out about a dozen of failures there.

154
0:21:13.520 --> 0:21:21.040
I can give you more information about the game. I have both a paper on it and a game

155
0:21:21.040 --> 0:21:23.040
available too.

156
0:21:51.040 --> 0:22:04.560
The question is how do you include the GDPR in front-end design?

157
0:22:04.560 --> 0:22:10.120
I love it. I have once been introduced at the conference as a person who laughs about

158
0:22:10.120 --> 0:22:19.520
the GDPR. The point is when you work like this, let's move on. We are now here. When

159
0:22:19.520 --> 0:22:29.160
you work like this, you have a far better way of handling privacy of your users than

160
0:22:29.160 --> 0:22:36.760
the GDPR says. For example, part of the pattern is that in the iterative pattern, you only

161
0:22:36.760 --> 0:22:42.360
ask for information when needed. Then you make clear why you need it.

162
0:22:42.360 --> 0:22:51.080
That is already awfully close to an official privacy question you may have to ask according

163
0:22:51.080 --> 0:22:58.640
to the GDPR. Maybe add a link to some additional information with all legal stuff in it. But

164
0:22:58.640 --> 0:23:05.880
they are already there. Then you have asked a meaningful consent question. That is just

165
0:23:05.880 --> 0:23:12.800
logical to do that when you are designing this way. Just design for privacy and then

166
0:23:12.800 --> 0:23:24.680
GDPR should be included already. Good to know. You are the dark side.

167
0:23:24.680 --> 0:23:37.280
Can you give a clear example of how you use reciprocity in information exchange?

168
0:23:37.280 --> 0:23:49.720
For example, there are two levels. Between humans, you can deliberately support, and

169
0:23:49.720 --> 0:23:55.000
we also had some design patterns, but they were UI design patterns, so they couldn't

170
0:23:55.000 --> 0:24:05.840
be forced on all apps. You can really support two-way communication before telling somebody

171
0:24:05.840 --> 0:24:15.880
something. You can really support, well, I asked you this, but I want to ask you this

172
0:24:15.880 --> 0:24:24.520
because then I know I can do this for you. You can support in your design of the chat

173
0:24:24.520 --> 0:24:35.040
interaction messages, patterns like that. You can also do that in human to machine things.

174
0:24:35.040 --> 0:24:43.880
The app asking, well, I really like, if you like to, then we can arrange it if you come

175
0:24:43.880 --> 0:24:51.720
back that I remember what you did. But to do so, I need an account.

176
0:24:51.720 --> 0:24:58.680
It's essentially about telling what you are doing.

177
0:24:58.680 --> 0:24:59.680
Exactly.

178
0:24:59.680 --> 0:25:01.680
Any time you ask for something, tell us about it.

179
0:25:01.680 --> 0:25:07.720
Exactly. Don't ask before it is already clear what you can give back for it. Start, for

180
0:25:07.720 --> 0:25:14.360
example, with anonymous accounts. Then people can see what the app is doing.

181
0:25:14.360 --> 0:25:20.960
Well, at a certain point, you can say, maybe it's interesting if I remember this. Tell

182
0:25:20.960 --> 0:25:29.800
which you like to make an account. Don't ask the account. Then for the account, ask only

183
0:25:29.800 --> 0:25:47.480
what is needed for the account. Don't ask anything more.

184
0:25:47.480 --> 0:26:05.320
One final question. For example, try to make a really taxonomy and architecture that would

185
0:26:05.320 --> 0:26:14.640
support all fine grained type of privacy patterns. That became a huge picture with lots of drawings

186
0:26:14.640 --> 0:26:24.920
and lines and data model that was really exploding on all sides. Nobody understood it anymore.

187
0:26:24.920 --> 0:26:30.880
And it was bonkers to even think about implementing it.

188
0:26:30.880 --> 0:26:39.840
Spend some days on drawing all the boxes. No, that's not the way to go. To give one

189
0:26:39.840 --> 0:26:52.360
example. Exactly. Yeah.

