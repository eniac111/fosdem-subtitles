1
0:00:00.000 --> 0:00:14.760
Okay. Hi, everyone. It's a big pleasure to be here. My name is Armin Purnaki, and I'm

2
0:00:14.760 --> 0:00:22.680
a PhD candidate in applied mathematics, and I work on building tools for discourse analysis.

3
0:00:22.680 --> 0:00:28.640
And we build tools for discourse analysis based on methods from graph theory, network

4
0:00:28.640 --> 0:00:34.120
science and natural language processing. And today I want to present a tool called the

5
0:00:34.120 --> 0:00:40.520
Twitter Explorer that is already a bit older, but that was built in the institute in the

6
0:00:40.520 --> 0:00:45.520
Max Planck Institute for Mathematics and the Sciences in my previous group. And the idea

7
0:00:45.520 --> 0:00:51.080
was to build a tool that allows researchers who don't necessarily have programming skills

8
0:00:51.080 --> 0:00:59.360
to collect Twitter data, visualize them using graphs, and explore the data and maybe generate

9
0:00:59.360 --> 0:01:06.280
hypotheses in their pipelines. So this kind of tool building and this research happens

10
0:01:06.280 --> 0:01:13.120
in the field called computational social science. So when I was preparing my slides two days

11
0:01:13.120 --> 0:01:17.120
ago, I thought it would be good to maybe give a little overview of computational social

12
0:01:17.120 --> 0:01:21.840
science, then say why we built the Twitter Explorer and where we saw somehow the need

13
0:01:21.840 --> 0:01:27.480
for a new tool. Of course, introduce the features of the tool because it's kind of a talk on

14
0:01:27.480 --> 0:01:34.040
programming, the architecture, and maybe give some insights on the usage. But when I was

15
0:01:34.040 --> 0:01:43.280
sitting down to make the slides two days ago, I was confronted with this. And of course,

16
0:01:43.280 --> 0:01:51.240
since the tool is essentially an entry point into the free API, there's also a part of

17
0:01:51.240 --> 0:01:58.200
it that uses the research API, which of course led us directly to this question, what happens

18
0:01:58.200 --> 0:02:06.680
to the research API? It's also not entirely clear, right? So I want to maybe instead of

19
0:02:06.680 --> 0:02:12.440
giving this talk the way I was planning to do it, I will do it, but maybe I wanted to

20
0:02:12.440 --> 0:02:17.440
ask a few questions first that we might then discuss maybe in the discussion also. And

21
0:02:17.440 --> 0:02:22.560
I think there is even something planned later, right? So some kind of panel discussion. So

22
0:02:22.560 --> 0:02:26.560
I'm just going to throw some questions out there that I think are really pressing now,

23
0:02:26.560 --> 0:02:32.480
especially in the research field. How serious is this? By this, I don't mean the implications

24
0:02:32.480 --> 0:02:37.400
of it because I know a few people whose thesis is now in jeopardy because they can't collect

25
0:02:37.400 --> 0:02:46.080
data in a way. But how serious is it in the sense? Will it actually happen? Or is it some

26
0:02:46.080 --> 0:02:53.280
scare tactic? So I think this is something that is hard to predict. And then these are

27
0:02:53.280 --> 0:02:58.960
questions also I think that we can discuss here is how, or is there a way for us as users

28
0:02:58.960 --> 0:03:04.320
and not necessarily only as researchers to claim our data or our digital traces that

29
0:03:04.320 --> 0:03:09.520
we use and that we leave on these platforms? And how can things like the Digital Services

30
0:03:09.520 --> 0:03:17.920
Act play a role in this? And the last question is very broad, but how do we move on in the

31
0:03:17.920 --> 0:03:25.440
sense of how can we see this as some kind of wake up call maybe? And how can we use

32
0:03:25.440 --> 0:03:30.720
this new development to maybe on one hand move to different platforms, but on the other

33
0:03:30.720 --> 0:03:37.200
hand also to think about how we do computational social science in the future? So with these

34
0:03:37.200 --> 0:03:43.080
questions that we're going to discuss later, I'm still going to give my original talk.

35
0:03:43.080 --> 0:03:50.280
So in computational social science, a typical pipeline for a project is you have a research

36
0:03:50.280 --> 0:03:56.040
question, then you collect data related to it. And in this case, it may be data from

37
0:03:56.040 --> 0:04:02.520
online social platforms. And then you analyze it and ideally you generated some more insights

38
0:04:02.520 --> 0:04:06.680
on the research question you had in the beginning. And sometimes the exploration and the analysis

39
0:04:06.680 --> 0:04:11.400
of the data can help you maybe refine also the questions you had in the beginning. So

40
0:04:11.400 --> 0:04:17.080
it's some kind of loop that you can see in this way. And the tool that I'm going to present

41
0:04:17.080 --> 0:04:23.240
the Twitter Explorer is precisely made for this second part, for both facilitating the

42
0:04:23.240 --> 0:04:30.840
collection and also the exploration of such data. And this pipeline is that we start with

43
0:04:30.840 --> 0:04:37.880
text. So in our case, it's tweets that are annotated with some kind of metadata. We have

44
0:04:37.880 --> 0:04:42.160
on Twitter different types of interactions. So you can mention someone, you can reply

45
0:04:42.160 --> 0:04:50.240
to someone or retweet. And we choose one type of metadata and cast it into an interaction

46
0:04:50.240 --> 0:04:58.280
network. And then we want to find the most significant, for instance, clusters or the

47
0:04:58.280 --> 0:05:06.040
significant correlations in this data by using 2D spatializations. And typically, these are

48
0:05:06.040 --> 0:05:10.640
done using forced layouts. But today, for instance, in the graph room, there were also

49
0:05:10.640 --> 0:05:14.960
some talks about new methods of node embedding. And so I think this is also something that

50
0:05:14.960 --> 0:05:21.840
we can discuss maybe in the questions section. But one reason why I think forced layouts

51
0:05:21.840 --> 0:05:27.200
are good is that especially if you use them in a context where you work with social science

52
0:05:27.200 --> 0:05:33.640
researchers who don't necessarily have a lot of knowledge about the latest machine learning

53
0:05:33.640 --> 0:05:38.600
algorithms, they are quite straightforward to explain in the sense that you have a spring

54
0:05:38.600 --> 0:05:45.520
system and nodes that are strongly connected tend to attract each other. And especially

55
0:05:45.520 --> 0:05:54.200
if you look at interaction networks on Twitter, since retweeting can be considered endorsement,

56
0:05:54.200 --> 0:05:59.760
clusters in such 2D spatializations can then correspond to something like opinion clusters.

57
0:05:59.760 --> 0:06:04.400
And there's a lot of research being done in that way. But one question that we always

58
0:06:04.400 --> 0:06:09.960
had when we look at these networks is how do we actually go back to the data that generated

59
0:06:09.960 --> 0:06:16.720
them? And this is something that we tried to tackle with building these tools. So why

60
0:06:16.720 --> 0:06:21.560
we built it is firstly to provide an interface for researchers without programming skills

61
0:06:21.560 --> 0:06:26.440
also to collect and visualize the data, because we were working a lot with social scientists

62
0:06:26.440 --> 0:06:32.360
that did not have these programming skills, but had a lot of hypotheses about the data

63
0:06:32.360 --> 0:06:38.600
that they could not test. Then, of course, to facilitate the exploration of controversial

64
0:06:38.600 --> 0:06:45.960
issues on social media. And this is the point that I was making before, is add some layer

65
0:06:45.960 --> 0:06:52.120
of interpretability to these 2D spatializations by providing an access from within the interface

66
0:06:52.120 --> 0:07:00.320
to the actual data that created these node positions. And finally, we see it in the context

67
0:07:00.320 --> 0:07:07.560
of a larger scientific scope of using the network paradigm as something like a sampling

68
0:07:07.560 --> 0:07:13.320
mechanism for the data. Because if you're confronted with a large number of tweets,

69
0:07:13.320 --> 0:07:17.240
for instance, of course, everyone knows that you can't read all of them manually. So you

70
0:07:17.240 --> 0:07:23.480
need some kind of way to get to the tweets that are relevant for you to read. And this

71
0:07:23.480 --> 0:07:29.440
is what we use the network for, essentially. So we can look, when we look at retweet networks,

72
0:07:29.440 --> 0:07:33.960
immediately identify, for instance, the most influential actors in the debate, and then

73
0:07:33.960 --> 0:07:39.800
read precisely those tweets that they made to maybe influence other actors. And we call

74
0:07:39.800 --> 0:07:45.440
this guided close reading, because if you do only close reading, then you have to read

75
0:07:45.440 --> 0:07:50.600
all the text. If you have distant reading, you kind of look only at the network on a

76
0:07:50.600 --> 0:08:01.920
structural level, and this is something in between. So what can the tool do? It collects

77
0:08:01.920 --> 0:08:11.880
tweets. I mean, I think we have like one week left for the V2 and the V1. So far, the V2

78
0:08:11.880 --> 0:08:16.440
academic is safe, but we don't know that. So you can search for it from the past seven

79
0:08:16.440 --> 0:08:22.720
days using the API. In the second part, in the visualizer, you can do, display just a

80
0:08:22.720 --> 0:08:28.400
simple time series of the tweets to see maybe if there's some kind of special activity during

81
0:08:28.400 --> 0:08:36.160
one day. You can build these interaction networks, build co-hashtag networks. So we divide it

82
0:08:36.160 --> 0:08:42.160
into some kind of two types of networks, which we call semantic networks and interaction

83
0:08:42.160 --> 0:08:49.480
networks. And then you can compute the typical measures people compute on networks, and especially

84
0:08:49.480 --> 0:08:58.120
compute clusters like using modularity-based algorithms. And all this happens in some kind

85
0:08:58.120 --> 0:09:05.320
of interactive interface using JavaScript and D3.js. And this is essentially the part

86
0:09:05.320 --> 0:09:08.640
where it gets interesting, because so far, all the other things you can do it with a

87
0:09:08.640 --> 0:09:14.400
lot of other tools, especially like Gefi. I think you can even collect tweets with some

88
0:09:14.400 --> 0:09:20.200
plugins. So I think all of this is not new, and this is kind of where it gets interesting.

89
0:09:20.200 --> 0:09:25.280
And I think this is time for a quick demo. I don't know how much. Okay, I have plenty

90
0:09:25.280 --> 0:09:43.200
of time. I think I talked too fast. Okay, so I've prepared some Python environments

91
0:09:43.200 --> 0:09:53.040
that already have the Twitter Explorer installed, but usually you would do it like this. And

92
0:09:53.040 --> 0:10:02.480
then all you need to do to fire up this interactive interface is type Twitter Explorer collector.

93
0:10:02.480 --> 0:10:10.960
This will open a browser window from which you can choose your API access, choose the

94
0:10:10.960 --> 0:10:17.800
path to which the tweets will be downloaded, and insert your search query, maybe adding

95
0:10:17.800 --> 0:10:23.920
some advanced settings and saving options. So I don't know. This is a question to the

96
0:10:23.920 --> 0:10:32.440
audience now, what we should search for. This is easy. And you're looking into the future.

97
0:10:32.440 --> 0:10:37.400
I already have this network prepared for the last slide. Sorry. Can you look at the reaction

98
0:10:37.400 --> 0:10:43.800
to the Twitter API? We could, but what would we look for then? Is there maybe a hashtag

99
0:10:43.800 --> 0:10:57.920
like API shutdown? Maybe we need to go to Twitter itself. API something like this. Ideally,

100
0:10:57.920 --> 0:11:08.680
we would find some kind of hashtag. Okay, let's just use maybe this as a search query.

101
0:11:08.680 --> 0:11:25.000
Okay. Now it's collecting in the background. Then we can open another browser window here.

102
0:11:25.000 --> 0:11:33.480
Fire up the visualizer. Now we see that while this is still collecting, we can already access

103
0:11:33.480 --> 0:11:49.080
this. There were only 400 tweets, so there seems to be. So we can look at a time series

104
0:11:49.080 --> 0:11:57.320
of tweets, and then we can choose different types of networks to create. We can filter

105
0:11:57.320 --> 0:12:02.800
them by language if we want. And this is the language that the Twitter API returns. So

106
0:12:02.800 --> 0:12:11.800
there's no language detection going on here. We can do some network reduction methods like

107
0:12:11.800 --> 0:12:19.320
taking only the largest connected component of the graph. Then we have this option here

108
0:12:19.320 --> 0:12:24.060
to remove the metadata of nodes that are not what we call public figures. So if you want

109
0:12:24.060 --> 0:12:32.240
to publish some explorable networks, it is advisable to do so. There is not, as far as

110
0:12:32.240 --> 0:12:38.600
I know, not a very distinctive or clear rule after which point one is considered such a

111
0:12:38.600 --> 0:12:44.000
public figure, but within our consortium, we decided that it's 5,000 followers. This

112
0:12:44.000 --> 0:12:51.120
is also something we could discuss. But since Twitter is public by default, in a way, anything

113
0:12:51.120 --> 0:12:58.480
you post is somehow potentially to be used and displayed somewhere. Then you can export

114
0:12:58.480 --> 0:13:05.560
the graph to all sorts of formats. Then you can aggregate nodes. This means that, for

115
0:13:05.560 --> 0:13:10.720
instance, removing them based on how many retweets they have or how many retweets they

116
0:13:10.720 --> 0:13:17.120
did themselves, and remove, for instance, nodes that only retweeted one person. So is

117
0:13:17.120 --> 0:13:36.480
there a chalk? Maybe somewhere? If you have a graph, and then there are some nodes that

118
0:13:36.480 --> 0:13:44.340
only retweet this person. I don't know if everyone can see that, but they tend to clutter

119
0:13:44.340 --> 0:13:50.200
the force-directed algorithm. Structurally, they do not necessarily add anything to the

120
0:13:50.200 --> 0:13:54.480
network. So if you have very, very large graphs, it makes sense to remove these and somehow

121
0:13:54.480 --> 0:14:08.440
englope them into this super node. Then you can do traditional community detection. Then

122
0:14:08.440 --> 0:14:24.440
it will be saved as an HTML that you can then open. So we see here that this is, again,

123
0:14:24.440 --> 0:14:29.600
in a retweet network, every node is a user, and the link is drawn from A to B if A retweets

124
0:14:29.600 --> 0:14:41.240
B. Now we can look at this user, T Chambers, and look at the actual tweets that were made

125
0:14:41.240 --> 0:14:54.240
for them to end up at this part of the visualization. Okay. So the data we collect is kind of sparse,

126
0:14:54.240 --> 0:15:05.800
so this network doesn't look that interesting, but I have prepared some fallback options.

127
0:15:05.800 --> 0:15:13.080
So what we did in a case study a few months ago was to look at the repercussion of some

128
0:15:13.080 --> 0:15:21.040
discussions in the US about red flag laws. And red flag laws are specific kinds of laws

129
0:15:21.040 --> 0:15:28.720
for gun control that allow state level judges to confiscate temporarily guns from people

130
0:15:28.720 --> 0:15:36.000
that are deemed to be a threat to themselves or to the public. And these laws created very

131
0:15:36.000 --> 0:15:42.200
big repercussions, especially on social media and especially in the conservative camps.

132
0:15:42.200 --> 0:15:48.480
And this is one typical example where people then can analyze on Twitter if there's something

133
0:15:48.480 --> 0:15:53.800
like echo chambers or if people then maybe retweet each other only from the similar camps,

134
0:15:53.800 --> 0:15:59.160
and then people draw very quick conclusions very fast. And what we want to do with this

135
0:15:59.160 --> 0:16:06.080
tool is to show that maybe things are not that simple as they seem. So I have prepared

136
0:16:06.080 --> 0:16:23.360
these networks, but I think I will make it a bit smaller. So this is now a bit bigger

137
0:16:23.360 --> 0:16:34.600
than what we had before. We have roughly 25,000 nodes and 90,000 links. And this is already

138
0:16:34.600 --> 0:16:38.600
one limitation of the tool that I think I would also like to discuss in the end is that

139
0:16:38.600 --> 0:16:43.760
you can't display mentally huge graphs. So 100,000 links approximately is kind of the

140
0:16:43.760 --> 0:16:50.080
limit. And I think this is also where integrating it with other tools such as Sigma or Gayfri

141
0:16:50.080 --> 0:16:58.880
might actually make a lot of sense. And so now I can color the nodes by the Luvan community.

142
0:16:58.880 --> 0:17:09.360
We can turn off the light also. And now we can wonder what are these two communities.

143
0:17:09.360 --> 0:17:15.520
And right now the node size is proportional to the indegree, meaning how often a given

144
0:17:15.520 --> 0:17:21.360
node was retweeted. But if we want to, so these may then be considered as something

145
0:17:21.360 --> 0:17:27.280
like the opinion leaders of the given camps. And so if we go here, we see for instance

146
0:17:27.280 --> 0:17:36.800
on this side Donald Trump Jr. And we can then look exactly at the tweets that led the visualization

147
0:17:36.800 --> 0:17:43.600
to put him where he was. So okay, we don't need to go into the details of what he said,

148
0:17:43.600 --> 0:17:50.760
but you see the point. We can also change the node size to the number of followers.

149
0:17:50.760 --> 0:17:56.980
And then we get an immediate view at who the main actors are that in general are also influential

150
0:17:56.980 --> 0:18:10.960
on Twitter. So we have the New York Times here. And Wall Street Journal. So we can see

151
0:18:10.960 --> 0:18:17.720
that we have something like of a more liberal versus a more conservative camp. But if we

152
0:18:17.720 --> 0:18:23.240
look only at the retweet behavior, we might think that okay, these are separated echo

153
0:18:23.240 --> 0:18:27.800
chambers and people do not talk to each other. But what is interesting is if we look at other

154
0:18:27.800 --> 0:18:33.800
types of networks in these examples. So we can look at the replies. I think I will make

155
0:18:33.800 --> 0:18:42.960
it a bit smaller. And all of the sudden we don't see this very strong segregated clustering

156
0:18:42.960 --> 0:18:52.920
anymore that we saw here. Maybe it's easier if I put it in. But we see something more

157
0:18:52.920 --> 0:19:04.120
of a hairball layout. And when we look at the nodes, we see that indeed the path of

158
0:19:04.120 --> 0:19:09.360
going for instance from Donald Trump to Hillary Clinton or New York Times of those people

159
0:19:09.360 --> 0:19:14.800
that were very far apart of the network is maybe not that long in the reply network.

160
0:19:14.800 --> 0:19:19.080
These opposing camps actually maybe do talk to each other. And it might be more interesting

161
0:19:19.080 --> 0:19:23.080
to see how they talk to each other and what they say. And this is something that you can

162
0:19:23.080 --> 0:19:32.320
do when you use this interface and look at the tweets and that actually replies. So it

163
0:19:32.320 --> 0:19:39.600
allows you to then actually go to the parts of the platform that generate this data and

164
0:19:39.600 --> 0:19:50.160
that then generate these networks. And finally as a small example of the semantic networks,

165
0:19:50.160 --> 0:20:04.200
you can look at the hashtags that I used. Again, I'll make it smaller. And you see that

166
0:20:04.200 --> 0:20:10.720
for instance there is one kind of hateful conservative hashtag cluster which and again

167
0:20:10.720 --> 0:20:15.600
maybe I should have said that in the hashtag networks every node is a hashtag and they

168
0:20:15.600 --> 0:20:23.120
are connected if they appear together in the same tweet. So this is a very low level way

169
0:20:23.120 --> 0:20:26.800
of seeing what is going on in the data in a way. You don't need to do some kind of topic

170
0:20:26.800 --> 0:20:31.800
modeling or complicated techniques. You can literally just by looking at the hashtags

171
0:20:31.800 --> 0:20:38.160
already get a hint at how the different camps speak about the same topic. So if you go here

172
0:20:38.160 --> 0:20:48.560
in this area, this is about confiscation laws. So Marxism in this case is also a good example.

173
0:20:48.560 --> 0:20:53.880
Right now we don't really know how it is used, right? And it can be used either by conservatives

174
0:20:53.880 --> 0:20:59.920
or by liberals and it's important to look at it in the context of the data. So then

175
0:20:59.920 --> 0:21:14.640
we would have to, okay, five minutes left. Good. I will go back to the slides. Okay.

176
0:21:14.640 --> 0:21:21.720
So under the hood, this whole backend of the collector and the visualizer is written in

177
0:21:21.720 --> 0:21:31.280
Python and it's using the streamlit Python library to serve it on a local front end.

178
0:21:31.280 --> 0:21:37.080
So this is actually a very convenient library and I guess a lot of people also know it,

179
0:21:37.080 --> 0:21:41.680
but you can write your code in Python and then it essentially serves it in interfaces

180
0:21:41.680 --> 0:21:54.560
that look like this. And the explorer is written in HTML and JavaScript and it uses D3 and

181
0:21:54.560 --> 0:22:03.240
prints the graph on canvas, which is also why it's probably not as fast as Sigma is,

182
0:22:03.240 --> 0:22:09.080
but it has some nice other features that are especially due to this force graph library.

183
0:22:09.080 --> 0:22:15.360
So I think if anyone has questions, I'm going to go into the details and the questions.

184
0:22:15.360 --> 0:22:20.360
So this is how we install it. It's fairly simple. If you have a running Python bigger

185
0:22:20.360 --> 0:22:26.620
than 3.7 and there's also an API. So of course, especially here, probably people will not

186
0:22:26.620 --> 0:22:31.980
be so interested in using the streamlit interface, but you may want to include it into some kind

187
0:22:31.980 --> 0:22:40.440
of existing code pipeline that you have. And this is essentially the API for semantic networks

188
0:22:40.440 --> 0:22:48.720
and interaction networks. So I invite you to try it out yourself while you still can.

189
0:22:48.720 --> 0:22:57.320
You have five days. Of course, if you have the research API, you might be able to use

190
0:22:57.320 --> 0:23:06.000
it for a bit longer, but otherwise go on these websites fast. And I will stop the talk with

191
0:23:06.000 --> 0:23:10.040
some questions. Actually, I came here with more questions than answers and I'm really

192
0:23:10.040 --> 0:23:15.280
hoping for a lively discussion now because I'm not originally a developer, so I kind

193
0:23:15.280 --> 0:23:21.160
of wrote this a bit on my own. And I wonder if this integration of Python and JavaScript

194
0:23:21.160 --> 0:23:24.880
is actually a good idea because in theory it would also be possible to probably do everything

195
0:23:24.880 --> 0:23:29.840
in JavaScript and maybe do it on the client side so you wouldn't have to install all these

196
0:23:29.840 --> 0:23:35.600
libraries. Then, okay, maybe one thing that I would like to show is that I experimented

197
0:23:35.600 --> 0:23:41.440
with temporal networks. So of course, doing temporal force layouts is kind of a non-trivial

198
0:23:41.440 --> 0:23:49.320
task, but we can kind of look a little bit at the temporality of these networks by at

199
0:23:49.320 --> 0:23:56.960
least displaying only the links that are active during a given day. So this is also kind of

200
0:23:56.960 --> 0:24:01.400
nice, I think, but I would like to discuss maybe other visualization paradigms for this

201
0:24:01.400 --> 0:24:07.160
kind of network. Then one thing that would be really interesting, I think, is to dig

202
0:24:07.160 --> 0:24:12.600
deeper into a visualization paradigm for hierarchical structure of communities, meaning

203
0:24:12.600 --> 0:24:18.360
that okay, in theory I can either run stochastic block models or luvain community detections

204
0:24:18.360 --> 0:24:22.840
and stop them at a certain level and then have some kind of hierarchical node structure.

205
0:24:22.840 --> 0:24:26.120
But how to visualize that is another question, but I think it would be very interesting, especially

206
0:24:26.120 --> 0:24:32.240
for very large graphs. Then another question is force layouts. Should we still use them

207
0:24:32.240 --> 0:24:37.080
now that everyone is doing node2vec and all these other things? I think yes, but maybe

208
0:24:37.080 --> 0:24:44.760
there's good arguments against it. And on a more deeper conceptual level is, and this

209
0:24:44.760 --> 0:24:48.680
is a question, the first one is a question for people who already have much more experience

210
0:24:48.680 --> 0:24:53.520
in building tools for the social sciences, is how do you kind of further integrate these

211
0:24:53.520 --> 0:25:00.920
kinds of methods into existing maybe also more qualitative social science pipelines?

212
0:25:00.920 --> 0:25:05.640
So yeah, it's kind of an open question. And how can we devise something like a research

213
0:25:05.640 --> 0:25:10.640
protocol for these kinds of interactive network visualizations? Because as you saw in my demo,

214
0:25:10.640 --> 0:25:16.040
we kind of look at the big nodes, we look at the tweets they made, and it gives us some

215
0:25:16.040 --> 0:25:20.920
kind of intuition of what's going on in the debate. But how can we formalize such kinds

216
0:25:20.920 --> 0:25:25.320
of visual network analysis? And I think, I mean, there's people in the audience who actually

217
0:25:25.320 --> 0:25:30.520
work on this, so it would be very interesting for me to talk about this. And finally, to

218
0:25:30.520 --> 0:25:39.440
end on actually maybe a bit nicer note is that there is the network of firsts as we

219
0:25:39.440 --> 0:25:44.440
had already said in the beginning on this website. So it is updated every 15 minutes

220
0:25:44.440 --> 0:25:50.800
thanks to a data collection done by my colleague, Beatrice. Thank you very much. And so if you

221
0:25:50.800 --> 0:25:59.040
go on this website, you can see the retweet network of firsts. And if you tweet, then

222
0:25:59.040 --> 0:26:07.400
you can find yourself in the network also. So yeah, what do we have here in the middle?

223
0:26:07.400 --> 0:26:20.600
OK, first them itself. Then there was Ubuntu, Debian somewhere. OK, time's up. Thank you.

224
0:26:20.600 --> 0:26:38.200
Do we have questions?

225
0:26:38.200 --> 0:26:59.400
Yes. So the question is, I mentioned that you can only collect tweets from the last

226
0:26:59.400 --> 0:27:06.800
seven days. With the free API, this is a limitation. But the tool itself just writes into existing

227
0:27:06.800 --> 0:27:11.400
CSV. It depends, basically. So if you do the same keyword search multiple times, then it

228
0:27:11.400 --> 0:27:20.120
will just append to a CSV. Yeah, so this is.

229
0:27:20.120 --> 0:27:25.840
Yes, I mean, this is the question right now. It depends because the question is what happens

230
0:27:25.840 --> 0:27:32.760
on Mastodon. I don't know. All these, like if you want to look at political controversies

231
0:27:32.760 --> 0:27:39.960
and such discussions, I don't know if Mastodon is mature enough yet to or adopted enough

232
0:27:39.960 --> 0:27:44.320
yet. I think if you want to look at the first income, it's great. So for this, yes. But

233
0:27:44.320 --> 0:27:45.320
yeah.

234
0:27:45.320 --> 0:28:03.520
I know computer science for more than 40 years. And I am very skeptical of this kind of thing,

235
0:28:03.520 --> 0:28:27.600
I'm one. You don't buy paper, right? Really?

236
0:28:27.600 --> 0:28:30.120
Yes.

237
0:28:30.120 --> 0:28:33.280
You know, dancing.

238
0:28:33.280 --> 0:28:36.800
computers, computers, computers, or many people

239
0:28:36.800 --> 0:28:38.360
visits which we have so often.

240
0:28:38.360 --> 0:28:42.400
So I'm very reactive about this kind of thing.

241
0:28:42.400 --> 0:28:44.520
I don't know what you should think about that.

242
0:28:44.520 --> 0:28:49.560
Well, I don't know which point exactly should I address,

243
0:28:49.560 --> 0:28:51.400
because you raised a lot of.

244
0:28:51.400 --> 0:28:54.160
So are you OK?

245
0:28:54.160 --> 0:28:56.360
If I can rephrase, so you are concerned

246
0:28:56.360 --> 0:28:59.640
about this kind of research also?

247
0:28:59.640 --> 0:29:02.680
Yes, because it can be used to track users

248
0:29:02.680 --> 0:29:03.800
across political camps.

249
0:29:03.800 --> 0:29:08.880
We have problems with computers of a popular section

250
0:29:08.880 --> 0:29:09.800
of population.

251
0:29:09.800 --> 0:29:14.880
There are no computers taking them in account.

252
0:29:14.880 --> 0:29:15.380
I see.

253
0:29:15.380 --> 0:29:17.960
So I think it's more about the representativity of Twitter

254
0:29:17.960 --> 0:29:21.280
data for the wider population, which of course,

255
0:29:21.280 --> 0:29:23.040
you're totally right.

256
0:29:23.040 --> 0:29:26.960
It is kind of a subset of highly politicized, maybe also

257
0:29:26.960 --> 0:29:29.360
a bit more educated than average people.

258
0:29:29.360 --> 0:29:30.200
So you cannot.

259
0:29:30.200 --> 0:29:32.160
But this is not what we're trying to do also.

260
0:29:32.160 --> 0:29:34.640
You're not trying to infer, I don't know,

261
0:29:34.640 --> 0:29:39.000
actual election results based on Twitter data.

262
0:29:39.000 --> 0:29:43.040
So yeah, I don't know if I addressed the point.

263
0:29:43.040 --> 0:29:44.480
Maybe we can take more about it.

264
0:29:44.480 --> 0:29:46.200
To be concerned about this kind of thing.

265
0:29:46.200 --> 0:29:47.320
Right.

266
0:29:47.320 --> 0:29:48.320
Thank you.

267
0:29:48.320 --> 0:30:17.600
Next time.

