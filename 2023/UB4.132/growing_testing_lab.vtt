WEBVTT

00:00.000 --> 00:12.680
Hello, everyone. Thanks for being here so early on a Sunday morning. My name is Laura.

00:12.680 --> 00:20.700
I work at Calabra. And today I'd like to share with you a word story about how we built and

00:20.700 --> 00:26.080
grew our laboratory for upstream testing. I'm going to share with you a little bit about

00:26.080 --> 00:31.680
our infrastructure, as well as some of the challenges that we have to face while scaling

00:31.680 --> 00:41.160
up. So our main goal was to build a big test bed for open source projects to use. So of course,

00:41.160 --> 00:46.400
we're going to need diverse ecosystem of devices. So many different devices of different

00:46.400 --> 00:51.200
architectures and from different vendors. Of course, we're going to need a software to

00:51.200 --> 00:57.960
automate the tests on the actual devices. We need a monitoring system, so a way to monitor

00:57.960 --> 01:04.800
and assess the health of the devices that we have in the lab. And we also need some recovery

01:04.800 --> 01:12.080
strategies. So mainly, when devices start to misbehave or don't behave, it's expected we need

01:12.080 --> 01:18.560
some way of recovering them automatically, or putting them offline automatically if they're

01:18.560 --> 01:27.040
not reliable to run tests. So it all starts with a commit. The developer pushes the changes into a

01:27.040 --> 01:34.960
development branch. The artifacts for the tests are built automatically. A test job is submitted

01:34.960 --> 01:42.760
and run. The results are gathered and parsed. And finally, a report is generated and sent back to the

01:42.760 --> 01:49.280
developer. So from the lab perspective, we're interested in the part that runs the test jobs

01:49.280 --> 01:57.000
and makes the results available. What we chose for our lab is lava, as we saw earlier. This is the

01:57.000 --> 02:04.280
linear automation and validation architecture. So it automates the boot and deploy phases of the

02:04.280 --> 02:13.080
apprentice system on the device. It has a really scalable scheduler. It allows to run thousands of

02:13.080 --> 02:22.120
jobs on hundreds of devices on a single instance. So that's really convenient for big labs. It

02:22.120 --> 02:29.760
handles the power on the devices. So it switches the power on and off on the devices when needed.

02:29.760 --> 02:37.960
And it also helps monitoring the serial output. And finally, it also makes the results of the

02:37.960 --> 02:44.840
tests available in many different formats, which is, again, pretty convenient. Lava, again, just

02:44.840 --> 02:51.080
takes care of this part of the CI loop, while all the other phases needs to be implemented with

02:51.080 --> 03:01.360
different tools. So in order to run devices in lava, we need to fulfill set of base requirements.

03:01.360 --> 03:07.120
Of course, we're going to need to be able to turn on and off the power on the devices remotely.

03:07.120 --> 03:15.520
We need access to a reliable serial console remotely. And finally, we need some way of

03:15.520 --> 03:23.520
booting an arbitrary combination of kernel, device tree, and root of us remotely. For all the devices

03:23.520 --> 03:32.720
that we have in the lab, we rely on TFTP. So we need network connectivity at the bootloader level.

03:32.720 --> 03:40.640
And that means that we often have to build our custom bootloaders and enable all the features

03:40.640 --> 03:47.200
that we need for debugging. So there are a few steps to prepare the devices before they enter

03:47.200 --> 03:55.600
the lab. As far as the configuration of the devices itself in lava, we have a couple of...

03:55.600 --> 04:02.800
You only need to define some Jinja 2 and YAML templates. So the device type template basically

04:02.800 --> 04:09.040
defines the characteristic of the device type. So for example, which kind of bootloader run on a

04:09.040 --> 04:15.840
certain device, which kind of command line options are needed for booting. Well, the device dictionary

04:15.840 --> 04:23.040
defines device specific characteristics. So for example, what command do we need to run to turn on

04:23.040 --> 04:29.680
and off the power or to access the serial console. And finally, we have the health check, which is a

04:29.680 --> 04:36.800
special kind of job associated to each device type. And the aim of the health check is to assess

04:36.800 --> 04:45.840
the health status of each device. It's supposed to be run on a fairly regular basis. We run a

04:45.840 --> 04:51.600
health check on every device that we have in the lab every day. And examples of tests that you can

04:51.600 --> 04:57.440
fit in a health check are, for example, a battery test, or you can check the temperature on the

04:57.440 --> 05:03.040
device to make sure it's not overheating. You can check the network connectivity. Basically, all the

05:03.040 --> 05:08.320
tests that you need to make sure that the device is functional, you can fit them in health check.

05:09.120 --> 05:15.680
And whenever a device fails its health check, lava automatically puts it offline.

05:16.560 --> 05:22.080
So it's really useful just to shut down all the devices that are not reliable at the moment.

05:24.000 --> 05:31.040
So Colabra maintains a laboratory running lava. And we have, as of a couple of days ago,

05:31.040 --> 05:40.640
217 devices of 38 different types spread across 16 racks. Each rack is controlled by its own server.

05:40.640 --> 05:46.720
And that's also where the lava dispatcher runs. And of course, besides all the device types,

05:46.720 --> 05:52.720
devices, we also have a bunch of hardware equipment that we need to automate the boot

05:52.720 --> 06:01.280
and test phases on our devices. And this is what the device distribution looked like in January.

06:02.160 --> 06:11.520
So the vast majority of our devices are x86, 64, and arm 64 platforms. And we also have some QAML

06:11.520 --> 06:19.200
instances that are mainly used by kernel CI. And the very vast majority of our devices are

06:19.200 --> 06:24.080
actually Chromebook laptops. But we also have some embedded SPC devices as well.

06:27.600 --> 06:35.360
So what kind of hardware do we have in the lab? So different devices as usually different

06:35.360 --> 06:42.080
requirements. So for embedded SPCs, what we use to control the power on them remotely are

06:42.080 --> 06:48.640
Ethernet control relays and PDUs. I left there some examples of the actual models that we currently

06:48.640 --> 06:56.000
have in the lab. Chromebooks are kind of a different beast. They have their own hardware

06:56.000 --> 07:03.440
debug interface, which is the Servo v4 and this user cables. So Servo v4 allows you to control

07:03.440 --> 07:10.240
the power on the device to access the serial consoles on the device, and also provides network

07:10.240 --> 07:15.920
connectivity through an internet port. So you can fit everything you need to automate the

07:15.920 --> 07:24.160
boot and test phases on a Chromebook that fits inside just one hardware box. As an alternative,

07:24.160 --> 07:31.040
you also have CZ cables, which pretty much have the same functionality except for the network

07:31.040 --> 07:38.240
connectivity that you have to provide through usually to a USB to Ethernet adapter. We have a

07:38.240 --> 07:45.760
couple of servers as well in the lab. And for those we use the IPMI standard protocol just to

07:45.760 --> 07:52.640
control the power and access the serial consoles. And for all the devices, of course, we're going

07:52.640 --> 08:04.000
to need a bunch of USB cables with their fragilities. And also we use USB hubs. We find especially

08:04.000 --> 08:09.760
useful the switchable hubs such as the YKush, especially for those devices that are controlled

08:09.760 --> 08:17.200
by just one USB connection such as the Chromebooks. So that's really convenient just not to have

08:17.200 --> 08:23.440
them to manually intervene every time you need to re-plug the USB connection. As for the software,

08:23.440 --> 08:30.560
I left here a couple of links. You want to check them out. We use PDU daemon to execute comments

08:30.560 --> 08:38.880
on the PDUs. We use Conservo to access the serial consoles and monitor the output. And the HDC tools

08:38.880 --> 08:43.280
are just for the Chromebooks. These are the software tools that allows you to interact

08:43.840 --> 08:50.160
with the servo v4 and with the CZ cable as well just to control the power and serial on the

08:50.160 --> 08:57.040
Chromebooks. For the interaction with lava, we use lava CLI. It's a command line interface.

08:57.600 --> 09:04.080
And that's useful to run the tests on the device and also configure and push the templates.

09:04.080 --> 09:11.920
Finally, we also have a lava GitLab runner that serves as a bridge between GitLab and lava.

09:13.520 --> 09:22.560
And yeah, that's pretty much it for the software side. So in our lab, we have two major users.

09:22.560 --> 09:28.560
One is kernel CI, which is focused on continuous testing of the Linux kernel.

09:28.560 --> 09:34.400
It's not only boot tests. We have a bunch of other test suites running on them. And

09:35.680 --> 09:40.160
the type of testing that kernel CI does is post merge. So changes are

09:40.160 --> 09:49.600
tested after they landed on a set of monitor trees. So after the tests have run, kernel CI

09:51.360 --> 09:57.360
will generate some build reports as well as some regression reports for every regression that is

09:57.360 --> 10:07.520
found. The other major player in our lab is mezoci. That's DCI for meza 3D. And it does

10:07.520 --> 10:13.440
conformance testing and also performance tracking. There are a bunch of test suites that are currently

10:13.440 --> 10:21.920
run by mezoci. I left the list here. A bunch of APIs and drivers are tested. And while kernel CI

10:21.920 --> 10:28.800
only does post merge testing, meza CI also does pre-merge conformance tests. So that's a little

10:28.800 --> 10:37.280
bit of both. And in this diagram, you can see what the usage of kernel CI and meza CI in our

10:38.080 --> 10:45.680
laboratory. As you can see, both projects keep our lab pretty busy. Kernel CI uses

10:45.680 --> 10:52.800
almost all the architectures that we have in the lab while meza CI is focused more on x86,

10:52.800 --> 11:04.800
64, and ARM 64. And with so many jobs running every day in our lab, of course, the impact

11:04.800 --> 11:12.320
of any error or unreliability in the infrastructure can be quite big. So for pre-merge tests,

11:12.320 --> 11:17.840
you have the merge requests from users can get blocked. And definitely, if a certain device type

11:17.840 --> 11:27.440
is not available. And also, yeah, there is a risk of merge requests getting rejected if there are

11:27.440 --> 11:32.400
many errors in the lab. So what we need to make sure from the lab perspective is that

11:33.840 --> 11:40.240
the merge requests from users do get rejected only because the change is introduced,

11:40.240 --> 11:47.120
made the test fail, and not because of any infrastructure error. Well, for post-merge

11:47.120 --> 11:53.760
tests, we have a risk of reporting false regressions. In this case, we want to make

11:53.760 --> 12:00.480
sure again that the infrastructure errors are reported as such by lava. And lava defines

12:01.840 --> 12:06.560
different types of exceptions that you can raise based on the type of error that occurs.

12:06.560 --> 12:12.240
We need just to make sure that the devices and lava itself is configured properly to do so.

12:14.960 --> 12:21.760
So yeah, this is just a minimal list of the common issues that we have seen over the years.

12:21.760 --> 12:26.880
Of course, there can be other degradation. You can have faulty cables at any time or

12:26.880 --> 12:34.480
batteries just failing, power chargers not working properly. All kinds of network issues can happen

12:34.480 --> 12:42.000
at any time and they can have quite a big impact. We also saw some issues related to the rack setup.

12:42.000 --> 12:49.200
So for example, we had some laptops where the lid was likely too closed because of how it was set

12:49.200 --> 12:55.520
up in the rack and it was causing the device to enter suspend unexpectedly. So we have all kind

12:55.520 --> 13:02.160
of different errors that can happen. Of course, we can have firmware bugs either in the firmware

13:02.160 --> 13:09.120
running or the actual devices or also firmware running in the hardware debug interface. So that's

13:09.120 --> 13:17.680
a lot of errors that can happen. I gather a few of my favorite pitfalls. These are issues, tricky ones

13:17.680 --> 13:27.040
that we have found recently and we're still dealing with some of those. So one of the things

13:27.040 --> 13:34.160
that we saw is that sometimes it happens that the serial console will just stop outputting anything

13:34.160 --> 13:41.680
on the serial console. And if this happens during the test phase, it's kind of hard to understand

13:41.680 --> 13:50.080
in an automated way whether the kernel is hanging or whether your USB cable connection has dropped

13:50.080 --> 13:57.760
or if it's just like a reliable serial connection. So that's usually a tricky one to deal with.

13:59.600 --> 14:08.400
Another serial related one is caused by interference. So not all devices can have multiple

14:08.400 --> 14:15.520
UART connections for debug. Most of our devices in the lab don't. So we have to share the same

14:15.520 --> 14:22.320
serial connection between the kernel and the test shell. And this sometimes can cause some

14:22.320 --> 14:30.560
interference and of course you will confuse lava about the outcome. So one way that we are

14:31.680 --> 14:38.400
thinking of many solutions to deal with this kind of serial issue stuff and one approach that we are

14:38.400 --> 14:46.000
looking into is actually using a docker container. So running a docker container, connecting to the

14:46.000 --> 14:54.080
device over SSH and run the tests on the SSH console. This way we can probably work around some of these

14:54.080 --> 15:01.680
serial issues. So as I said there are also network connectivity issues from time to time and

15:01.680 --> 15:09.520
if the network drops during the bootloader phase that's usually something we can easily catch

15:10.240 --> 15:16.960
because lava of course monitors the serial output. And if our bootloader is nice enough to

15:18.640 --> 15:25.040
print error messages we can just catch the right patterns at the right time and just raise

15:25.040 --> 15:32.720
an infrastructure error so that won't officiate like the outcome of the test. When this happens

15:32.720 --> 15:39.040
we can also configure lava to retry the job if needed. So when this happens it's useful to catch

15:39.760 --> 15:46.160
error patterns. If network decides to drop during the test phase that's usually

15:47.520 --> 15:54.400
worse especially for devices that rely on a network phase system. So it's usually pretty

15:54.400 --> 16:01.840
hard to recover from this. We have seen occasional USB disconnection for whatever reason and yet it's

16:01.840 --> 16:10.000
hard to recover from these kind of issues usually. So these are some of the best practices we came

16:10.000 --> 16:20.320
upon while working on these issues. So the first one is about writing robust health checks. So as

16:20.320 --> 16:28.160
I said devices will be put offline by lava if the health check fails so we need to make sure that

16:28.160 --> 16:38.000
the health checks catch as many issues as possible automatically. We found very useful to monitor the

16:38.560 --> 16:45.120
lava infrastructure error exceptions and this is mainly to catch issues with specific racks or

16:45.120 --> 16:54.560
specific device types. We usually try to monitor also the device itself and the job queue as well

16:54.560 --> 17:00.160
and this is to make sure that we have enough devices of a certain device type to feed all

17:00.160 --> 17:10.000
the pipelines for the projects and also to minimize if a certain device type goes offline

17:10.000 --> 17:17.760
and if we have redundancy we're able to recover from that. Last but not least as I said

17:18.400 --> 17:24.080
one best practice is to try and isolate the test shell output and kernel messages whenever possible.

17:25.040 --> 17:28.160
If not we're trying to work around some of these issues.

17:30.480 --> 17:36.800
So next steps for our lab is of course increase the lab capacity and try to cover even more

17:36.800 --> 17:43.920
platform and different vendors as soon as they come out. While doing this we are continuing to

17:43.920 --> 17:48.640
improve our infrastructure and our monitoring tools. I haven't included in this presentation

17:48.640 --> 17:55.680
how we actually monitor things but yeah lava does have some APIs that you can use to monitor

17:55.680 --> 18:04.160
the status of each device and also of the server and yeah of course while keeping to keep adding

18:04.160 --> 18:09.840
new lab devices we also want to increase the coverage of test suites so we're working on

18:09.840 --> 18:18.320
adding even more tests to it on kernel ci and mezoci as well and that's it. If you have any

18:18.320 --> 18:34.800
questions I think I have time right.

18:34.800 --> 18:38.160
Any questions?

18:38.960 --> 18:46.800
So in your experience how often is something wrong with the lab? Very often I'd say.

18:48.080 --> 18:49.440
Can you give me a specific question?

18:52.560 --> 18:59.280
Yeah I don't have data at hand with the actual failures but yeah it happens pretty often. I mean

18:59.280 --> 19:07.120
we have so many jobs running every day and we rely heavily on USB which is kind of not great,

19:07.120 --> 19:14.480
like it breaks pretty often. I'd say the most common issues that we have is usually due to the

19:14.480 --> 19:22.720
serial consoles being not too reliable. The vast majority of devices that we have are Chromebooks

19:22.720 --> 19:29.600
and we're using like these hardware debug interfaces that were meant for debugging so sometimes like

19:29.600 --> 19:35.520
the serial connection is not great and that caused all kind of issues so we try at least to retry

19:35.520 --> 19:55.440
the jobs when possible and catch the infrastructure errors as they come out.

19:55.440 --> 20:01.760
I'd say we don't have to manually intervene every day. I'd go like every couple of days we need to

20:01.760 --> 20:07.760
maybe replug some of the devices because we as I said we use the switchable hubs to try and

20:08.320 --> 20:14.880
avoid having to reset the connection manually but we don't have this set up on each and every

20:14.880 --> 20:22.400
device that we have. We're working on it but yeah I'd say like at least a couple of times a week.

20:22.400 --> 20:27.680
I haven't like really checked the frequency of it but yeah of course there's people in the lab

20:27.680 --> 20:31.840
actually taking care of all the devices.

20:41.600 --> 20:47.520
So from the lab perspective we don't really care about the test suites running. It's more the

20:47.520 --> 20:55.280
responsibility of kernel CI and mazoci. You can check out the the links that I left like everything

20:55.280 --> 20:58.960
is of course open source so you can check out all the test suites and how they work.

21:00.880 --> 21:07.680
Yeah some tests you just need a run disk. Some other tests rely the most heavier ones

21:07.680 --> 21:09.280
rely on natural practice system.

21:09.280 --> 21:15.280
Yeah.

21:29.200 --> 21:34.960
I mean it depends on the type of tests that you need to run. We use a lot of Chromebooks

21:34.960 --> 21:40.960
because we need Chromebooks so yeah you cannot really emulate one.

21:40.960 --> 21:46.960
Yeah.

21:46.960 --> 22:08.160
All right no more questions so thank you. Thank you.
