WEBVTT

00:00.000 --> 00:12.280
Hello, thank you very much for having us here.

00:12.280 --> 00:16.840
I'm Daniel Aguido, together with my colleague, Elizabeth Garrard.

00:16.840 --> 00:24.920
We are coming from the University of Luxembourg, from the Center of Contemporary Digital History,

00:24.920 --> 00:34.080
where we're running this new journal, this new idea of journal, together with a well-known publisher

00:34.080 --> 00:38.040
in the Open Access publication, which is the Groiter.

00:38.040 --> 00:46.400
The idea is the journal of digitalhistory.org, and then the idea is how to bring reproducible

00:46.400 --> 00:53.120
papers in the humanities and in digital history in our specific case.

00:53.120 --> 00:57.600
And then that's why we joined forces with them, so it's a joint venture with them directly,

00:57.600 --> 01:04.600
so the team is relatively small compared to other projects, and then we have two perspectives

01:04.600 --> 01:07.960
that we decided to put together.

01:07.960 --> 01:16.160
On our side, we understand that academic publishing is a bit too traditional, especially in history.

01:16.160 --> 01:23.800
And then our researchers, they currently work on Jupyter Notebook to run their own experiment

01:23.800 --> 01:31.520
and so on, so the idea was, can we pass from experiment on Jupyter Notebook to actual publication,

01:31.520 --> 01:33.160
also in our domain?

01:33.160 --> 01:37.320
And on the other side, they wanted to test out this hypothesis, because they really want

01:37.320 --> 01:44.040
to engage with new publication practices, and this joint venture would just be a good

01:44.040 --> 01:46.080
match.

01:46.080 --> 01:55.160
And well, reproducible papers in digital history means a lot of things, because first of all,

01:55.160 --> 02:03.480
we have now massive digitization process of primary sources and secondary literature and

02:03.480 --> 02:10.960
also new digital-born material, like the Twitter archive that we've been seeing before.

02:10.960 --> 02:15.520
On one side, the detail of the code, Cruz also sharing the dataset is one thing, but

02:15.520 --> 02:19.240
the other thing is really how this data has been created.

02:19.240 --> 02:22.400
So what were the conditions of the production of this data?

02:22.400 --> 02:30.520
So this is very important for us as historians, not for me, for my colleague, and then interpretation.

02:30.520 --> 02:37.800
So how the dataset has been built, which were the limits, all this questions need to be

02:37.800 --> 02:41.080
addressed in a different way.

02:41.080 --> 02:45.800
And then at the same time, we have, of course, new standards, not only digital history, but

02:45.800 --> 02:54.680
also the famous FAIR principle, so findable, accessible, interoperable, and reusable data.

02:54.680 --> 03:01.040
And we need to meet this criteria also with our journal.

03:01.040 --> 03:04.760
And this idea of Moolen is the one of a Braden narrative.

03:04.760 --> 03:09.720
So he advocates for bringing together two things.

03:09.720 --> 03:14.080
One is the narrative of the argumentation of our publication.

03:14.080 --> 03:19.560
The other one is the interpretation of data and say that they can be done in a narrative

03:19.560 --> 03:21.400
way.

03:21.400 --> 03:26.400
This is where we put these so-called multi-layers together.

03:26.400 --> 03:33.280
So this one is like every article published in our journal has a fingerprint sort of identity

03:33.280 --> 03:39.880
where this level, like the narrative, the hermeneutic level, and the data layer are

03:39.880 --> 03:40.880
together.

03:40.880 --> 03:47.720
So this is the representation of one Jupyter notebook, which is normally linear cell by

03:47.720 --> 03:48.720
cell.

03:48.720 --> 03:54.120
We just distorted, we put it in a circle, and here you can test it out.

03:54.120 --> 04:00.400
So this was also a tool, it is also a tool for our authors, which we own them a lot because

04:00.400 --> 04:05.040
they are our primary tester.

04:05.040 --> 04:08.560
It is still an experimental journal.

04:08.560 --> 04:14.440
And there you could tweak with data, you can change the content, and you see how the fingerprint

04:14.440 --> 04:15.440
is changing.

04:15.440 --> 04:25.720
This was just an experiment at the beginning, but then it really becomes integrated into

04:25.720 --> 04:29.640
the main interface of the journal.

04:29.640 --> 04:33.080
And we saw that indeed they were very different.

04:33.080 --> 04:35.680
Okay, yes, I run.

04:35.680 --> 04:42.440
They were very different, and we can see also the code style of every Jupyter notebook,

04:42.440 --> 04:45.920
how they also decided to narrate the arguments.

04:45.920 --> 04:48.800
So I will go quickly, sorry.

04:48.800 --> 04:53.960
And then this is like the basic layer, so the narrative layer that looks like an MV

04:53.960 --> 04:59.240
viewer with steroids in the sense that we have figures, we have tables, we have bibliography.

04:59.240 --> 05:02.960
With Zotero and Site2C.

05:02.960 --> 05:09.320
And then above all, it's a very thin layer on top of the Jupyter notebook because we

05:09.320 --> 05:12.560
use the usual output of the notebook.

05:12.560 --> 05:17.360
So this is very like an augmented MV viewer.

05:17.360 --> 05:24.080
And then we have, as it is a braided narrative, we decided to have this metaphor of this level

05:24.080 --> 05:25.740
one on top of the other.

05:25.740 --> 05:30.520
So this is a sort of animation, so on the left you see the full hermeneutic layer,

05:30.520 --> 05:37.780
and on the other side you can see how it slides through the, like behind the narrative layer.

05:37.780 --> 05:46.000
And the data layer is for the moment the part on top, right top, which we use MyBinder,

05:46.000 --> 05:50.680
fantastic service to publish online your notebooks.

05:50.680 --> 05:57.440
And we wanted this article not only to be a show off of the dataset, but also a small

05:57.440 --> 06:05.080
history lab so that people could just click on a button and get to the data and understand

06:05.080 --> 06:07.200
how the data will be composed.

06:07.200 --> 06:12.320
The good thing is that we decided to keep this, MyBinder, as this source of truth.

06:12.320 --> 06:20.040
So the article that you see published is exactly the same copy, with just a different way of

06:20.040 --> 06:22.440
interacting with this different layer.

06:22.440 --> 06:27.780
So this is how it looks like on MyBinder, so it's a classical Jupyter notebook.

06:27.780 --> 06:35.520
And for every notebook we have a GitHub repo, where we store all the requirements and all

06:35.520 --> 06:38.480
the images in the dataset.

06:38.480 --> 06:47.240
We have to put together the fair metadata, but still, so it's under construction.

06:47.240 --> 06:51.240
Then what does it mean having Jupyter notebooks for publishing?

06:51.240 --> 06:58.020
We see that in the literature there are a lot of critics, shouldn't use Jupyter notebooks

06:58.020 --> 07:04.800
because it's too complex, it's impossible to replicate and so on and so forth.

07:04.800 --> 07:09.680
But then for us, it was really the simplest solution.

07:09.680 --> 07:15.280
So at the same time, to publish with Jupyter, we had to make our pipeline a bit more complex

07:15.280 --> 07:21.800
as usual, so we have a first review directly on the abstract, where we start communicating

07:21.800 --> 07:27.960
with the authors, understanding their needs, creating a writing environment for them that

07:27.960 --> 07:34.480
can be replicated with Docker containers for Python and Air.

07:34.480 --> 07:38.360
And then the first technical review, she's in charge of the first technical review, which

07:38.360 --> 07:42.960
is the most complicated one, because there's a lot of checks.

07:42.960 --> 07:47.400
We saw some projects already, we needed to have checks.

07:47.400 --> 07:52.440
And then we have a lot of other open source software that enters this pipeline, like for

07:52.440 --> 07:58.520
the preview of the notebook, we took it up, and the viewer, we have my binder, and this

07:58.520 --> 08:03.440
is just for the first technical review, because then the article is being sent to the reviewer

08:03.440 --> 08:05.280
for the double banana review.

08:05.280 --> 08:11.000
So before even reviewing, we had to do this huge job because they have to review also

08:11.000 --> 08:14.280
the data and the pertinence of the dataset.

08:14.280 --> 08:21.440
And then finally, there is one important thing, so it's English editing, so how to edit something

08:21.440 --> 08:25.640
that which is already being run, so without running itself.

08:25.640 --> 08:30.920
So this could be a tool for translators, tool for correctors, that they're not into the

08:30.920 --> 08:33.040
Jupyter world, so how to do that?

08:33.040 --> 08:38.900
We have Jupyter text, we're still testing some plugin to see if this could work without

08:38.900 --> 08:41.640
touching the final output.

08:41.640 --> 08:46.280
And then the final technical review, so after all this has been shipped, we have a DOI,

08:46.280 --> 08:51.640
so the article is now published, needs to be indexing, and there is the problem of long-term

08:51.640 --> 08:56.320
archiving, which is a big problem for many reasons.

08:56.320 --> 09:05.600
First of all, libraries that get deprecated, also API that disappeared, so how to really

09:05.600 --> 09:08.140
reproduce this in the future.

09:08.140 --> 09:13.040
And then finally, the dataset needs to be included into, we have dataverse, but we are

09:13.040 --> 09:17.560
looking for Zenodo in order to match the fair metadata.

09:17.560 --> 09:22.480
And time is up, I have a question for you, of course.

09:22.480 --> 09:24.000
Thank you very much, first of all.

09:24.000 --> 09:31.680
And then if you want to contact us, just collaborate or work together on Jupyter publication, jdhadmin

09:31.680 --> 09:33.340
at uni.lu.

09:33.340 --> 09:37.800
And then the question are, how can we actually collaborate on something which is a notebook

09:37.800 --> 09:45.400
that requires quite a threshold of expertise, not only for the researcher, but for the people

09:45.400 --> 09:51.920
that are around, and how to maintain all this and how to make this history lab living for

09:51.920 --> 09:53.960
more than one year.

09:53.960 --> 09:54.960
Thank you.

09:54.960 --> 10:10.840
Thank you very much.

10:10.840 --> 10:29.880
Yeah, well, I repeat the question, so he asked me if the double blind review, how can we

10:29.880 --> 10:33.880
keep it actually an actual double blind.

10:33.880 --> 10:37.940
So she anonymized the data on GitHub.

10:37.940 --> 10:43.600
So we have specific repository that have been created after the communication with the authors,

10:43.600 --> 10:50.320
where we only have the code without the names, but then you still have the bibliography,

10:50.320 --> 10:54.160
so it's easy to, it's a very small word, one of the digital history.

10:54.160 --> 10:58.280
But still, this is the way to maintain double blind.

10:58.280 --> 11:05.960
And then we're going to send the review where both the MyBinder and the version of the article

11:05.960 --> 11:10.360
on our website with a hidden URL.

11:10.360 --> 11:12.520
So this is the only thing that we can do.

11:12.520 --> 11:20.000
For sure, the double blind, we have the problems that we cannot really use the peer request

11:20.000 --> 11:22.600
directly on the GitHub repository.

11:22.600 --> 11:26.800
In fact, there is some replication between the GitHub repository.

11:26.800 --> 11:37.080
After with the peer review, there is some recursivity that come back to technical review

11:37.080 --> 11:39.160
because there is a revision.

11:39.160 --> 11:43.800
There is this question about how we synchronize the notebook together.

11:43.800 --> 11:52.480
There is some authors that they have good enough with GitHub.

11:52.480 --> 11:59.360
But to review a notebook with the output with the metadata to track what has been changed.

11:59.360 --> 12:10.920
This, it was the question that you have, we are testing with ReviewNB, or also to maybe

12:10.920 --> 12:19.720
use some markdown or just Python script to produce several output in order to not sometimes

12:19.720 --> 12:26.720
touch about these metadata that they are inside the notebook.

12:26.720 --> 12:29.760
And there was another question, but I don't know if we have time.

12:29.760 --> 12:30.760
Yes?

12:30.760 --> 12:31.760
Yes.

12:31.760 --> 12:32.760
Yes.

12:32.760 --> 12:33.760
Please.

12:33.760 --> 12:36.760
I'm sorry, auto assess?

12:36.760 --> 12:45.200
Yeah, that's the very big, big, big question.

12:45.200 --> 12:52.440
So the idea behind the narrative is then you tell the story around the data on one side,

12:52.440 --> 12:59.880
and on the other side, you keep the data like with the Zenodo metadata coherent, or probably

12:59.880 --> 13:09.520
with what Paul showed us before with Ricardo, so having like an external check on the metadata

13:09.520 --> 13:11.320
and on the dataset itself.

13:11.320 --> 13:17.320
At the same time, the initial, the first technical review is the one where we assess actually

13:17.320 --> 13:18.320
the data.

13:18.320 --> 13:24.120
So if the data sets are complete, coherent, we don't judge them because then we know that

13:24.120 --> 13:29.600
there are conditions of production that needs to be, we try to make this as more explicit

13:29.600 --> 13:30.600
as possible.

13:30.600 --> 13:31.600
Yes, exactly.

13:31.600 --> 13:40.120
And this, like that's why the long term maintenance, so now we only have nine articles, but we

13:40.120 --> 13:45.320
have 28 in the pipeline in the coming year.

13:45.320 --> 13:50.080
So it's really now it's getting up speed, and we have more and more interaction with

13:50.080 --> 13:52.760
all those, which makes things more complicated.

13:52.760 --> 13:53.760
Thank you.

13:53.760 --> 13:54.760
Okay.

13:54.760 --> 14:11.480
Okay.
