1
0:00:00.000 --> 0:00:16.380
All right, so I'll start a little bit earlier of the, actually, the 2PM.

2
0:00:16.380 --> 0:00:20.240
Just by saying that, so I'm wearing this t-shirt because I'm one of the Dev Room Manager here

3
0:00:20.240 --> 0:00:25.660
in this room, and I'm taking over, actually, a talk slot that has been cancelled.

4
0:00:25.660 --> 0:00:33.800
So we were supposed to hear a talk by Maya Arias de Reina, who couldn't make it today,

5
0:00:33.800 --> 0:00:34.960
unfortunately.

6
0:00:34.960 --> 0:00:42.120
She was supposed to talk about data flowing in the right way, which is a talk about a

7
0:00:42.120 --> 0:00:49.200
tool called Kauto, which implements data workflows with a low-code, no-code approach.

8
0:00:49.200 --> 0:00:53.320
This is what it looks like, so of course I can't talk about this tool because I don't

9
0:00:53.320 --> 0:00:54.320
know it.

10
0:00:54.320 --> 0:00:57.520
It really looks pretty cool.

11
0:00:57.520 --> 0:01:06.400
So we are very sorry for Maya not being here, and we hope we can host her next time.

12
0:01:06.400 --> 0:01:19.600
So I will speak about a project, a research project, called Ricardo in Digital Maintenance,

13
0:01:19.600 --> 0:01:25.200
which I've done with, oops, sorry, are we going for me?

14
0:01:25.200 --> 0:01:32.840
Yes, I worked with Bertrice de Dangere from Sciences Po, she is a historian, and I am

15
0:01:32.840 --> 0:01:33.840
Paul Girard.

16
0:01:33.840 --> 0:01:40.800
I am from a company called Westware, a small agency, specialized into developing data-oriented

17
0:01:40.800 --> 0:01:45.040
web applications, and we do work a lot with researchers.

18
0:01:45.040 --> 0:01:50.600
Today I'm here to talk about how actually a collaboration between a researcher, Bertrice

19
0:01:50.600 --> 0:01:56.640
and Dangere, myself, can be fostered by using data control systems.

20
0:01:56.640 --> 0:02:01.880
By data control systems, I mean making sure we care about the data we are using in the

21
0:02:01.880 --> 0:02:09.600
research by documentation, version control, and also quality control.

22
0:02:09.600 --> 0:02:14.920
So the research is about the history of trade.

23
0:02:14.920 --> 0:02:22.600
So together with Bertrice, we build a database of trade flows between nations, well, between

24
0:02:22.600 --> 0:02:29.760
geopolitical entities in the world in the 19th century, which means that we have the

25
0:02:29.760 --> 0:02:38.640
main data is we know how much amount of money in different currencies has been exchanged

26
0:02:38.640 --> 0:02:42.480
between different geopolitical entities in the 19th century.

27
0:02:42.480 --> 0:02:47.360
We know important exports, and we know this with a bilateral view, which means that we

28
0:02:47.360 --> 0:02:54.880
know the trade from France to UK, for instance, and the reverse two from two different sources,

29
0:02:54.880 --> 0:02:58.880
which makes quite a nightmare to deal with, but still.

30
0:02:58.880 --> 0:03:04.120
So this is basically the main publication we already achieved.

31
0:03:04.120 --> 0:03:13.200
So we started by releasing in 2016-17 a web exploration tool, I'll show you, and also

32
0:03:13.200 --> 0:03:16.440
a paper about how we build this database.

33
0:03:16.440 --> 0:03:24.720
And then in 2021, we released a new database called Geopol East, which is basically a dataset

34
0:03:24.720 --> 0:03:30.000
that explained trying to track which geopolitical entity.

35
0:03:30.000 --> 0:03:35.040
So I'm using this weird word because we have countries, of course, but also entities that

36
0:03:35.040 --> 0:03:40.440
are part of countries, but we also have trade with entities that were colonies at that time

37
0:03:40.440 --> 0:03:43.720
and all the kind of weird political statuses.

38
0:03:43.720 --> 0:03:49.600
And because of that, we built this Geopoly database where we tried to track which geopolitical

39
0:03:49.600 --> 0:03:55.320
entities were controlled by which other one in time.

40
0:03:55.320 --> 0:04:04.520
And recently, we released a new version of the database, adding 230,000 new trade flows.

41
0:04:04.520 --> 0:04:11.040
And this releasing of new data, because actually Beatriz discovered new archives, new archival

42
0:04:11.040 --> 0:04:19.360
book about trade, this massive updates needed a tool to make sure we can actually release

43
0:04:19.360 --> 0:04:25.160
the data which are clean the structure the way we want it without having to deal with

44
0:04:25.160 --> 0:04:28.120
all those kind of issues manually.

45
0:04:28.120 --> 0:04:30.000
I will speak about that a little bit later.

46
0:04:30.000 --> 0:04:33.480
So this is what the websites, the main website looks like.

47
0:04:33.480 --> 0:04:40.040
It's a web application you can go where you can explore basically the trade of the world

48
0:04:40.040 --> 0:04:42.040
in the 19th century.

49
0:04:42.040 --> 0:04:43.920
So we have different kinds of visualizations.

50
0:04:43.920 --> 0:04:49.480
I will not go through all of them because I don't want to focus too much on this today.

51
0:04:49.480 --> 0:04:54.840
Well if you have questions about this, we can go back to that later.

52
0:04:54.840 --> 0:05:01.120
We also have these websites, Geopoly, that allows you to actually explore the political

53
0:05:01.120 --> 0:05:10.960
evolutions of the links, sovereignty, links between the different entities.

54
0:05:10.960 --> 0:05:17.480
I'll show you a little bit what it's like just afterwards I think.

55
0:05:17.480 --> 0:05:23.120
So just as we totally announced, this slide deck is actually something I already presented

56
0:05:23.120 --> 0:05:25.600
in another conference.

57
0:05:25.600 --> 0:05:32.320
So I wanted to speak about the visual data exploration tools we made, the frictionless

58
0:05:32.320 --> 0:05:33.680
data integration.

59
0:05:33.680 --> 0:05:37.280
So this is the main point I want to speak about today, point two.

60
0:05:37.280 --> 0:05:44.600
And also the third point was how we can actually analyze heterogeneous data in the long run,

61
0:05:44.600 --> 0:05:47.200
like one century of data.

62
0:05:47.200 --> 0:05:55.400
My main, I will try to convince you that actually using data integration is a very nice and

63
0:05:55.400 --> 0:06:00.520
important tool to foster this long lasting collaboration we had between Bertrice, historian,

64
0:06:00.520 --> 0:06:03.560
and I, a data engineer.

65
0:06:03.560 --> 0:06:07.360
So that collaboration I just put a link to a conference I gave a few years ago about

66
0:06:07.360 --> 0:06:10.760
this specific subject.

67
0:06:10.760 --> 0:06:15.600
So visual data exploration, so I will really go quickly on this part to focus more on the

68
0:06:15.600 --> 0:06:18.400
second part.

69
0:06:18.400 --> 0:06:26.480
Our main objective here is to propose basically a tool, a set of interactive data visualizations

70
0:06:26.480 --> 0:06:34.520
on the web that all those researchers are basically people exploring this to change

71
0:06:34.520 --> 0:06:41.240
points of view on the data, looking at, for instance, the total trade of the world, then

72
0:06:41.240 --> 0:06:49.600
focusing on one specific country, then on one specific currency, and to be able to add

73
0:06:49.600 --> 0:06:54.080
all the different ways to look at the data in the same tool.

74
0:06:54.080 --> 0:06:58.480
We also like to offer visual documentation, like visualization is a really nice important

75
0:06:58.480 --> 0:07:08.080
tool to spot issues or surprises or errors in the data and to unfold the complexity of

76
0:07:08.080 --> 0:07:09.680
the whole phenomenon.

77
0:07:09.680 --> 0:07:16.680
So this is, for instance, the world view, so we are able to retrace the world trade

78
0:07:16.680 --> 0:07:20.460
in a century, but as you can see there is more than one curve, so we have different

79
0:07:20.460 --> 0:07:23.320
ways we can calculate that actually from the data.

80
0:07:23.320 --> 0:07:31.560
We can, for instance, take some researchers that did re-estimations of this total trade

81
0:07:31.560 --> 0:07:36.080
by correcting sources and all this kind of stuff, so that's one way to do it.

82
0:07:36.080 --> 0:07:42.480
That's the green curve in this visualization, but we also can actually sum all the totals

83
0:07:42.480 --> 0:07:43.480
we have in our data.

84
0:07:43.480 --> 0:07:50.480
This is the yellow one, and we also have the, so here we are summing all the flows we have,

85
0:07:50.480 --> 0:07:55.520
the yellow, and the red is more, we are summing only the total that were in the archive books,

86
0:07:55.520 --> 0:07:57.160
and it's not the same thing.

87
0:07:57.160 --> 0:08:01.760
If you sum what we have or if you take the sum that's done at the time, you don't have

88
0:08:01.760 --> 0:08:03.760
the same results.

89
0:08:03.760 --> 0:08:09.080
Welcome to the nightmare of dealing with archive data.

90
0:08:09.080 --> 0:08:13.800
In this visualization, for instance, we are focusing on one country, Denmark, and then

91
0:08:13.800 --> 0:08:22.800
we can actually spot the trade on the long run of this specific country, and we can also

92
0:08:22.800 --> 0:08:31.880
visualize, so here is Germany's on the right, we can also depict actually not only the total

93
0:08:31.880 --> 0:08:38.120
trade, but also the difference trade, bilateral trade between Germany and its trade partner

94
0:08:38.120 --> 0:08:39.520
a long time.

95
0:08:39.520 --> 0:08:45.560
Okay, so this is an objective, so geopolysts here, for instance, we see like when we talk

96
0:08:45.560 --> 0:08:49.440
about Germany's overline, what are we talking about?

97
0:08:49.440 --> 0:08:56.120
And we are talking about geopolitical entity that had a different statisticism on time,

98
0:08:56.120 --> 0:09:02.200
you can see here, and then you can see on the bottom line which other parts, all other

99
0:09:02.200 --> 0:09:08.840
geopolitical entities were actually part of Germany through time.

100
0:09:08.840 --> 0:09:17.480
Because sometimes we have trade with only Saxony or Waldeck, and we want to know eventually

101
0:09:17.480 --> 0:09:21.640
if those entities are part of another one.

102
0:09:21.640 --> 0:09:26.000
So frictionless data integration.

103
0:09:26.000 --> 0:09:32.300
So we are using a data package from frictionless data here from the Open Knowledge Foundation.

104
0:09:32.300 --> 0:09:39.920
So actually there is a talk from Evgeny from frictionless team later today in the online

105
0:09:39.920 --> 0:09:46.080
part of our room, we'll talk about the new tool fostering data package, and actually

106
0:09:46.080 --> 0:09:50.620
I'm very interested into that, but I will talk about what I've done myself.

107
0:09:50.620 --> 0:09:59.180
So about this project, the main, yeah sorry, the first thing we do is versioning the data.

108
0:09:59.180 --> 0:10:05.320
So we put data as CSVs into a version control system like a Git, simply.

109
0:10:05.320 --> 0:10:10.360
Here it's on GitHub, and you can see that you can track actually just the same way we

110
0:10:10.360 --> 0:10:14.400
do with code, who changed which data, when and why.

111
0:10:14.400 --> 0:10:20.640
So here for instance is Beatrice who actually corrected a typo in the flow number, adding

112
0:10:20.640 --> 0:10:24.760
the comma at the right place, and we have the commentary here.

113
0:10:24.760 --> 0:10:30.520
This is very important to keep track of what's going on with the, because we have like hundreds

114
0:10:30.520 --> 0:10:33.360
and even thousands of files like that.

115
0:10:33.360 --> 0:10:40.000
So it's very important to have that also to know if we have issues, if that happens, where

116
0:10:40.000 --> 0:10:42.000
it comes from.

117
0:10:42.000 --> 0:10:43.000
So data package.

118
0:10:43.000 --> 0:10:49.040
Data package is a formalism where it's basically a JSON file where you will actually describe

119
0:10:49.040 --> 0:10:52.240
the data you have, adding a documentation.

120
0:10:52.240 --> 0:10:56.680
So the first interest of using data package is to actually document your data set to make

121
0:10:56.680 --> 0:11:02.040
it easier for other people to actually understand what you want it to do.

122
0:11:02.040 --> 0:11:05.840
And it's very important for publication at the end, open science.

123
0:11:05.840 --> 0:11:12.200
So here we have the title of the project, we have the license, the contributors, that's

124
0:11:12.200 --> 0:11:14.440
also very important to have.

125
0:11:14.440 --> 0:11:17.400
And then we describe resources.

126
0:11:17.400 --> 0:11:20.040
Resources can be seen as a data table.

127
0:11:20.040 --> 0:11:25.640
So here from some of the RIC entities, and for each resources, which is a CSV here, we

128
0:11:25.640 --> 0:11:27.720
describe the fields we have in the table.

129
0:11:27.720 --> 0:11:33.300
So we know that the RIC name table is a string, it's unique, and it is required.

130
0:11:33.300 --> 0:11:37.040
So it's really like a relational database schema.

131
0:11:37.040 --> 0:11:41.600
It's really kind of the same spirit, but an images and format.

132
0:11:41.600 --> 0:11:44.520
The reason why you can do that, as I said, is documenting.

133
0:11:44.520 --> 0:11:51.840
The second reason is actually to control your data, so doing right, driving data validation.

134
0:11:51.840 --> 0:11:59.400
So if you have a data package, describe like that, you can then use a Python library, frictionless,

135
0:11:59.400 --> 0:12:05.120
it's called frictionless, which will actually check all your data line if each data line

136
0:12:05.120 --> 0:12:08.360
you have respects the schema you wrote.

137
0:12:08.360 --> 0:12:14.920
And if it's not the case, it will actually provide you with a report with errors like,

138
0:12:14.920 --> 0:12:20.840
for instance, here I have a foreign key error because the modified currency here is not

139
0:12:20.840 --> 0:12:24.480
known in the table that's supposed to have this data.

140
0:12:24.480 --> 0:12:29.560
So it's a very nice way to actually, we get new data, and then we check, okay, where do

141
0:12:29.560 --> 0:12:35.440
we stand about what we want to achieve at the end, which is to respect the data package

142
0:12:35.440 --> 0:12:37.800
formalism we wrote.

143
0:12:37.800 --> 0:12:40.160
So that's very nice.

144
0:12:40.160 --> 0:12:49.200
It's very cool for data engineers, but as I said, our goal with Beatrice is that we

145
0:12:49.200 --> 0:12:55.920
work together and she, because the thing is like when she enters new data in the system,

146
0:12:55.920 --> 0:13:01.720
she has an historian to take decisions on how to interpret the data that were in the

147
0:13:01.720 --> 0:13:02.720
archive.

148
0:13:02.720 --> 0:13:04.480
I count, that's not my job.

149
0:13:04.480 --> 0:13:07.160
I thought about responsibility, I don't have the skills to do it.

150
0:13:07.160 --> 0:13:13.720
So we need something to allow her to actually correct the data, update the data that comes

151
0:13:13.720 --> 0:13:16.840
in into the data package format.

152
0:13:16.840 --> 0:13:21.400
And she can't use a common line tools in Python script and that kind of stuff.

153
0:13:21.400 --> 0:13:27.560
So we need something, we missed a tool here to let actually humanist researchers, in this

154
0:13:27.560 --> 0:13:33.600
case, but people in general, to be able to interact with the data flow with something

155
0:13:33.600 --> 0:13:36.780
else than actually a two-technical interface.

156
0:13:36.780 --> 0:13:43.840
So we built, we developed a specific web application that actually helps Beatrice to integrate

157
0:13:43.840 --> 0:13:49.180
new data by using the data package as a validation system.

158
0:13:49.180 --> 0:13:51.000
So all of this is done in JavaScript.

159
0:13:51.000 --> 0:13:55.220
You also have a JavaScript library for the data package.

160
0:13:55.220 --> 0:13:56.780
So basically, this is the steps.

161
0:13:56.780 --> 0:14:03.460
So the idea is Beatrice will upload a data spreadsheet, so a new data file, transcription

162
0:14:03.460 --> 0:14:09.840
of one new archival book she found, the tool which first checks the spreadsheet format,

163
0:14:09.840 --> 0:14:12.720
saying like, do we have all the columns we want and everything?

164
0:14:12.720 --> 0:14:20.920
If it's correct, then it will go through all the data points, checking all the errors and

165
0:14:20.920 --> 0:14:30.680
grouping them to propose a creation interface for her to correct all source issues through

166
0:14:30.680 --> 0:14:32.040
a form.

167
0:14:32.040 --> 0:14:39.640
And we tried to do, to develop something that makes this process, which is tedious and a

168
0:14:39.640 --> 0:14:46.200
long process, as easy and as fast as we could for her, Beatrice, to actually go through

169
0:14:46.200 --> 0:14:47.200
this.

170
0:14:47.200 --> 0:14:54.800
At the very end, this tool will actually commit to and push, commit it into a Git repository

171
0:14:54.800 --> 0:14:57.080
and push it into the GitHub repository.

172
0:14:57.080 --> 0:15:02.200
It follows that down into a serverless web application, which means that I didn't have

173
0:15:02.200 --> 0:15:07.340
had to introduce Git command line to Beatrice neither.

174
0:15:07.340 --> 0:15:10.240
The tool does that for her.

175
0:15:10.240 --> 0:15:11.240
So this is what it looks like.

176
0:15:11.240 --> 0:15:14.280
So it's a React web application.

177
0:15:14.280 --> 0:15:20.760
Here we have the schema validation summary where we see for the fields, so the different

178
0:15:20.760 --> 0:15:24.860
fields we have for which we have errors, the kind of the error we have.

179
0:15:24.860 --> 0:15:30.840
And at the end, we have the error overview which says how many rows that has an issue.

180
0:15:30.840 --> 0:15:36.840
For instance, here in the source column, we have two different invalid values that impact

181
0:15:36.840 --> 0:15:40.200
169 rows.

182
0:15:40.200 --> 0:15:47.580
The idea here is to correct all this group of 169 rows with only one edit.

183
0:15:47.580 --> 0:15:55.040
So once we have all sorts of errors, basically the process of workflow using this tool will

184
0:15:55.040 --> 0:15:59.240
be to go through the error groups one by one.

185
0:15:59.240 --> 0:16:06.480
The web application will actually generate a form with inputs to actually help Beatrice

186
0:16:06.480 --> 0:16:07.480
to decide.

187
0:16:07.480 --> 0:16:10.520
So in this example, we have a value for a partner.

188
0:16:10.520 --> 0:16:12.320
So partner is a trade partner.

189
0:16:12.320 --> 0:16:14.440
So it's geopolitical entity.

190
0:16:14.440 --> 0:16:15.440
Here it's in French.

191
0:16:15.440 --> 0:16:17.500
It's il de célon.

192
0:16:17.500 --> 0:16:22.280
So we use English-based vocabularies to translate the partner.

193
0:16:22.280 --> 0:16:27.520
So we need to decide what is il de célon in our vocabulary in the rest of the data.

194
0:16:27.520 --> 0:16:32.960
And this is where we have search input where actually Beatrice can actually search for

195
0:16:32.960 --> 0:16:39.040
célon which is called in our vocabulary, scélonca, célon, in parenthesis.

196
0:16:39.040 --> 0:16:45.600
And once she should chose that, actually the tool we correct this column and put the data

197
0:16:45.600 --> 0:16:54.240
at the right place to make sure we will translate il de célon to the scélonca célon.

198
0:16:54.240 --> 0:17:01.200
At the end, once she went through all the process, we have summary here explaining all

199
0:17:01.200 --> 0:17:03.560
the corrections she made.

200
0:17:03.560 --> 0:17:10.240
So in the first line, for instance, a year was misspelled.

201
0:17:10.240 --> 0:17:11.600
All that kind of thing.

202
0:17:11.600 --> 0:17:15.480
We change the source name and everything is fixed.

203
0:17:15.480 --> 0:17:21.720
So once all the errors have been corrected through the form, the data form I just showed

204
0:17:21.720 --> 0:17:32.200
you, then she can move on with the last step which is actually to publish this new data

205
0:17:32.200 --> 0:17:37.000
that is now valid because we know it's valid because we can control it with the data package

206
0:17:37.000 --> 0:17:41.040
into the GitHub repository.

207
0:17:41.040 --> 0:17:49.240
And this is how basically the RAC-to-web application will really prepare the data.

208
0:17:49.240 --> 0:17:58.120
So I could go into details into what does that mean later and make it possible for Beatrice

209
0:17:58.120 --> 0:18:06.720
to actually take the right decisions to adapt the raw data into the data package we worked

210
0:18:06.720 --> 0:18:09.440
with.

211
0:18:09.440 --> 0:18:12.880
So I have a little bit more time.

212
0:18:12.880 --> 0:18:14.400
So this is the analysis.

213
0:18:14.400 --> 0:18:24.880
Maybe I can try to demo a little bit the tool live.

214
0:18:24.880 --> 0:18:29.040
So the very important thing is it's a several or less web application.

215
0:18:29.040 --> 0:18:35.520
So here it's on my local host on my laptop, but actually it's hosted on GitHub directly.

216
0:18:35.520 --> 0:18:43.160
So what is the media lab, actually a lot of this work has been done at the media lab,

217
0:18:43.160 --> 0:18:45.520
my previous employer.

218
0:18:45.520 --> 0:18:51.040
So congrats to them too because they contributed to that work too a lot.

219
0:18:51.040 --> 0:18:59.280
So this is the tool.

220
0:18:59.280 --> 0:19:02.640
It's hosted on GitHub.io because it doesn't need any server.

221
0:19:02.640 --> 0:19:07.720
All the logging process with GitHub is done through a personal token, which is a very

222
0:19:07.720 --> 0:19:12.800
specific long key that you have to produce in your GitHub account for once.

223
0:19:12.800 --> 0:19:16.600
Then you use that as a login mechanism.

224
0:19:16.600 --> 0:19:19.680
So this is what it looks like.

225
0:19:19.680 --> 0:19:27.960
Once I am logged in, I can fetch the data from GitHub to make sure I have the last version

226
0:19:27.960 --> 0:19:31.200
of the data before adding new things.

227
0:19:31.200 --> 0:19:37.480
In here I can prepare the little file here, which normally should have some errors.

228
0:19:37.480 --> 0:19:42.280
So the first thing here you see like this green message here on the bottom says that

229
0:19:42.280 --> 0:19:47.600
actually this CSV file is valid structure wise.

230
0:19:47.600 --> 0:19:53.720
And the errors of the columns are good, which is a good first step.

231
0:19:53.720 --> 0:20:00.200
And then this is the all the errors I have in my file.

232
0:20:00.200 --> 0:20:04.520
This is a nice step because you want to review what kind of mess you are going to on top

233
0:20:04.520 --> 0:20:06.920
of where you are before starting the process.

234
0:20:06.920 --> 0:20:11.320
Because if you have too many, maybe you want to do that later or asking for help.

235
0:20:11.320 --> 0:20:15.280
So once you've seen that, you can start.

236
0:20:15.280 --> 0:20:19.640
So this is basically all the things we have to do.

237
0:20:19.640 --> 0:20:21.280
So this is the first one.

238
0:20:21.280 --> 0:20:27.840
I can move to the next error or go back, even though I haven't corrected it yet.

239
0:20:27.840 --> 0:20:36.000
And here I say like, okay, so the value is parallel to, commercial A, I don't know, whatever.

240
0:20:36.000 --> 0:20:39.600
This character is not actually a unit because the unit should be an integer.

241
0:20:39.600 --> 0:20:42.320
Yeah, it's true.

242
0:20:42.320 --> 0:20:44.520
So it's better with a, oops, sorry.

243
0:20:44.520 --> 0:20:46.360
So better with a one.

244
0:20:46.360 --> 0:20:48.800
And I can confirm this fix.

245
0:20:48.800 --> 0:20:50.080
And now we're good.

246
0:20:50.080 --> 0:20:51.080
Unit is one.

247
0:20:51.080 --> 0:20:52.680
Now I move to the next one.

248
0:20:52.680 --> 0:20:55.160
You see here I am in two on nine.

249
0:20:55.160 --> 0:20:58.840
So all the processors are trying to make that as smooth as we can.

250
0:20:58.840 --> 0:21:04.880
So as soon as I fix it, so here I have, it's written in French, it's Milne-Lafsant-Trent-Witz.

251
0:21:04.880 --> 0:21:07.880
But actually we want that to be an integer again.

252
0:21:07.880 --> 0:21:10.880
I don't want the later version of the year.

253
0:21:10.880 --> 0:21:14.840
So Milne-Lafsant-Trent-Witz has a number.

254
0:21:14.840 --> 0:21:18.080
As soon as I confirm the fix, I move to the next page.

255
0:21:18.080 --> 0:21:22.680
So that's, we can try to make that process as seamless as possible.

256
0:21:22.680 --> 0:21:25.320
So here I have a source, so this is a foreign key.

257
0:21:25.320 --> 0:21:31.200
So in the source, in the data table, the source is actually, it's a key that is referring,

258
0:21:31.200 --> 0:21:33.720
which is referring to the source table.

259
0:21:33.720 --> 0:21:36.560
And say like, so here basically foreign key source violation.

260
0:21:36.560 --> 0:21:40.100
So it means that this sort doesn't exist in our system.

261
0:21:40.100 --> 0:21:42.560
So here I have two choice.

262
0:21:42.560 --> 0:21:47.720
Or I was supposed to, okay, normally I should, oh yeah, sorry.

263
0:21:47.720 --> 0:21:49.560
Can you dad?

264
0:21:49.560 --> 0:21:51.220
No.

265
0:21:51.220 --> 0:21:54.560
So normally, okay, so whatever.

266
0:21:54.560 --> 0:22:01.880
So I can search for it and find it, or, and in which case it will, the edit will be only

267
0:22:01.880 --> 0:22:06.180
replacing the key, or I can create a new item.

268
0:22:06.180 --> 0:22:09.360
And here you can see that, here I'm creating a new source because sometimes the source

269
0:22:09.360 --> 0:22:10.360
doesn't exist yet.

270
0:22:10.360 --> 0:22:14.280
So I have to go through all the, you see this form is much, much longer because here I'm

271
0:22:14.280 --> 0:22:18.320
creating a new line into the source table.

272
0:22:18.320 --> 0:22:23.120
I will not do that because it's too long, I will just give me something please.

273
0:22:23.120 --> 0:22:25.440
And that will make it, okay.

274
0:22:25.440 --> 0:22:26.440
And so on and so forth.

275
0:22:26.440 --> 0:22:33.440
Again, we have an issue with the, sorry, this example is a little bit up.

276
0:22:33.440 --> 0:22:35.960
Okay, here it's a Trini-Dad and Tobago.

277
0:22:35.960 --> 0:22:43.400
It's a duopatical line time, I don't know because it's a Trini-Dad and Tobago, not a.

278
0:22:43.400 --> 0:22:44.400
Okay.

279
0:22:44.400 --> 0:22:45.400
Up.

280
0:22:45.400 --> 0:22:49.520
And we are good.

281
0:22:49.520 --> 0:22:54.720
Australia with a lot of E at the end is not correct.

282
0:22:54.720 --> 0:22:55.720
It's Australia.

283
0:22:55.720 --> 0:22:56.720
Yep.

284
0:22:56.720 --> 0:23:00.720
Sorry, it's very long.

285
0:23:00.720 --> 0:23:03.720
Yeah, whatever.

286
0:23:03.720 --> 0:23:05.720
Dollar.

287
0:23:05.720 --> 0:23:12.600
Let's say it's this, it's this crap.

288
0:23:12.600 --> 0:23:14.600
Don't do that, right?

289
0:23:14.600 --> 0:23:15.600
Okay.

290
0:23:15.600 --> 0:23:16.600
Ah.

291
0:23:16.600 --> 0:23:28.800
Okay, so you see, that's important point is like, so we are based on the data package,

292
0:23:28.800 --> 0:23:29.960
in the data package.

293
0:23:29.960 --> 0:23:36.000
So we are using foreign keys, the specification and so on, but actually we had to add specific

294
0:23:36.000 --> 0:23:37.800
forms for our case.

295
0:23:37.800 --> 0:23:39.600
So the application here is not generic.

296
0:23:39.600 --> 0:23:43.680
You can't just put a new data package of yourself with your data and it will not work because

297
0:23:43.680 --> 0:23:51.600
we had actually for UX and UI reasons to make specific cases like that where the forms are

298
0:23:51.600 --> 0:23:55.200
not exactly as the data package described it.

299
0:23:55.200 --> 0:24:00.320
It was too complex to make it very generic, but actually with more work that could be

300
0:24:00.320 --> 0:24:02.160
achieved maybe at some point.

301
0:24:02.160 --> 0:24:05.480
And actually the talk from the evergreen is this afternoon, we'll talk a little bit about

302
0:24:05.480 --> 0:24:07.840
that kind of stuff.

303
0:24:07.840 --> 0:24:10.560
So here we are.

304
0:24:10.560 --> 0:24:14.600
I'll stop the demo here.

305
0:24:14.600 --> 0:24:19.560
Just to finish, why do we do all kinds of stuffs?

306
0:24:19.560 --> 0:24:21.960
Because we want to analyze trade in the long run.

307
0:24:21.960 --> 0:24:23.880
We have lots of trade values as you can see.

308
0:24:23.880 --> 0:24:27.040
A lot of trading entities, very too much.

309
0:24:27.040 --> 0:24:32.760
And then at the end we try to, this is a visual documentation where we depict the kind of

310
0:24:32.760 --> 0:24:37.480
different source we use in the database.

311
0:24:37.480 --> 0:24:45.920
And at the end we try to do something like that where actually here we have the trade

312
0:24:45.920 --> 0:24:50.040
of the world in 1890.

313
0:24:50.040 --> 0:24:56.080
So each node here, circles, are geopolitical entities.

314
0:24:56.080 --> 0:24:59.040
And the links between them are the trade of that year.

315
0:24:59.040 --> 0:25:01.240
So it's total trade.

316
0:25:01.240 --> 0:25:02.960
We could choose import or export here.

317
0:25:02.960 --> 0:25:05.320
I just summed it up.

318
0:25:05.320 --> 0:25:10.720
The important part here is the color here is based on the type of geopolitical entity

319
0:25:10.720 --> 0:25:11.720
we have.

320
0:25:11.720 --> 0:25:19.240
So in this orange or kind of yellow thing, it's what we call the GPH entity.

321
0:25:19.240 --> 0:25:25.080
It's entities that geopolitical entity we know, mainly countries but not always.

322
0:25:25.080 --> 0:25:27.480
In green, so are colonial areas.

323
0:25:27.480 --> 0:25:28.480
So it's not a colony.

324
0:25:28.480 --> 0:25:30.560
It's not a country which is a colony.

325
0:25:30.560 --> 0:25:34.000
It's like French Africa.

326
0:25:34.000 --> 0:25:36.920
It's like, we know it's a colony.

327
0:25:36.920 --> 0:25:39.120
We don't know which one exactly.

328
0:25:39.120 --> 0:25:42.880
Like here, for instance, we have European Russia, which is a city part of.

329
0:25:42.880 --> 0:25:46.000
It's from Russia, but it's the European part of it.

330
0:25:46.000 --> 0:25:48.160
And this is what we find in the archive books.

331
0:25:48.160 --> 0:25:51.000
So we can't really decide what that means exactly.

332
0:25:51.000 --> 0:25:57.600
So we try to analyze this kind of, so we have this, there is this gap between very heterogeneous

333
0:25:57.600 --> 0:26:03.960
data, very difficult to interpret, but still try to do quantification and analyze this

334
0:26:03.960 --> 0:26:11.680
like this networks on top of this very complex and rich dataset.

335
0:26:11.680 --> 0:26:16.520
I think I'm good with what I wanted to share with you today.

336
0:26:16.520 --> 0:26:23.200
We can move to question if you have.

337
0:26:23.200 --> 0:26:28.200
Yes, please.

338
0:26:28.200 --> 0:26:33.200
There was a slide.

339
0:26:33.200 --> 0:26:39.200
The slide was had a title, which was development of a specific web application to integrate

340
0:26:39.200 --> 0:26:40.200
new data.

341
0:26:40.200 --> 0:26:41.200
Yes.

342
0:26:41.200 --> 0:26:48.720
You might not have time to tell me, but please tell me if you have time at some point, what

343
0:26:48.720 --> 0:26:51.680
was the conversation with your historian?

344
0:26:51.680 --> 0:26:53.680
How did this happen?

345
0:26:53.680 --> 0:26:56.520
What did you actually do to plan?

346
0:26:56.520 --> 0:27:04.000
So the question is like how Beatrice and I ended up, decided to do that.

347
0:27:04.000 --> 0:27:08.040
So basically the whole point is very like a collaboration because we worked without that

348
0:27:08.040 --> 0:27:09.640
for a very long time.

349
0:27:09.640 --> 0:27:13.760
And the process was we had to meet in the same room.

350
0:27:13.760 --> 0:27:18.680
I was doing the script, checking the data, editing the data, because editing the data

351
0:27:18.680 --> 0:27:23.960
in a spreadsheet data wise that doesn't mess up your numbers and everything is not easy

352
0:27:23.960 --> 0:27:24.960
actually.

353
0:27:24.960 --> 0:27:27.920
And we were working together on that.

354
0:27:27.920 --> 0:27:31.760
It was necessary and actually very nice to do because we had to exchange.

355
0:27:31.760 --> 0:27:36.480
So she was explaining to me why she was taking this decision or not.

356
0:27:36.480 --> 0:27:40.040
And she was taking this and I was just putting data.

357
0:27:40.040 --> 0:27:46.380
But at some point we ended up with the fact that we had so much more to add that this

358
0:27:46.380 --> 0:27:48.840
process couldn't scale basically.

359
0:27:48.840 --> 0:27:54.600
So we had to find something else to make sure she could do this process on her own.

360
0:27:54.600 --> 0:28:00.760
And I would intervene once the data is in the GitHub repository, checking myself with

361
0:28:00.760 --> 0:28:06.440
quantification and scripts and everything again because you always need to check everything

362
0:28:06.440 --> 0:28:08.660
many times.

363
0:28:08.660 --> 0:28:14.120
And then it makes the whole process much more smoother.

364
0:28:14.120 --> 0:28:16.840
Yes?

365
0:28:16.840 --> 0:28:46.200
Sorry, I don't get it.

366
0:28:46.200 --> 0:28:54.080
So, the question is like, would it be beneficial or possible to actually commit the data before

367
0:28:54.080 --> 0:28:57.200
checking it and put it in GitHub?

368
0:28:57.200 --> 0:28:59.680
So yes and no.

369
0:28:59.680 --> 0:29:04.280
The reason why we don't do that is because, the first one, because I need Bethz to take

370
0:29:04.280 --> 0:29:11.520
the decision of documenting the raw data to make it compatible with all the nice visualization

371
0:29:11.520 --> 0:29:13.080
I showed you.

372
0:29:13.080 --> 0:29:14.360
And she needs to take the decision.

373
0:29:14.360 --> 0:29:16.480
She needs to do it.

374
0:29:16.480 --> 0:29:23.960
So that's why we put this data into the GitHub after she has done this work of data creation.

375
0:29:23.960 --> 0:29:31.240
We could actually host the data as a raw file first and then do that later on that kind

376
0:29:31.240 --> 0:29:32.240
of stuff.

377
0:29:32.240 --> 0:29:40.360
Still, we still need a web interface that lets Bertris, the historian, take the decision.

378
0:29:40.360 --> 0:29:44.880
So no.

379
0:29:44.880 --> 0:29:45.880
Any more?

380
0:29:45.880 --> 0:29:46.880
Yeah?

381
0:29:46.880 --> 0:29:47.880
Just a few things.

382
0:29:47.880 --> 0:29:54.200
I saw on the last slide that the board gave me the idea of organizing.

383
0:29:54.200 --> 0:29:55.200
Yes.

384
0:29:55.200 --> 0:30:00.000
So, this tool I'm using here is actually brand new.

385
0:30:00.000 --> 0:30:01.880
It's a Giphy but on the web.

386
0:30:01.880 --> 0:30:05.080
We are working on this with my company, Westware.

387
0:30:05.080 --> 0:30:07.720
And we are very close to release it.

388
0:30:07.720 --> 0:30:14.080
It's basically the same thing as the Giphy but lighter version and the web version.

389
0:30:14.080 --> 0:30:19.720
It's already there, but you shouldn't go because it's not live yet.

390
0:30:19.720 --> 0:30:21.720
Thank you.

391
0:30:21.720 --> 0:30:38.080
Thank you.

