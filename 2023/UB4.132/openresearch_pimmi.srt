1
0:00:00.000 --> 0:00:01.000
Hi everyone.

2
0:00:01.000 --> 0:00:14.440
Well, I'm very impressed to have such a large audience for such a small tool.

3
0:00:14.440 --> 0:00:16.200
But, well, I'm Beatrice.

4
0:00:16.200 --> 0:00:19.680
I work at the French Media Lab.

5
0:00:19.680 --> 0:00:27.120
And today I'm going to present PIMI, which is a tool to study image propagation.

6
0:00:27.120 --> 0:00:34.800
The Media Lab is a lab where social scientists try to, among other things, study the traces

7
0:00:34.800 --> 0:00:39.600
that people leave online.

8
0:00:39.600 --> 0:00:45.240
And from now they are quite well equipped with tools to study text.

9
0:00:45.240 --> 0:00:54.360
But when they ask me, okay, how can I study image propagation?

10
0:00:54.360 --> 0:00:59.960
And I'm still struggling to give them answers.

11
0:00:59.960 --> 0:01:02.540
So what does it mean to study mean propagation?

12
0:01:02.540 --> 0:01:08.720
It means, okay, being able to recognize that some parts of an image are copied or partially

13
0:01:08.720 --> 0:01:12.600
copied.

14
0:01:12.600 --> 0:01:17.200
So what this tool does, it's very simple.

15
0:01:17.200 --> 0:01:26.240
It's able to create clusters of images and group together images that are total or partial

16
0:01:26.240 --> 0:01:28.640
copies of each other.

17
0:01:28.640 --> 0:01:34.360
It's able to deal with image transformation, so if the image is cropped or zoomed.

18
0:01:34.360 --> 0:01:39.000
And it's able to adapt to copper's characteristics.

19
0:01:39.000 --> 0:01:45.600
So it will try to make the best of your data sets, depending on the number of images you

20
0:01:45.600 --> 0:01:49.920
have or the type of images you have.

21
0:01:49.920 --> 0:01:57.240
What PIMI is not able to do is to cluster semantically similar images.

22
0:01:57.240 --> 0:02:03.120
So it's not the tool that you are going to use if you want to create clusters of cats

23
0:02:03.120 --> 0:02:13.560
and clusters of dogs or, I don't know, find images of violence versus images of peace.

24
0:02:13.560 --> 0:02:16.120
And it's not able to do some face recognition.

25
0:02:16.120 --> 0:02:26.440
So again, you will not be able to make some clusters of pictures of Elizabeth II versus

26
0:02:26.440 --> 0:02:36.280
clusters of images of Emmanuel Macron.

27
0:02:36.280 --> 0:02:43.280
What you could imagine doing, and we could imagine also work together if you are a researcher

28
0:02:43.280 --> 0:02:50.560
working on those subjects, is to study the propagation of MIMA on social networks, as

29
0:02:50.560 --> 0:02:52.480
I was saying.

30
0:02:52.480 --> 0:02:59.200
But also you could study the usage of press agency photos in a press corpus or stock photos

31
0:02:59.200 --> 0:03:01.880
as well.

32
0:03:01.880 --> 0:03:08.160
You could also study the dissemination of fake news based on image montage.

33
0:03:08.160 --> 0:03:15.160
Or you could study the editorial choices between different media, depending on whether they

34
0:03:15.160 --> 0:03:18.920
use the same images or not.

35
0:03:18.920 --> 0:03:26.920
So let me do a quick demo of how it looks for now.

36
0:03:26.920 --> 0:03:30.640
It's not on the screen.

37
0:03:30.640 --> 0:03:46.520
OK, let's forget about that.

38
0:03:46.520 --> 0:04:15.840
OK, I'm very sorry.

39
0:04:15.840 --> 0:04:26.000
I'll try to make it work.

40
0:04:26.000 --> 0:04:33.640
OK, well, still not showing totally all clusters.

41
0:04:33.640 --> 0:04:36.920
So we create clusters of images.

42
0:04:36.920 --> 0:04:41.240
So this is a data set that is created by the French INRIA.

43
0:04:41.240 --> 0:04:47.480
And that is presenting some degradation on images.

44
0:04:47.480 --> 0:04:54.400
So they take an original picture and they apply some filters or they crop the images

45
0:04:54.400 --> 0:04:57.080
to see if they are able to put the images together.

46
0:04:57.080 --> 0:05:03.240
So we can see that we have pretty correct results on that data set.

47
0:05:03.240 --> 0:05:10.880
And this is our results on some images that we collected ourselves on Twitter using Ellen

48
0:05:10.880 --> 0:05:14.240
Musk as a query.

49
0:05:14.240 --> 0:05:17.760
And so we try to cluster those images.

50
0:05:17.760 --> 0:05:22.560
So as you can see, we have images of Ellen Musk.

51
0:05:22.560 --> 0:05:30.200
We are able to group together some images that are crops of others.

52
0:05:30.200 --> 0:05:37.520
So this is probably the source image of the montage that has been done here.

53
0:05:37.520 --> 0:05:42.760
But we can also see that we have some problems with the tool.

54
0:05:42.760 --> 0:05:51.200
For example, here we have a cluster with two images that have been assembled together and

55
0:05:51.200 --> 0:05:55.480
we create a cluster of actually two images.

56
0:05:55.480 --> 0:06:02.680
But well, that's the state of the tool for now.

57
0:06:02.680 --> 0:06:15.880
And now I'll try to come back to my slides.

58
0:06:15.880 --> 0:06:25.960
OK.

59
0:06:25.960 --> 0:06:29.400
So how does it work?

60
0:06:29.400 --> 0:06:35.600
For people who work in computer vision, I'm going for a bit to say some things that are

61
0:06:35.600 --> 0:06:42.880
quite basic, but I try to make it clear for people who do not do computer vision.

62
0:06:42.880 --> 0:06:45.640
So it is not based on colors at all.

63
0:06:45.640 --> 0:06:49.240
It's used like the grayscale of images.

64
0:06:49.240 --> 0:06:54.320
And it tries to detect points of interest on a picture.

65
0:06:54.320 --> 0:07:02.760
And then it uses these local key points as vectors.

66
0:07:02.760 --> 0:07:14.280
And then those vectors are indexed in a database that is able to perform some very quick similarity

67
0:07:14.280 --> 0:07:24.240
approach.

68
0:07:44.280 --> 0:07:46.340
you

69
0:08:14.280 --> 0:08:16.340
you

70
0:08:44.280 --> 0:08:46.340
you

71
0:09:14.280 --> 0:09:16.340
you

72
0:09:44.280 --> 0:09:46.340
you

73
0:10:14.280 --> 0:10:16.340
you

74
0:10:44.280 --> 0:10:46.340
you

75
0:11:14.280 --> 0:11:16.340
you

76
0:11:44.280 --> 0:11:46.340
you

77
0:12:14.280 --> 0:12:16.340
you

78
0:12:44.280 --> 0:12:46.340
you

79
0:13:14.280 --> 0:13:16.340
you

80
0:13:27.440 --> 0:13:36.280
of the tool as I say there is that problem of parts of images that create

81
0:13:36.280 --> 0:13:41.960
clusters that are bigger than they should be so our plan is to be able to

82
0:13:41.960 --> 0:13:47.720
detect images that are actually those links between two clusters so to be able

83
0:13:47.720 --> 0:13:53.960
that's to detect that this image is actually containing two images and to be

84
0:13:53.960 --> 0:14:01.880
able to to deal with part of images and also what we would like to do is to show

85
0:14:01.880 --> 0:14:08.480
images in their context to be able to show the tweets that that contains those

86
0:14:08.480 --> 0:14:15.400
images or Instagram posts etc or at least to show additional metadata for

87
0:14:15.400 --> 0:14:24.000
the users and also we would like to show you the graph of image similarities so

88
0:14:24.000 --> 0:14:29.560
that the clusters that are resulting from that graph are not like not

89
0:14:29.560 --> 0:14:40.360
interpretable and to improve our tool we need your use cases because for now like

90
0:14:40.360 --> 0:14:52.800
we have those two three databases but but we would be very glad to like do some

91
0:14:52.800 --> 0:14:58.320
partnerships with other researchers to improve the tool thank you very much for

92
0:14:58.320 --> 0:15:00.320
your attention

93
0:15:00.880 --> 0:15:06.520
he she wanted to look at the slides we have the references to all the images

94
0:15:06.520 --> 0:15:16.600
used and to the papers of the algorithms used by teeny I'm open for questions

95
0:15:16.600 --> 0:15:28.040
we had a bit of trouble with the sound stream but it's back online so yeah you

96
0:15:28.040 --> 0:15:33.000
should repeat the question okay I'll do

97
0:15:33.000 --> 0:15:45.960
yes

98
0:16:03.320 --> 0:16:12.720
well trying to find similarities in oh sorry I have to repeat the question so

99
0:16:12.720 --> 0:16:21.640
the question was if I understand well well how to reproduce that that use case

100
0:16:21.640 --> 0:16:27.840
not on images but on other types of documents that would be I guess some

101
0:16:27.840 --> 0:16:40.640
features 3d counterparts and I'd say well as long as you can you like represent

102
0:16:40.640 --> 0:16:47.920
your data in the shape of vectors then you ready to use face to do like do some

103
0:16:47.920 --> 0:16:54.560
some search from for nearest neighbor in your database and then you can go for

104
0:16:54.560 --> 0:17:00.760
the whole pipeline create some graphs find communities in graph and go for it

105
0:17:00.760 --> 0:17:08.960
but I'm not sure PME is your tool but but well the the architecture of PME

106
0:17:08.960 --> 0:17:13.840
could be of course a model

107
0:17:14.840 --> 0:17:20.760
is there any project current or you're going that media labs has used them for

108
0:17:20.760 --> 0:17:29.040
or is it still largely in development it is largely in the development sorry I

109
0:17:29.040 --> 0:17:36.520
repeat the question so are there some projects at the media lab that are

110
0:17:36.520 --> 0:17:45.440
currently using PME and the response is no yes

111
0:17:45.440 --> 0:17:50.280
sorry can you

112
0:17:50.280 --> 0:18:07.520
well yes have you considered other ways of presenting picture similarity or

113
0:18:07.520 --> 0:18:13.440
using picture similarity all the types of image similarity if you understand

114
0:18:13.440 --> 0:18:26.840
well well I'd say that that that was what I was saying in my second slide

115
0:18:26.840 --> 0:18:31.600
there are other types of image similarity for example semantic or

116
0:18:31.600 --> 0:18:40.320
similarity and well maybe in a few months if we have like a robust

117
0:18:40.320 --> 0:18:51.240
architecture we could maybe include some other types of vectorization of images

118
0:18:51.240 --> 0:19:00.760
but for now well they are already tools that do that like there is something

119
0:19:00.760 --> 0:19:10.280
called clip server that that helps you like find similar images from clip

120
0:19:10.280 --> 0:19:16.280
vectors that are like semantical vectors so you you you could use that tool it's

121
0:19:16.280 --> 0:19:33.040
great yes yes

122
0:19:46.280 --> 0:19:53.640
so the question is is the tool really able to distinguish the thing that is of

123
0:19:53.640 --> 0:20:01.320
interest to us the fact that we are talking about a dog so the tool is only

124
0:20:01.320 --> 0:20:08.160
able to find partial copies in an image so the tool would probably be able to

125
0:20:08.160 --> 0:20:17.600
say that all those images contain the same parts of face of a dog so it would

126
0:20:17.600 --> 0:20:22.520
probably be able to group all those images together the problem is that if

127
0:20:22.520 --> 0:20:29.080
there are other images in the database that contain the rest of the images then

128
0:20:29.080 --> 0:20:36.080
they would probably also be grouped in the same cluster so that's why what we

129
0:20:36.080 --> 0:20:43.120
are currently doing about parts of images who let us improve the cluster

130
0:20:43.120 --> 0:20:53.080
so that it's purified from the rest of the images and we could have a cluster

131
0:20:53.080 --> 0:21:02.120
of the face of that specific dog and then a cluster of that taco in the second

132
0:21:02.120 --> 0:21:19.080
cluster yes well for now we have the best result with excuse me what kind of

133
0:21:19.080 --> 0:21:24.480
cluster ization do you use on the graph for now we have our best results using

134
0:21:24.480 --> 0:21:32.880
pure connected components so actually the specification we do on the graph to

135
0:21:32.880 --> 0:21:38.520
reduce the number of links between images is enough to have separated

136
0:21:38.520 --> 0:21:43.200
connected components in the graph and so we take each connected component and

137
0:21:43.200 --> 0:21:53.200
it's our cluster what we would like to do is to try to like mix with some

138
0:21:53.200 --> 0:21:59.600
wrong from detection but actually for now it's not the thing that works best

139
0:22:00.960 --> 0:22:03.960
yes

140
0:22:03.960 --> 0:22:30.600
I'm not sure I understand the question can you try to rephrase it okay what

141
0:22:30.600 --> 0:22:45.200
what things are you looking at to improve the model well there are many

142
0:22:45.200 --> 0:22:54.760
things where we we are looking at for now mainly we we look at techniques to

143
0:22:54.760 --> 0:23:07.800
to do a better graph specification in order to find more coherent clusters we

144
0:23:07.800 --> 0:23:14.560
are not so much working on the on the like local descriptors parts of the

145
0:23:14.560 --> 0:23:41.800
tool for now yes have you considered and used the direct

146
0:23:41.800 --> 0:23:50.240
clique to to the Twitter images or social media images online did I repeat

147
0:23:50.240 --> 0:23:59.520
everything well yes we would like people to be able to to see images in their

148
0:23:59.520 --> 0:24:04.320
context because actually they they won't understand what's happening if they just

149
0:24:04.320 --> 0:24:11.400
have images they need to see okay why was this image published who answered

150
0:24:11.400 --> 0:24:20.880
etc so this would probably mean that we need to to add at least the links to to

151
0:24:20.880 --> 0:24:43.880
the pulse and or maybe some some kind of visualization of it

152
0:24:50.880 --> 0:24:52.920
you

