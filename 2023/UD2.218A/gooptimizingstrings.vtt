WEBVTT

00:00.000 --> 00:13.240
Okay, our next speaker is going to talk about something we all used in Go, which is strings.

00:13.240 --> 00:16.680
If you didn't have a user in Go, what are you doing here?

00:16.680 --> 00:19.640
So let's give a round of applause for Matej.

00:19.640 --> 00:22.520
Thank you, everyone.

00:22.520 --> 00:23.680
Thank you.

00:23.680 --> 00:29.800
Excited to be here, excited to see so many faces, excited to speak first time at the

00:29.800 --> 00:35.720
FOSDEM, also a bit intimidating, but hopefully I can show you a thing or two about string

00:35.720 --> 00:38.920
optimization in Go.

00:38.920 --> 00:41.000
About me, my name is Matej Gera.

00:41.000 --> 00:45.000
I work as a software engineer at a company called Corelogix, where we're building an

00:45.000 --> 00:46.000
observability platform.

00:46.000 --> 00:51.640
Apart from that, I am active in different open source communities, mostly within the

00:51.640 --> 00:58.140
cloud-native computing foundation, specifically in the observability area.

00:58.140 --> 01:00.280
I work a lot with metrics.

01:00.280 --> 01:06.480
I'm a maintainer of the TANOS project, which I will also talk a bit about during my presentation.

01:06.480 --> 01:11.980
And apart from that, I contribute to a couple different projects, most interestingly open

01:11.980 --> 01:12.980
telemetry.

01:12.980 --> 01:15.640
And, yeah, these are my handles.

01:15.640 --> 01:18.640
I'm not that active on social media.

01:18.640 --> 01:23.080
Best is to reach me on the GitHub issues directly or PRs.

01:23.080 --> 01:25.080
And let's get into it.

01:25.080 --> 01:32.000
So if anything else, I'd like you to take at least three things today from this presentation.

01:32.000 --> 01:37.160
So first of all, I'd like you to understand how strings work behind the scenes in Go.

01:37.160 --> 01:42.040
This might be old news for many people who are more experienced with Go or might be a

01:42.040 --> 01:48.920
new knowledge for newbies, but I want to set kind of a common ground from which we then

01:48.920 --> 01:50.520
can talk about the optimization.

01:50.520 --> 01:55.800
Secondly, I want to tell you about the use cases in context of which I have been thinking

01:55.800 --> 02:00.080
about string optimization and where I think the presented strategies can be useful.

02:00.080 --> 02:05.660
And lastly, I want to tell you about the actual optimization strategies and show some examples

02:05.660 --> 02:09.880
of how they can be applied or where they have been applied.

02:09.880 --> 02:15.880
I won't be talking today that much about stack versus heap, although a lot of this has to

02:15.880 --> 02:18.240
do with memory.

02:18.240 --> 02:24.040
For the presentation, I assume we'll be talking more about the heap and kind of the long-term

02:24.040 --> 02:27.640
storage of strings in memory.

02:27.640 --> 02:32.600
Also I'll be going into encoding or related types like runes and chars, although it's

02:32.600 --> 02:38.020
all kind of related, but it's outside of the scope for today.

02:38.020 --> 02:41.940
So let me first tell you what kind of brought me to this topic, what was the inspiration

02:41.940 --> 02:42.940
behind this talk.

02:42.940 --> 02:48.040
As I already said, I worked primarily in the observability landscape with metrics and over

02:48.040 --> 02:53.520
the past almost two years, I was working a lot on the TANOS project, which I mentioned

02:53.520 --> 02:58.080
and which you can for simple city here imagine as a distributed database for storing time

02:58.080 --> 02:59.640
series.

02:59.640 --> 03:06.320
And with this goes, it's intended to store millions of time series, even up to or more

03:06.320 --> 03:07.320
than billion series.

03:07.320 --> 03:11.440
We have heard also about deployments like that.

03:11.440 --> 03:16.640
And as I was working with TANOS and learning about these various aspects and components,

03:16.640 --> 03:20.480
one particular issue that has been standing out to me was the amount of memory needed

03:20.480 --> 03:24.600
for certain TANOS components to operate.

03:24.600 --> 03:31.120
And this is due, this is partly due to the fact that the time series data is stored in

03:31.120 --> 03:33.920
memory in a time series database.

03:33.920 --> 03:38.640
And this is where I decided to focus my attention, where I started to explore like how we can,

03:38.640 --> 03:44.280
what are some possible avenues where we could optimize the performance here.

03:44.280 --> 03:47.880
Digital here was played by doing this in a data driven way.

03:47.880 --> 03:54.920
So I started looking at different data points from TANOS I got like metrics, profiles, benchmarks,

03:54.920 --> 03:59.840
and this is a small side note because I considered data driven performance optimization to be

03:59.840 --> 04:04.280
what most importance when you're improving the efficiency of your program.

04:04.280 --> 04:08.120
And so I don't want to diverge here, but I highly recommend for you to check out a talk

04:08.120 --> 04:12.160
by Bartik Plotka, who's I think is in the room here.

04:12.160 --> 04:17.600
So he's talking couple of thoughts after me, who is kind of dedicating a lot of his time

04:17.600 --> 04:21.720
into this data driven approach to efficiency in Go ecosystem.

04:21.720 --> 04:25.960
I don't have it on the slide, but also the presentation that's after me that has to do

04:25.960 --> 04:28.680
with squeezing Go functions, it seemed interesting.

04:28.680 --> 04:34.560
So a lot of optimization talks today, which I love to see.

04:34.560 --> 04:39.560
And you might also ask why string specifically, what makes them so interesting or so optimization

04:39.560 --> 04:41.800
worthy?

04:41.800 --> 04:48.120
And although I've been looking at TANOS for some time, something clicked after I've seen

04:48.120 --> 04:50.520
this particular image at the different presentations.

04:50.520 --> 04:56.440
So this was presentation from Brian Borum, and it should be also somewhere around Fosdem,

04:56.440 --> 05:02.320
who is working on a kind of a neighboring project called Prometheus, which is a time

05:02.320 --> 05:05.200
series database on which TANOS is built.

05:05.200 --> 05:11.640
So if TANOS is kind of a distributed version of the Prometheus, we reuse a lot of the code

05:11.640 --> 05:16.480
from Prometheus and also the actual time series database code.

05:16.480 --> 05:21.200
So he showed, based on the profile and on the icicle graph that you see here, that the

05:21.200 --> 05:25.400
labels take most of the memory in Prometheus, and that was around one third.

05:25.400 --> 05:30.160
And when I thought about it, the result was rather surprising to me, because the labels

05:30.160 --> 05:36.680
of the time series, we could think of them as some kind of metadata or some kind of contextual

05:36.680 --> 05:41.180
data about the actual data points, about the samples, as we call them.

05:41.180 --> 05:45.480
And these were taking up more spaces than those actual data points, those actual samples

05:45.480 --> 05:46.680
themselves.

05:46.680 --> 05:51.720
So there's been a lot of thought and work put into optimization and compression of the

05:51.720 --> 05:56.400
samples, of the actual time series data, but Brian's finding indicated that there can be

05:56.400 --> 05:59.160
more, can be squeezed out of labels.

05:59.160 --> 06:01.200
And what are actually labels?

06:01.200 --> 06:06.860
Labels are key value pairs attached to a given time series to kind of characterize it.

06:06.860 --> 06:10.880
So in principle, they are nothing more than pairs of strings.

06:10.880 --> 06:13.960
So this is what brought me in the end to the strings.

06:13.960 --> 06:17.720
And it inspired me to talk about this topic to a larger audience.

06:17.720 --> 06:23.240
I thought it might be useful to look at this from kind of a more general perspective, even

06:23.240 --> 06:28.520
though we're dealing with this problem in a limited space of observability.

06:28.520 --> 06:33.840
I think it can be also, some learnings from this can be gained and used also in different,

06:33.840 --> 06:37.420
in other types of programs.

06:37.420 --> 06:42.000
So first let's lay foundations to our talk by taking a look at what string actually is

06:42.000 --> 06:43.000
in Go.

06:43.000 --> 06:46.360
So most of you probably are familiar with different properties of strings.

06:46.360 --> 06:47.360
They are immutable.

06:47.360 --> 06:52.600
They can be converted easily into slices of bytes, can be concatenated, sliced, et cetera,

06:52.600 --> 06:53.600
et cetera.

06:53.600 --> 06:57.240
However, talking about the qualities of strings does not answer the question what strings

06:57.240 --> 06:58.520
really are.

06:58.520 --> 07:02.720
And if you look at the source code of Go, you'll see that the strings are actually represented

07:02.720 --> 07:05.240
by the string struct struct.

07:05.240 --> 07:07.680
So strings are structs.

07:07.680 --> 07:08.840
Shocking, right?

07:08.840 --> 07:13.120
You can also get the runtime representation of this from the reflect package, which contains

07:13.120 --> 07:15.320
the string header type.

07:15.320 --> 07:19.920
So based on these two types, we see that the string consists of a pointer to the actual

07:19.920 --> 07:24.360
string data in the memory, an integer which gives the information about the size of the

07:24.360 --> 07:25.360
string.

07:25.360 --> 07:28.600
When Go creates a string, it allocates storage corresponding to the provided string size

07:28.600 --> 07:32.800
and then sets the string content as a slice of byte.

07:32.800 --> 07:36.200
As you've seen, the string data is stored as a contained slice of bytes memory.

07:36.200 --> 07:41.080
The size of the strings stays the same during its lifetime since, as I mentioned previously,

07:41.080 --> 07:42.080
the string is immutable.

07:42.080 --> 07:45.840
And this also means that the size and the capacity of the backing slice of bytes stays

07:45.840 --> 07:47.480
the same.

07:47.480 --> 07:51.320
When you put this all together, the total size of the string will consist of the overhead

07:51.320 --> 07:56.000
of the string header, which is equal to 16 bytes, and I show in a bit why, and the byte

07:56.000 --> 07:57.660
length of the string.

07:57.660 --> 08:03.280
We can break this down on this small example of the string I created with FOSDEM, space,

08:03.280 --> 08:04.820
waving hand emoji.

08:04.820 --> 08:05.960
So this is just a snippet.

08:05.960 --> 08:12.860
I don't think it would compile this code, but for brevity, I decided to show these three

08:12.860 --> 08:14.720
small lines.

08:14.720 --> 08:19.480
And by calling the size method on the string type from the reflect package, you would see

08:19.480 --> 08:22.200
it return number 16.

08:22.200 --> 08:23.200
Don't be fooled.

08:23.200 --> 08:27.560
The size method returns only the information of the size of the type, not size of the

08:27.560 --> 08:28.560
whole string.

08:28.560 --> 08:33.360
Therefore, it correctly tells us it's 16 bytes, 18 bytes due to pointer pointing to the string

08:33.360 --> 08:37.000
in memory, and 8 bytes for keeping the string length information.

08:37.000 --> 08:41.920
To get the size of the actual string data, we have to use the good old len method.

08:41.920 --> 08:43.760
This tells us it's 11 bytes.

08:43.760 --> 08:50.340
Since the string literal, here is UTF-8 encoded, we count one byte per each letter and space,

08:50.340 --> 08:54.380
and we need actually four bytes to encode the waving hand emoji.

08:54.380 --> 08:58.200
And this brings our total to 27 bytes.

08:58.200 --> 09:01.960
Interestingly for such a short string, the overhead of storing it is bigger than the

09:01.960 --> 09:05.720
string data itself.

09:05.720 --> 09:09.560
It's also important to realize what happens if we declare a new string variable that is

09:09.560 --> 09:11.040
copying an existing string.

09:11.040 --> 09:16.000
In this case, co-creates what we can consider a shallow copy, meaning the data the string

09:16.000 --> 09:18.760
refers to is shared between the variables.

09:18.760 --> 09:21.360
Let's break it down again on the example of our FOSDEM string.

09:21.360 --> 09:27.520
So we declare a new string literal FOSDEM waving hand emoji, and then create a new str

09:27.520 --> 09:32.560
or new string variable, and set it to value equal to string or str.

09:32.560 --> 09:34.120
What happens behind the scenes?

09:34.120 --> 09:37.800
If you would look at the values, pointer of each of the strings, you would see different

09:37.800 --> 09:43.080
addresses, making it obvious that these are two different strings strictly speaking.

09:43.080 --> 09:47.720
By looking at their headers, we would see identical information, same pointer to string

09:47.720 --> 09:49.600
data and same length.

09:49.600 --> 09:55.360
Excuse me, can we turn the light on the front off?

09:55.360 --> 09:57.520
I cannot, sorry.

09:57.520 --> 10:03.320
Yeah, it's a bit light, right?

10:03.320 --> 10:05.120
Sorry.

10:05.120 --> 10:10.760
But anyway, so these are two different strings strictly speaking, and looking at the header

10:10.760 --> 10:16.760
information, we would see that they point to same string data and have same length.

10:16.760 --> 10:20.640
Because they are two different strings, we need to be mindful of the fact that the new

10:20.640 --> 10:23.000
STR comes with a brand new string header.

10:23.000 --> 10:28.640
So the bottom line is when we do this copying, there is again, even the data is shared, the

10:28.640 --> 10:32.600
overhead of 16 bytes is still there.

10:32.600 --> 10:36.440
So I briefly talked about my inspiration for this talk, but I also wanted to expand a bit

10:36.440 --> 10:42.240
on the context of the problems where I think the string optimization strategies can be

10:42.240 --> 10:43.240
useful.

10:43.240 --> 10:48.000
I think in general, many programs with characteristics of in-memory stores may face performance

10:48.000 --> 10:49.000
issue.

10:49.000 --> 10:52.320
I will talk about in this slide such programs.

10:52.320 --> 10:57.160
I already mentioned numerous times the time series database, DNS resolvers, or any other

10:57.160 --> 11:02.560
kind of key value store, where we come with an assumption that these are some long-running

11:02.560 --> 11:09.960
programs, and over the runtime of the program, we will keep the number of strings we'll keep

11:09.960 --> 11:12.160
accumulating.

11:12.160 --> 11:15.040
So we can be talking potentially billions of strings.

11:15.040 --> 11:19.320
There's also potential for repetitions of strings since many of these stored values

11:19.320 --> 11:21.160
may repeat themselves.

11:21.160 --> 11:26.160
For example, if we associate each of our entries with a label denoting which cluster they belong

11:26.160 --> 11:31.440
to, we are guaranteed to have repeated values since we have a finite and often small amount

11:31.440 --> 11:32.680
of clusters.

11:32.680 --> 11:38.840
So the string cluster will be stored as many times as many entries there are in our database.

11:38.840 --> 11:42.720
There are also certain caveats when it comes to handling of incoming data.

11:42.720 --> 11:50.480
Data will often come in the form of requests through HTTP or GRPC or any other protocol.

11:50.480 --> 11:56.280
And usually we handle this data in our program by un-marshalling them into a struct, and

11:56.280 --> 12:03.800
then we might want to store some information, some string from this struct in the memory

12:03.800 --> 12:05.120
for future use.

12:05.120 --> 12:09.600
However, the side effect of this is that the whole struct will be prevented from being

12:09.600 --> 12:14.520
garbage collected because as long as the string or as a matter of fact any other field from

12:14.520 --> 12:21.400
a struct is being referenced by our database in memory, the garbage collection won't kick

12:21.400 --> 12:25.760
in and eventually will lead to bloat in the memory consumption.

12:25.760 --> 12:32.260
I think the second kind of different type of programs where string optimization can

12:32.260 --> 12:39.400
be useful are kind of one-off data processing situations as opposed to the long-running

12:39.400 --> 12:40.720
programs.

12:40.720 --> 12:46.000
So we can take an example of handling some large JSON file.

12:46.000 --> 12:50.760
Perhaps it can be some data set from a study or a health data, which I think were some

12:50.760 --> 12:53.680
good examples I've seen out in the wild.

12:53.680 --> 12:58.280
And such processing will require a larger amount of memory to decode the data during

12:58.280 --> 12:59.280
processing.

12:59.280 --> 13:03.000
So in general we might be processing same strings that repeat themselves over and over

13:03.000 --> 13:05.600
again such as the keys in the JSON document.

13:05.600 --> 13:09.280
We're having to allocate such strings in you each time.

13:09.280 --> 13:15.920
So now that we have a better understanding of the problem zones, let's look at the actual

13:15.920 --> 13:17.880
optimization strategies.

13:17.880 --> 13:25.640
So the first strategy is related to the issue I mentioned a couple of slides before where

13:25.640 --> 13:32.880
we are wasting memory by keeping whole structs in memory when we only need part of the struct

13:32.880 --> 13:35.640
that is represented by the string.

13:35.640 --> 13:40.080
So what we want to do here is to have a mechanism that will allow us to quote unquote detach

13:40.080 --> 13:45.240
the string from the struct so that the rest of the struct can be garbage collected.

13:45.240 --> 13:48.960
Previously this was also possible to achieve with some unsafe manipulation of strings,

13:48.960 --> 13:55.080
but since Go 118 there's a new method called clone in the string standard library that

13:55.080 --> 13:57.480
makes it quite straightforward.

13:57.480 --> 14:00.080
The clone creates a new fresh copy of the string.

14:00.080 --> 14:04.280
This decouples the string from the struct, meaning the struct can be garbage collected

14:04.280 --> 14:08.620
in the long term and will retain only the new copy of the string.

14:08.620 --> 14:13.060
So remember previously I showed that when we copy strings we create shallow copies.

14:13.060 --> 14:14.360
Here we want to achieve the opposite.

14:14.360 --> 14:19.340
We want to truly copy the string and create a fresh copy of the underlying string data

14:19.340 --> 14:24.100
so the original string can be garbage collected together with the struct it's part of.

14:24.100 --> 14:28.200
So this you can refer to as deep copying.

14:28.200 --> 14:32.600
The next most interesting and I'd say one of the most widely used strategies in software

14:32.600 --> 14:35.040
in general is string interning.

14:35.040 --> 14:38.800
String interning is a technique which makes it possible to store only a single copy of

14:38.800 --> 14:43.400
each distinct string and subsequently we keep referencing the same underlying string in

14:43.400 --> 14:44.640
the memory.

14:44.640 --> 14:49.440
This concept is somewhat more common in other languages such as Java or Python but can be

14:49.440 --> 14:52.160
implemented effortlessly in Go as well.

14:52.160 --> 14:56.560
There are even some ready-made solutions out in the open that you can use.

14:56.560 --> 15:02.520
So it's simple as you could achieve this by having a simple map string string and you

15:02.520 --> 15:08.200
can keep the references to the string in this map which we can call our interning map or

15:08.200 --> 15:13.240
cache or anything like that.

15:13.240 --> 15:16.480
First complication comes with the concurrency, right?

15:16.480 --> 15:21.240
Because we need a mechanism to prevent concurrent write and read to our interning map so obvious

15:21.240 --> 15:27.280
choice would be to use Mutex which have our incurs performance penalty but so be it.

15:27.280 --> 15:31.800
Our concurrency save map version from the sync standard library.

15:31.800 --> 15:36.280
The second complication or the noteworthy fact is that with each new reference string

15:36.280 --> 15:41.440
we are incurring the 16 bytes overhead as I explained a couple of slides back.

15:41.440 --> 15:47.880
So even though we're saving on the actual string data it's not we're still incurring

15:47.880 --> 15:49.120
the overhead.

15:49.120 --> 15:55.560
So with millions of strings 16 bytes for every string it's not a trivial it's a non-trivial

15:55.560 --> 15:57.600
amount.

15:57.600 --> 16:02.200
Third complication comes from the unknown lifetime of the string in our interning map.

16:02.200 --> 16:07.000
At some point in the lifetime of the program there might be no more references to a particular

16:07.000 --> 16:09.600
string so it can be safely dropped.

16:09.600 --> 16:12.800
But how to know when these conditions are met?

16:12.800 --> 16:18.120
Ideally we don't want to be keeping unused strings as in an extreme case this can be

16:18.120 --> 16:25.520
a denial of service vector leading to exhaustion of memory if we allow the map to grow unbounded.

16:25.520 --> 16:29.720
One option could be to periodically clear the map or give the entry a certain time to

16:29.720 --> 16:34.840
live so after a given period the map or the given entries are dropped from the map and

16:34.840 --> 16:39.600
if a string reappears after such deletion we simply create the entry in the interning

16:39.600 --> 16:43.120
map so kind of like cache.

16:43.120 --> 16:47.600
And naturally this can lead to some unnecessary churning and unnecessary allocations because

16:47.600 --> 16:51.080
we don't know exactly which strings are no longer needed or referenced but we might be

16:51.080 --> 16:54.160
still dropping them.

16:54.160 --> 16:59.840
Second and more elaborate way to do this is to keep counting the number of references

16:59.840 --> 17:05.520
of the used strings and this naturally requires a more eloquent and complex implementation

17:05.520 --> 17:10.280
but you can see here I linked work done in the Prometheus project where I think is a

17:10.280 --> 17:17.680
good example of how this can be implemented with counting the references.

17:17.680 --> 17:20.080
We can take this even to the next level.

17:20.080 --> 17:25.160
As I recently learned there is an implementation of an interning library that is capable of

17:25.160 --> 17:27.880
automatically dropping unused references.

17:27.880 --> 17:33.400
The goal for.org intern library is capable of doing this thanks to somewhat controversial

17:33.400 --> 17:37.800
concept of the finalizers in the Go runtime.

17:37.800 --> 17:42.360
The first set very plainly make it possible to attach a function that will be called on

17:42.360 --> 17:47.440
a variable that is deemed to be garbage collection ready by the garbage collector.

17:47.440 --> 17:52.360
At that point this library checks the sentinel boolean on the reference value and if it finds

17:52.360 --> 17:57.440
this is the last reference to that value it drops it from a map.

17:57.440 --> 18:01.680
The library also cleverly boxes the string header down to a single pointer which brings

18:01.680 --> 18:06.020
the overhead down to eight bytes instead of 16.

18:06.020 --> 18:10.760
So as fascinating as this implementation is to me it makes use of some potentially unsafe

18:10.760 --> 18:15.720
code behavior hence the dark arts reference in the slide title.

18:15.720 --> 18:19.500
However the library is deemed stable and mature enough and has been created by some well known

18:19.500 --> 18:26.120
names in the Go community so if you're interested I encourage you to study and look at the code

18:26.120 --> 18:32.360
it's just one file but it's quite interesting and you're sure to learn a thing or two about

18:32.360 --> 18:37.200
some less known parts of Go.

18:37.200 --> 18:42.720
And as an example I've recently tried this library in the last blood point in the Thanos

18:42.720 --> 18:48.280
project again I linked you the PR with the usage with the implementation which I think

18:48.280 --> 18:55.880
is rather straightforward and we ran some synthetic benchmarks on this version with

18:55.880 --> 18:59.640
interning turned on this was the result.

18:59.640 --> 19:04.720
On the left side you can see probably not very clearly unfortunately but there is a

19:04.720 --> 19:12.360
graph showing metrics for both both reported by the Go runtime how many bytes we have in

19:12.360 --> 19:21.960
the heap and metrics reported by the container itself and you can see the differences between

19:21.960 --> 19:27.960
the green and yellow line and the blue and red line so it's it came up to roughly two

19:27.960 --> 19:35.480
to three gigabytes improvement per instance so this is averaged per I think across six

19:35.480 --> 19:40.760
or nine instances so for instance this was around two to three gigabytes so we can count

19:40.760 --> 19:46.160
overall improvements around ten to twelve gigabytes but more interestingly on the right

19:46.160 --> 19:52.280
side of the slide there is another graph to kind of confirm that the interning is doing

19:52.280 --> 19:59.080
something that is working then we can we can see we're following again metric reported

19:59.080 --> 20:04.080
by the Go runtime and we're looking at the number of objects held in the memory so we

20:04.080 --> 20:12.080
can see that it dropped it dropped almost by by half when we look at the average.

20:12.080 --> 20:15.600
Finally there's a string interning with a slightly different flavor I would say which

20:15.600 --> 20:20.800
I refer to a string interning with symbol tables and in this alternative instead of

20:20.800 --> 20:26.240
keeping a reference string we replace it with another referring symbol such as for example

20:26.240 --> 20:31.360
an integer so that integer one will correspond to string apple or integer two will correspond

20:31.360 --> 20:36.360
to string banana and so on and this can be beneficial with scenarios with a lot of duplicated

20:36.360 --> 20:42.280
strings again this brings me to my home home field and to the time series databases where

20:42.280 --> 20:49.040
there is generally a high probability of the labels so also the strings being repeated

20:49.040 --> 20:53.640
and especially when such strings are being sent over the wire so instead of sending all

20:53.640 --> 20:59.280
the duplicated strings we can send a symbol table in their place and we can replace the

20:59.280 --> 21:05.600
strings with the references in this table so where this idea come from or where I got

21:05.600 --> 21:11.080
inspired for this was also in tunnels but this was by one of my fellow maintainers so

21:11.080 --> 21:16.600
you can you can look at that PR who implemented this for data series being sent over the network

21:16.600 --> 21:23.520
between tunnels components so instead of sending all the long and unduplicated label keys and

21:23.520 --> 21:28.520
values so instead of sending all of these strings we build a symbol table that we send

21:28.520 --> 21:35.880
together with the the duplicated label data that includes that contains only references

21:35.880 --> 21:40.560
instead of the strings so that all we have to do on the other side once we receive the

21:40.560 --> 21:46.120
data is to replace the references by the actual strings based on the symbol table which saves

21:46.120 --> 21:51.880
us on one hand the cost of the network since the requests are smaller and also the allocations

21:51.880 --> 21:59.280
once we're dealing with the data on the receiving side. Lastly you could try putting all of

21:59.280 --> 22:04.680
the strings into one big structure into one big string and this can be useful to decrease

22:04.680 --> 22:10.200
the total overhead of the strings as this eliminates the already mentioned overhead

22:10.200 --> 22:19.280
of the string header so yeah since this is always 16 bytes plus the byte length of the

22:19.280 --> 22:23.160
string which consists which creates the size of the of the string by putting all the strings

22:23.160 --> 22:30.400
into the one we can effectively decrease the overhead of those string headers so of course

22:30.400 --> 22:35.280
this is not without added complexity because now we have to deal with how to look up those

22:35.280 --> 22:41.360
substrings or those smaller strings within the big within the bigger structure and so

22:41.360 --> 22:46.240
you need a mechanism because you cannot simply look them up in a map or symbol table and

22:46.240 --> 22:51.440
obviously another already mentioned complications such as concurrent access you also have to

22:51.440 --> 22:55.880
also still have to deal with this and I think particularly interesting attempt that this

22:55.880 --> 23:01.400
is going on in the Prometheus project which again this is this is done by Brian Boren

23:01.400 --> 23:06.960
who I mentioned in the previous slides so if you're interested feel free to feel free

23:06.960 --> 23:16.400
to check out this this PR so I will conclude with a few words of caution so I have show

23:16.400 --> 23:19.960
you some optimization techniques that I found particularly interesting when I was doing

23:19.960 --> 23:24.440
my research but let's not be naive these are not magic ones that will make your program

23:24.440 --> 23:30.180
suddenly work faster and with fewer resources this is still a balancing exercise so many

23:30.180 --> 23:34.120
of the presented techniques can save memory but will actually increase the time it takes

23:34.120 --> 23:39.480
to retrieve a string so when I mean optimization this is mostly in a situation where we want

23:39.480 --> 23:44.680
to decrease expensive memory footprint of our application while sacrificing a bit more

23:44.680 --> 23:51.000
CPU a trade-off that I believe is reasonable in such setting most of making any concrete

23:51.000 --> 23:57.360
claims about performance improvements of various techniques as you have seen and I think this

23:57.360 --> 24:02.240
nicely ties into the introduction of my talk where I talked about the need of data data

24:02.240 --> 24:06.860
driven optimization so I believe there's still more data points needed to show how well these

24:06.860 --> 24:12.960
techniques work in practice how well they can work in your specific use case how they

24:12.960 --> 24:18.080
compare with each other when it comes to performance and whether there are some other real world

24:18.080 --> 24:24.880
implications or maybe properties of go or compiler or the runtime that might not render

24:24.880 --> 24:33.920
them useful in practice or the performance gain might be negligible so just to say that

24:33.920 --> 24:43.800
your mileage might vary but I think these ideas are worth exploring and can be interesting

24:43.800 --> 24:57.160
and that is all from my side thank you for your attention. Also included a couple more

24:57.160 --> 25:14.760
resources for those interested you can find the slides in the in the panta bar.
