1
0:00:00.000 --> 0:00:16.600
It's four o'clock, so let's look at our preview.

2
0:00:16.600 --> 0:00:18.400
Sorry, now next talk.

3
0:00:18.400 --> 0:00:24.280
I have been doing some matting in Go, but building a database I honestly have strong

4
0:00:24.280 --> 0:00:25.600
respect for.

5
0:00:25.600 --> 0:00:31.200
So next up is Etienne, who is going to tell us everything about crazy good journeys in

6
0:00:31.200 --> 0:00:32.200
Go.

7
0:00:32.200 --> 0:00:33.200
Thank you.

8
0:00:33.200 --> 0:00:34.200
Thank you.

9
0:00:34.200 --> 0:00:39.840
Yeah, welcome to our mad journey of building a database in Go.

10
0:00:39.840 --> 0:00:43.040
And yeah, it's pretty mad to build a database at all.

11
0:00:43.040 --> 0:00:51.600
It may be even worse or even a matter to build a database in Go when most are built in...

12
0:00:51.600 --> 0:00:52.600
Okay.

13
0:00:52.600 --> 0:00:53.600
Closer?

14
0:00:53.600 --> 0:00:55.600
Okay.

15
0:00:55.600 --> 0:00:57.600
Cool.

16
0:00:57.600 --> 0:00:59.400
Let me start over in case you didn't hear it.

17
0:00:59.400 --> 0:01:00.600
Hi, my name is Etienne.

18
0:01:00.600 --> 0:01:03.960
Welcome to our mad journey of building a vector database in Go.

19
0:01:03.960 --> 0:01:08.040
So building a database at all could already be pretty mad at doing it in Go when most

20
0:01:08.040 --> 0:01:10.440
are built in C or C++.

21
0:01:10.440 --> 0:01:13.480
Could be even even matter or even more exciting.

22
0:01:13.480 --> 0:01:18.160
And we definitely encountered a couple of unique problems that led us to create creative

23
0:01:18.160 --> 0:01:21.920
solutions and there's lots of shouts out in there and also a couple of wish lists that

24
0:01:21.920 --> 0:01:26.800
we just released Go window 20 and of course the occasional madness.

25
0:01:26.800 --> 0:01:29.760
So let's get one question out of the way right away.

26
0:01:29.760 --> 0:01:32.640
Why does the world even need yet another database?

27
0:01:32.640 --> 0:01:38.400
There's so many out there already but probably you've seen this thing called chat GPT because

28
0:01:38.400 --> 0:01:42.400
that was pretty much everywhere and it's kind of hard to hide from it.

29
0:01:42.400 --> 0:01:48.020
And chat GPT is a large language model and it's really good at putting text together

30
0:01:48.020 --> 0:01:54.880
that sounds really sophisticated and sounds nice and sometimes is completely wrong.

31
0:01:54.880 --> 0:01:59.040
And so in this case we're asking is it math to write a database and go I might disagree

32
0:01:59.040 --> 0:02:00.440
with that.

33
0:02:00.440 --> 0:02:04.400
But either way basically we're now in a situation where on the one hand we have these machine

34
0:02:04.400 --> 0:02:08.160
learning models that can do all the cool stuff and do this sort of interactively and on the

35
0:02:08.160 --> 0:02:12.480
fly and on the other side we have traditional databases and those traditional databases

36
0:02:12.480 --> 0:02:15.920
they have the facts because that's kind of what databases are for.

37
0:02:15.920 --> 0:02:19.600
So wouldn't it be cool if we could somehow combine those two.

38
0:02:19.600 --> 0:02:25.500
So for example on the query side if I ask Wikipedia why can airplanes fly then the kind

39
0:02:25.500 --> 0:02:29.800
of passage that I want that has the answer in it is titled the physics of flight.

40
0:02:29.800 --> 0:02:33.480
But that is difficult for a traditional search engine because if you look at keyword overlap

41
0:02:33.480 --> 0:02:35.960
there's almost none in there.

42
0:02:35.960 --> 0:02:40.400
But a vector search engine can use machine learning models basically that can tell you

43
0:02:40.400 --> 0:02:45.640
these two things are the same and searching through that at scale is a big problem.

44
0:02:45.640 --> 0:02:51.000
Then there's that sort of chat GPT side where you don't just want to search through it but

45
0:02:51.000 --> 0:02:55.400
maybe you also want to say like take those results summarize them and also translate

46
0:02:55.400 --> 0:02:56.400
them to German.

47
0:02:56.400 --> 0:03:00.640
So basically not just return exactly what's in the database but do something with it and

48
0:03:00.640 --> 0:03:03.520
basically generate more data from it.

49
0:03:03.520 --> 0:03:05.120
And that is exactly where VVA come in.

50
0:03:05.120 --> 0:03:09.720
So VVA does a vector search engine which basically helps us solve this kind of searching by meaning

51
0:03:09.720 --> 0:03:15.800
instead of keywords without sort of losing what we've done in 20 plus years of search

52
0:03:15.800 --> 0:03:17.440
engine research.

53
0:03:17.440 --> 0:03:22.520
And now most recently you can also interact with these models such as chat GPT GPT 3 and

54
0:03:22.520 --> 0:03:26.160
of course also the open source versions of it.

55
0:03:26.160 --> 0:03:28.700
So VVA is written in go.

56
0:03:28.700 --> 0:03:29.740
Is that a good idea?

57
0:03:29.740 --> 0:03:34.040
Is that a bad idea or have we just gone playing that?

58
0:03:34.040 --> 0:03:35.040
So we're not alone.

59
0:03:35.040 --> 0:03:36.040
That's good.

60
0:03:36.040 --> 0:03:38.960
So you probably probably recognize these things.

61
0:03:38.960 --> 0:03:43.760
They're all bigger brands at the moment than VVA but VVA is growing fast.

62
0:03:43.760 --> 0:03:48.320
And some of those vendors have really great blog posts where you see some of the like

63
0:03:48.320 --> 0:03:51.540
optimization topics and some of the crazy stuff that they have to do.

64
0:03:51.540 --> 0:03:55.720
So if you've contributed to one of those some of the things I'm going to say might sound

65
0:03:55.720 --> 0:03:56.720
familiar.

66
0:03:56.720 --> 0:03:59.320
If not then buckle up.

67
0:03:59.320 --> 0:04:00.920
It's going to get mad.

68
0:04:00.920 --> 0:04:05.760
So first stop on our mad journey memory allocation then that also brings us to our friend the

69
0:04:05.760 --> 0:04:07.080
garbage collector.

70
0:04:07.080 --> 0:04:13.000
So for any high performance go application sooner or later you're going to talk about

71
0:04:13.000 --> 0:04:14.000
memory allocations.

72
0:04:14.000 --> 0:04:17.280
And I definitely consider a database a high performance application at least I consider

73
0:04:17.280 --> 0:04:19.800
VVA a high performance application.

74
0:04:19.800 --> 0:04:24.400
And if you think of what databases do like in essence basically you have something on

75
0:04:24.400 --> 0:04:26.080
disk and you want to serve it to the user.

76
0:04:26.080 --> 0:04:30.040
That's like one of the most important user journeys in a database.

77
0:04:30.040 --> 0:04:32.040
And here this is represented by just a number.

78
0:04:32.040 --> 0:04:33.960
So it went for UN32.

79
0:04:33.960 --> 0:04:39.040
So that's just four bytes on disk and basically you can see these four bytes.

80
0:04:39.040 --> 0:04:43.960
If you parse them into go they would have the value of 16 in that UN32.

81
0:04:43.960 --> 0:04:48.200
And this is essentially something very much simplified that the database needs to do and

82
0:04:48.200 --> 0:04:50.960
it needs to do it over and over again.

83
0:04:50.960 --> 0:04:56.520
So the standard library gives us the encoding binary package and there we have this binary

84
0:04:56.520 --> 0:04:59.840
dot read method with which I think looks really cool.

85
0:04:59.840 --> 0:05:04.560
To me it looks like idiomatic go because it has the IODA reader interface like everyone's

86
0:05:04.560 --> 0:05:08.280
favorite interface and you can put all of that stuff in there.

87
0:05:08.280 --> 0:05:12.040
And if you run this code and there's no error then basically you get exactly what you want.

88
0:05:12.040 --> 0:05:17.080
You can turn those sort of four bytes that were somewhere on disk, turn them into our

89
0:05:17.080 --> 0:05:20.920
in memory representation of that UN32.

90
0:05:20.920 --> 0:05:27.520
So is this a good idea to do that exactly like well if you do it once or maybe twice

91
0:05:27.520 --> 0:05:29.180
could be a good idea.

92
0:05:29.180 --> 0:05:32.640
If you do it a billion times this is what happens.

93
0:05:32.640 --> 0:05:38.840
So for those of you who are new to CPU profiles in Go this is pretty bad.

94
0:05:38.840 --> 0:05:44.320
So first of all you see it in the center parsing those one billion numbers took 26 seconds

95
0:05:44.320 --> 0:05:49.240
and 26 seconds is not the kind of time that we ever have in the database.

96
0:05:49.240 --> 0:05:55.120
But worse than that if you look at that profile we have stuff like runtime, MLGc, runtime,

97
0:05:55.120 --> 0:05:57.500
memmove, runtime, and advise.

98
0:05:57.500 --> 0:06:01.840
So all these things they're related to memory allocations or to garbage collection.

99
0:06:01.840 --> 0:06:05.800
What they're not related to is parsing data which is what we wanted to do right.

100
0:06:05.800 --> 0:06:10.000
So how much time of that 20 seconds that we spend what we wanted to do.

101
0:06:10.000 --> 0:06:13.120
Don't know doesn't even show up in the profile.

102
0:06:13.120 --> 0:06:18.000
So and to understand why that is the case we need to quickly talk about the stack and

103
0:06:18.000 --> 0:06:19.600
the heap.

104
0:06:19.600 --> 0:06:24.160
So you can think of the stack as basically your function stack so you call one function

105
0:06:24.160 --> 0:06:28.240
that calls another function and then at some point basically you go back through the stack

106
0:06:28.240 --> 0:06:32.520
and this is very short-lived and this is cheap and fast to allocate and why is it cheap?

107
0:06:32.520 --> 0:06:37.280
Because you know exactly the runtime of your variables or the life cycle of your variables

108
0:06:37.280 --> 0:06:39.120
so you don't even need to involve the garbage collector.

109
0:06:39.120 --> 0:06:41.480
So no garbage collector cheap and fast.

110
0:06:41.480 --> 0:06:45.920
Then on the other side you have the heap and the heap is basically this sort of long-lived

111
0:06:45.920 --> 0:06:49.920
kind of memory and that's expensive and slow to allocate and why?

112
0:06:49.920 --> 0:06:51.440
And also to deallocate.

113
0:06:51.440 --> 0:06:54.060
Why because it involves the garbage collector.

114
0:06:54.060 --> 0:06:58.000
So if the stack is so much cheaper then we can just always allocate on the stack right.

115
0:06:58.000 --> 0:07:00.400
So warning this is not real go.

116
0:07:00.400 --> 0:07:02.100
Please do not do this.

117
0:07:02.100 --> 0:07:05.840
This is sort of a fictional example of allocating a buffer of size 8 and then we're going to

118
0:07:05.840 --> 0:07:09.600
say like yeah please put this on the stack and that is not how it works.

119
0:07:09.600 --> 0:07:12.800
And for most of you you probably say like this is pretty good that it's not that it

120
0:07:12.800 --> 0:07:15.000
works that way because why would you want to deal with that?

121
0:07:15.000 --> 0:07:19.160
But for me just trying to build a database and go sometimes like this something like

122
0:07:19.160 --> 0:07:21.440
this may be good or maybe not.

123
0:07:21.440 --> 0:07:22.920
So how does it work?

124
0:07:22.920 --> 0:07:25.620
Go does something that's called escape analysis.

125
0:07:25.620 --> 0:07:32.300
So if you compile your code with gcflax-m then go annotates your code basically and

126
0:07:32.300 --> 0:07:34.240
tells you sort of what's happening there.

127
0:07:34.240 --> 0:07:40.600
So here you can see in the second line that this num variable that we used was moved to

128
0:07:40.600 --> 0:07:45.680
the heap and then in the next point you see the bytes.reader which represents our iodo.reader

129
0:07:45.680 --> 0:07:46.880
escaped to the heap.

130
0:07:46.880 --> 0:07:49.880
So two times we see that something happened to the or went to the heap.

131
0:07:49.880 --> 0:07:54.360
We don't exactly know what happened yet but at least there's proof that we have this kind

132
0:07:54.360 --> 0:07:56.960
of allocation problem.

133
0:07:56.960 --> 0:07:58.360
So what can we do?

134
0:07:58.360 --> 0:07:59.880
Well we can simplify it a bit.

135
0:07:59.880 --> 0:08:05.220
Turns out that the binary or encoding binary package also has another method that looks

136
0:08:05.220 --> 0:08:11.000
like this which is just called view in 32 on the little nd package and it kind of does

137
0:08:11.000 --> 0:08:12.000
the same thing.

138
0:08:12.000 --> 0:08:15.240
You just put in the buffer on the one side so no reader this time you just put in the

139
0:08:15.240 --> 0:08:19.760
raw buffer basically with the position offset and on the other side you get the number out.

140
0:08:19.760 --> 0:08:24.000
And the crazy thing is this one line needs no memory allocations.

141
0:08:24.000 --> 0:08:30.320
So if we do that again our one billion numbers that took 26 seconds before now take 600 milliseconds.

142
0:08:30.320 --> 0:08:35.280
Now we're starting to get into a range where like this is acceptable for a data basis.

143
0:08:35.280 --> 0:08:39.880
And more importantly what we see on that profile the profile is so much simpler right now.

144
0:08:39.880 --> 0:08:45.000
There's basically just this one function there and that is that is yeah it's what we wanted

145
0:08:45.000 --> 0:08:46.000
to do.

146
0:08:46.000 --> 0:08:49.160
So admittedly we're not doing much other than parsing the data at the moment but at least

147
0:08:49.160 --> 0:08:54.240
we got sort of rid of all the noise and you can see the speed up.

148
0:08:54.240 --> 0:08:59.360
Okay so quickly to recap if we say a database is nothing but reading data and sort of parsing

149
0:08:59.360 --> 0:09:04.960
it to serve it to the user then we do that over and over again and we need to take care

150
0:09:04.960 --> 0:09:06.160
of memory allocations.

151
0:09:06.160 --> 0:09:10.520
And the fix in this case was super simple we changed two lines of code and reduced it

152
0:09:10.520 --> 0:09:13.600
from 26 seconds to 600 milliseconds.

153
0:09:13.600 --> 0:09:17.280
But why we had to do that wasn't very intuitive like that it wasn't very obvious.

154
0:09:17.280 --> 0:09:22.000
In fact I haven't even told you yet why this binary dot little nd dot read why that escaped

155
0:09:22.000 --> 0:09:23.000
to the heap.

156
0:09:23.000 --> 0:09:26.560
And in this case it's because we passed in pointer and we passed in an interface and

157
0:09:26.560 --> 0:09:30.600
that's kind of a hint basically that something might escape to the heap.

158
0:09:30.600 --> 0:09:36.160
So what I would wish is yes this is not a topic that you need every day you write go

159
0:09:36.160 --> 0:09:42.160
but maybe if you do need this would be cool if there was better education.

160
0:09:42.160 --> 0:09:45.680
Okay so second step delayed decoding.

161
0:09:45.680 --> 0:09:51.480
So this is kind of the idea that we wouldn't want to do the same work twice and we're sticking

162
0:09:51.480 --> 0:09:57.040
with our example of serving data from disk but now while the number example was a bit

163
0:09:57.040 --> 0:10:00.560
too too simple so let's make it make it slightly more complex.

164
0:10:00.560 --> 0:10:07.800
We have this nested array here basically a sort of slice off slice of view in 64 and

165
0:10:07.800 --> 0:10:12.080
that's representative now for a more complex object on your database.

166
0:10:12.080 --> 0:10:14.960
Of course in reality you'd have like string props and other kind of things but just sort

167
0:10:14.960 --> 0:10:18.560
of to show that there's more going on than a single number.

168
0:10:18.560 --> 0:10:22.600
And let's say we have 80 million of them so 10 million of the outer slice and then eight

169
0:10:22.600 --> 0:10:25.920
elements in each inner slice and our task is just to sum those up.

170
0:10:25.920 --> 0:10:28.800
So these are 80 million numbers and we want to know what is the sum of them.

171
0:10:28.800 --> 0:10:36.480
So that is actually kind of a realistic database task for an OLAP kind of database.

172
0:10:36.480 --> 0:10:41.640
We need to somehow represent that data on disk and we're looking at two ways to do this.

173
0:10:41.640 --> 0:10:45.840
The first one is JSON representation and then the second one would be sort of binary encoding

174
0:10:45.840 --> 0:10:48.200
and then there'll be more.

175
0:10:48.200 --> 0:10:51.160
So JSON is basically just here for completeness.

176
0:10:51.160 --> 0:10:53.000
We can basically rule it out immediately.

177
0:10:53.000 --> 0:10:58.240
When you're building a database you're probably not using JSON to store stuff on disk unless

178
0:10:58.240 --> 0:11:00.440
it's sort of a JSON database.

179
0:11:00.440 --> 0:11:05.160
Why because it's space inefficient so if you want to represent those numbers on disk like

180
0:11:05.160 --> 0:11:09.560
space and JSON basically uses strings for it and then you have all these control characters

181
0:11:09.560 --> 0:11:13.440
like your curly braces and your quotes and your columns and everything that takes up

182
0:11:13.440 --> 0:11:14.440
space.

183
0:11:14.440 --> 0:11:18.680
So in our fictional example that would take up 1.6 gigabyte and you'll see soon that we

184
0:11:18.680 --> 0:11:20.140
can do that more efficient.

185
0:11:20.140 --> 0:11:25.120
But also it's slow and part of why it's slow is again because we have these memory allocations

186
0:11:25.120 --> 0:11:27.640
but also the whole parsing just takes time.

187
0:11:27.640 --> 0:11:34.000
So in our example this took 14 seconds to sum up those 80 million numbers and yeah as

188
0:11:34.000 --> 0:11:39.840
I said before you just don't have double digit seconds in a database.

189
0:11:39.840 --> 0:11:44.820
So we can do something that's a bit smarter which is called length encoding.

190
0:11:44.820 --> 0:11:50.680
So we're encoding this basically as binary and we're spending one in this case one byte

191
0:11:50.680 --> 0:11:54.400
so that's basically a U and eight and we're using that as a length indicator.

192
0:11:54.400 --> 0:11:57.440
So basically that tells us that when we're reading this from disk that just tells us

193
0:11:57.440 --> 0:11:58.440
what's coming up.

194
0:11:58.440 --> 0:12:02.120
So in this case it says we have eight elements coming up and then we know that our elements

195
0:12:02.120 --> 0:12:05.520
in this example is U and 32 so that's four bytes each.

196
0:12:05.520 --> 0:12:09.820
So basically the next 32 bytes that we're reading are going to be our eight in arrays

197
0:12:09.820 --> 0:12:10.820
and then we just continue.

198
0:12:10.820 --> 0:12:17.280
Then we basically read the next length indicator and this way we can encode the stuff in one

199
0:12:17.280 --> 0:12:19.040
continuous thing.

200
0:12:19.040 --> 0:12:23.440
Then of course we have to decode it somehow and we can do that because we've learned from

201
0:12:23.440 --> 0:12:27.760
our previous example right so we're not going to use binary.littleendian.read but we're

202
0:12:27.760 --> 0:12:30.560
doing this in an allocation free way.

203
0:12:30.560 --> 0:12:32.880
You can see it in the length line basically.

204
0:12:32.880 --> 0:12:38.520
And yeah our goal is to take that data and put it into our nested sort of go slice of

205
0:12:38.520 --> 0:12:44.720
slice of U in 64 and the code here basically you see we're reading the length and then

206
0:12:44.720 --> 0:12:48.160
we're increasing our offsets and we know where to read from and then we're basically repeating

207
0:12:48.160 --> 0:12:54.760
this for the inner slice which is just hinted at here by the decode inner function.

208
0:12:54.760 --> 0:12:57.000
So what happens when we do this?

209
0:12:57.000 --> 0:13:03.200
First of all the good news 660 megabytes that's way less than our 1.6 gigabyte before so basically

210
0:13:03.200 --> 0:13:09.600
just by using a more space efficient way to represent data we've done exactly that.

211
0:13:09.600 --> 0:13:11.320
We've reduced our size.

212
0:13:11.320 --> 0:13:18.160
Also it's much much faster so we were at 14 seconds before and now it's down to 260 milliseconds

213
0:13:18.160 --> 0:13:23.400
but this is our mad journey of building a database so we're not done here yet because

214
0:13:23.400 --> 0:13:25.400
there's some hidden madness.

215
0:13:25.400 --> 0:13:31.040
And the hidden madness is that we actually spend 250 milliseconds decoding while we spend

216
0:13:31.040 --> 0:13:33.840
10 milliseconds summing up those 80 million numbers.

217
0:13:33.840 --> 0:13:37.240
So again we're kind of in that situation where we're doing something that we never really

218
0:13:37.240 --> 0:13:41.960
set out to do like we wanted to do something else but we're spending our time on yeah doing

219
0:13:41.960 --> 0:13:43.840
something that we didn't want to do.

220
0:13:43.840 --> 0:13:46.280
So where does that come from?

221
0:13:46.280 --> 0:13:51.240
And the first problem is basically that what we did what we set out to do was fought from

222
0:13:51.240 --> 0:13:55.480
the get go because we said we want to decode so we're basically thinking in the same way

223
0:13:55.480 --> 0:13:58.280
that we're thinking as we were with Jason.

224
0:13:58.280 --> 0:14:02.240
We said that we want to decode this entire thing into this go data structure but that

225
0:14:02.240 --> 0:14:06.560
means that you see we need to allocate this massive slice again and that also means that

226
0:14:06.560 --> 0:14:10.160
we need to in each inner slice we also need to allocate again so we're basically allocating

227
0:14:10.160 --> 0:14:14.360
and allocating over and over again where our task is not to allocate our task was to sum

228
0:14:14.360 --> 0:14:16.160
up numbers.

229
0:14:16.160 --> 0:14:21.440
So we can actually just simplify this a bit and we can basically just not decode it like

230
0:14:21.440 --> 0:14:25.520
while we're looping over that data anyway instead of storing it in an array we can just

231
0:14:25.520 --> 0:14:29.840
do with it what we plan to do and in this case this would be summing up the data.

232
0:14:29.840 --> 0:14:36.280
So basically getting rid of that decoding step helps us to make this way faster.

233
0:14:36.280 --> 0:14:41.280
So now we're at 46 milliseconds of course our footprint of the data on disk hasn't changed

234
0:14:41.280 --> 0:14:45.160
because it's the same data that we're reading which is reading it in a slightly more efficient

235
0:14:45.160 --> 0:14:49.360
way but yeah we don't have to allocate slices and also because we don't have these nested

236
0:14:49.360 --> 0:14:53.140
slices we don't have slices that basically have pointers to other slices so we have better

237
0:14:53.140 --> 0:15:00.560
memory locality and now we're at 46 milliseconds and that is cool so 46 milliseconds is basically

238
0:15:00.560 --> 0:15:05.320
the time frame that can be acceptable for a database.

239
0:15:05.320 --> 0:15:10.200
So quickly in recap we immediately ruled out JSON because it just wasn't space efficient

240
0:15:10.200 --> 0:15:13.820
and we knew that we needed something more space efficient and also way faster binary

241
0:15:13.820 --> 0:15:19.360
encoding already made it much faster which is great but if we decode it upfront then

242
0:15:19.360 --> 0:15:24.040
yeah we still lost a lot of time and it can be worth it in these kind of high performance

243
0:15:24.040 --> 0:15:28.720
situations if you either sort of delay the decoding as late as possible until you really

244
0:15:28.720 --> 0:15:33.080
need it or just don't do it at all or do it in sort of small parts where we need it.

245
0:15:33.080 --> 0:15:38.560
No wish list here but an honorary mention so Go 1.20 they've actually removed it from

246
0:15:38.560 --> 0:15:43.440
the from the release notes because it's so experimental but Go 1.20 has support for memory

247
0:15:43.440 --> 0:15:48.400
arenas the idea for memory arenas is basically that you can bypass the garbage collector

248
0:15:48.400 --> 0:15:52.200
and sort of manually free that data so if you have something that you know has the same

249
0:15:52.200 --> 0:15:56.440
sort of life cycle then you can say okay put it in the arena and basically in the end free

250
0:15:56.440 --> 0:16:01.440
the entire arena which would sort of bypass the garbage collector so that could also be

251
0:16:01.440 --> 0:16:05.400
a solution in this case if that ever makes it like right now it's super experimental

252
0:16:05.400 --> 0:16:09.840
and it basically tells you we might just remove it so don't use it.

253
0:16:09.840 --> 0:16:15.280
Point stop is something that when I first heard it almost sounded like too good to be

254
0:16:15.280 --> 0:16:17.680
true so something called SIMD.

255
0:16:17.680 --> 0:16:22.120
I'll get to what that is in a second but first question to the audience who here remembers

256
0:16:22.120 --> 0:16:28.680
this thing raise your hands okay cool so you're just as old as I am so this is the Intel

257
0:16:28.680 --> 0:16:36.000
Pentium 2 processor and this came out in late 90s I think 1997 and was sold for a couple

258
0:16:36.000 --> 0:16:40.760
of years and back then I did not build databases definitely not in Go because that also didn't

259
0:16:40.760 --> 0:16:45.260
exist yet but what I would do was sort of try to play 3D video games and I would urge

260
0:16:45.260 --> 0:16:49.720
my parents to get one of those new computers with an Intel Pentium 2 processor and one

261
0:16:49.720 --> 0:16:53.160
of the arguments that I could have used in that discussion was hey it comes with MMX

262
0:16:53.160 --> 0:16:59.440
technology and of course I had no idea what that is and it probably took me 10 or so more

263
0:16:59.440 --> 0:17:04.240
years to find out what MMX is but it's the first in a long list of SIMD instructions

264
0:17:04.240 --> 0:17:07.720
I haven't explained what SIMD is yet but I will in a second.

265
0:17:07.720 --> 0:17:12.320
Some of those especially the one in the top line they're not really used anymore these

266
0:17:12.320 --> 0:17:17.920
days but the bottom line like AVX2 and AVX512 you may have heard them in fact for many open

267
0:17:17.920 --> 0:17:22.640
source projects sometimes you sort of slap that label and read me like yeah yeah has

268
0:17:22.640 --> 0:17:27.440
AVX2 optimizations and that kind of signals you yeah we care about speed because it's

269
0:17:27.440 --> 0:17:32.020
like low level optimized and VVA does the exact same thing by the way.

270
0:17:32.020 --> 0:17:37.000
So to understand how we could make use of that I quickly need to talk about vector embeddings

271
0:17:37.000 --> 0:17:42.560
because I said before that VVA doesn't search through data by keywords but rather through

272
0:17:42.560 --> 0:17:47.600
its meaning and it uses vector embeddings as a tool for that so this is basically just

273
0:17:47.600 --> 0:17:52.120
a long list of numbers in this case floats and then a machine learning model comes in

274
0:17:52.120 --> 0:17:56.320
and basically it says do something with my input and then you get this vector out and

275
0:17:56.320 --> 0:17:59.880
if you do this on all the objects then you can compare your vectors so you basically

276
0:17:59.880 --> 0:18:04.480
can do a vector similarity comparison and that tells you if something is close to one

277
0:18:04.480 --> 0:18:08.840
another or not so for example the query and the object that we have before.

278
0:18:08.840 --> 0:18:14.800
So without any SIMD we can use something called the dot product the dot product is a simple

279
0:18:14.800 --> 0:18:20.440
calculation where basically you use you multiply each element of the first vector with the

280
0:18:20.440 --> 0:18:25.220
same corresponding element of the second vector and then you just sum up all of those elements

281
0:18:25.220 --> 0:18:31.000
and we can think of this like multiplication and summing as two instructions so if we look

282
0:18:31.000 --> 0:18:35.520
out first shout out here to the compiler explorer which is a super cool tool to see like what

283
0:18:35.520 --> 0:18:40.600
your go code compiles to we can see that this indeed turns into two instructions so this

284
0:18:40.600 --> 0:18:44.360
is a bit of a lie because there's more stuff going on because it's in a loop etc but let's

285
0:18:44.360 --> 0:18:50.600
just pretend that indeed we have these two instructions to multiply it and to add it.

286
0:18:50.600 --> 0:18:56.040
So how could we possibly optimize this even further if we're already at such a low level

287
0:18:56.040 --> 0:19:01.720
well we can because this is our mad journey so all we have to do is introduce some madness

288
0:19:01.720 --> 0:19:08.620
and what we're doing now is a practice that's called unrolling so the idea here is that

289
0:19:08.620 --> 0:19:13.120
instead of looping over one element at a time we're now looping over eight elements at a

290
0:19:13.120 --> 0:19:17.360
time but we've gained nothing like this is we're still doing the same kind of work like

291
0:19:17.360 --> 0:19:21.440
we're doing 16 instructions now in a single loop and we're just doing fewer iterations

292
0:19:21.440 --> 0:19:27.160
so by this point nothing gained but why would we do that well here comes the part where

293
0:19:27.160 --> 0:19:32.720
I thought it was too good to be true what if we could do those 16 operations for the

294
0:19:32.720 --> 0:19:38.800
cost of just two instructions sounds crazy right well no because Cindy I'm finally revealing

295
0:19:38.800 --> 0:19:44.000
what the acronym stands for it stands for single instructions multiple data and that

296
0:19:44.000 --> 0:19:48.000
is exactly what we're doing here so we want to do the same thing over and over again which

297
0:19:48.000 --> 0:19:54.040
is multiplication and then additions and this is exactly what these SIMD instructions provide

298
0:19:54.040 --> 0:19:58.520
so in this case we can multiply eight floats with other eight floats and then we can add

299
0:19:58.520 --> 0:20:06.320
them up so all this perfect here maybe not because there's a catch of course it's our

300
0:20:06.320 --> 0:20:13.960
mad journey how do you tell go to use these AVX two instructions you don't you write assembly

301
0:20:13.960 --> 0:20:19.000
code because go has no way to do that directly the good part is that assembly code integrates

302
0:20:19.000 --> 0:20:23.960
really nicely into go and in the in the standard library it's used over and over again so it's

303
0:20:23.960 --> 0:20:29.480
kind of a standard practice and there is tooling here so shout out to avo really cool too that

304
0:20:29.480 --> 0:20:35.040
helps you basically you're still writing assembly with with avo but you're writing it in go

305
0:20:35.040 --> 0:20:38.080
and then it generates the assembly so you still need to know what you're doing but it's

306
0:20:38.080 --> 0:20:47.400
like it protects you a bit so it definitely helped us a lot so SIMD recap using avx instructions

307
0:20:47.400 --> 0:20:52.960
or other SIMD instructions you can basically trick your CPU into doing more work for free

308
0:20:52.960 --> 0:20:58.000
but you need to sort of also trick go to use assembly and with this tooling such as avo

309
0:20:58.000 --> 0:21:02.520
it can be better but it would be even nicer if the language had some sort of support for

310
0:21:02.520 --> 0:21:06.080
it and you made my saying now okay this is this mad guy on stage that wants to build

311
0:21:06.080 --> 0:21:10.480
a database but no one else does needs that but we have this issue here that was opened

312
0:21:10.480 --> 0:21:14.960
recently and unfortunately also most recently because no consensus could be reached but

313
0:21:14.960 --> 0:21:19.800
it comes up back and back basically that go users are saying like hey we want something

314
0:21:19.800 --> 0:21:24.720
in the language such as intrinsic so intrinsics are basically the idea of having high level

315
0:21:24.720 --> 0:21:30.920
language instructions to do these these sort of avx or SIMD instructions and C or C++ has

316
0:21:30.920 --> 0:21:36.120
that for example one way to do that and maybe you're wondering like okay if you have such

317
0:21:36.120 --> 0:21:40.560
a performance hot path like why don't you just write that in C and you see go or write

318
0:21:40.560 --> 0:21:45.520
it in Rust or something like that sounds good in theory but the problem is that the call

319
0:21:45.520 --> 0:21:51.320
overhead to call C or C++ is so high that you actually have to outsource quite a bit

320
0:21:51.320 --> 0:21:56.240
of your code for that to to pay off again so if you do that you basically end up writing

321
0:21:56.240 --> 0:22:00.760
more and more and more in that language and then you're not writing go anymore so fortunately

322
0:22:00.760 --> 0:22:07.960
that's not or it can be in some ways but it's not always a great idea so demo time this

323
0:22:07.960 --> 0:22:12.480
was going to be a live demo and maybe it still is because I prepared this running nicely

324
0:22:12.480 --> 0:22:16.680
in a docker container and then my docker network just broke everything and it didn't work but

325
0:22:16.680 --> 0:22:21.320
I just rebuild it without docker and I think it might work if not I have screenshots basically

326
0:22:21.320 --> 0:22:28.560
that do a backup so example query here I'm a big wine nerd so what I did is I put wine

327
0:22:28.560 --> 0:22:33.440
reviews into vv8 and I want to search them now and one way to do it to show you basically

328
0:22:33.440 --> 0:22:38.080
that the keyword that you don't need a keyword match but can search by meaning is for example

329
0:22:38.080 --> 0:22:47.040
if I go for an affordable Italian wine let's see if the internet connection works it does

330
0:22:47.040 --> 0:22:52.960
so what we got back is basically this this a wine review that I wrote about a Barolo

331
0:22:52.960 --> 0:22:57.880
that I recently drank it and you can see it doesn't say Italy anywhere it doesn't say

332
0:22:57.880 --> 0:23:02.840
affordable what it says like without breaking the bank so this is a vector search that basically

333
0:23:02.840 --> 0:23:08.720
happened in the in the background we can take this one step further by using the generative

334
0:23:08.720 --> 0:23:15.040
side so this is basically the chat GPT part we can now ask for database based on the review

335
0:23:15.040 --> 0:23:19.080
which is what I wrote when is this wine going to be ready to drink so let's see you saw

336
0:23:19.080 --> 0:23:22.120
before that was the failed career when the internet didn't work now now it's actually

337
0:23:22.120 --> 0:23:27.000
working so that's nice and here in this case you can see that so this is using open AI

338
0:23:27.000 --> 0:23:31.560
but you can plug in other tools can plug in open source versions of it this is using open

339
0:23:31.560 --> 0:23:35.880
AI because that's nice to be hosted at a service I don't have to run the machine learning model

340
0:23:35.880 --> 0:23:39.680
on my laptop then you can see it tells you the wine is not ready to drink yet we will

341
0:23:39.680 --> 0:23:43.120
need at least five more years which is sort of a good summary of this and then you can

342
0:23:43.120 --> 0:23:47.240
see another one is ready to drink right now it's in the perfect drinking window so for

343
0:23:47.240 --> 0:23:52.880
the final demo let's combine those two let's do a semantic search to identify something

344
0:23:52.880 --> 0:23:59.000
and then do an AI generation basically so in this case we're saying find me an aged

345
0:23:59.000 --> 0:24:04.760
classic Riesling best best wine in the world Riesling and based on the review would you

346
0:24:04.760 --> 0:24:08.880
consider this wine to be a fruit bomb so let's have sort of an opinion from the machine learning

347
0:24:08.880 --> 0:24:16.440
model in it and here we got one of my favorite wines and the model says no I would not consider

348
0:24:16.440 --> 0:24:20.560
this a fruit bomb while it does have some fruity notes it is balanced by the minerality

349
0:24:20.560 --> 0:24:25.240
and acidity which keeps it from being overly sweet or fruity which is if you read the text

350
0:24:25.240 --> 0:24:30.480
like this is nowhere in there so this is kind of cool that the model was able to do this

351
0:24:30.480 --> 0:24:35.600
okay so let's go back now is the demo time by the way have a GitHub repo with like this

352
0:24:35.600 --> 0:24:43.600
example so you can run it yourself and yeah try it out yourself so this was our mad journey

353
0:24:43.600 --> 0:24:49.960
and are we mad at go are we mad to do this well I would pretty much say no because yes

354
0:24:49.960 --> 0:24:54.920
there were a couple of parts we have to give get really creative and have to do some some

355
0:24:54.920 --> 0:25:00.320
yeah rather unique stuff but that was also basically like the highlight reload building

356
0:25:00.320 --> 0:25:04.560
database and all the other parts like it didn't even show the parts that went great like concurrency

357
0:25:04.560 --> 0:25:09.560
handling and the powerful standard library and of course all of you basically representing

358
0:25:09.560 --> 0:25:15.160
the gopher community which is super helpful and yeah this was my way to basically give

359
0:25:15.160 --> 0:25:19.520
back to all of you so if you ever want to build a database or run into other kind of

360
0:25:19.520 --> 0:25:21.680
high performance problems then maybe some of those

