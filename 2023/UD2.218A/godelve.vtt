WEBVTT

00:00.000 --> 00:09.120
Okay, thank you.

00:09.120 --> 00:11.360
We have two traditions here in the Go Dev Room.

00:11.360 --> 00:14.400
That is that we start with a state of Go and then it's around lunchtime.

00:14.400 --> 00:18.220
We always have the next state, which is the state of Delve.

00:18.220 --> 00:20.640
So let's all get into Delve.

00:20.640 --> 00:29.880
Let's go debugging.

00:29.880 --> 00:32.360
You might be muted to that me quickly.

00:32.360 --> 00:33.840
There is a switch on there.

00:33.840 --> 00:34.840
Test.

00:34.840 --> 00:35.840
Good?

00:35.840 --> 00:36.840
Perfect.

00:36.840 --> 00:37.840
All right, let's try this again.

00:37.840 --> 00:38.840
Hello, everybody.

00:38.840 --> 00:40.960
My name is Derek Parker.

00:40.960 --> 00:49.240
I am the author of Delve and I continue to maintain the project along with my lead co-maintainer,

00:49.240 --> 00:53.440
Alessandro, who is also in the crowd today.

00:53.440 --> 01:00.840
And as mentioned, it's been kind of a tradition here at Bosdom to piggyback on the state

01:00.840 --> 01:03.240
of Go and talk about the state of Delve.

01:03.240 --> 01:06.400
So this talk will be kind of a two-parter.

01:06.400 --> 01:11.040
I'll start with the state of Delve and then I'll go into the main talk, which is debugging

01:11.040 --> 01:12.760
Go programs with eBPF.

01:12.760 --> 01:20.400
Now if you're unfamiliar with what eBPF is as a technology, fret not.

01:20.400 --> 01:25.960
I will go in and kind of explain it in more detail throughout the course of the talk as

01:25.960 --> 01:30.800
we kind of get into the real meat of everything.

01:30.800 --> 01:34.320
So just to introduce myself a little bit more, again, my name is Derek Parker.

01:34.320 --> 01:38.040
I'm a senior software engineer at Red Hat.

01:38.040 --> 01:42.100
If you would like to follow me, I am Dirk the Daring on Twitter.

01:42.100 --> 01:48.960
And at Red Hat I work on Delve and Go itself.

01:48.960 --> 01:54.920
So the first thing that I want to start and talk about is the state of Delve.

01:54.920 --> 02:02.560
So what I'll go through is essentially what's changed since the last Bosdom and actually

02:02.560 --> 02:04.800
since the last in-person Bosdom.

02:04.800 --> 02:10.840
So I was actually here in 2020 presenting a different talk before the world ended.

02:10.840 --> 02:16.980
And I'm happy to be here again in person with everybody and being able to kind of talk and

02:16.980 --> 02:18.760
catch up and present these things.

02:18.760 --> 02:21.760
So thanks everybody for coming and for attending this talk.

02:21.760 --> 02:30.440
I really appreciate it.

02:30.440 --> 02:34.880
So one of the big milestones that I kind of want to call out is that Delve turns nine

02:34.880 --> 02:36.400
this year.

02:36.400 --> 02:40.400
So to celebrate that on the count of three, everybody in the room, we're going to sing

02:40.400 --> 02:41.400
happy birthday.

02:41.400 --> 02:45.180
One, two, I'm just kidding.

02:45.180 --> 02:46.880
Maybe next year for the 10th anniversary.

02:46.880 --> 02:49.480
I'll hold that off for a little bit.

02:49.480 --> 02:55.560
But the Delve project was started in 2014.

02:55.560 --> 02:59.320
And yeah, it turns nine still going strong.

02:59.320 --> 03:03.880
And I appreciate everybody who uses it, contributes to it.

03:03.880 --> 03:07.240
It's just really fantastic to see.

03:07.240 --> 03:13.720
So I'll go into some statistics a little bit about what's happened in the last couple of

03:13.720 --> 03:14.720
years.

03:14.720 --> 03:19.240
So as far as in-person FOSDAM, we've done 18 different releases.

03:19.240 --> 03:27.480
Now the way we do releases of Delve is somewhat scheduled, somewhat ad hoc.

03:27.480 --> 03:35.800
So we always produce a new release when the first release candidate of the new Go version

03:35.800 --> 03:36.800
comes out.

03:36.800 --> 03:43.200
So anytime a new Go version comes out, we ensure that the day that it's released, you

03:43.200 --> 03:44.200
can debug it.

03:44.200 --> 03:47.680
Once you compile your code, do everything, you have a debugger that's going to work with

03:47.680 --> 03:49.880
that version.

03:49.880 --> 03:53.840
And then aside from that, we have kind of minor releases that come out throughout the

03:53.840 --> 03:54.840
year.

03:54.840 --> 04:00.680
And in between the releases, the fixed bugs add new features, things like that.

04:00.680 --> 04:07.760
So within that time frame, we've added support for numerous different Go versions.

04:07.760 --> 04:10.880
So 114 all the way through 120.

04:10.880 --> 04:14.920
And as I mentioned, 120 was just released the other day.

04:14.920 --> 04:17.800
We've supported it since the first RC.

04:17.800 --> 04:24.600
So you always have a debugger to kind of go through your code even before the official

04:24.600 --> 04:28.880
release actually comes out.

04:28.880 --> 04:33.400
During that time, we've also added four new operating system and architecture combinations.

04:33.400 --> 04:40.240
So with Delve, we strive to enable you to debug on any operating system and architecture

04:40.240 --> 04:42.800
that Go itself supports.

04:42.800 --> 04:46.960
We're getting closer and closer to that goal with each passing release.

04:46.960 --> 04:51.880
So I'm proud to say, you know, within the last few years, we added support for four

04:51.880 --> 04:54.560
new platforms.

04:54.560 --> 05:02.520
And there's a few more already in the works, and we'll be releasing later this year.

05:02.520 --> 05:07.480
So I want to call out a few major new features that have been developed.

05:07.480 --> 05:14.760
The first is we've integrated a DAP server into Delve, which is probably not something

05:14.760 --> 05:19.880
that's super relevant to everybody here unless you're like the author of like an editor or

05:19.880 --> 05:20.960
something like that.

05:20.960 --> 05:23.160
It's really for editor integration.

05:23.160 --> 05:31.200
But from a user's perspective, it really improves the usability of Delve within editors, such

05:31.200 --> 05:35.200
as VS Code and things like that.

05:35.200 --> 05:42.200
We've added support for Apple Silicon, and that happened, you know, really quickly once

05:42.200 --> 05:48.400
we were able to kind of get our hands on the hardware and everything like that.

05:48.400 --> 05:51.320
We added the ability to generate core dumps from running processes.

05:51.320 --> 05:56.600
So while you're in a debug session, you can ad hoc, generate a core dump, save that away,

05:56.600 --> 06:00.920
and use that, debug it later.

06:00.920 --> 06:05.120
We've added support for hardware watch points, which I think is a really, really cool feature.

06:05.120 --> 06:13.800
And kind of difficult to do with Go due to some kind of internal things of how Go kind

06:13.800 --> 06:18.760
of looks at the stack and changes stack and stuff like changes Go routine stacks as the

06:18.760 --> 06:20.640
stack grows, things like that.

06:20.640 --> 06:24.000
But we were able to implement them and get them working.

06:24.000 --> 06:29.120
So if you're unaware of what hardware watch points are, it's a really cool feature where

06:29.120 --> 06:34.200
you can say like, I want to watch this particular variable or this particular address and memory,

06:34.200 --> 06:38.840
and I want to know, I want the debugger to stop any time that value is read or changed,

06:38.840 --> 06:39.840
right?

06:39.840 --> 06:45.520
So it's, you're basically just saying like, telling the debugger what you want to do and

06:45.520 --> 06:47.800
letting it do the heavy lifting for you.

06:47.800 --> 06:51.160
Really cool feature.

06:51.160 --> 06:56.280
And as was just shown in the previous talk, we've improved some of the filtering and grouping

06:56.280 --> 06:57.800
for Go routine output.

06:57.800 --> 07:01.720
So you can filter by label, you can filter by all different kinds of things.

07:01.720 --> 07:07.320
So in massively concurrent and parallel programs where you might have tons and tons of different

07:07.320 --> 07:14.500
Go routines, we've improved a lot of the introspection on that and being able to kind of filter out

07:14.500 --> 07:19.080
and get the information that you really need.

07:19.080 --> 07:22.360
We've also added an experimental EBPF-based tracing backend.

07:22.360 --> 07:26.840
So that's what I'm going to be talking about today.

07:26.840 --> 07:29.680
And we also added support for debug info find.

07:29.680 --> 07:34.080
So this is really cool for a lot of operating systems where maybe you're debugging a package

07:34.080 --> 07:39.960
that you installed via your package manager and the like the dwarf, the debug information

07:39.960 --> 07:41.480
is not included with the binary.

07:41.480 --> 07:44.040
Maybe it's in a different package or something like that.

07:44.040 --> 07:48.360
We've integrated with debug info find to be able to automatically download those debug

07:48.360 --> 07:54.440
packages for you so that you can have a fruitful and successful debugging session.

07:54.440 --> 07:56.040
And there's also been a lot more.

07:56.040 --> 08:00.760
If you want a look at all of the details, go ahead and check out the changelog and the

08:00.760 --> 08:01.760
repo.

08:01.760 --> 08:03.560
It'll detail all of the changes that we've made.

08:03.560 --> 08:12.120
Next thing I want to talk about is a few little upcoming features that I want to tease.

08:12.120 --> 08:17.520
So one of the biggest ones is we're working on support for two new architectures.

08:17.520 --> 08:23.200
So PowerPC 64LE and S390X.

08:23.200 --> 08:29.300
My colleague Alejandro is working on the PowerPC 64LE port and he's in the crowd as well.

08:29.300 --> 08:34.680
So thank you for your work on that.

08:34.680 --> 08:37.320
We're looking at some more improvements to the EBPF tracing backend.

08:37.320 --> 08:42.520
I'll go into some more detail on that as well during this talk.

08:42.520 --> 08:48.000
We're also working on the ability to debug multiple processes simultaneously.

08:48.000 --> 08:53.440
My co-maintainer Alejandro is working on that and we're hoping to land that pretty

08:53.440 --> 08:54.440
soon.

08:54.440 --> 09:00.160
So that would be like if your process forks or anything like that creates new child processes,

09:00.160 --> 09:05.400
you can debug all of them within a single session.

09:05.400 --> 09:09.360
Another thing that we want to work on this year is improved function call support across

09:09.360 --> 09:10.360
architectures.

09:10.360 --> 09:15.680
So that was a big feature that landed in Delve as well, the ability to call a function while

09:15.680 --> 09:18.360
you're debugging your program.

09:18.360 --> 09:20.720
It's very architecture specific.

09:20.720 --> 09:24.960
So one of the things that we want to do throughout this year is improve support for that across

09:24.960 --> 09:27.600
different architectures.

09:27.600 --> 09:28.960
And there's tons more.

09:28.960 --> 09:35.640
We're always working on new things and we also always try to gather community feedback

09:35.640 --> 09:37.240
and user feedback and stuff like that.

09:37.240 --> 09:44.280
So since I'm here and other maintainers of Delve are here, if you want to come and tell

09:44.280 --> 09:47.200
us something that you would like us to implement or something that you would like to focus

09:47.200 --> 09:52.440
on, feel free to come chat with us.

09:52.440 --> 09:58.600
So now with that said and done, I want to move on to the real portion of this talk which

09:58.600 --> 10:03.440
is debugging and tracing Go programs with EBPF.

10:03.440 --> 10:09.720
Now it's really cool that this talk comes after the talk right before because I think

10:09.720 --> 10:13.960
like the tracing feature in Delve is somewhat underutilized.

10:13.960 --> 10:18.640
And I think it's really good for debugging concurrent programs and seeing the interactions

10:18.640 --> 10:21.940
between Go routines as your program is actually running.

10:21.940 --> 10:29.240
So if you're unfamiliar with what tracing is, I'll show a little demo, but essentially

10:29.240 --> 10:34.120
what we're talking about here is instead of going into a full on debug session, what you're

10:34.120 --> 10:36.280
really doing is kind of like spying on your program.

10:36.280 --> 10:40.480
So if you're familiar with S trace, it's the same concept except for the functions that

10:40.480 --> 10:44.560
you're writing as opposed to the interactions with the kernel, right, like the system calls

10:44.560 --> 10:45.880
and things like that.

10:45.880 --> 10:52.400
So you can kind of spy and see like what functions are being executed, what are their inputs,

10:52.400 --> 10:58.040
what are the outputs, what Go routines are executing that function, so on and so forth.

10:58.040 --> 11:08.920
So to show a little demo of it real quick, let me increase my screen size a little bit.

11:08.920 --> 11:15.880
It may still be hard for folks in the back to see, but hopefully that's good enough.

11:15.880 --> 11:20.680
So what I've done here is instead of typing everything directly on the console, I've created

11:20.680 --> 11:25.320
a little makefile just so that you can see kind of the commands up there and they don't

11:25.320 --> 11:26.880
disappear as I run them.

11:26.880 --> 11:30.640
But the first thing that we're going to do is we're just going to run a simple trace.

11:30.640 --> 11:36.520
So to do this, we use the trace subcommand of Delve and what you provide to it as an

11:36.520 --> 11:43.640
argument is a regular expression and what Delve will do internally is set a trace point

11:43.640 --> 11:45.860
on any function that matches that regular expression.

11:45.860 --> 11:53.160
So you can do something like main.star to trace anything in the main package, extrapolate

11:53.160 --> 11:56.900
that out to any other package, and it's a really cool feature.

11:56.900 --> 12:03.680
So just to kind of show how it works, we can go here and say make trace and we see the

12:03.680 --> 12:04.680
output there.

12:04.680 --> 12:10.520
So to explain the output a little bit, you have like the single line or the single arrow

12:10.520 --> 12:15.140
is the call, the double arrow is the return.

12:15.140 --> 12:20.960
You can see there it labels what go routine is running and calling that function.

12:20.960 --> 12:25.380
You can see the arguments to that function and then you can also see the return value.

12:25.380 --> 12:29.260
So again, really cool and useful for like if you have a bunch of different go routines,

12:29.260 --> 12:34.360
you can kind of see the interactions of them and see what go routines are doing at any

12:34.360 --> 12:36.240
given time.

12:36.240 --> 12:42.320
Another option that you can do is you can say if you pass the stack flag and give it

12:42.320 --> 12:46.680
an argument, you can get a stack trace any time one of the trace points are hit.

12:46.680 --> 12:55.520
So if we say trace with stack, you see we get kind of a similar output but we get a

12:55.520 --> 12:56.520
stack trace as well.

12:56.520 --> 13:03.040
So you can kind of see a little bit more detailed information as your program is being traced.

13:03.040 --> 13:14.400
So, the real like meat of this talk is how we improve the tracing back end to make it

13:14.400 --> 13:22.040
more efficient because what you, especially when you're doing something like tracing and

13:22.040 --> 13:25.440
things like that, the lower overhead the better, right?

13:25.440 --> 13:30.440
We don't want to make your program run significantly slower because that's just going to frustrate

13:30.440 --> 13:33.920
you and it's going to take longer to get to root cause analysis which is what you're really

13:33.920 --> 13:36.120
trying to do if you're using a debugger in the first place.

13:36.120 --> 13:41.620
So we'll talk about quickly how things are currently implemented and then how we can

13:41.620 --> 13:44.760
improve upon that using eBPF.

13:44.760 --> 13:52.780
So right now, Delve uses, or traditionally Delve uses Ptrace syscall to implement the

13:52.780 --> 13:53.940
tracing back end.

13:53.940 --> 13:59.560
It's how Ptrace is useful for, like it's used by pretty much every debugger, every kind

13:59.560 --> 14:01.040
of tool like this.

14:01.040 --> 14:04.240
Delve is no exception.

14:04.240 --> 14:08.360
And if you look at the man page, it'll explain a little bit more about what it is, but it

14:08.360 --> 14:12.400
essentially allows you to control the execution of another process and kind of examine the

14:12.400 --> 14:16.080
state of it, memory and things like that.

14:16.080 --> 14:19.400
So the problem is Ptrace is slow.

14:19.400 --> 14:22.560
And it can be very slow.

14:22.560 --> 14:29.920
So I ran some tests kind of a while ago when I was implementing the first iteration of

14:29.920 --> 14:32.160
this eBPF back end.

14:32.160 --> 14:39.720
And I measured a simple program execution that executed in 23.7 microseconds.

14:39.720 --> 14:44.080
And then the overhead with the Ptrace base tracing, the traditional base tracing, it

14:44.080 --> 14:45.560
went up to 2.3 seconds.

14:45.560 --> 14:48.840
So that's several orders of magnitude of overhead, right?

14:48.840 --> 14:51.600
Which is definitely not what you want.

14:51.600 --> 14:59.040
But why is Ptrace so slow?

14:59.040 --> 15:04.200
So part of the reason is syscall overhead.

15:04.200 --> 15:07.680
We have to, you know, Ptrace is a syscall.

15:07.680 --> 15:12.320
So whenever you invoke a syscall, you trap into the kernel, you switch context, right?

15:12.320 --> 15:19.840
So that has its own kind of overhead, which can be pretty significant.

15:19.840 --> 15:23.760
And as I mentioned, the user space kernel context switching, the overhead of that can

15:23.760 --> 15:26.400
be really expensive.

15:26.400 --> 15:34.840
And it's amplified by the fact that Ptrace is in a sense very like directed.

15:34.840 --> 15:42.760
So when we're tracing these functions, we often have to make multiple Ptrace calls per

15:42.760 --> 15:45.560
like function entry and function exit.

15:45.560 --> 15:50.000
So if you think about it, we need to read the registers, we need to read all of the

15:50.000 --> 15:54.840
different like function arguments that are there.

15:54.840 --> 15:56.680
There's a bunch of different things that we need to do.

15:56.680 --> 16:01.760
So it kind of balloons up really, really quickly where we get into this situation where we're

16:01.760 --> 16:06.400
doing a ton of these user space kernel context switching per every time you hit one of these

16:06.400 --> 16:11.480
trace points.

16:11.480 --> 16:17.100
And on top of that, all of these operations have to happen twice per function, right?

16:17.100 --> 16:19.000
So the entry and the exit.

16:19.000 --> 16:25.200
So it's a lot of overhead, a lot of context switching, essentially a lot of unnecessary

16:25.200 --> 16:31.560
work and a lot of work that just slows down your program and adds a lot of overhead.

16:31.560 --> 16:40.920
So the way that we can improve upon this and work around this is by using eBPF.

16:40.920 --> 16:47.000
So eBPF is a lot more efficient, a lot quicker to do this kind of work.

16:47.000 --> 16:54.760
So with the same test, again, as I mentioned before, the original program, 23 microseconds

16:54.760 --> 16:58.520
with Ptrace, 2.3 actual seconds.

16:58.520 --> 17:04.560
And with the eBPF-based tracing, we have like 683 microseconds, which is still measurable

17:04.560 --> 17:11.320
overhead but significantly less than the traditional method of doing it.

17:11.320 --> 17:17.120
So I've been talking about this technology a lot, eBPF, eBPF, eBPF, right?

17:17.120 --> 17:20.360
But what actually is it?

17:20.360 --> 17:29.480
So eBPF is a technology that enables the kernel to run sandbox programs directly.

17:29.480 --> 17:35.920
So eBPF programs are written primarily in like a limited C. I'll get into some of the

17:35.920 --> 17:39.640
limitations later.

17:39.640 --> 17:44.840
But it gets compiled to a bytecode, loaded into the kernel where it's executed and jitted

17:44.840 --> 17:48.240
as it's ran.

17:48.240 --> 17:53.200
And it has a lot of use cases, observability, networking, debugging, and a lot more.

17:53.200 --> 17:54.880
So you'll hear a lot about eBPF.

17:54.880 --> 18:00.360
I'm sure a lot of folks in this room have already heard of it in some shape or another.

18:00.360 --> 18:07.560
Typically it started as a technology for networking and kind of ballooned from there.

18:07.560 --> 18:16.400
So originally it was like BPF, which is Berkeley packet filtering, and it came into extended

18:16.400 --> 18:17.400
Berkeley packet filtering.

18:17.400 --> 18:19.680
And now the acronym doesn't really mean anything anymore.

18:19.680 --> 18:25.440
eBPF is just eBPF because it's way more than just what it originally was.

18:25.440 --> 18:29.360
And the cool thing is these programs that are loaded into the kernel, they can be triggered

18:29.360 --> 18:30.520
by certain events.

18:30.520 --> 18:34.760
And I'll talk about how we can trigger those events ourselves.

18:34.760 --> 18:41.720
But they run in response to something happening.

18:41.720 --> 18:48.240
So why is eBPF so fast in comparison to the way that we're traditionally doing things?

18:48.240 --> 18:52.560
The first thing is these eBPF programs run in the kernel.

18:52.560 --> 18:59.440
So there's a lot less context switching overhead.

18:59.440 --> 19:02.960
We're already in the kernel, so we don't have to keep asking the kernel for more and more

19:02.960 --> 19:08.240
and more and more information to get what we actually want.

19:08.240 --> 19:12.560
Relative to traditional syscall and a bunch of syscalls, the context switching is a lot

19:12.560 --> 19:15.580
cheaper.

19:15.580 --> 19:23.500
You get small targeted programs that, again, execute really quickly and can do everything

19:23.500 --> 19:29.080
that you need or want to do in essentially one shot.

19:29.080 --> 19:33.960
And a single program can execute many tasks that we would traditionally use multiple ptrace

19:33.960 --> 19:34.960
calls for.

19:34.960 --> 19:40.920
So you can, like, you have access to the current registers.

19:40.920 --> 19:47.720
You can read memory and a lot of other things like that.

19:47.720 --> 19:54.120
Now when I was looking to implement this back end, I had a few requirements that I wanted

19:54.120 --> 19:59.840
to make sure can be satisfied with this eBPF-based approach.

19:59.840 --> 20:03.840
So the first one was the ability to trace arbitrary functions, right?

20:03.840 --> 20:07.320
As a user, you just want to say, I want to trace everything in the main package.

20:07.320 --> 20:12.840
Or I want to trace this specific function or whatever.

20:12.840 --> 20:17.320
This new back end had to be able to satisfy that requirement as well.

20:17.320 --> 20:23.800
We had to be able to retrieve the go routine ID from within the eBPF program.

20:23.800 --> 20:27.240
We had to be able to read function input arguments.

20:27.240 --> 20:33.800
And we had to be able to read function return arguments.

20:33.800 --> 20:39.320
Now let's talk a little bit about tracing arbitrary functions.

20:39.320 --> 20:46.120
So just as a little bit of background, how Delve uses eBPF from the go side of things

20:46.120 --> 20:48.720
is we use the cilium eBPF package.

20:48.720 --> 20:55.040
There's a few other go-based eBPF packages out there.

20:55.040 --> 20:58.960
Originally it was implemented, I implemented using one from Aqua security but ended up

20:58.960 --> 21:04.400
switching to cilium for a few various different reasons.

21:04.400 --> 21:09.520
But the first thing that we need to do when we're tracing these arbitrary functions is

21:09.520 --> 21:14.600
we need to first load the eBPF program into the kernel so that we can start triggering

21:14.600 --> 21:17.200
it with some of these events.

21:17.200 --> 21:22.880
Once we've loaded the eBPF program, we attach UProbes to each symbol.

21:22.880 --> 21:27.120
And this slide is actually a little bit outdated because we don't actually use URepProbes.

21:27.120 --> 21:35.720
So UProbes can kind of be attached arbitrarily to different addresses and things like that

21:35.720 --> 21:36.720
within the binary.

21:36.720 --> 21:44.200
URepProbes are typically used to hook into the return of a function, which seems like

21:44.200 --> 21:46.120
something that would be super, super useful.

21:46.120 --> 21:47.240
And in theory it is.

21:47.240 --> 21:52.760
But with go it doesn't work very well because of how go manages go routine stacks.

21:52.760 --> 22:04.760
So when go has to inspect the stack, it kind of reads up the stack to unwind it a little

22:04.760 --> 22:05.860
bit.

22:05.860 --> 22:08.800
And if it sees anything that doesn't look right, it'll panic.

22:08.800 --> 22:17.080
So URepProbes work by overwriting the return address of the function that we're trying

22:17.080 --> 22:19.500
to probe.

22:19.500 --> 22:25.160
And so go notices that during its runtime work and kind of freaks out.

22:25.160 --> 22:27.240
So we just use UProbes.

22:27.240 --> 22:32.240
And again, we want to do as much in the kernel as possible to limit overhead.

22:32.240 --> 22:38.800
And we have to communicate function argument and return values to the eBPF program and

22:38.800 --> 22:42.600
get those values back from the eBPF program.

22:42.600 --> 22:44.200
So first we load it.

22:44.200 --> 22:48.080
So first thing we have to do is write the eBPF program.

22:48.080 --> 22:50.880
Second thing, compile the program and generate some helpers.

22:50.880 --> 22:54.100
So this is kind of what the Cilium package helps us with.

22:54.100 --> 22:56.420
And then we have to load the programs in the kernel.

22:56.420 --> 22:58.200
So these are actually links.

22:58.200 --> 22:59.680
I'll publish these slides.

22:59.680 --> 23:02.520
You can kind of follow along at home.

23:02.520 --> 23:05.660
But I'll show a little bit of the code here.

23:05.660 --> 23:16.040
So this is an example of the eBPF program that we use written in like C basically.

23:16.040 --> 23:22.080
And we have access to a bunch of different eBPF based data structures, like maps, ring

23:22.080 --> 23:23.600
buffers.

23:23.600 --> 23:26.960
And these are just different ways to be able to communicate with the eBPF program running

23:26.960 --> 23:32.180
in the kernel and the Go program that's running in user space.

23:32.180 --> 23:35.480
So I won't go through all of this exhaustively for time.

23:35.480 --> 23:41.100
But again, if you want to look at it yourself, go ahead and follow the link.

23:41.100 --> 23:46.080
So the second thing that we have to do is go ahead and actually compile this eBPF program

23:46.080 --> 23:48.160
and make it usable from Go.

23:48.160 --> 23:53.760
So the Cilium eBPF package has a really nice helper that you can just use with Go generate

23:53.760 --> 23:57.880
to be able to compile the object file that is your eBPF program.

23:57.880 --> 24:01.840
And it generates a bunch of helpers for you that you can call to be able to load it and

24:01.840 --> 24:06.020
interact with that eBPF program.

24:06.020 --> 24:12.280
And then finally, we have to load the eBPF program into the kernel.

24:12.280 --> 24:19.760
So again, the Cilium eBPF library has a ton of helpers to be able to facilitate that.

24:19.760 --> 24:27.920
So we open up the executable that represents the process that we're debugging.

24:27.920 --> 24:32.440
We call this helper that the package generated for us.

24:32.440 --> 24:38.560
And then we initialize some of the things that we need to do, like the ring buffer and

24:38.560 --> 24:44.880
the map data structure that we use to pass values back and forth.

24:44.880 --> 24:49.540
The next thing we have to do is attach our UProbes.

24:49.540 --> 24:52.360
So first we find an offset to attach to.

24:52.360 --> 24:55.080
We attach the probe to that offset.

24:55.080 --> 24:56.880
And then we go from there.

24:56.880 --> 25:06.160
So we have a little helper here to take an address within the program to an offset.

25:06.160 --> 25:16.420
And the offset is just like an offset within the binary itself as it's loaded into memory.

25:16.420 --> 25:19.720
And then from there, we attach our probe.

25:19.720 --> 25:24.840
And it's as simple as the executable that we opened earlier.

25:24.840 --> 25:29.560
We have that attached to this eBPF context here.

25:29.560 --> 25:36.080
And we just call this UProbe method and pass it the offset and the PID.

25:36.080 --> 25:44.120
So the next thing about this is you pass along the PID so that this eBPF program is constrained

25:44.120 --> 25:46.360
to just the process that you're trying to debug.

25:46.360 --> 25:50.340
Because these programs that you load in are actually global.

25:50.340 --> 25:59.840
So they're not really by themselves attached to any specific process.

25:59.840 --> 26:04.040
Then from there, we need to actually communicate with this program.

26:04.040 --> 26:07.800
So we need to store function parameter information.

26:07.800 --> 26:11.600
And then we need to communicate that information with the program.

26:11.600 --> 26:16.040
Now I won't go too much into the code in this for the sake of time.

26:16.040 --> 26:24.760
But essentially, we need to tell the eBPF program all of the function argument information,

26:24.760 --> 26:29.240
the return value information, where they're located, are they on the stack, are they in

26:29.240 --> 26:34.860
registers, and let it know where to find it so that it can read all of this information

26:34.860 --> 26:39.720
and send it back to the user space program.

26:39.720 --> 26:45.320
When we want to get the data back, we use a ring buffer to, again, communicate between

26:45.320 --> 26:49.040
user space and our program running in kernel space.

26:49.040 --> 26:52.840
And essentially, it's just a stream of all of the information coming back.

26:52.840 --> 26:56.840
So all of the information that's being read and picked up by the eBPF program.

26:56.840 --> 27:03.440
And then that's ultimately what gets displayed to you as we run the trace command.

27:03.440 --> 27:09.480
So I'll go through another quick demo of actually using the eBPF backend.

27:09.480 --> 27:16.540
So all you have to do to enable it for now is just add dash dash eBPF to the trace command.

27:16.540 --> 27:26.000
So if I run our make command here, nobody look at my password.

27:26.000 --> 27:27.980
We see that the trace happens.

27:27.980 --> 27:31.900
And from here, you can't really tell that it's significantly faster.

27:31.900 --> 27:34.020
But the output is a little bit different.

27:34.020 --> 27:39.040
As I mentioned, this is still kind of like an experimental work in progress backend.

27:39.040 --> 27:41.340
So some of the output is a little bit different.

27:41.340 --> 27:48.180
And it doesn't have exact parity with the traditional more established tracing backend.

27:48.180 --> 27:50.580
But you can see it works.

27:50.580 --> 27:54.920
You see the arguments, the return values, and everything like that.

27:54.920 --> 27:59.540
And this is all happening with significantly less overhead.

27:59.540 --> 28:02.880
So a few downsides of the eBPF approach.

28:02.880 --> 28:07.900
The programs are written in a constrained version of C, so you're not writing go.

28:07.900 --> 28:10.420
You end up having to fight the verifier a lot.

28:10.420 --> 28:14.380
If you don't know what that means, that's great for you.

28:14.380 --> 28:18.580
Congratulations.

28:18.580 --> 28:23.580
There's a lot of constraints on stack sizes and stuff like that within eBPF programs,

28:23.580 --> 28:27.980
which is going to be kind of gnarly to deal with.

28:27.980 --> 28:33.500
It's different to write some control flow, like loops and stuff like that.

28:33.500 --> 28:37.820
And as I mentioned, URep probes do not play well with go programs at all.

28:37.820 --> 28:38.940
Do not use them.

28:38.940 --> 28:41.500
Do not try.

28:41.500 --> 28:42.500
And that's it.

28:42.500 --> 28:43.500
Thank you very much.

28:43.500 --> 28:44.500
Thank you.

28:44.500 --> 28:45.580
Unfortunately, we do not have time for questions.

28:45.580 --> 28:58.820
But if you see them in the hallway track, you can always ask him any questions, improvements,

28:58.820 --> 28:59.820
bug fixes, et cetera.

28:59.820 --> 29:16.500
If you leave, it's better to do so on this side.

29:16.500 --> 29:18.020
You may pause the stage.

29:18.020 --> 29:22.180
And there is also a swag table diagram.

29:22.180 --> 29:30.180
It's looking pretty easy.
