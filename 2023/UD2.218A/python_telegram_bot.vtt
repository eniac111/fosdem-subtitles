WEBVTT

00:00.000 --> 00:02.000
I'm going to turn it over to you.

00:06.000 --> 00:08.000
Hello, everybody.

00:10.000 --> 00:13.000
Welcome to the Python, the room of Fuzzdam. I'm really happy to

00:13.000 --> 00:17.000
welcome all of you here and to welcome Mark for his talk and

00:17.000 --> 00:21.000
introduction to async programming. Thanks for everyone

00:21.000 --> 00:25.000
for coming. Also, so early for this talk that I'm really

00:25.000 --> 00:27.000
looking forward to see with you.

00:27.000 --> 00:31.000
Thank you for the introduction. It's really nice to see so many

00:31.000 --> 00:35.000
people here on a Sunday morning at 9 o'clock. I'm really happy

00:35.000 --> 00:39.000
about this. I wouldn't have expected so many people. I hope

00:39.000 --> 00:43.000
this is going to be interesting for you. There is a lot of text

00:43.000 --> 00:46.000
on these slides. I uploaded the slides to the website so you

00:46.000 --> 00:50.000
don't have to write down everything and read everything.

00:50.000 --> 00:54.000
There's a lot to talk about in async. So what I wanted to do is

00:54.000 --> 00:58.000
give a short introduction to async and frame everything into

00:58.000 --> 01:03.000
writing a telegram bot. Because that was the motivation behind

01:03.000 --> 01:08.000
the talk and how I came to doing this. So a few words about

01:08.000 --> 01:14.000
myself. I'm Mark. I'm from DÃ¼sseldorf in Germany. I've

01:14.000 --> 01:18.000
been with Python for a very long time, since 1994. I'm a core

01:18.000 --> 01:22.000
developer. I've done lots of work in the various organizations

01:22.000 --> 01:27.000
around Python. So with your Python chair, for example, I was

01:27.000 --> 01:31.000
on the board of the PSF. I'm a PSF fellow. And I've done lots

01:31.000 --> 01:35.000
of work in that area. In my day job, I am a consulting CTO or

01:35.000 --> 01:40.000
do senior architectures. Also do coaching a bit. So if you have

01:40.000 --> 01:46.000
a need in that area, just ping me. So the motivation for the

01:46.000 --> 01:50.000
talk is writing a telegram and a spam bot. Now, why do we have

01:50.000 --> 01:55.000
to do that? We have a user group in Germany, the Python meeting

01:55.000 --> 02:00.000
DÃ¼sseldorf, and we're using a telegram group to communicate.

02:00.000 --> 02:05.000
And early last year, we started seeing lots of signups to that

02:05.000 --> 02:08.000
group. Because it's a public group, anyone can just sign up

02:08.000 --> 02:12.000
to that group. We started seeing lots of signups from strange

02:12.000 --> 02:17.000
people. And the people then usually started to, you know,

02:17.000 --> 02:24.000
send spam, send crypto, links, you know, link spam. Many of

02:24.000 --> 02:28.000
those people were actually not people but bots, and they

02:28.000 --> 02:31.000
scraped the contact info and started sending DMs to the

02:31.000 --> 02:37.000
various members. So it was, you know, getting to a point where

02:37.000 --> 02:41.000
it was not possible for us as admins to handle this anymore

02:41.000 --> 02:44.000
because most of these people or bots, they were actually

02:44.000 --> 02:47.000
signing up during the night so there was no one there to

02:47.000 --> 02:51.000
handle these. And so the idea was to write a bot which

02:51.000 --> 02:55.000
basically tries to, you know, check whether people are human,

02:55.000 --> 02:59.000
check whether the signups are actually from people who know

02:59.000 --> 03:06.000
Python. And that's what I did last year. So the idea was to

03:06.000 --> 03:11.000
have a scalable bot because it needs to run 24-7. It also needs

03:11.000 --> 03:15.000
to be very stable because, you know, at night no one is there

03:15.000 --> 03:19.000
to basically restart it. We needed something that is low

03:19.000 --> 03:23.000
resource because we wanted to have it on one of the VMs that

03:23.000 --> 03:28.000
we have to run. And so we decided to look out for, you

03:28.000 --> 03:32.000
know, a library that we could use. And there is a very nice

03:32.000 --> 03:36.000
library called pyrogram which you can use for creating these

03:36.000 --> 03:43.000
bots. It's LJPL3. It's fairly new. It's well documented and

03:43.000 --> 03:47.000
it's actually maintained. So basically all the checks are

03:47.000 --> 03:50.000
there. And we started to use that and we had great success

03:50.000 --> 03:57.000
with it. It is an async library. So what is this

03:57.000 --> 04:01.000
asynchronous programming? I'm going to go through the three

04:01.000 --> 04:06.000
different models that you have for execution in Python. And

04:06.000 --> 04:10.000
let's start with the synchronous execution. So what is synchronous

04:10.000 --> 04:13.000
execution? Basically you write your program from top to

04:13.000 --> 04:16.000
bottom. The Python interpreter then takes all the different

04:16.000 --> 04:19.000
steps that you have in your program and runs them one by

04:19.000 --> 04:23.000
one, one after the other. So there's no parallel processing

04:23.000 --> 04:28.000
going on. Everything happens one after the other thing. If you

04:28.000 --> 04:32.000
have to wait for I.O., for example, then the cord just sits

04:32.000 --> 04:36.000
there. It doesn't do anything. And of course waiting is not

04:36.000 --> 04:40.000
really very efficient. So what can you do about that if you

04:40.000 --> 04:45.000
want to scale up? One way to scale up in Python is to use

04:45.000 --> 04:49.000
threads. And probably many of you know about the gil. I'm

04:49.000 --> 04:53.000
going to talk about that in a bit. But let's just mention

04:53.000 --> 04:56.000
what threaded programming is. Threaded programming is where

04:56.000 --> 05:00.000
the operating system basically assigns slices to your

05:00.000 --> 05:04.000
application. And then each slice can run for a certain amount

05:04.000 --> 05:07.000
of time. And then the operating system switches to the next

05:07.000 --> 05:11.000
slice and the next thread. So everything is controlled by the

05:11.000 --> 05:15.000
OS. This is a very nice and very elegant way to scale up. You

05:15.000 --> 05:20.000
can use all the cores that you have in your CPU. You can, you

05:20.000 --> 05:24.000
know, in the past you usually had multiple processes in the

05:24.000 --> 05:28.000
servers and you could use those multiple processes as well.

05:28.000 --> 05:31.000
There's one catch, though, with thread programming. Because

05:31.000 --> 05:34.000
it's controlled by the OS and not by the application, so not

05:34.000 --> 05:39.000
by Python, it is possible for two threads to try to access

05:39.000 --> 05:43.000
the same object, let's say, or same memory area in your

05:43.000 --> 05:47.000
application and do things on those memory areas. For

05:47.000 --> 05:51.000
example, you know, take a list, append to it, delete from it,

05:51.000 --> 05:55.000
and so on. And if two threads start doing that at the same

05:55.000 --> 05:58.000
time, you can have clashes. And in order to prevent that,

05:58.000 --> 06:01.000
because you don't want to have data loss, for example, you

06:01.000 --> 06:04.000
have to put locks around these things to make everything

06:04.000 --> 06:09.000
work. So there is a bit of extra work to be done there. You

06:09.000 --> 06:12.000
have to consider how things work in the thread environment

06:12.000 --> 06:16.000
and you have to put locks around areas that can be shared

06:16.000 --> 06:19.000
between the different threads that you have. It is an

06:19.000 --> 06:22.000
efficient use of resources. So this is actually something

06:22.000 --> 06:26.000
that people try to get working. With Python, it's a bit

06:26.000 --> 06:31.000
harder. And because it's a bit harder, some years ago,

06:31.000 --> 06:35.000
async support was added to Python. So what is asynchronous?

06:35.000 --> 06:38.000
Asynchronous is basically focusing on a single thread, on

06:38.000 --> 06:42.000
a single core. It looks very much like a synchronous

06:42.000 --> 06:47.000
program, except that whenever you do I.O., the application,

06:47.000 --> 06:51.000
Python in that case, can then say, okay, I'm going to run

06:51.000 --> 06:54.000
this function until I hit a spot where I have I.O., for

06:54.000 --> 06:57.000
example, or I have to wait for something. And then I give

06:57.000 --> 07:01.000
back control to something called an event loop. And that

07:01.000 --> 07:05.000
event loop is then going to take, going to go through the

07:05.000 --> 07:08.000
list of everything that is scheduled to be executed and

07:08.000 --> 07:11.000
then just run the next thing that's on that list. And so

07:11.000 --> 07:15.000
whenever you wait for I.O., you can tell the program, okay,

07:15.000 --> 07:18.000
that's the thing that's going to be done with this part of

07:18.000 --> 07:22.000
the application. And now I'm going to switch to a different

07:22.000 --> 07:26.000
part and run that part. So that's a way to work around the

07:26.000 --> 07:30.000
threading issues I just talked about. It's also a way to, you

07:30.000 --> 07:35.000
know, write code that scales up very neatly, very fast. It's

07:35.000 --> 07:40.000
a bit limited in terms of focusing on just one core. So

07:40.000 --> 07:44.000
you, for example, you cannot use multiple cores that way, or

07:44.000 --> 07:48.000
you can use multiple cores that way. And if you want to use

07:48.000 --> 07:52.000
multiple cores, you can push work that is being done in the

07:52.000 --> 07:56.000
application that's not running Python on these other cores.

07:56.000 --> 07:59.000
That's certainly possible. But if you want to scale up, use

07:59.000 --> 08:02.000
all the cores, then you basically have to use multiple

08:02.000 --> 08:05.000
of these applications, run them in different processes, and

08:05.000 --> 08:09.000
then use up all the cores that you have. There's one catch

08:09.000 --> 08:13.000
with this. I mean, there's no free lunch, right? So all the

08:13.000 --> 08:16.000
cores that you have to collaborate. Because it's

08:16.000 --> 08:20.000
application-driven, all the parts that you have need to have

08:20.000 --> 08:23.000
certain places where they say, okay, I'm going to give up

08:23.000 --> 08:26.000
control back to the event loop at this point because I'm

08:26.000 --> 08:29.000
waiting for something. Because Python cannot know that you're

08:29.000 --> 08:33.000
trying to wait for something. And so you have to tell Python

08:33.000 --> 08:39.000
that this is a good place to give up control. Now, why do

08:39.000 --> 08:43.000
you want to do this? Because it's a good place to go. And this

08:43.000 --> 08:46.000
slide is about the global interpreter lock. How many of

08:46.000 --> 08:50.000
you know the global interpreter lock? Just a few. That's

08:50.000 --> 08:55.000
interesting. So what Python does is it keeps a one big lock

08:55.000 --> 08:59.000
around the Python virtual machine that executes the

08:59.000 --> 09:03.000
Python bytecode. And it does this because it wants to

09:03.000 --> 09:08.000
support multiple processes, sorry, multiple threads. But

09:08.000 --> 09:11.000
it's not the same thing. So when Python was added, threading

09:11.000 --> 09:16.000
was actually very new. Python is, you know, it's more than

09:16.000 --> 09:21.000
30 years old now. So there's been a lot of development going

09:21.000 --> 09:25.000
on since Python started. Because Guido wanted to start

09:25.000 --> 09:28.000
supporting threading right from the beginning, he added this

09:28.000 --> 09:32.000
global interpreter lock to make sure that the logic that's

09:32.000 --> 09:37.000
inside Python is only used by one thread at any point in

09:37.000 --> 09:43.000
the world. So what happens is the Python starts running

09:43.000 --> 09:48.000
code, Python code in one thread, then reaches a certain

09:48.000 --> 09:52.000
point, and then gives up control back to the OS and says, okay,

09:52.000 --> 09:56.000
you can switch to a different thread now. But it does this

09:56.000 --> 10:00.000
under the control of this global interpreter lock. So it makes

10:00.000 --> 10:04.000
sure that no other thread is running Python at the moment.

10:04.000 --> 10:09.000
So the other thread will have been waiting for the Python

10:09.000 --> 10:13.000
thread to lock to get the lock. And then we'll start executing.

10:13.000 --> 10:16.000
And this goes on, you know, for all the threads that you have

10:16.000 --> 10:20.000
in your application. So in a multithreaded program that's

10:20.000 --> 10:25.000
running Python, you can just have one thread execute Python

10:25.000 --> 10:28.000
code at any point in time. And this is something that, of

10:28.000 --> 10:34.000
course, is very scalable. It's also not a very big issue, as

10:34.000 --> 10:40.000
some, you know, may tell you. Because if you're clever and you

10:40.000 --> 10:44.000
put all the logic that you need to run on multiple cores or

10:44.000 --> 10:48.000
multiple threads into parts of the program that don't need

10:48.000 --> 10:52.000
Python, for example, if you're running machine learning and you

10:52.000 --> 10:55.000
want to train a model, then you can just easily push off

10:55.000 --> 10:59.000
something. And you can just run a code that doesn't need

10:59.000 --> 11:04.000
Python. And that can very well run next to Python in another

11:04.000 --> 11:08.000
thread. So that's certainly possible. But, of course, you

11:08.000 --> 11:11.000
know, sometimes you don't have a chance to do that. And then you

11:11.000 --> 11:14.000
need to look for other things. And this is where async becomes

11:14.000 --> 11:18.000
very nice. So let's have a look at how threaded code executes

11:18.000 --> 11:23.000
in Python. The image on the right there basically explains

11:23.000 --> 11:29.000
how you can do that. So you have three threads. The orange

11:29.000 --> 11:33.000
is Python running. Then the yellow is basically the thread,

11:33.000 --> 11:37.000
the Python interpreter in those threads waiting for the Gil.

11:37.000 --> 11:42.000
And then you have some waiting for IO that happens in between.

11:42.000 --> 11:46.000
So if you look closely, you will see that it's not a very

11:46.000 --> 11:49.000
efficient use here. Because there's lots of waiting, lots

11:49.000 --> 11:55.000
of blue waiting for IO. Let's have a closer look at this.

11:55.000 --> 11:59.000
So this again is the picture that I had on the other slide.

11:59.000 --> 12:03.000
And I moved out all the waiting and all the execution. And if

12:03.000 --> 12:06.000
you move all the execution together, you will see that only

12:06.000 --> 12:10.000
one thread is running at any point in time. So the other

12:10.000 --> 12:14.000
threads are basically just sitting there doing nothing.

12:14.000 --> 12:20.000
Now, how can you work around this? You can use async

12:20.000 --> 12:24.000
programming for this. And async programming has the need

12:24.000 --> 12:28.000
feature that you can actually saturate a single core very

12:28.000 --> 12:32.000
efficiently without doing too much work. So again, you have

12:32.000 --> 12:35.000
the execution here. You don't have three threads. This is

12:35.000 --> 12:40.000
just one thread that you have or one core. But you have three

12:40.000 --> 12:44.000
tasks running in that one thread. And the different tasks,

12:44.000 --> 12:48.000
they share the execution. And again, you have the orange

12:48.000 --> 12:51.000
here executing Python. You have some waiting for IO in here

12:51.000 --> 12:56.000
or could also be waiting for calculations to happen. And if

12:56.000 --> 12:59.000
you move everything together, you will see that it's really

12:59.000 --> 13:03.000
the thread, the core is saturated. So everything is

13:03.000 --> 13:07.000
working out nicely. And it's very efficient. So how does

13:07.000 --> 13:12.000
this work? How many of you know coroutines? Okay, about

13:12.000 --> 13:17.000
like one-third. So a coroutine basically is very much like a

13:17.000 --> 13:23.000
normal function except that it's possible to have certain

13:23.000 --> 13:27.000
spots in the coroutine, in the function, where it says, okay,

13:27.000 --> 13:30.000
at this point, you can give up control back to the caller of

13:30.000 --> 13:33.000
that function. And this is essentially how async

13:33.000 --> 13:37.000
programming works. You have something called an event loop.

13:37.000 --> 13:40.000
The event loop calls these coroutines. The coroutine

13:40.000 --> 13:44.000
executes until it hits one of these spots where you can give

13:44.000 --> 13:48.000
up control. The function, the coroutine gives back control to

13:48.000 --> 13:51.000
the event loop at that point. And then the event loop can

13:51.000 --> 13:54.000
execute something else in your application. And then at a

13:54.000 --> 13:58.000
later point, it comes back to that coroutine and continues

13:58.000 --> 14:02.000
executing where it left off. In order to make that easy to

14:02.000 --> 14:06.000
define and easy to use in Python, we have new keywords.

14:06.000 --> 14:10.000
We have async dev, which is a way to define these

14:10.000 --> 14:17.000
coroutines. And we have these await statements in Python,

14:17.000 --> 14:21.000
which are basically places where the coroutine says, okay,

14:21.000 --> 14:25.000
you can give up control and you can pass back control to the

14:25.000 --> 14:28.000
event loop because I'm waiting for, let's say, I owe or for

14:28.000 --> 14:34.000
longer running calculation that you want to do. And everything

14:34.000 --> 14:39.000
around this, all the support for this is bundled in this

14:39.000 --> 14:42.000
package called async I owe, which is part of the standard

14:42.000 --> 14:48.000
library. So let's have a look at how this works in Python to

14:48.000 --> 14:52.000
compare synchronous code and async code. So on the left, you

14:52.000 --> 14:55.000
have a very simple function. You have a time sleep in there,

14:55.000 --> 14:59.000
which is like a simulation for the I owe. So something, the

14:59.000 --> 15:03.000
application needs to wait for something. And then you call

15:03.000 --> 15:08.000
that function. And if you run this, the simple example, you

15:08.000 --> 15:12.000
see that, you know, it starts executing, it starts working,

15:12.000 --> 15:15.000
then it sleeps for two seconds, and then it's done, and then

15:15.000 --> 15:19.000
it's the end of that function. Now, in the async case, it

15:19.000 --> 15:22.000
works a bit differently. So what you do is you put the async

15:22.000 --> 15:25.000
in front of the function, so you have to turn it into a

15:25.000 --> 15:29.000
coroutine. And then inside that function, we use the await

15:29.000 --> 15:33.000
statement to say, okay, at this point, I can give up control

15:33.000 --> 15:37.000
back to the event loop. And so what happens here is that you

15:37.000 --> 15:41.000
have a special function called async I owe sleep, which is a

15:41.000 --> 15:46.000
way to, you know, wait for a certain amount of time in async.

15:46.000 --> 15:51.000
But when waiting for these two seconds, you can actually

15:51.000 --> 15:55.000
go back and you can execute something else. It's not

15:55.000 --> 15:59.000
possible to use await and then time.sleep for this, because

15:59.000 --> 16:03.000
time.sleep is actually a blocking function, right? It

16:03.000 --> 16:07.000
doesn't give back control. So you have to make sure that

16:07.000 --> 16:11.000
whatever you use with await is actually a coroutine so that

16:11.000 --> 16:16.000
it can pass back control to your coroutine that's calling

16:16.000 --> 16:20.000
this coroutine. And this is what I meant with everything has

16:20.000 --> 16:23.000
to collaborate. If you have places in your application that

16:23.000 --> 16:27.000
are not compatible with async, you have to be careful and you

16:27.000 --> 16:32.000
have to use certain work arounds to make it happen. So the

16:32.000 --> 16:36.000
next thing is that, you know, now you have a coroutine,

16:36.000 --> 16:40.000
calling the coroutine will do nothing. Basically, all that

16:40.000 --> 16:44.000
happens is you get back a coroutine object. So it doesn't

16:44.000 --> 16:48.000
run. So in order to run it, you have to actually start the

16:48.000 --> 16:52.000
coroutine inside the event loop. And this is what async I O

16:52.000 --> 16:57.000
dot run does at the very bottom. And this, if you look at

16:57.000 --> 17:03.000
it, it takes, it defines two tasks. So basically two

17:03.000 --> 17:07.000
instances of that coroutine. Puts them into this tuple. The

17:07.000 --> 17:11.000
tuple is passed to this async I O gather, which is a special

17:11.000 --> 17:14.000
function I'm going to come to in one of the next slides. It

17:14.000 --> 17:18.000
basically just takes the coroutines, creates tasks

17:18.000 --> 17:22.000
objects, and then executes them until all of them are done.

17:22.000 --> 17:26.000
And then passes back control. So this is how you would run

17:26.000 --> 17:32.000
an async application. I ran through these so I can basically

17:32.000 --> 17:38.000
just skip these. So what are the things in the async I O

17:38.000 --> 17:43.000
package module? A very important function is this async

17:43.000 --> 17:46.000
I O run. This is basically the function that needs to be

17:46.000 --> 17:49.000
called in order to set up the event loop, to run everything in

17:49.000 --> 17:53.000
that event loop. You typically just have one of these calls in

17:53.000 --> 17:56.000
your application. Basically just starting the event loop and

17:56.000 --> 18:00.000
then running anything that's being scheduled. Then you have

18:00.000 --> 18:04.000
this gather function. Gather is, like I already mentioned,

18:04.000 --> 18:10.000
it's a function where you can pass in coroutines or tasks.

18:10.000 --> 18:14.000
And then it runs all of these tasks until completion and then

18:14.000 --> 18:18.000
it returns. You also have a couple of functions down here

18:18.000 --> 18:22.000
for waiting for certain things. So sometimes in an application

18:22.000 --> 18:25.000
you need to synchronize between various different parts. So

18:25.000 --> 18:29.000
there are some handy functions for this as well. Now what is

18:29.000 --> 18:33.000
this task object I keep mentioning? Task objects are

18:33.000 --> 18:37.000
basically just coroutine calls that are being scheduled. And

18:37.000 --> 18:41.000
it's a way for the event loop to manage everything that happens

18:41.000 --> 18:46.000
in the event loop. So whenever something is scheduled to be

18:46.000 --> 18:49.000
run, you create a task object. And this is done behind the

18:49.000 --> 18:52.000
scenes for you. You don't have to create these objects

18:52.000 --> 18:55.000
yourself. In fact, you should not create these objects

18:55.000 --> 18:58.000
yourself. You should always use one of the functions for this,

18:58.000 --> 19:04.000
like this create task that you have down here. And then these

19:04.000 --> 19:07.000
task objects go into the event loop, are run, and everything

19:07.000 --> 19:10.000
happens for you. There are also some query functions down here

19:10.000 --> 19:13.000
if you're interested in what's currently scheduled on the

19:13.000 --> 19:17.000
event loop. You can have a look at the documentation for those.

19:17.000 --> 19:21.000
So how does this event loop work? It's basically just a way

19:21.000 --> 19:26.000
just to do the same kind of management as the OS is doing

19:26.000 --> 19:31.000
for threads, except that it's done in Python. And the

19:31.000 --> 19:36.000
asyncio package manages one of these event loops. Now event

19:36.000 --> 19:41.000
loops can actually be defined by multiple different libraries.

19:41.000 --> 19:44.000
But what's important is that there should only be one event

19:44.000 --> 19:49.000
loop per thread. So you can have multiple threads, of course,

19:49.000 --> 19:53.000
also run. Then again, you hit the same kind of roadblock as

19:53.000 --> 20:00.000
you've seen with the gil. But there might be ways in your

20:00.000 --> 20:04.000
application to make use of that. So that would be possible as

20:04.000 --> 20:07.000
well. Typically what you have in the async program is you just

20:07.000 --> 20:11.000
have a single thread. And so you just call this run function

20:11.000 --> 20:16.000
once. Now, I mentioned blocking code. So blocking code basically

20:16.000 --> 20:21.000
is code that doesn't collaborate with this async logic. And you

20:21.000 --> 20:24.000
have that quite often in Python. For example, let's say you're

20:24.000 --> 20:28.000
using one of the database modules, not one of the async

20:28.000 --> 20:32.000
modules, but the regular ones. Those will all be synchronous.

20:32.000 --> 20:36.000
So you call, let's say, an execute to run some SQL. And

20:36.000 --> 20:39.000
that will actually wait until the database comes back with

20:39.000 --> 20:44.000
the results. So in order to run this kind of code in an async

20:44.000 --> 20:48.000
application, you have to use special functions. There's a

20:48.000 --> 20:52.000
very nice function called asyncio2thread, which was

20:52.000 --> 20:57.000
added in Python 3.9, which makes this easy. So what these

20:57.000 --> 21:03.000
functions do is they spin up a thread in your async

21:03.000 --> 21:07.000
application, run the code inside that thread, and then pass back

21:07.000 --> 21:12.000
control via the threading logic to your event loop. So you can

21:12.000 --> 21:15.000
still run synchronous code because the synchronous code is

21:15.000 --> 21:19.000
most likely going to give up the gil. For example, if you have

21:19.000 --> 21:23.000
a good database module, then when you execute something,

21:23.000 --> 21:26.000
typically what these database modules do is they give back

21:26.000 --> 21:30.000
control to other threads running Python code because they're

21:30.000 --> 21:34.000
just running C code at that time. So this is a way to make

21:34.000 --> 21:39.000
everything work together. And of course, there's lots more.

21:39.000 --> 21:42.000
I'm not going to talk about these things because I don't

21:42.000 --> 21:45.000
have enough time for that. In fact, I'm already almost out

21:45.000 --> 21:49.000
of time, so I have to speed up a bit. Let's just do a very

21:49.000 --> 21:54.000
quick view, overview of what's in the async ecosystem. So of

21:54.000 --> 21:58.000
course, we have the async IO standard library package. We

21:58.000 --> 22:02.000
have event loops inside the async IO package. If you want

22:02.000 --> 22:05.000
fast loops, then you can use UV loop, which is the faster

22:05.000 --> 22:09.000
implementation, speeds up your async by almost four times.

22:09.000 --> 22:12.000
You can also have a look at other stacks that implement

22:12.000 --> 22:15.000
event loops, like trio, for example, or you can use the

22:15.000 --> 22:20.000
package any IO, which abstracts these things. So you can use

22:20.000 --> 22:25.000
async IO loops underneath in your application when using any

22:25.000 --> 22:29.000
IO and in abstracts away other details. Now, there's a rather

22:29.000 --> 22:34.000
large system of modules and packages around the async world

22:34.000 --> 22:40.000
in Python. Many of these are grouped under the AIO libs. So

22:40.000 --> 22:45.000
if you go to GitHub to that URL, then you will find lots of

22:45.000 --> 22:50.000
examples there. There are database packages there. There

22:50.000 --> 22:54.000
are things for doing HTTP DNS and so on. Something to watch

22:54.000 --> 22:58.000
out is the database modules typically don't support

22:58.000 --> 23:03.000
transactions, which can be a bummer sometimes. At the

23:03.000 --> 23:07.000
higher level, you have, of course, web frameworks again,

23:07.000 --> 23:11.000
because, you know, everyone loves web frameworks. And of

23:11.000 --> 23:15.000
course, you have API frameworks. The most popular one right

23:15.000 --> 23:19.000
now is fast API for doing the rest APIs, and then

23:19.000 --> 23:24.000
strawberries is coming very strongly as a GraphQL server.

23:24.000 --> 23:29.000
Both operate async. Even Django does or starts is starting

23:29.000 --> 23:33.000
to do async right now. It's not fully there yet. If you're

23:33.000 --> 23:37.000
using Flask for synchronous code, then you might want to

23:37.000 --> 23:40.000
look at a quart, which is like an async implementation using

23:40.000 --> 23:45.000
a similar API. And the most famous one probably in the

23:45.000 --> 23:49.000
async space is tornado, which some of you may know. It's

23:49.000 --> 23:54.000
very fast. Right. So let's go back to the bot. If you want

23:54.000 --> 23:57.000
to see the bot code, it's on GitHub. Just search for

23:57.000 --> 24:01.000
eugenics telegram and then you'll find it. How does it

24:01.000 --> 24:04.000
work? Very easy. You just subclass the client of that

24:04.000 --> 24:07.000
package. You do some configuration. You do some

24:07.000 --> 24:11.000
observability. So logging for things. I'm lazy. So I just

24:11.000 --> 24:15.000
put all the admin messages into another telegram chat that I

24:15.000 --> 24:19.000
can manage so I can see what's happening without having to go

24:19.000 --> 24:25.000
to the server. Because we actually want to catch all the

24:25.000 --> 24:29.000
messages of these people signing up to the telegram

24:29.000 --> 24:33.000
group, and not just people who want to run bot commands,

24:33.000 --> 24:37.000
slash something. We have to use the catch all in here. And

24:37.000 --> 24:40.000
that's also why we need something that's very scalable

24:40.000 --> 24:43.000
because literally the bot sees all the messages that go into

24:43.000 --> 24:48.000
that group and it has to handle all these messages. And then

24:48.000 --> 24:53.000
what you do is basically you just do these awaits whenever

24:53.000 --> 24:56.000
you have to do I.O. And if you look at it, it looks very

24:56.000 --> 24:59.000
much like synchronous code, right? Except that you have

24:59.000 --> 25:02.000
these awaits in front of certain things. Wherever something

25:02.000 --> 25:05.000
happens where you need to do some I.O., you put the await

25:05.000 --> 25:09.000
in front of it. And then everything else looks very

25:09.000 --> 25:13.000
natural. Looks like a very much like a synchronous program.

25:13.000 --> 25:22.000
So what are the results of doing this? Writing this bot,

25:22.000 --> 25:26.000
it's actually helped us a lot. We've had over almost 800

25:26.000 --> 25:34.000
spam signups since April 2022 when we started to use it. And

25:34.000 --> 25:37.000
this has, I mean, this is one part, this is just the admin

25:37.000 --> 25:40.000
part that saved us a lot of work. But of course, you know,

25:40.000 --> 25:44.000
every single signup would have caused spam messages. And so

25:44.000 --> 25:47.000
that was very successful. So the time spent on actually

25:47.000 --> 25:52.000
writing things was well invested and basically mission

25:52.000 --> 25:56.000
accomplished. So what's the main takeaway of the talk?

25:56.000 --> 26:02.000
I think it's great. And give it a try if you can. Thank you

26:02.000 --> 26:07.000
for your attention. Thank you for your attention.

26:16.000 --> 26:18.000
Thank you, Mark. Thanks, everyone, for attending this

26:18.000 --> 26:22.000
talk. Don't hesitate to reach to Mark and Ray if you have

26:22.000 --> 26:27.000
any questions or want to discuss this further. Thanks a

26:27.000 --> 26:31.000
lot. Thank you. Thank you very much for coming. Let me just

26:31.000 --> 26:49.000
show you a picture. Excellent.
