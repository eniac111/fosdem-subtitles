1
0:00:00.000 --> 0:00:02.000
I'm going to turn it over to you.

2
0:00:06.000 --> 0:00:08.000
Hello, everybody.

3
0:00:10.000 --> 0:00:13.000
Welcome to the Python, the room of Fuzzdam. I'm really happy to

4
0:00:13.000 --> 0:00:17.000
welcome all of you here and to welcome Mark for his talk and

5
0:00:17.000 --> 0:00:21.000
introduction to async programming. Thanks for everyone

6
0:00:21.000 --> 0:00:25.000
for coming. Also, so early for this talk that I'm really

7
0:00:25.000 --> 0:00:27.000
looking forward to see with you.

8
0:00:27.000 --> 0:00:31.000
Thank you for the introduction. It's really nice to see so many

9
0:00:31.000 --> 0:00:35.000
people here on a Sunday morning at 9 o'clock. I'm really happy

10
0:00:35.000 --> 0:00:39.000
about this. I wouldn't have expected so many people. I hope

11
0:00:39.000 --> 0:00:43.000
this is going to be interesting for you. There is a lot of text

12
0:00:43.000 --> 0:00:46.000
on these slides. I uploaded the slides to the website so you

13
0:00:46.000 --> 0:00:50.000
don't have to write down everything and read everything.

14
0:00:50.000 --> 0:00:54.000
There's a lot to talk about in async. So what I wanted to do is

15
0:00:54.000 --> 0:00:58.000
give a short introduction to async and frame everything into

16
0:00:58.000 --> 0:01:03.000
writing a telegram bot. Because that was the motivation behind

17
0:01:03.000 --> 0:01:08.000
the talk and how I came to doing this. So a few words about

18
0:01:08.000 --> 0:01:14.000
myself. I'm Mark. I'm from Düsseldorf in Germany. I've

19
0:01:14.000 --> 0:01:18.000
been with Python for a very long time, since 1994. I'm a core

20
0:01:18.000 --> 0:01:22.000
developer. I've done lots of work in the various organizations

21
0:01:22.000 --> 0:01:27.000
around Python. So with your Python chair, for example, I was

22
0:01:27.000 --> 0:01:31.000
on the board of the PSF. I'm a PSF fellow. And I've done lots

23
0:01:31.000 --> 0:01:35.000
of work in that area. In my day job, I am a consulting CTO or

24
0:01:35.000 --> 0:01:40.000
do senior architectures. Also do coaching a bit. So if you have

25
0:01:40.000 --> 0:01:46.000
a need in that area, just ping me. So the motivation for the

26
0:01:46.000 --> 0:01:50.000
talk is writing a telegram and a spam bot. Now, why do we have

27
0:01:50.000 --> 0:01:55.000
to do that? We have a user group in Germany, the Python meeting

28
0:01:55.000 --> 0:02:00.000
Düsseldorf, and we're using a telegram group to communicate.

29
0:02:00.000 --> 0:02:05.000
And early last year, we started seeing lots of signups to that

30
0:02:05.000 --> 0:02:08.000
group. Because it's a public group, anyone can just sign up

31
0:02:08.000 --> 0:02:12.000
to that group. We started seeing lots of signups from strange

32
0:02:12.000 --> 0:02:17.000
people. And the people then usually started to, you know,

33
0:02:17.000 --> 0:02:24.000
send spam, send crypto, links, you know, link spam. Many of

34
0:02:24.000 --> 0:02:28.000
those people were actually not people but bots, and they

35
0:02:28.000 --> 0:02:31.000
scraped the contact info and started sending DMs to the

36
0:02:31.000 --> 0:02:37.000
various members. So it was, you know, getting to a point where

37
0:02:37.000 --> 0:02:41.000
it was not possible for us as admins to handle this anymore

38
0:02:41.000 --> 0:02:44.000
because most of these people or bots, they were actually

39
0:02:44.000 --> 0:02:47.000
signing up during the night so there was no one there to

40
0:02:47.000 --> 0:02:51.000
handle these. And so the idea was to write a bot which

41
0:02:51.000 --> 0:02:55.000
basically tries to, you know, check whether people are human,

42
0:02:55.000 --> 0:02:59.000
check whether the signups are actually from people who know

43
0:02:59.000 --> 0:03:06.000
Python. And that's what I did last year. So the idea was to

44
0:03:06.000 --> 0:03:11.000
have a scalable bot because it needs to run 24-7. It also needs

45
0:03:11.000 --> 0:03:15.000
to be very stable because, you know, at night no one is there

46
0:03:15.000 --> 0:03:19.000
to basically restart it. We needed something that is low

47
0:03:19.000 --> 0:03:23.000
resource because we wanted to have it on one of the VMs that

48
0:03:23.000 --> 0:03:28.000
we have to run. And so we decided to look out for, you

49
0:03:28.000 --> 0:03:32.000
know, a library that we could use. And there is a very nice

50
0:03:32.000 --> 0:03:36.000
library called pyrogram which you can use for creating these

51
0:03:36.000 --> 0:03:43.000
bots. It's LJPL3. It's fairly new. It's well documented and

52
0:03:43.000 --> 0:03:47.000
it's actually maintained. So basically all the checks are

53
0:03:47.000 --> 0:03:50.000
there. And we started to use that and we had great success

54
0:03:50.000 --> 0:03:57.000
with it. It is an async library. So what is this

55
0:03:57.000 --> 0:04:01.000
asynchronous programming? I'm going to go through the three

56
0:04:01.000 --> 0:04:06.000
different models that you have for execution in Python. And

57
0:04:06.000 --> 0:04:10.000
let's start with the synchronous execution. So what is synchronous

58
0:04:10.000 --> 0:04:13.000
execution? Basically you write your program from top to

59
0:04:13.000 --> 0:04:16.000
bottom. The Python interpreter then takes all the different

60
0:04:16.000 --> 0:04:19.000
steps that you have in your program and runs them one by

61
0:04:19.000 --> 0:04:23.000
one, one after the other. So there's no parallel processing

62
0:04:23.000 --> 0:04:28.000
going on. Everything happens one after the other thing. If you

63
0:04:28.000 --> 0:04:32.000
have to wait for I.O., for example, then the cord just sits

64
0:04:32.000 --> 0:04:36.000
there. It doesn't do anything. And of course waiting is not

65
0:04:36.000 --> 0:04:40.000
really very efficient. So what can you do about that if you

66
0:04:40.000 --> 0:04:45.000
want to scale up? One way to scale up in Python is to use

67
0:04:45.000 --> 0:04:49.000
threads. And probably many of you know about the gil. I'm

68
0:04:49.000 --> 0:04:53.000
going to talk about that in a bit. But let's just mention

69
0:04:53.000 --> 0:04:56.000
what threaded programming is. Threaded programming is where

70
0:04:56.000 --> 0:05:00.000
the operating system basically assigns slices to your

71
0:05:00.000 --> 0:05:04.000
application. And then each slice can run for a certain amount

72
0:05:04.000 --> 0:05:07.000
of time. And then the operating system switches to the next

73
0:05:07.000 --> 0:05:11.000
slice and the next thread. So everything is controlled by the

74
0:05:11.000 --> 0:05:15.000
OS. This is a very nice and very elegant way to scale up. You

75
0:05:15.000 --> 0:05:20.000
can use all the cores that you have in your CPU. You can, you

76
0:05:20.000 --> 0:05:24.000
know, in the past you usually had multiple processes in the

77
0:05:24.000 --> 0:05:28.000
servers and you could use those multiple processes as well.

78
0:05:28.000 --> 0:05:31.000
There's one catch, though, with thread programming. Because

79
0:05:31.000 --> 0:05:34.000
it's controlled by the OS and not by the application, so not

80
0:05:34.000 --> 0:05:39.000
by Python, it is possible for two threads to try to access

81
0:05:39.000 --> 0:05:43.000
the same object, let's say, or same memory area in your

82
0:05:43.000 --> 0:05:47.000
application and do things on those memory areas. For

83
0:05:47.000 --> 0:05:51.000
example, you know, take a list, append to it, delete from it,

84
0:05:51.000 --> 0:05:55.000
and so on. And if two threads start doing that at the same

85
0:05:55.000 --> 0:05:58.000
time, you can have clashes. And in order to prevent that,

86
0:05:58.000 --> 0:06:01.000
because you don't want to have data loss, for example, you

87
0:06:01.000 --> 0:06:04.000
have to put locks around these things to make everything

88
0:06:04.000 --> 0:06:09.000
work. So there is a bit of extra work to be done there. You

89
0:06:09.000 --> 0:06:12.000
have to consider how things work in the thread environment

90
0:06:12.000 --> 0:06:16.000
and you have to put locks around areas that can be shared

91
0:06:16.000 --> 0:06:19.000
between the different threads that you have. It is an

92
0:06:19.000 --> 0:06:22.000
efficient use of resources. So this is actually something

93
0:06:22.000 --> 0:06:26.000
that people try to get working. With Python, it's a bit

94
0:06:26.000 --> 0:06:31.000
harder. And because it's a bit harder, some years ago,

95
0:06:31.000 --> 0:06:35.000
async support was added to Python. So what is asynchronous?

96
0:06:35.000 --> 0:06:38.000
Asynchronous is basically focusing on a single thread, on

97
0:06:38.000 --> 0:06:42.000
a single core. It looks very much like a synchronous

98
0:06:42.000 --> 0:06:47.000
program, except that whenever you do I.O., the application,

99
0:06:47.000 --> 0:06:51.000
Python in that case, can then say, okay, I'm going to run

100
0:06:51.000 --> 0:06:54.000
this function until I hit a spot where I have I.O., for

101
0:06:54.000 --> 0:06:57.000
example, or I have to wait for something. And then I give

102
0:06:57.000 --> 0:07:01.000
back control to something called an event loop. And that

103
0:07:01.000 --> 0:07:05.000
event loop is then going to take, going to go through the

104
0:07:05.000 --> 0:07:08.000
list of everything that is scheduled to be executed and

105
0:07:08.000 --> 0:07:11.000
then just run the next thing that's on that list. And so

106
0:07:11.000 --> 0:07:15.000
whenever you wait for I.O., you can tell the program, okay,

107
0:07:15.000 --> 0:07:18.000
that's the thing that's going to be done with this part of

108
0:07:18.000 --> 0:07:22.000
the application. And now I'm going to switch to a different

109
0:07:22.000 --> 0:07:26.000
part and run that part. So that's a way to work around the

110
0:07:26.000 --> 0:07:30.000
threading issues I just talked about. It's also a way to, you

111
0:07:30.000 --> 0:07:35.000
know, write code that scales up very neatly, very fast. It's

112
0:07:35.000 --> 0:07:40.000
a bit limited in terms of focusing on just one core. So

113
0:07:40.000 --> 0:07:44.000
you, for example, you cannot use multiple cores that way, or

114
0:07:44.000 --> 0:07:48.000
you can use multiple cores that way. And if you want to use

115
0:07:48.000 --> 0:07:52.000
multiple cores, you can push work that is being done in the

116
0:07:52.000 --> 0:07:56.000
application that's not running Python on these other cores.

117
0:07:56.000 --> 0:07:59.000
That's certainly possible. But if you want to scale up, use

118
0:07:59.000 --> 0:08:02.000
all the cores, then you basically have to use multiple

119
0:08:02.000 --> 0:08:05.000
of these applications, run them in different processes, and

120
0:08:05.000 --> 0:08:09.000
then use up all the cores that you have. There's one catch

121
0:08:09.000 --> 0:08:13.000
with this. I mean, there's no free lunch, right? So all the

122
0:08:13.000 --> 0:08:16.000
cores that you have to collaborate. Because it's

123
0:08:16.000 --> 0:08:20.000
application-driven, all the parts that you have need to have

124
0:08:20.000 --> 0:08:23.000
certain places where they say, okay, I'm going to give up

125
0:08:23.000 --> 0:08:26.000
control back to the event loop at this point because I'm

126
0:08:26.000 --> 0:08:29.000
waiting for something. Because Python cannot know that you're

127
0:08:29.000 --> 0:08:33.000
trying to wait for something. And so you have to tell Python

128
0:08:33.000 --> 0:08:39.000
that this is a good place to give up control. Now, why do

129
0:08:39.000 --> 0:08:43.000
you want to do this? Because it's a good place to go. And this

130
0:08:43.000 --> 0:08:46.000
slide is about the global interpreter lock. How many of

131
0:08:46.000 --> 0:08:50.000
you know the global interpreter lock? Just a few. That's

132
0:08:50.000 --> 0:08:55.000
interesting. So what Python does is it keeps a one big lock

133
0:08:55.000 --> 0:08:59.000
around the Python virtual machine that executes the

134
0:08:59.000 --> 0:09:03.000
Python bytecode. And it does this because it wants to

135
0:09:03.000 --> 0:09:08.000
support multiple processes, sorry, multiple threads. But

136
0:09:08.000 --> 0:09:11.000
it's not the same thing. So when Python was added, threading

137
0:09:11.000 --> 0:09:16.000
was actually very new. Python is, you know, it's more than

138
0:09:16.000 --> 0:09:21.000
30 years old now. So there's been a lot of development going

139
0:09:21.000 --> 0:09:25.000
on since Python started. Because Guido wanted to start

140
0:09:25.000 --> 0:09:28.000
supporting threading right from the beginning, he added this

141
0:09:28.000 --> 0:09:32.000
global interpreter lock to make sure that the logic that's

142
0:09:32.000 --> 0:09:37.000
inside Python is only used by one thread at any point in

143
0:09:37.000 --> 0:09:43.000
the world. So what happens is the Python starts running

144
0:09:43.000 --> 0:09:48.000
code, Python code in one thread, then reaches a certain

145
0:09:48.000 --> 0:09:52.000
point, and then gives up control back to the OS and says, okay,

146
0:09:52.000 --> 0:09:56.000
you can switch to a different thread now. But it does this

147
0:09:56.000 --> 0:10:00.000
under the control of this global interpreter lock. So it makes

148
0:10:00.000 --> 0:10:04.000
sure that no other thread is running Python at the moment.

149
0:10:04.000 --> 0:10:09.000
So the other thread will have been waiting for the Python

150
0:10:09.000 --> 0:10:13.000
thread to lock to get the lock. And then we'll start executing.

151
0:10:13.000 --> 0:10:16.000
And this goes on, you know, for all the threads that you have

152
0:10:16.000 --> 0:10:20.000
in your application. So in a multithreaded program that's

153
0:10:20.000 --> 0:10:25.000
running Python, you can just have one thread execute Python

154
0:10:25.000 --> 0:10:28.000
code at any point in time. And this is something that, of

155
0:10:28.000 --> 0:10:34.000
course, is very scalable. It's also not a very big issue, as

156
0:10:34.000 --> 0:10:40.000
some, you know, may tell you. Because if you're clever and you

157
0:10:40.000 --> 0:10:44.000
put all the logic that you need to run on multiple cores or

158
0:10:44.000 --> 0:10:48.000
multiple threads into parts of the program that don't need

159
0:10:48.000 --> 0:10:52.000
Python, for example, if you're running machine learning and you

160
0:10:52.000 --> 0:10:55.000
want to train a model, then you can just easily push off

161
0:10:55.000 --> 0:10:59.000
something. And you can just run a code that doesn't need

162
0:10:59.000 --> 0:11:04.000
Python. And that can very well run next to Python in another

163
0:11:04.000 --> 0:11:08.000
thread. So that's certainly possible. But, of course, you

164
0:11:08.000 --> 0:11:11.000
know, sometimes you don't have a chance to do that. And then you

165
0:11:11.000 --> 0:11:14.000
need to look for other things. And this is where async becomes

166
0:11:14.000 --> 0:11:18.000
very nice. So let's have a look at how threaded code executes

167
0:11:18.000 --> 0:11:23.000
in Python. The image on the right there basically explains

168
0:11:23.000 --> 0:11:29.000
how you can do that. So you have three threads. The orange

169
0:11:29.000 --> 0:11:33.000
is Python running. Then the yellow is basically the thread,

170
0:11:33.000 --> 0:11:37.000
the Python interpreter in those threads waiting for the Gil.

171
0:11:37.000 --> 0:11:42.000
And then you have some waiting for IO that happens in between.

172
0:11:42.000 --> 0:11:46.000
So if you look closely, you will see that it's not a very

173
0:11:46.000 --> 0:11:49.000
efficient use here. Because there's lots of waiting, lots

174
0:11:49.000 --> 0:11:55.000
of blue waiting for IO. Let's have a closer look at this.

175
0:11:55.000 --> 0:11:59.000
So this again is the picture that I had on the other slide.

176
0:11:59.000 --> 0:12:03.000
And I moved out all the waiting and all the execution. And if

177
0:12:03.000 --> 0:12:06.000
you move all the execution together, you will see that only

178
0:12:06.000 --> 0:12:10.000
one thread is running at any point in time. So the other

179
0:12:10.000 --> 0:12:14.000
threads are basically just sitting there doing nothing.

180
0:12:14.000 --> 0:12:20.000
Now, how can you work around this? You can use async

181
0:12:20.000 --> 0:12:24.000
programming for this. And async programming has the need

182
0:12:24.000 --> 0:12:28.000
feature that you can actually saturate a single core very

183
0:12:28.000 --> 0:12:32.000
efficiently without doing too much work. So again, you have

184
0:12:32.000 --> 0:12:35.000
the execution here. You don't have three threads. This is

185
0:12:35.000 --> 0:12:40.000
just one thread that you have or one core. But you have three

186
0:12:40.000 --> 0:12:44.000
tasks running in that one thread. And the different tasks,

187
0:12:44.000 --> 0:12:48.000
they share the execution. And again, you have the orange

188
0:12:48.000 --> 0:12:51.000
here executing Python. You have some waiting for IO in here

189
0:12:51.000 --> 0:12:56.000
or could also be waiting for calculations to happen. And if

190
0:12:56.000 --> 0:12:59.000
you move everything together, you will see that it's really

191
0:12:59.000 --> 0:13:03.000
the thread, the core is saturated. So everything is

192
0:13:03.000 --> 0:13:07.000
working out nicely. And it's very efficient. So how does

193
0:13:07.000 --> 0:13:12.000
this work? How many of you know coroutines? Okay, about

194
0:13:12.000 --> 0:13:17.000
like one-third. So a coroutine basically is very much like a

195
0:13:17.000 --> 0:13:23.000
normal function except that it's possible to have certain

196
0:13:23.000 --> 0:13:27.000
spots in the coroutine, in the function, where it says, okay,

197
0:13:27.000 --> 0:13:30.000
at this point, you can give up control back to the caller of

198
0:13:30.000 --> 0:13:33.000
that function. And this is essentially how async

199
0:13:33.000 --> 0:13:37.000
programming works. You have something called an event loop.

200
0:13:37.000 --> 0:13:40.000
The event loop calls these coroutines. The coroutine

201
0:13:40.000 --> 0:13:44.000
executes until it hits one of these spots where you can give

202
0:13:44.000 --> 0:13:48.000
up control. The function, the coroutine gives back control to

203
0:13:48.000 --> 0:13:51.000
the event loop at that point. And then the event loop can

204
0:13:51.000 --> 0:13:54.000
execute something else in your application. And then at a

205
0:13:54.000 --> 0:13:58.000
later point, it comes back to that coroutine and continues

206
0:13:58.000 --> 0:14:02.000
executing where it left off. In order to make that easy to

207
0:14:02.000 --> 0:14:06.000
define and easy to use in Python, we have new keywords.

208
0:14:06.000 --> 0:14:10.000
We have async dev, which is a way to define these

209
0:14:10.000 --> 0:14:17.000
coroutines. And we have these await statements in Python,

210
0:14:17.000 --> 0:14:21.000
which are basically places where the coroutine says, okay,

211
0:14:21.000 --> 0:14:25.000
you can give up control and you can pass back control to the

212
0:14:25.000 --> 0:14:28.000
event loop because I'm waiting for, let's say, I owe or for

213
0:14:28.000 --> 0:14:34.000
longer running calculation that you want to do. And everything

214
0:14:34.000 --> 0:14:39.000
around this, all the support for this is bundled in this

215
0:14:39.000 --> 0:14:42.000
package called async I owe, which is part of the standard

216
0:14:42.000 --> 0:14:48.000
library. So let's have a look at how this works in Python to

217
0:14:48.000 --> 0:14:52.000
compare synchronous code and async code. So on the left, you

218
0:14:52.000 --> 0:14:55.000
have a very simple function. You have a time sleep in there,

219
0:14:55.000 --> 0:14:59.000
which is like a simulation for the I owe. So something, the

220
0:14:59.000 --> 0:15:03.000
application needs to wait for something. And then you call

221
0:15:03.000 --> 0:15:08.000
that function. And if you run this, the simple example, you

222
0:15:08.000 --> 0:15:12.000
see that, you know, it starts executing, it starts working,

223
0:15:12.000 --> 0:15:15.000
then it sleeps for two seconds, and then it's done, and then

224
0:15:15.000 --> 0:15:19.000
it's the end of that function. Now, in the async case, it

225
0:15:19.000 --> 0:15:22.000
works a bit differently. So what you do is you put the async

226
0:15:22.000 --> 0:15:25.000
in front of the function, so you have to turn it into a

227
0:15:25.000 --> 0:15:29.000
coroutine. And then inside that function, we use the await

228
0:15:29.000 --> 0:15:33.000
statement to say, okay, at this point, I can give up control

229
0:15:33.000 --> 0:15:37.000
back to the event loop. And so what happens here is that you

230
0:15:37.000 --> 0:15:41.000
have a special function called async I owe sleep, which is a

231
0:15:41.000 --> 0:15:46.000
way to, you know, wait for a certain amount of time in async.

232
0:15:46.000 --> 0:15:51.000
But when waiting for these two seconds, you can actually

233
0:15:51.000 --> 0:15:55.000
go back and you can execute something else. It's not

234
0:15:55.000 --> 0:15:59.000
possible to use await and then time.sleep for this, because

235
0:15:59.000 --> 0:16:03.000
time.sleep is actually a blocking function, right? It

236
0:16:03.000 --> 0:16:07.000
doesn't give back control. So you have to make sure that

237
0:16:07.000 --> 0:16:11.000
whatever you use with await is actually a coroutine so that

238
0:16:11.000 --> 0:16:16.000
it can pass back control to your coroutine that's calling

239
0:16:16.000 --> 0:16:20.000
this coroutine. And this is what I meant with everything has

240
0:16:20.000 --> 0:16:23.000
to collaborate. If you have places in your application that

241
0:16:23.000 --> 0:16:27.000
are not compatible with async, you have to be careful and you

242
0:16:27.000 --> 0:16:32.000
have to use certain work arounds to make it happen. So the

243
0:16:32.000 --> 0:16:36.000
next thing is that, you know, now you have a coroutine,

244
0:16:36.000 --> 0:16:40.000
calling the coroutine will do nothing. Basically, all that

245
0:16:40.000 --> 0:16:44.000
happens is you get back a coroutine object. So it doesn't

246
0:16:44.000 --> 0:16:48.000
run. So in order to run it, you have to actually start the

247
0:16:48.000 --> 0:16:52.000
coroutine inside the event loop. And this is what async I O

248
0:16:52.000 --> 0:16:57.000
dot run does at the very bottom. And this, if you look at

249
0:16:57.000 --> 0:17:03.000
it, it takes, it defines two tasks. So basically two

250
0:17:03.000 --> 0:17:07.000
instances of that coroutine. Puts them into this tuple. The

251
0:17:07.000 --> 0:17:11.000
tuple is passed to this async I O gather, which is a special

252
0:17:11.000 --> 0:17:14.000
function I'm going to come to in one of the next slides. It

253
0:17:14.000 --> 0:17:18.000
basically just takes the coroutines, creates tasks

254
0:17:18.000 --> 0:17:22.000
objects, and then executes them until all of them are done.

255
0:17:22.000 --> 0:17:26.000
And then passes back control. So this is how you would run

256
0:17:26.000 --> 0:17:32.000
an async application. I ran through these so I can basically

257
0:17:32.000 --> 0:17:38.000
just skip these. So what are the things in the async I O

258
0:17:38.000 --> 0:17:43.000
package module? A very important function is this async

259
0:17:43.000 --> 0:17:46.000
I O run. This is basically the function that needs to be

260
0:17:46.000 --> 0:17:49.000
called in order to set up the event loop, to run everything in

261
0:17:49.000 --> 0:17:53.000
that event loop. You typically just have one of these calls in

262
0:17:53.000 --> 0:17:56.000
your application. Basically just starting the event loop and

263
0:17:56.000 --> 0:18:00.000
then running anything that's being scheduled. Then you have

264
0:18:00.000 --> 0:18:04.000
this gather function. Gather is, like I already mentioned,

265
0:18:04.000 --> 0:18:10.000
it's a function where you can pass in coroutines or tasks.

266
0:18:10.000 --> 0:18:14.000
And then it runs all of these tasks until completion and then

267
0:18:14.000 --> 0:18:18.000
it returns. You also have a couple of functions down here

268
0:18:18.000 --> 0:18:22.000
for waiting for certain things. So sometimes in an application

269
0:18:22.000 --> 0:18:25.000
you need to synchronize between various different parts. So

270
0:18:25.000 --> 0:18:29.000
there are some handy functions for this as well. Now what is

271
0:18:29.000 --> 0:18:33.000
this task object I keep mentioning? Task objects are

272
0:18:33.000 --> 0:18:37.000
basically just coroutine calls that are being scheduled. And

273
0:18:37.000 --> 0:18:41.000
it's a way for the event loop to manage everything that happens

274
0:18:41.000 --> 0:18:46.000
in the event loop. So whenever something is scheduled to be

275
0:18:46.000 --> 0:18:49.000
run, you create a task object. And this is done behind the

276
0:18:49.000 --> 0:18:52.000
scenes for you. You don't have to create these objects

277
0:18:52.000 --> 0:18:55.000
yourself. In fact, you should not create these objects

278
0:18:55.000 --> 0:18:58.000
yourself. You should always use one of the functions for this,

279
0:18:58.000 --> 0:19:04.000
like this create task that you have down here. And then these

280
0:19:04.000 --> 0:19:07.000
task objects go into the event loop, are run, and everything

281
0:19:07.000 --> 0:19:10.000
happens for you. There are also some query functions down here

282
0:19:10.000 --> 0:19:13.000
if you're interested in what's currently scheduled on the

283
0:19:13.000 --> 0:19:17.000
event loop. You can have a look at the documentation for those.

284
0:19:17.000 --> 0:19:21.000
So how does this event loop work? It's basically just a way

285
0:19:21.000 --> 0:19:26.000
just to do the same kind of management as the OS is doing

286
0:19:26.000 --> 0:19:31.000
for threads, except that it's done in Python. And the

287
0:19:31.000 --> 0:19:36.000
asyncio package manages one of these event loops. Now event

288
0:19:36.000 --> 0:19:41.000
loops can actually be defined by multiple different libraries.

289
0:19:41.000 --> 0:19:44.000
But what's important is that there should only be one event

290
0:19:44.000 --> 0:19:49.000
loop per thread. So you can have multiple threads, of course,

291
0:19:49.000 --> 0:19:53.000
also run. Then again, you hit the same kind of roadblock as

292
0:19:53.000 --> 0:20:00.000
you've seen with the gil. But there might be ways in your

293
0:20:00.000 --> 0:20:04.000
application to make use of that. So that would be possible as

294
0:20:04.000 --> 0:20:07.000
well. Typically what you have in the async program is you just

295
0:20:07.000 --> 0:20:11.000
have a single thread. And so you just call this run function

296
0:20:11.000 --> 0:20:16.000
once. Now, I mentioned blocking code. So blocking code basically

297
0:20:16.000 --> 0:20:21.000
is code that doesn't collaborate with this async logic. And you

298
0:20:21.000 --> 0:20:24.000
have that quite often in Python. For example, let's say you're

299
0:20:24.000 --> 0:20:28.000
using one of the database modules, not one of the async

300
0:20:28.000 --> 0:20:32.000
modules, but the regular ones. Those will all be synchronous.

301
0:20:32.000 --> 0:20:36.000
So you call, let's say, an execute to run some SQL. And

302
0:20:36.000 --> 0:20:39.000
that will actually wait until the database comes back with

303
0:20:39.000 --> 0:20:44.000
the results. So in order to run this kind of code in an async

304
0:20:44.000 --> 0:20:48.000
application, you have to use special functions. There's a

305
0:20:48.000 --> 0:20:52.000
very nice function called asyncio2thread, which was

306
0:20:52.000 --> 0:20:57.000
added in Python 3.9, which makes this easy. So what these

307
0:20:57.000 --> 0:21:03.000
functions do is they spin up a thread in your async

308
0:21:03.000 --> 0:21:07.000
application, run the code inside that thread, and then pass back

309
0:21:07.000 --> 0:21:12.000
control via the threading logic to your event loop. So you can

310
0:21:12.000 --> 0:21:15.000
still run synchronous code because the synchronous code is

311
0:21:15.000 --> 0:21:19.000
most likely going to give up the gil. For example, if you have

312
0:21:19.000 --> 0:21:23.000
a good database module, then when you execute something,

313
0:21:23.000 --> 0:21:26.000
typically what these database modules do is they give back

314
0:21:26.000 --> 0:21:30.000
control to other threads running Python code because they're

315
0:21:30.000 --> 0:21:34.000
just running C code at that time. So this is a way to make

316
0:21:34.000 --> 0:21:39.000
everything work together. And of course, there's lots more.

317
0:21:39.000 --> 0:21:42.000
I'm not going to talk about these things because I don't

318
0:21:42.000 --> 0:21:45.000
have enough time for that. In fact, I'm already almost out

319
0:21:45.000 --> 0:21:49.000
of time, so I have to speed up a bit. Let's just do a very

320
0:21:49.000 --> 0:21:54.000
quick view, overview of what's in the async ecosystem. So of

321
0:21:54.000 --> 0:21:58.000
course, we have the async IO standard library package. We

322
0:21:58.000 --> 0:22:02.000
have event loops inside the async IO package. If you want

323
0:22:02.000 --> 0:22:05.000
fast loops, then you can use UV loop, which is the faster

324
0:22:05.000 --> 0:22:09.000
implementation, speeds up your async by almost four times.

325
0:22:09.000 --> 0:22:12.000
You can also have a look at other stacks that implement

326
0:22:12.000 --> 0:22:15.000
event loops, like trio, for example, or you can use the

327
0:22:15.000 --> 0:22:20.000
package any IO, which abstracts these things. So you can use

328
0:22:20.000 --> 0:22:25.000
async IO loops underneath in your application when using any

329
0:22:25.000 --> 0:22:29.000
IO and in abstracts away other details. Now, there's a rather

330
0:22:29.000 --> 0:22:34.000
large system of modules and packages around the async world

331
0:22:34.000 --> 0:22:40.000
in Python. Many of these are grouped under the AIO libs. So

332
0:22:40.000 --> 0:22:45.000
if you go to GitHub to that URL, then you will find lots of

333
0:22:45.000 --> 0:22:50.000
examples there. There are database packages there. There

334
0:22:50.000 --> 0:22:54.000
are things for doing HTTP DNS and so on. Something to watch

335
0:22:54.000 --> 0:22:58.000
out is the database modules typically don't support

336
0:22:58.000 --> 0:23:03.000
transactions, which can be a bummer sometimes. At the

337
0:23:03.000 --> 0:23:07.000
higher level, you have, of course, web frameworks again,

338
0:23:07.000 --> 0:23:11.000
because, you know, everyone loves web frameworks. And of

339
0:23:11.000 --> 0:23:15.000
course, you have API frameworks. The most popular one right

340
0:23:15.000 --> 0:23:19.000
now is fast API for doing the rest APIs, and then

341
0:23:19.000 --> 0:23:24.000
strawberries is coming very strongly as a GraphQL server.

342
0:23:24.000 --> 0:23:29.000
Both operate async. Even Django does or starts is starting

343
0:23:29.000 --> 0:23:33.000
to do async right now. It's not fully there yet. If you're

344
0:23:33.000 --> 0:23:37.000
using Flask for synchronous code, then you might want to

345
0:23:37.000 --> 0:23:40.000
look at a quart, which is like an async implementation using

346
0:23:40.000 --> 0:23:45.000
a similar API. And the most famous one probably in the

347
0:23:45.000 --> 0:23:49.000
async space is tornado, which some of you may know. It's

348
0:23:49.000 --> 0:23:54.000
very fast. Right. So let's go back to the bot. If you want

349
0:23:54.000 --> 0:23:57.000
to see the bot code, it's on GitHub. Just search for

350
0:23:57.000 --> 0:24:01.000
eugenics telegram and then you'll find it. How does it

351
0:24:01.000 --> 0:24:04.000
work? Very easy. You just subclass the client of that

352
0:24:04.000 --> 0:24:07.000
package. You do some configuration. You do some

353
0:24:07.000 --> 0:24:11.000
observability. So logging for things. I'm lazy. So I just

354
0:24:11.000 --> 0:24:15.000
put all the admin messages into another telegram chat that I

355
0:24:15.000 --> 0:24:19.000
can manage so I can see what's happening without having to go

356
0:24:19.000 --> 0:24:25.000
to the server. Because we actually want to catch all the

357
0:24:25.000 --> 0:24:29.000
messages of these people signing up to the telegram

358
0:24:29.000 --> 0:24:33.000
group, and not just people who want to run bot commands,

359
0:24:33.000 --> 0:24:37.000
slash something. We have to use the catch all in here. And

360
0:24:37.000 --> 0:24:40.000
that's also why we need something that's very scalable

361
0:24:40.000 --> 0:24:43.000
because literally the bot sees all the messages that go into

362
0:24:43.000 --> 0:24:48.000
that group and it has to handle all these messages. And then

363
0:24:48.000 --> 0:24:53.000
what you do is basically you just do these awaits whenever

364
0:24:53.000 --> 0:24:56.000
you have to do I.O. And if you look at it, it looks very

365
0:24:56.000 --> 0:24:59.000
much like synchronous code, right? Except that you have

366
0:24:59.000 --> 0:25:02.000
these awaits in front of certain things. Wherever something

367
0:25:02.000 --> 0:25:05.000
happens where you need to do some I.O., you put the await

368
0:25:05.000 --> 0:25:09.000
in front of it. And then everything else looks very

369
0:25:09.000 --> 0:25:13.000
natural. Looks like a very much like a synchronous program.

370
0:25:13.000 --> 0:25:22.000
So what are the results of doing this? Writing this bot,

371
0:25:22.000 --> 0:25:26.000
it's actually helped us a lot. We've had over almost 800

372
0:25:26.000 --> 0:25:34.000
spam signups since April 2022 when we started to use it. And

373
0:25:34.000 --> 0:25:37.000
this has, I mean, this is one part, this is just the admin

374
0:25:37.000 --> 0:25:40.000
part that saved us a lot of work. But of course, you know,

375
0:25:40.000 --> 0:25:44.000
every single signup would have caused spam messages. And so

376
0:25:44.000 --> 0:25:47.000
that was very successful. So the time spent on actually

377
0:25:47.000 --> 0:25:52.000
writing things was well invested and basically mission

378
0:25:52.000 --> 0:25:56.000
accomplished. So what's the main takeaway of the talk?

379
0:25:56.000 --> 0:26:02.000
I think it's great. And give it a try if you can. Thank you

380
0:26:02.000 --> 0:26:07.000
for your attention. Thank you for your attention.

381
0:26:16.000 --> 0:26:18.000
Thank you, Mark. Thanks, everyone, for attending this

382
0:26:18.000 --> 0:26:22.000
talk. Don't hesitate to reach to Mark and Ray if you have

383
0:26:22.000 --> 0:26:27.000
any questions or want to discuss this further. Thanks a

384
0:26:27.000 --> 0:26:31.000
lot. Thank you. Thank you very much for coming. Let me just

385
0:26:31.000 --> 0:26:49.000
show you a picture. Excellent.

