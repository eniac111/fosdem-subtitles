1
0:00:00.000 --> 0:00:13.680
Let's welcome Pedro Halanda for his talk on DuckDB and Magnificent Snake Duck.

2
0:00:13.680 --> 0:00:17.400
Yeah, you guys can be surprised at all.

3
0:00:17.400 --> 0:00:20.200
Anything you can find as a rubber duck these days, you know.

4
0:00:20.200 --> 0:00:22.920
All right, so I'm Pedro Halanda.

5
0:00:22.920 --> 0:00:28.440
I am one of the main contributors of the DuckDB projects, which is an open source database

6
0:00:28.440 --> 0:00:29.440
system.

7
0:00:29.440 --> 0:00:32.640
I'm the COO of DuckDB Labs.

8
0:00:32.640 --> 0:00:36.640
And today I'm going to be talking a little bit about how DuckDB can bring analytical

9
0:00:36.640 --> 0:00:40.480
SQL power directly into your Python shell.

10
0:00:40.480 --> 0:00:44.920
So to give you a little bit of an idea of how these talks look like, I'm going to start

11
0:00:44.920 --> 0:00:46.640
with what is DuckDB?

12
0:00:46.640 --> 0:00:50.080
So I'm here talking about one more database system.

13
0:00:50.080 --> 0:00:54.360
I'm going to motivate you guys that we actually needed to do one more database system.

14
0:00:54.360 --> 0:00:57.560
The other ones didn't solve the problems we had.

15
0:00:57.560 --> 0:01:00.360
And then I'm going to go over the main characteristics of DuckDB.

16
0:01:00.360 --> 0:01:03.720
So what actually makes it special?

17
0:01:03.720 --> 0:01:11.120
Then I'm going to go over DuckDB Python line, so how DuckDB integrates in the Python ecosystem.

18
0:01:11.120 --> 0:01:12.760
I'm going to do a little demo.

19
0:01:12.760 --> 0:01:18.160
The basic idea is that we're going to use the infamous New York City taxi data sets.

20
0:01:18.160 --> 0:01:23.060
And we're going to try to do some estimation of fair costs.

21
0:01:23.060 --> 0:01:27.400
And we're going to use DuckDB pandas and PySpark just to see a couple of the differences.

22
0:01:27.400 --> 0:01:30.120
Of the things I'm going to be talking over.

23
0:01:30.120 --> 0:01:32.440
And then some summary of the talk.

24
0:01:32.440 --> 0:01:33.440
So what is DuckDB?

25
0:01:33.440 --> 0:01:39.260
Well, DuckDB was actually born at CWI, which is the Research Center of Mathematics and

26
0:01:39.260 --> 0:01:41.720
Computer Science in the Netherlands.

27
0:01:41.720 --> 0:01:45.760
And what we actually had there is that a lot of the projects, the PhDs in the projects,

28
0:01:45.760 --> 0:01:48.800
the master projects, they are very data sciencey.

29
0:01:48.800 --> 0:01:52.360
So usually you have a data science problem and you want to throw a database management

30
0:01:52.360 --> 0:01:56.160
system at a data science problem because you're handling data.

31
0:01:56.160 --> 0:02:02.080
So initially we're like, okay, we can probably use a database server, use a database connection,

32
0:02:02.080 --> 0:02:07.600
and then just transfer the data from the relational database to your Python terminal, for example,

33
0:02:07.600 --> 0:02:08.600
right?

34
0:02:08.600 --> 0:02:10.400
Like where your analytical tools are.

35
0:02:10.400 --> 0:02:14.920
And it turns out that's quite a bad idea because you are transferring a lot of data.

36
0:02:14.920 --> 0:02:16.880
So that's pretty costly.

37
0:02:16.880 --> 0:02:19.760
And then we're like, okay, this is really not solving our problem.

38
0:02:19.760 --> 0:02:22.200
Can we draw inspiration from somewhere else?

39
0:02:22.200 --> 0:02:25.800
And then of course there are SQLites, the most famous database out there, at least the

40
0:02:25.800 --> 0:02:28.120
most used one.

41
0:02:28.120 --> 0:02:33.600
And it has a quite a nice property, which is being an embedded database system.

42
0:02:33.600 --> 0:02:37.320
Being an embedded database system, it means it can run inside your Python process.

43
0:02:37.320 --> 0:02:41.400
So you can eliminate this data transfer cost.

44
0:02:41.400 --> 0:02:46.440
SQLite comes with one design decision that is a transactional database.

45
0:02:46.440 --> 0:02:50.920
So it's actually super optimized for small updates, but it's not really optimized for

46
0:02:50.920 --> 0:02:52.720
analytics.

47
0:02:52.720 --> 0:02:57.760
So we kind of wanted to use SQLites in terms of being easy to use and eliminating this

48
0:02:57.760 --> 0:03:03.160
data transfer cost, but focusing on analytical queries.

49
0:03:03.160 --> 0:03:07.480
And that's kind of how the code was born, and that's also why we frame it as a SQLite

50
0:03:07.480 --> 0:03:09.560
for analytics.

51
0:03:09.560 --> 0:03:11.240
It also has a very simple installation.

52
0:03:11.240 --> 0:03:14.760
So if you think about Python, it's just a big solve and you're good.

53
0:03:14.760 --> 0:03:16.840
Since it's embedded, there's no server management.

54
0:03:16.840 --> 0:03:21.080
So let's say you just want to, I don't know, query a per K file, two lines of code you

55
0:03:21.080 --> 0:03:22.080
can already query it.

56
0:03:22.080 --> 0:03:24.880
There's no starting of server.

57
0:03:24.880 --> 0:03:27.280
There's no schema creation.

58
0:03:27.280 --> 0:03:28.960
The schema is inferred from the object.

59
0:03:28.960 --> 0:03:31.240
So it's very easy, very fast.

60
0:03:31.240 --> 0:03:35.680
And we also really focus on this fast transfer between analytical languages.

61
0:03:35.680 --> 0:03:39.000
And there are tools like in Python and R to DuckDB.

62
0:03:39.000 --> 0:03:41.840
DuckDB is currently in pre-release.

63
0:03:41.840 --> 0:03:46.760
I think the last version we released was 0.6, 0.7 is coming up soon.

64
0:03:46.760 --> 0:03:52.400
In the web page, there's a little bit more details about all the things that are in each

65
0:03:52.400 --> 0:03:53.400
release.

66
0:03:53.400 --> 0:03:54.400
All right.

67
0:03:54.400 --> 0:04:01.760
So I'm going to go over some of the main characteristics of DuckDB, particularly the color data storage,

68
0:04:01.760 --> 0:04:02.760
a little bit about compression.

69
0:04:02.760 --> 0:04:05.600
I'm going to talk about vectorized execution.

70
0:04:05.600 --> 0:04:07.720
So these are all like core database stuff.

71
0:04:07.720 --> 0:04:12.840
Actually talking about vectorized execution engine, it's going to be difficult because

72
0:04:12.840 --> 0:04:15.640
Professor Bong is here and he actually created that.

73
0:04:15.640 --> 0:04:20.040
So I'll try to do it correctly.

74
0:04:20.040 --> 0:04:25.160
A little bit about intimate corruptimization, parallelism and beyond memory execution.

75
0:04:25.160 --> 0:04:26.760
So color data storage.

76
0:04:26.760 --> 0:04:30.000
Well, there's basically two ways that you can do it.

77
0:04:30.000 --> 0:04:32.320
One is a row store, a scan store.

78
0:04:32.320 --> 0:04:34.920
As an example, row store, we have SQLites.

79
0:04:34.920 --> 0:04:39.760
And the whole thing about the whole idea is that you're storing your data consecutively

80
0:04:39.760 --> 0:04:42.920
in memory per row.

81
0:04:42.920 --> 0:04:47.400
So that basically means that if you want to fetch a row, that's very cheap because it's

82
0:04:47.400 --> 0:04:48.920
contiguous in memory.

83
0:04:48.920 --> 0:04:52.200
However, you always have to fetch all the columns.

84
0:04:52.200 --> 0:04:58.160
So analytical queries usually you have very wide tables, but you just want to really get

85
0:04:58.160 --> 0:05:00.240
a couple of these columns.

86
0:05:00.240 --> 0:05:02.020
So what if you only want to use a field?

87
0:05:02.020 --> 0:05:08.120
So in this example, what if you just are interested in the price of a product, but not the stores

88
0:05:08.120 --> 0:05:11.440
they sold, right?

89
0:05:11.440 --> 0:05:19.200
In a column store, you actually have your layouts that the data of the column is consecutively

90
0:05:19.200 --> 0:05:20.200
in memory.

91
0:05:20.200 --> 0:05:24.440
So if you want to access just a couple columns, you can actually have immense savings on disk

92
0:05:24.440 --> 0:05:27.240
I.O. and memory bandwidth.

93
0:05:27.240 --> 0:05:31.320
So that's why this type of format is really optimized for analytics.

94
0:05:31.320 --> 0:05:35.840
So to give you a more concrete example, let's say that we have a one terabyte table with

95
0:05:35.840 --> 0:05:38.040
100 columns.

96
0:05:38.040 --> 0:05:40.720
For simplicity, let's say all the columns have the same size.

97
0:05:40.720 --> 0:05:44.540
And we just require five columns of the table in our analytical queries.

98
0:05:44.540 --> 0:05:49.760
So in a row store like SQLites, reading this whole table, if you have a disk with around

99
0:05:49.760 --> 0:05:52.600
100 megabytes per second, it will take you three hours.

100
0:05:52.600 --> 0:05:57.160
If you were using a column store model, which is what Pandas and Ducktivitas, for example,

101
0:05:57.160 --> 0:06:00.840
reading these five columns from disk takes you eight minutes.

102
0:06:00.840 --> 0:06:08.280
So there's a huge improvement by just setting up the correct storage formats for your workload.

103
0:06:08.280 --> 0:06:12.840
Compression, well, I'm not going to go into a lot of detail about the compression algorithms

104
0:06:12.840 --> 0:06:15.760
that we implement in Ducktivi.

105
0:06:15.760 --> 0:06:20.120
But what I can tell you is because of having a column store, you're going to have your

106
0:06:20.120 --> 0:06:26.560
data from your column continuously in memory, which gives you a very good advantage to compress

107
0:06:26.560 --> 0:06:31.160
units, because usually the data from the same column is somewhat similar.

108
0:06:31.160 --> 0:06:41.080
So you can apply cool things like RLE, FSSC, and CHIMP for floating point numbers, FSSC

109
0:06:41.080 --> 0:06:42.080
for strings.

110
0:06:42.080 --> 0:06:45.800
So you can start applying all these algorithms and really decrease the size of your data.

111
0:06:45.800 --> 0:06:51.480
So in this table here, we actually have, I think this is from one year ago, one year

112
0:06:51.480 --> 0:06:55.600
and a half, 0.2.8 from Ducktivi.

113
0:06:55.600 --> 0:06:57.040
We had no compression at that point.

114
0:06:57.040 --> 0:07:01.840
And then a year and a half later, we actually managed to implement all these things which

115
0:07:01.840 --> 0:07:09.680
got us five times better compression line 19, for example, 3.18 better compression in

116
0:07:09.680 --> 0:07:12.640
the taxi data set that I'm going to be using later.

117
0:07:12.640 --> 0:07:13.840
And why is compression so important?

118
0:07:13.840 --> 0:07:19.040
Well, if we go back again to the same example where we're reading our five columns and it

119
0:07:19.040 --> 0:07:23.880
was causing us to read them from disk eight minutes because of the storage formats, if

120
0:07:23.880 --> 0:07:29.040
we compress these columns, we suddenly don't have to read 50 gigabytes anymore, right?

121
0:07:29.040 --> 0:07:30.120
You read less.

122
0:07:30.120 --> 0:07:34.000
And then of course, you apply the best case from what I showed you from the last table,

123
0:07:34.000 --> 0:07:35.000
five times.

124
0:07:35.000 --> 0:07:40.600
The RLE increases the cost to one minute and 40 seconds.

125
0:07:40.600 --> 0:07:42.600
So execution.

126
0:07:42.600 --> 0:07:47.080
Well, there's three ways of doing query execution.

127
0:07:47.080 --> 0:07:49.720
There's actually one more, but it's not in the slides.

128
0:07:49.720 --> 0:07:53.480
But SQLite uses a top-order time processing, which means that you process one row at a

129
0:07:53.480 --> 0:07:54.480
time.

130
0:07:54.480 --> 0:07:59.480
Pandas uses column-at-a-time processing, which means that you process one column at a time.

131
0:07:59.480 --> 0:08:04.920
Inductive B uses kind of like a mix of the both, which is a technique developed by Peter,

132
0:08:04.920 --> 0:08:09.720
the vectorized processing where you process batches of a column at a time.

133
0:08:09.720 --> 0:08:13.760
So basically, the top-order time model from SQLite, it was optimized for a time where

134
0:08:13.760 --> 0:08:15.720
computers didn't have a lot of memory.

135
0:08:15.720 --> 0:08:20.760
There was low memory to be used because you only need to really keep one row in memory

136
0:08:20.760 --> 0:08:23.840
throughout your whole query plan.

137
0:08:23.840 --> 0:08:25.560
So the memory was expensive.

138
0:08:25.560 --> 0:08:26.880
That's what you could do.

139
0:08:26.880 --> 0:08:31.480
But this comes with a high CPU overhead per tuple because you constantly resetting your

140
0:08:31.480 --> 0:08:33.000
cache.

141
0:08:33.000 --> 0:08:38.120
You don't have any cache-conscious algorithm running that piece of data up to the production

142
0:08:38.120 --> 0:08:40.000
of the query results.

143
0:08:40.000 --> 0:08:43.840
If you go to the column at a time, which is what Pandas uses, this already brings better

144
0:08:43.840 --> 0:08:44.840
CPU utilization.

145
0:08:44.840 --> 0:08:47.320
It allows for SIMD.

146
0:08:47.320 --> 0:08:52.040
But it comes with the cost of materializing large intermediates in memory.

147
0:08:52.040 --> 0:08:56.040
It basically means that you need the whole column in memory at that point to process

148
0:08:56.040 --> 0:08:57.360
for that operator.

149
0:08:57.360 --> 0:09:00.240
And of course, the intermediates can be gigabytes each.

150
0:09:00.240 --> 0:09:03.080
So that's pretty problematic when the data sizes are large.

151
0:09:03.080 --> 0:09:06.800
And that's why you see, for example, that Pandas, if your data doesn't fit in memory,

152
0:09:06.800 --> 0:09:07.800
what does it happen?

153
0:09:07.800 --> 0:09:10.480
It crashes.

154
0:09:10.480 --> 0:09:15.680
And then if you go to the vectorized query processing, it's actually optimized for CPU

155
0:09:15.680 --> 0:09:17.000
cache locality.

156
0:09:17.000 --> 0:09:19.240
You can see the instructions, pipelining.

157
0:09:19.240 --> 0:09:25.000
And the whole idea is that your intermediates are actually going to fit here in a L1 cache.

158
0:09:25.000 --> 0:09:28.520
So basically, you're going to be paying this latency of 100 milliseconds to be accessing

159
0:09:28.520 --> 0:09:33.280
your data throughout your query plan instead of paying the latency of a memory, which is

160
0:09:33.280 --> 0:09:37.320
also the case of a column database, which is 100 milliseconds.

161
0:09:37.320 --> 0:09:38.880
It seems like a small difference.

162
0:09:38.880 --> 0:09:46.320
But when you're constantly executing this, this really becomes a bottleneck.

163
0:09:46.320 --> 0:09:49.240
And so end-to-end query optimization is, of course, something that we have in DuckDB.

164
0:09:49.240 --> 0:09:54.200
So we have stuff like expression rewriting, join ordering, sub-query flattening, filtering

165
0:09:54.200 --> 0:09:59.440
and projection pushdown, which is a bit more simple, but it's extremely important and brings

166
0:09:59.440 --> 0:10:02.380
a huge difference in the cost of a query.

167
0:10:02.380 --> 0:10:05.720
So here's an example of a projection pushdown.

168
0:10:05.720 --> 0:10:10.400
Let's say you have a table with five columns, A, B, C, D, E. And you want to run a query.

169
0:10:10.400 --> 0:10:11.400
That's pretty small.

170
0:10:11.400 --> 0:10:17.240
The query is like it selects minimum from column A, where there's a filtering column

171
0:10:17.240 --> 0:10:21.600
A saying the column A is bigger than zero, and you group by B. So the whole point of

172
0:10:21.600 --> 0:10:24.800
the query is that you're only using two columns off the table.

173
0:10:24.800 --> 0:10:29.400
So what the DuckDB optimizer will do is like, OK, in the scanner, I know I don't need all

174
0:10:29.400 --> 0:10:30.400
the columns.

175
0:10:30.400 --> 0:10:31.400
I just need NMB.

176
0:10:31.400 --> 0:10:34.320
And you just don't have to read the other ones.

177
0:10:34.320 --> 0:10:38.720
If you do the same one in pandas, for example, you can apply your filter, and then you have

178
0:10:38.720 --> 0:10:41.000
the filter to group by the aggregator.

179
0:10:41.000 --> 0:10:43.960
But at the time you're doing this filter, you're still filtering all the other columns

180
0:10:43.960 --> 0:10:45.880
you're not going to be using your query.

181
0:10:45.880 --> 0:10:49.360
Of course, you can manually make this optimization.

182
0:10:49.360 --> 0:10:54.160
But it's pretty nice that the database system can do that for you.

183
0:10:54.160 --> 0:10:58.400
Of course, DuckDB also has automatic parallelism and beyond memory execution.

184
0:10:58.400 --> 0:11:02.000
So DuckDB has parallel versions of most of its operators.

185
0:11:02.000 --> 0:11:08.280
I think all of our scanners, including with insertion, all your preservation of parallelized

186
0:11:08.280 --> 0:11:14.680
now, aggregations, joins, pandas, for example, only supports single-threaded execution.

187
0:11:14.680 --> 0:11:17.240
We all have pretty good laptops these days, right?

188
0:11:17.240 --> 0:11:21.640
So it's a shame if you cannot really take advantage of parallelism.

189
0:11:21.640 --> 0:11:26.400
And DuckDB, again, supports the execution of data that does not fit in memory.

190
0:11:26.400 --> 0:11:28.760
It's kind of the never-give-up, never-surrender approach.

191
0:11:28.760 --> 0:11:32.120
It's like, we're going to execute this query.

192
0:11:32.120 --> 0:11:35.960
We try to always have graceful degradation also, that it just doesn't suddenly crash

193
0:11:35.960 --> 0:11:36.960
the performance.

194
0:11:36.960 --> 0:11:41.720
And the whole goal is really to never crash and always execute the query.

195
0:11:41.720 --> 0:11:43.080
All right.

196
0:11:43.080 --> 0:11:46.960
So a little bit about DuckDB in the Python lens.

197
0:11:46.960 --> 0:11:48.960
Basically we have an API.

198
0:11:48.960 --> 0:11:51.960
It's a DB API, it's a.org compliant.

199
0:11:51.960 --> 0:11:57.080
So very similar to what SQLite has, for example, where you can create a connection and you

200
0:11:57.080 --> 0:11:58.400
can start executing queries.

201
0:11:58.400 --> 0:12:04.280
But we also wanted to have something similar to the DataFrame API that still could, people

202
0:12:04.280 --> 0:12:08.360
that come from pandas, for example, could still have something familiar to work on.

203
0:12:08.360 --> 0:12:11.440
So here in this example, you can also create a connection.

204
0:12:11.440 --> 0:12:14.240
You can create this relation, which kind of looks like a DataFrame.

205
0:12:14.240 --> 0:12:16.480
It's just pointing to a table.

206
0:12:16.480 --> 0:12:21.560
You can do a show to inspect what the table is inside, and you can apply, for example,

207
0:12:21.560 --> 0:12:22.680
these chaining operators, right?

208
0:12:22.680 --> 0:12:24.520
Like a filter, a projection.

209
0:12:24.520 --> 0:12:29.480
So in the end, this is all lazily executed.

210
0:12:29.480 --> 0:12:33.880
And this also allows you to take advantage of the optimizer of DuckDB, even if you do

211
0:12:33.880 --> 0:12:38.080
the chaining operations.

212
0:12:38.080 --> 0:12:42.920
Of course, I talked to you about memory transfer.

213
0:12:42.920 --> 0:12:48.520
So we were very careful as well into being very integrated with these very common libraries

214
0:12:48.520 --> 0:12:49.600
in Python.

215
0:12:49.600 --> 0:12:57.040
So with pandas and PyArrow, for example, what we actually do is that in the end, for pandas,

216
0:12:57.040 --> 0:13:02.160
the columns are usually not PyArrows, which turns out they are C, vectors, which turns

217
0:13:02.160 --> 0:13:04.280
out it's also kind of what we use.

218
0:13:04.280 --> 0:13:09.040
So with a little bit of makeup in the metadata, we can just directly read them.

219
0:13:09.040 --> 0:13:10.720
And they're all in the same process, right?

220
0:13:10.720 --> 0:13:16.520
So we have access to that piece of memory, which in the end means that you can actually

221
0:13:16.520 --> 0:13:21.720
access the data from pandas in DuckDB without paying any transfer costs, at least constant

222
0:13:21.720 --> 0:13:26.400
transfer costs just for doing the metadata makeup, let's say.

223
0:13:26.400 --> 0:13:28.120
And there's the same thing with PyArrow.

224
0:13:28.120 --> 0:13:30.280
We also have what we call zero copy.

225
0:13:30.280 --> 0:13:36.880
So we can read error objects and output error objects without any extra cost.

226
0:13:36.880 --> 0:13:42.960
With NumPy, we also support SQL, and in IBIS, we're actually the default backend from them,

227
0:13:42.960 --> 0:13:48.400
I think, since six months ago.

228
0:13:48.400 --> 0:13:49.640
A little bit of usage.

229
0:13:49.640 --> 0:13:55.680
So as you can see, this is our PyPy download count.

230
0:13:55.680 --> 0:13:58.440
The Python library is actually our most downloaded API.

231
0:13:58.440 --> 0:14:01.720
We have APIs for all sorts of languages.

232
0:14:01.720 --> 0:14:05.680
And you can see that in the last month, we had like $900,000.

233
0:14:05.680 --> 0:14:12.200
So there are a lot of people that are trying out and using DuckDB in their Python scripts.

234
0:14:12.200 --> 0:14:14.600
So now is the demo time.

235
0:14:14.600 --> 0:14:18.640
We got this.

236
0:14:18.640 --> 0:14:21.880
All right.

237
0:14:21.880 --> 0:14:23.080
This looks like you can see.

238
0:14:23.080 --> 0:14:29.280
So this is just installing DuckDB, PySpark, and getting our yellow trip data set.

239
0:14:29.280 --> 0:14:32.040
I executed this already.

240
0:14:32.040 --> 0:14:35.320
Just importing the stuff we're going to be using.

241
0:14:35.320 --> 0:14:41.040
And here is just getting a connection from DuckDB, creating a relation that's just, okay,

242
0:14:41.040 --> 0:14:45.080
we're going to, as a parquet file, DuckDB can read parquet files, and then you can just

243
0:14:45.080 --> 0:14:47.080
print to inspect what's out there.

244
0:14:47.080 --> 0:14:51.320
So if we run this, we can see, okay, these are the columns we have.

245
0:14:51.320 --> 0:14:56.200
We have vendor ID, we have pickup dates, time, passenger counts, you have the types of the

246
0:14:56.200 --> 0:14:57.200
columns.

247
0:14:57.200 --> 0:15:01.160
You can also have a little result preview to have an idea of what it looks like.

248
0:15:01.160 --> 0:15:05.040
So I think this data set has about like 20 columns maybe.

249
0:15:05.040 --> 0:15:12.200
And there's just information about the taxi rides in New York in 2016.

250
0:15:12.200 --> 0:15:15.400
And then you can also, for example, run a simple query here.

251
0:15:15.400 --> 0:15:19.080
I'm just doing like accounts to know how many tuples are there.

252
0:15:19.080 --> 0:15:22.680
And we have about 10 million tuples on this data set.

253
0:15:22.680 --> 0:15:23.680
All right.

254
0:15:23.680 --> 0:15:27.200
So this function here is just to do a little bit of benchmarking coming from academia.

255
0:15:27.200 --> 0:15:31.360
We do have to do something that's kind of fair, I guess.

256
0:15:31.360 --> 0:15:35.280
So I run just five times and take the median time of everything.

257
0:15:35.280 --> 0:15:37.160
And then this is actually where our demo starts.

258
0:15:37.160 --> 0:15:39.640
So we start off with a data frame.

259
0:15:39.640 --> 0:15:41.640
So Pandas can also read parquet files.

260
0:15:41.640 --> 0:15:46.400
And the whole thing about DuckDB, again, is that it's not here as a replacement for Pandas.

261
0:15:46.400 --> 0:15:50.560
This is not what I'm trying to sell, but it's something that can work together with Pandas.

262
0:15:50.560 --> 0:15:57.000
So the cool thing is that we can, again, read and output data frames without any extra cost.

263
0:15:57.000 --> 0:16:01.120
So let's say that in the query here, we're just getting the passenger counts.

264
0:16:01.120 --> 0:16:07.480
Then the average trip amounts of trips that had a short distance, right?

265
0:16:07.480 --> 0:16:10.680
And the group by passengers, by the number of passengers.

266
0:16:10.680 --> 0:16:18.400
So what we want to know is for short trips, does the amount of tip matters by the number

267
0:16:18.400 --> 0:16:22.000
of passengers in that rides?

268
0:16:22.000 --> 0:16:26.520
And what you can see here is that you can, again, read from the data frame.

269
0:16:26.520 --> 0:16:27.640
That's what we're doing.

270
0:16:27.640 --> 0:16:32.480
And we just have to use the data frame name in our SQL query.

271
0:16:32.480 --> 0:16:37.120
And if you call.df from the query results, you also output your data frame.

272
0:16:37.120 --> 0:16:42.360
And it's pretty cool because the data frames have these plot bars, they have plotting capabilities

273
0:16:42.360 --> 0:16:47.240
that DuckDB doesn't have, and you can get easily a very nice chart.

274
0:16:47.240 --> 0:16:52.960
So you see here, apparently, there's some dirty data because people are getting tips

275
0:16:52.960 --> 0:16:55.560
when they don't have anyone in their rides.

276
0:16:55.560 --> 0:16:56.760
Not sure what that is.

277
0:16:56.760 --> 0:17:01.640
But apparently, if you have more people 7 to 9, maybe the more expensive cars, you get

278
0:17:01.640 --> 0:17:03.480
a higher tip.

279
0:17:03.480 --> 0:17:05.720
And you can do the same thing in Pandas, of course, right?

280
0:17:05.720 --> 0:17:09.200
So in Pandas you don't have SQL, you're going to have to use their own language to do the

281
0:17:09.200 --> 0:17:13.520
group by, the average, and you can directly use the plots.

282
0:17:13.520 --> 0:17:17.600
And the whole point here is to show the different execution time.

283
0:17:17.600 --> 0:17:19.200
Like now we're waiting.

284
0:17:19.200 --> 0:17:22.160
Okay, so it took a second.

285
0:17:22.160 --> 0:17:25.920
And DuckDB took 0.2, so this is like a 5x, right?

286
0:17:25.920 --> 0:17:28.120
0.25, so like 4x.

287
0:17:28.120 --> 0:17:31.920
And you also have to consider that we're using not a very beefy machine, right?

288
0:17:31.920 --> 0:17:33.480
This is a Colab machine.

289
0:17:33.480 --> 0:17:37.720
If you had more cores, this difference would also be bigger.

290
0:17:37.720 --> 0:17:40.120
And then I added Spark for fun.

291
0:17:40.120 --> 0:17:46.000
So actually, Spark can also read data frames, but it crashes out of memory in my Colab machine,

292
0:17:46.000 --> 0:17:50.040
so I had to give up on that and read directly from Parquet files.

293
0:17:50.040 --> 0:17:54.880
But it does output it as a data frame.

294
0:17:54.880 --> 0:17:56.960
I think we're going to have to wait a little bit.

295
0:17:56.960 --> 0:18:00.840
But as me, it's best.

296
0:18:00.840 --> 0:18:06.720
So of course Spark is not designed for small data sets, but turns out there are a lot of

297
0:18:06.720 --> 0:18:08.720
use cases where you use small data sets.

298
0:18:08.720 --> 0:18:09.720
It's still going.

299
0:18:09.720 --> 0:18:14.120
It's warming up a little bit.

300
0:18:14.120 --> 0:18:15.120
It's good for the winter.

301
0:18:15.120 --> 0:18:18.320
It produces some energy, I think.

302
0:18:18.320 --> 0:18:20.320
All right.

303
0:18:20.320 --> 0:18:21.320
Okay.

304
0:18:21.320 --> 0:18:26.680
So it took two seconds, 2.2 seconds.

305
0:18:26.680 --> 0:18:31.240
The extra execution, and that's already like what?

306
0:18:31.240 --> 0:18:32.800
More than two times what Pandas was.

307
0:18:32.800 --> 0:18:33.800
So yeah.

308
0:18:33.800 --> 0:18:39.560
Anyway, for the demo, of course, I showed you something that's fairly simple.

309
0:18:39.560 --> 0:18:41.280
Can you do actually very complicated things?

310
0:18:41.280 --> 0:18:43.760
Maybe not very complicated, but more complicated.

311
0:18:43.760 --> 0:18:47.280
So here I'm not really going to go over the query, but the whole idea is that we can just

312
0:18:47.280 --> 0:18:50.840
like for example use DuckDB to run a linear regression, right?

313
0:18:50.840 --> 0:18:58.600
So can we predict, can we estimate the fare with the trip distance?

314
0:18:58.600 --> 0:19:04.460
And turns out you can just calculate the alpha and beta with not such a crazy query, right?

315
0:19:04.460 --> 0:19:08.680
And then you can again export it to Pandas, and you have a very nice figure there.

316
0:19:08.680 --> 0:19:14.520
So you can really combine these two to get the best out of both.

317
0:19:14.520 --> 0:19:17.280
All right.

318
0:19:17.280 --> 0:19:18.280
That was a demo.

319
0:19:18.280 --> 0:19:19.280
Summary.

320
0:19:19.280 --> 0:19:20.720
Oh, that's my last slide.

321
0:19:20.720 --> 0:19:21.720
Very good.

322
0:19:21.720 --> 0:19:24.640
So yeah, DuckDB is an embedded database system.

323
0:19:24.640 --> 0:19:26.040
Again it's completely open source.

324
0:19:26.040 --> 0:19:28.480
It is under the MIT license.

325
0:19:28.480 --> 0:19:31.360
Since it came from academia, this is something that we're always worried about.

326
0:19:31.360 --> 0:19:36.520
It's to also give it back to everyone because it was initially funded by taxpayers money

327
0:19:36.520 --> 0:19:37.960
so everyone can use it.

328
0:19:37.960 --> 0:19:40.640
100% of what we do is actually open source.

329
0:19:40.640 --> 0:19:43.880
There's nothing that's closed source.

330
0:19:43.880 --> 0:19:46.080
It's designed for analytical queries.

331
0:19:46.080 --> 0:19:51.360
So data analysis, data science has binding for many languages.

332
0:19:51.360 --> 0:19:53.680
So of course I'm at the Python dev room.

333
0:19:53.680 --> 0:19:55.160
I'm talking about Python.

334
0:19:55.160 --> 0:19:57.480
But we have R, Java.

335
0:19:57.480 --> 0:19:59.880
Turns out that Java is one of our most downloaded APIs.

336
0:19:59.880 --> 0:20:03.760
So I guess that's an interesting sign.

337
0:20:03.760 --> 0:20:06.120
JavaScript and a bunch of others.

338
0:20:06.120 --> 0:20:09.520
It has very tight integrations with the Python ecosystem.

339
0:20:09.520 --> 0:20:15.280
Again the whole idea is that you eliminate transfer costs, implement the database in

340
0:20:15.280 --> 0:20:16.280
relation to APIs.

341
0:20:16.280 --> 0:20:20.120
The relation to API again is this more data frame like.

342
0:20:20.120 --> 0:20:21.520
It has full SQL support.

343
0:20:21.520 --> 0:20:27.520
So anything you can imagine like window functions or whatnot, you can just express them using

344
0:20:27.520 --> 0:20:28.520
DuckDB.

345
0:20:28.520 --> 0:20:30.040
And that's it.

346
0:20:30.040 --> 0:20:35.520
Thank you very much for paying attention.

347
0:20:35.520 --> 0:20:52.680
Happy to answer questions.

348
0:20:52.680 --> 0:21:02.920
Thank you, we have five minutes for questions.

349
0:21:02.920 --> 0:21:13.520
You mentioned beyond memory execution and that it tries not to degrade as much.

350
0:21:13.520 --> 0:21:19.080
Can you shine a little bit more light on what happens under the hood and how much degradation

351
0:21:19.080 --> 0:21:20.960
happens?

352
0:21:20.960 --> 0:21:26.000
Of course I think there's only the ordering operator that actually does that.

353
0:21:26.000 --> 0:21:29.020
We have Lawrence that's doing his PTC.

354
0:21:29.020 --> 0:21:33.120
So there's a lot of operators that need to do research to be developed.

355
0:21:33.120 --> 0:21:36.040
That's more of a goal than something that actually happens now.

356
0:21:36.040 --> 0:21:40.400
But the whole goal is that you really don't have this sudden spike in the future.

357
0:21:40.400 --> 0:21:57.260
But there's research going on in the future that will be more to be shared for sure.

358
0:21:57.260 --> 0:22:03.600
Thank you very much for the talk and it's very exciting to see such a powerful tool.

359
0:22:03.600 --> 0:22:09.920
I'm working usually with data warehouses and I saw on the website that you do not recommend

360
0:22:09.920 --> 0:22:12.800
using this with data warehouses.

361
0:22:12.800 --> 0:22:15.320
I would like to know why.

362
0:22:15.320 --> 0:22:19.520
So of course there's no one solution for our problems.

363
0:22:19.520 --> 0:22:23.680
There are cases that data warehouses are very good fits.

364
0:22:23.680 --> 0:22:29.120
It turns out that for data science, for example, which is kind of what we preach the most,

365
0:22:29.120 --> 0:22:33.120
they're usually not good because then you fall back to the senior data outside your

366
0:22:33.120 --> 0:22:34.120
database system.

367
0:22:34.120 --> 0:22:38.000
You're not really going to be running your Python codes inside the system.

368
0:22:38.000 --> 0:22:40.960
You can do that for UDS, for example, but they are messy.

369
0:22:40.960 --> 0:22:42.000
They're a bit nasty.

370
0:22:42.000 --> 0:22:46.560
So you want really to have it embedded in your Python process so you completely eliminate

371
0:22:46.560 --> 0:22:48.200
the data transfer costs.

372
0:22:48.200 --> 0:22:51.640
Because usually what you do is like, okay, I have a table, 10 columns, I'm going over

373
0:22:51.640 --> 0:22:56.560
four columns but I'm really hitting, reading like huge chunks of it.

374
0:22:56.560 --> 0:23:04.920
So that's a bottleneck we try to eliminate.

375
0:23:04.920 --> 0:23:05.920
How do you handle updates?

376
0:23:05.920 --> 0:23:10.920
Yeah, although we are in the Lidical database system, we do do updates.

377
0:23:10.920 --> 0:23:14.320
So Mark, I don't know where he is, but he's there.

378
0:23:14.320 --> 0:23:18.120
He developed a MVCC algorithm for OLAF.

379
0:23:18.120 --> 0:23:23.120
So we have same asset transaction capabilities that you would expect from a transactional

380
0:23:23.120 --> 0:23:24.120
database.

381
0:23:24.120 --> 0:23:30.800
Of course, if you have a transaction workloads, you should still go for Postgres or SQLIs

382
0:23:30.800 --> 0:23:34.000
or a database that handles this type of transactions.

383
0:23:34.000 --> 0:23:39.280
But Mark has developed like a full-on algorithm to handle updates completely.

384
0:23:39.280 --> 0:23:40.280
Yeah?

385
0:23:40.280 --> 0:23:43.160
How do you compare to Vertica?

386
0:23:43.160 --> 0:23:44.640
How do you compare to Vertica?

387
0:23:44.640 --> 0:23:53.440
Good question, I think in terms of analytical queries, TPCA is probably similar performance.

388
0:23:53.440 --> 0:23:58.560
But then again, the whole point is that if you go again for the Python process, the data

389
0:23:58.560 --> 0:24:02.320
transfer costs will take most of the time there.

390
0:24:02.320 --> 0:24:11.320
And then it's really catered for this type of scenario, the embedded scenario.

391
0:24:11.320 --> 0:24:14.840
We have one minute left for one more question.

392
0:24:14.840 --> 0:24:19.800
Yeah, I actually have a repo somewhere for a bunch of examples as well.

393
0:24:19.800 --> 0:24:20.800
I'm very happy to share it.

394
0:24:20.800 --> 0:24:22.800
I don't know where I'll post it.

395
0:24:22.800 --> 0:24:25.800
Ah, there's only the first one thing, I guess.

396
0:24:25.800 --> 0:24:26.800
All right.

397
0:24:26.800 --> 0:24:27.800
All right, thank you a lot, Pedro.

398
0:24:27.800 --> 0:24:28.800
Thanks a lot.

399
0:24:28.800 --> 0:24:29.800
Thank you very much.

400
0:24:29.800 --> 0:24:46.800
Any other questions or comments?

