1
0:00:00.000 --> 0:00:09.480
Okay, welcome back.

2
0:00:09.480 --> 0:00:13.640
So while you all have been walking in, I've been quickly reading this book, Fish and Go,

3
0:00:13.640 --> 0:00:18.640
it reads very quickly, and now Bartek has made sure that my code is ten times quicker,

4
0:00:18.640 --> 0:00:19.840
so tell us everything about it.

5
0:00:19.840 --> 0:00:21.000
Thank you.

6
0:00:21.000 --> 0:00:28.560
Thank you very much, everybody.

7
0:00:28.560 --> 0:00:29.560
You're welcome.

8
0:00:29.560 --> 0:00:31.640
I hope your travels went well.

9
0:00:31.640 --> 0:00:37.560
Mine were top-party, you know, Play, Cancel, Change your pre-route, so I had lots of adventures,

10
0:00:37.560 --> 0:00:41.600
but generally I'm super happy I made it, and we are at the FOSDEM.

11
0:00:41.600 --> 0:00:47.960
So in this talk, I would like to invite you to learn more about efficiency of our Go programs,

12
0:00:47.960 --> 0:00:53.960
and there was already two talks that I had been on who mentioned, you know, optimizations

13
0:00:53.960 --> 0:00:58.280
in its name and, like, generally how to make software more efficient.

14
0:00:58.280 --> 0:01:02.720
I wonder where this, I don't know, it's not hype, but it's already three talks about one

15
0:01:02.720 --> 0:01:03.720
topic.

16
0:01:03.720 --> 0:01:05.640
Why it's so popular?

17
0:01:05.640 --> 0:01:08.080
Is it because everybody saved me money?

18
0:01:08.080 --> 0:01:13.780
That might be a reason, but I'm super happy we are really uncovering this for Go, because

19
0:01:13.780 --> 0:01:18.960
Go alone might be fast, but that doesn't mean that we cannot, you know, doesn't need to

20
0:01:18.960 --> 0:01:26.680
care about, you know, making it better, and use resources when we execute it, right?

21
0:01:26.680 --> 0:01:31.280
So let's learn about that, and it turns out that, you know, you can save literally millions

22
0:01:31.280 --> 0:01:35.840
of dollars if you, you know, optimize some code, sometimes in production long-term, so

23
0:01:35.840 --> 0:01:37.920
it really matters, right?

24
0:01:37.920 --> 0:01:42.040
But before we start, short introduction, my name is Bartolome Podka, I'm an engineer at

25
0:01:42.040 --> 0:01:50.680
Google, normally I work at Google Cloud, Google Managed Promoteer Service, but generally I'm

26
0:01:50.680 --> 0:01:56.720
open source passionists, I love Go, I love distributed systems, observability topics,

27
0:01:56.720 --> 0:02:02.600
I maintain TANOS, which is like open source, scalable Prometheus system, I maintain Prometheus

28
0:02:02.600 --> 0:02:07.960
as well, and generally, yeah, lots of things in open source, I mentor a lot, and I suggest

29
0:02:07.960 --> 0:02:12.760
you to check, you know, also try to mentor others, it's super important to bring new

30
0:02:12.760 --> 0:02:20.080
generation of people up to the speed in the open source, and yeah, I'm active in the CNCF.

31
0:02:20.080 --> 0:02:27.720
And recently, as you see, I published a book, and I think, you know, it's kind of unique,

32
0:02:27.720 --> 0:02:32.680
everybody's doing TikToks now, and YouTube, and I was like, yeah, let's be old school,

33
0:02:32.680 --> 0:02:36.360
because you need to be unique sometimes in the world, and I really enjoyed that, I learned

34
0:02:36.360 --> 0:02:41.600
a lot during that, and I would love you to learn as well, so I'm kind of summarizing

35
0:02:41.600 --> 0:02:46.520
some concepts from my book here in the stock, so let's go.

36
0:02:46.520 --> 0:02:52.760
And I would like to start with this story, and, you know, apparently, some of the talks,

37
0:02:52.760 --> 0:02:56.320
one of the best talks have to start with the story, but this is something that kind of

38
0:02:56.320 --> 0:03:00.080
maybe triggered me to write a book, right?

39
0:03:00.080 --> 0:03:05.240
So imagine that, I mean, yeah, that was kind of five years ago, we just started the project

40
0:03:05.240 --> 0:03:10.400
called TANOS open source, really doesn't matter what it does right now, but, you know, what

41
0:03:10.400 --> 0:03:15.680
happens is that it has microservices, it has, you know, I think six different microservices

42
0:03:15.680 --> 0:03:20.120
written in Go like you put in Kubernetes or any other cloud, and it's just a distributed

43
0:03:20.120 --> 0:03:25.120
database, and one part of this database is compact or it's like a component, again, doesn't

44
0:03:25.120 --> 0:03:29.880
matter much what it does, what it matters is that it touches object storage, and it

45
0:03:29.880 --> 0:03:35.360
processes, you know, sometimes gigabytes or terabytes daily of metrics, right, of some

46
0:03:35.360 --> 0:03:42.800
data, so what happened is that the very, very beginning of implementation, as you can imagine,

47
0:03:42.800 --> 0:03:48.720
you know, we implemented MVP, it kind of functionally worked, but of course, you know, the implementation

48
0:03:48.720 --> 0:03:53.560
was kind of naive, definitely not optimized, we didn't even run any benchmark, right, at

49
0:03:53.560 --> 0:03:59.200
all, other than just running on production and just, yeah, kind of works, so, and you're

50
0:03:59.200 --> 0:04:03.440
laughing, but this is usually, you know, what development in a high-level ST looks like,

51
0:04:03.440 --> 0:04:09.640
and it was working very well until, of course, more people put load into this, and, you know,

52
0:04:09.640 --> 0:04:16.240
we have some issues, like OOMS, you know, one user pointed us to some graphs of, you

53
0:04:16.240 --> 0:04:22.520
know, incredibly, you know, high spike of memory usage on the heap, on the Golang heap,

54
0:04:22.520 --> 0:04:26.120
right, and you can see it's a drop, which means, you know, there was a restart or someone

55
0:04:26.120 --> 0:04:33.120
killed this, and, yeah, and the numbers are not small, like 15 gigs, I mean, for large

56
0:04:33.120 --> 0:04:39.880
data set, maybe it's fine, but it was kind of problematic, right, so it was really interesting

57
0:04:39.880 --> 0:04:45.560
to see what different feedback and what different suggestions community were giving us, and

58
0:04:45.560 --> 0:04:49.960
I mean, community, everybody, like users, other developers, maybe product managers,

59
0:04:49.960 --> 0:04:54.400
we don't know sometimes who their role are, but, you know, probably depending on their

60
0:04:54.400 --> 0:05:00.280
background, the answers, the proposals were totally different, right, so I would like

61
0:05:00.280 --> 0:05:06.960
you to kind of, you know, check, and like check if you had the same situations in your

62
0:05:06.960 --> 0:05:13.560
experience, because, you know, this is kind of like very ongoing problem, and I would

63
0:05:13.560 --> 0:05:17.960
like to, yeah, showcase this, so, you know, first suggestion was that can you give me

64
0:05:17.960 --> 0:05:23.520
a configuration that doesn't OOM, and it's like, what, do you expect me, like, in the

65
0:05:23.520 --> 0:05:28.260
like very new project to have like flags, not OOM or like useless memory?

66
0:05:28.260 --> 0:05:32.840
This is not as simple as that, yet many, many users are asking us this question of a person's

67
0:05:32.840 --> 0:05:37.360
project, probably you heard this question, okay, what configuration I should use so it

68
0:05:37.360 --> 0:05:42.240
uses less memory, right, or like it just, it's more optimized, how can I optimize using

69
0:05:42.240 --> 0:05:43.240
configuration?

70
0:05:43.240 --> 0:05:46.920
It's just, you know, it's not as simple as that, I guess, you know, maybe in Java, in

71
0:05:46.920 --> 0:05:51.800
JVM, you have lots of performance flags, you sometimes tune things and it's better, but,

72
0:05:51.800 --> 0:05:56.360
you know, it's not so simple, it's a goal, like, kind of low level, you, I mean, yeah,

73
0:05:56.360 --> 0:05:59.560
you need to do more than that, right?

74
0:05:59.560 --> 0:06:04.080
Another, you know, interesting approach, but very, very good in some way, is that just,

75
0:06:04.080 --> 0:06:09.000
okay, I will just put this process into bigger machine and it's done, and that's totally

76
0:06:09.000 --> 0:06:14.080
valid, you know, solution, maybe short term, maybe sometimes it's enough, but, you know,

77
0:06:14.080 --> 0:06:18.880
in our case, it was not sustainable, because, of course, you couldn't grow vertically more

78
0:06:18.880 --> 0:06:23.440
and more, and also, even if you would maybe find a big enough machine that was working

79
0:06:23.440 --> 0:06:29.720
for your dataset, then, you know, obviously you were overpaying a lot if the code is naive

80
0:06:29.720 --> 0:06:33.200
and maybe wasting a lot of memory, right?

81
0:06:33.200 --> 0:06:38.560
Then finally, you know, the most fun approach, okay, let's split this one microservice into,

82
0:06:38.560 --> 0:06:43.000
you know, like a scheduler and then, you know, work errors and then we'll just replicate

83
0:06:43.000 --> 0:06:48.680
in my super nice computer, you know, communities cluster and, you know, it will just horizontally

84
0:06:48.680 --> 0:06:53.080
scale so I can use many, many hundreds of small machines, so it will work.

85
0:06:53.080 --> 0:06:59.720
Yes, but, you know, you are putting on small kind of microservice so much complexity that

86
0:06:59.720 --> 0:07:02.440
it will be, like, more expensive generally, right?

87
0:07:02.440 --> 0:07:08.400
Because of the network cost, like, distributed systems, you know, injects, you know, things

88
0:07:08.400 --> 0:07:10.400
that you have to replicate data finally.

89
0:07:10.400 --> 0:07:16.560
So if you overpaint more and more and more, and you are kind of distributing this non-optimized

90
0:07:16.560 --> 0:07:20.480
code to different places, that's not always a solution.

91
0:07:20.480 --> 0:07:25.160
Sometimes the code cannot be optimized more and we can, you know, we should probably horizontally

92
0:07:25.160 --> 0:07:28.600
scale but not in the very beginning of the project, right?

93
0:07:28.600 --> 0:07:32.320
Yet that was the first suggestion from the community, right?

94
0:07:32.320 --> 0:07:34.920
Of course you can just switch from that to something else, right?

95
0:07:34.920 --> 0:07:35.920
That's also a solution.

96
0:07:35.920 --> 0:07:39.720
And then if you have this approach and probably you would just jump through project, this

97
0:07:39.720 --> 0:07:45.880
is not super efficient but maybe, you know, some parts of the project are better or some

98
0:07:45.880 --> 0:07:46.880
worse.

99
0:07:46.880 --> 0:07:47.880
That's an option.

100
0:07:47.880 --> 0:07:51.320
Some suggestion, some suggested, of course, paying for vendor, right?

101
0:07:51.320 --> 0:07:54.760
They would solve the absorbance for me for real money.

102
0:07:54.760 --> 0:07:58.320
So, but, yeah, that's not always a good solution.

103
0:07:58.320 --> 0:08:00.240
That's just giving up.

104
0:08:00.240 --> 0:08:06.840
And also, you know, migration of data, huge cost of learning new tools and so on.

105
0:08:06.840 --> 0:08:13.720
And you know, all of this work, we're in the code, we have this.

106
0:08:13.720 --> 0:08:19.720
And it's like, you know, it's bumping and super easy ways that you could be avoided,

107
0:08:19.720 --> 0:08:20.720
right?

108
0:08:20.720 --> 0:08:21.880
And, yeah, I understand.

109
0:08:21.880 --> 0:08:25.000
So you know, of course, that was malloc.

110
0:08:25.000 --> 0:08:28.500
So in Bughal we don't have malloc and so on.

111
0:08:28.500 --> 0:08:33.160
But you know, memory overhead, memory leaks like that are very common in GOR.

112
0:08:33.160 --> 0:08:37.320
Just imagine how many GOR routines sometimes you put, you created, you forgot to close

113
0:08:37.320 --> 0:08:41.280
some kind of abstraction and the GOR routine is leaking and so you are leaking memory like

114
0:08:41.280 --> 0:08:43.200
this malloc, right?

115
0:08:43.200 --> 0:08:49.920
And you know, what actually, you know, was the solution was some contributor finally

116
0:08:49.920 --> 0:08:56.160
came up, investigated, thought about this efficiency problem on the code level, algorithm

117
0:08:56.160 --> 0:08:57.600
and code level, right?

118
0:08:57.600 --> 0:09:02.360
And rewrote or like rewrote small part of the compactor to stream data, right?

119
0:09:02.360 --> 0:09:10.120
So instead of building maybe the kind of resulted object that the compactor is doing in memory,

120
0:09:10.120 --> 0:09:13.400
it was as soon as possible streaming that to FireSystem.

121
0:09:13.400 --> 0:09:14.400
Easy.

122
0:09:14.400 --> 0:09:16.600
Generally easy, easy, easy change.

123
0:09:16.600 --> 0:09:21.240
Yet there was lots of discussions, lots of stress, lots of weird ideas and I would just

124
0:09:21.240 --> 0:09:27.600
find it like over time amusing that this story was repeating in many, many cases, right?

125
0:09:27.600 --> 0:09:33.960
So and you know, that's not only, you know, of course, experience so many kind of nice

126
0:09:33.960 --> 0:09:38.640
examples where only small character change, two character change there and you know, so

127
0:09:38.640 --> 0:09:42.160
much kind of like improvement over like large system.

128
0:09:42.160 --> 0:09:48.440
So sometimes, sometimes there are like very easy ways that we can just pick it up and

129
0:09:48.440 --> 0:09:49.840
just do it, right?

130
0:09:49.840 --> 0:09:51.520
But we need to know how, right?

131
0:09:51.520 --> 0:09:54.120
So kind of two learnings from the story.

132
0:09:54.120 --> 0:09:59.800
One is that software efficiency and code level and algorithms for changing code, you know,

133
0:09:59.800 --> 0:10:05.600
matters and learning how to do it can be useful.

134
0:10:05.600 --> 0:10:12.120
And second learning is that there is common pitfall, I think, generally in the years because

135
0:10:12.120 --> 0:10:14.520
in the past we have premature optimizations.

136
0:10:14.520 --> 0:10:18.080
Everybody was playing with the code and trying to over optimize things.

137
0:10:18.080 --> 0:10:23.720
I think now we are lazy and we are more like into DevOps, into changing, you know, configuration,

138
0:10:23.720 --> 0:10:27.280
into horizontally scaling because we have this power, we have cloud.

139
0:10:27.280 --> 0:10:31.520
And this is usually, you know, more chosen solution than actually checking the code,

140
0:10:31.520 --> 0:10:32.520
right?

141
0:10:32.520 --> 0:10:37.560
I call it closed box thinking and I think this is a thread a little bit in our system.

142
0:10:37.560 --> 0:10:40.800
So we should acknowledge that there are different levels.

143
0:10:40.800 --> 0:10:41.880
We can sometimes scale.

144
0:10:41.880 --> 0:10:43.920
We can sometimes put more bigger machine.

145
0:10:43.920 --> 0:10:46.320
We can sometimes write to Rust if that makes sense.

146
0:10:46.320 --> 0:10:50.680
But you know, that's not the first solution that should come to your mind, right?

147
0:10:50.680 --> 0:10:51.680
Okay.

148
0:10:51.680 --> 0:10:57.360
Before we go forward, I have five books to share and I will share the link to quiz at

149
0:10:57.360 --> 0:10:58.720
the end.

150
0:10:58.720 --> 0:11:00.880
And it was super simple, but pay attention, right?

151
0:11:00.880 --> 0:11:05.680
Because maybe there will be some questions around and you need to answer, send me an

152
0:11:05.680 --> 0:11:11.960
email and I will just, you know, just choose five people, lucky people to have my book.

153
0:11:11.960 --> 0:11:15.040
So yeah, pay attention.

154
0:11:15.040 --> 0:11:16.040
All right.

155
0:11:16.040 --> 0:11:18.560
Five steps.

156
0:11:18.560 --> 0:11:23.760
Five steps, yeah, for efficiency progress.

157
0:11:23.760 --> 0:11:28.640
One thing I want to mention, I don't know if you have been in the previous talk or like

158
0:11:28.640 --> 0:11:35.600
more previous, he kind of explained a lot of optimization ideas.

159
0:11:35.600 --> 0:11:41.360
Like I think, like Mache before, like he mentioned string optimization with internings.

160
0:11:41.360 --> 0:11:48.880
Has just mentioned, I think, something around, you know, pre-allocations and many kind of

161
0:11:48.880 --> 0:11:54.480
like, I think, padding, struct padding and generally, you know, all those kind of ideas.

162
0:11:54.480 --> 0:11:55.480
This is fine.

163
0:11:55.480 --> 0:11:57.720
But it's optimizing stuff.

164
0:11:57.720 --> 0:12:01.880
It's not like looking for dictionary of things I did in the past.

165
0:12:01.880 --> 0:12:03.640
It's kind of more fuzzy, more involved.

166
0:12:03.640 --> 0:12:09.400
So what I would like you to focus, it's not a particular way of how we optimize an example

167
0:12:09.400 --> 0:12:14.040
I will show, because it's super simple and trivial, but how we get there, right?

168
0:12:14.040 --> 0:12:18.600
How we found what to optimize, how we found if we should even optimize, okay?

169
0:12:18.600 --> 0:12:19.800
So focus on that.

170
0:12:19.800 --> 0:12:24.280
So first step, first suggestion I would have, and this is from book, I kind of found, yeah,

171
0:12:24.280 --> 0:12:30.840
I don't know, I define this name TFBO, which is essentially a flow for development, efficient

172
0:12:30.840 --> 0:12:33.160
aware development that worked for me.

173
0:12:33.160 --> 0:12:36.320
And generally I see other professionals doing that a lot as well.

174
0:12:36.320 --> 0:12:38.520
So test fix benchmark optimize.

175
0:12:38.520 --> 0:12:43.440
So essentially what it is, it's like a TDD with something else.

176
0:12:43.440 --> 0:12:46.760
And TDD you are probably familiar with, test driven development.

177
0:12:46.760 --> 0:12:52.160
You test first, as you can see, and only then you kind of like implement or fix it until

178
0:12:52.160 --> 0:12:53.840
the test is passing, right?

179
0:12:53.840 --> 0:12:58.200
I would like to kind of do the same for optimizations as well.

180
0:12:58.200 --> 0:13:03.800
So we have benchmark driven optimizations, because as you can see, we benchmark first,

181
0:13:03.800 --> 0:13:07.360
then we optimize, and then we profile, right?

182
0:13:07.360 --> 0:13:11.660
And I will tell you later why, but all of this is a close loop, right?

183
0:13:11.660 --> 0:13:14.040
So after optimizations we have to test as well.

184
0:13:14.040 --> 0:13:20.280
All right, so it feels complex, but we'll make one loop, actually maybe two, doing the

185
0:13:20.280 --> 0:13:22.120
stock on a simple code.

186
0:13:22.120 --> 0:13:23.400
So let's do it.

187
0:13:23.400 --> 0:13:28.480
So let's introduce a simple function, super simple, super stupid.

188
0:13:28.480 --> 0:13:33.560
We are creating a million elements, I mean, a slice with million elements, and each of

189
0:13:33.560 --> 0:13:38.800
those elements are just a string, a constant string for them.

190
0:13:38.800 --> 0:13:39.800
Super simple.

191
0:13:39.800 --> 0:13:44.480
It's the first, you know, kind of first iteration of this program we want to write.

192
0:13:44.480 --> 0:13:49.640
So what we do regarding TFBO, okay, so we test, right?

193
0:13:49.640 --> 0:13:53.320
I mean, now we have a code, for example, and we want to maybe improve it.

194
0:13:53.320 --> 0:13:57.140
We test of development, so let's assume I already had the test, right?

195
0:13:57.140 --> 0:14:00.480
But the test could look like this.

196
0:14:00.480 --> 0:14:05.080
And then, you know, I'm ensuring, okay, it's passing, so nothing functionally I have to

197
0:14:05.080 --> 0:14:06.800
fix.

198
0:14:06.800 --> 0:14:08.800
So what next?

199
0:14:08.800 --> 0:14:10.240
So next is this measurement.

200
0:14:10.240 --> 0:14:11.240
It's a benchmark.

201
0:14:11.240 --> 0:14:16.360
And, again, it has already mentioned how to make benchmarks, but I have some additions,

202
0:14:16.360 --> 0:14:19.840
to that, that you might find helpful.

203
0:14:19.840 --> 0:14:24.880
Something I want to mention is that, you know, we were talking about microbenchmarks, because

204
0:14:24.880 --> 0:14:30.320
the same level of testing behavior, like, for example, like, for this small function,

205
0:14:30.320 --> 0:14:35.400
we have this create, you know, unit test is totally enough, right?

206
0:14:35.400 --> 0:14:38.440
This is on micro level, we are making just unit test, it's fine.

207
0:14:38.440 --> 0:14:41.960
But sometimes if you have a bigger system, you need to have, you know, to do something

208
0:14:41.960 --> 0:14:46.000
on macro level, like integration test, end-to-end test, whatever bigger, right?

209
0:14:46.000 --> 0:14:48.160
And the same happens in the benchmarks, right?

210
0:14:48.160 --> 0:14:49.160
This is microbenchmark.

211
0:14:49.160 --> 0:14:52.160
This is a kind of unit benchmark.

212
0:14:52.160 --> 0:14:56.800
There are also microbenchmarks I covered in my book.

213
0:14:56.800 --> 0:15:01.440
And then you need to have more sophisticated kind of setup with low testing, with maybe

214
0:15:01.440 --> 0:15:06.040
some automation, with some observability, like, you know, Prometheus, maybe, which measures

215
0:15:06.040 --> 0:15:07.480
over time some resources.

216
0:15:07.480 --> 0:15:12.320
But here we can, we have a simple unit create function, we can just make it simple with

217
0:15:12.320 --> 0:15:13.320
microbenchmarks.

218
0:15:13.320 --> 0:15:17.400
And, you know, it has already mentioned, but, you know, there is a special signature in

219
0:15:17.400 --> 0:15:19.880
a test file you have to put.

220
0:15:19.880 --> 0:15:25.240
And then there are optional helpers, for example, that I like, actually, to put almost everywhere.

221
0:15:25.240 --> 0:15:29.320
Report allocs, which is by default making sure that this function will measure allocations

222
0:15:29.320 --> 0:15:30.320
as well.

223
0:15:30.320 --> 0:15:34.640
And the reset timer, which is super cool because it resets the measurement.

224
0:15:34.640 --> 0:15:39.400
So anything before you allocate, you spend time on, it will be discarded from benchmark

225
0:15:39.400 --> 0:15:40.400
result.

226
0:15:40.400 --> 0:15:46.880
So benchmark will only focus on what will happen within this loop iteration, right?

227
0:15:46.880 --> 0:15:48.920
And then this for loop, you cannot change it.

228
0:15:48.920 --> 0:15:49.960
Don't try to change it.

229
0:15:49.960 --> 0:15:50.960
Always copy.

230
0:15:50.960 --> 0:15:54.280
This is a boilerplate that has to be there, right?

231
0:15:54.280 --> 0:16:02.000
Because it allows Go to make repeatability, check the repeatability of your test by running

232
0:16:02.000 --> 0:16:04.680
it, you know, hundreds of times.

233
0:16:04.680 --> 0:16:10.480
Okay, so how we execute it already, again, as mentioned, but this is, you know, how I

234
0:16:10.480 --> 0:16:12.600
do it, like, focus to one test.

235
0:16:12.600 --> 0:16:16.240
But this is not enough, in my opinion, right?

236
0:16:16.240 --> 0:16:20.160
By default, it runs only one test, one second.

237
0:16:20.160 --> 0:16:25.920
I recommend to actually make sure you explicitly state some parameters, right?

238
0:16:25.920 --> 0:16:31.720
And I have one liner, one liner in bash, for example, that I often use.

239
0:16:31.720 --> 0:16:36.680
So what it is, essentially, I'm kind of creating some variables so I can reference this result

240
0:16:36.680 --> 0:16:39.080
later on in the short-term future.

241
0:16:39.080 --> 0:16:40.600
V1, for example.

242
0:16:40.600 --> 0:16:46.680
So this will create a v1.txt file in my locally.

243
0:16:46.680 --> 0:16:47.680
It will run this benchmark.

244
0:16:47.680 --> 0:16:53.320
It will actually run it, you know, sometime, I specify, again, which is super amazing,

245
0:16:53.320 --> 0:16:57.160
because it was like, okay, so I have this v1 file, what I was doing with it, and then

246
0:16:57.160 --> 0:17:00.760
you check in your bash key story, okay, oh, that was one second, and then that was something

247
0:17:00.760 --> 0:17:01.760
else, right?

248
0:17:01.760 --> 0:17:03.080
So it's kind of useful.

249
0:17:03.080 --> 0:17:04.760
And then this is crucial.

250
0:17:04.760 --> 0:17:07.760
This is something I don't know why I didn't learn in the beginning.

251
0:17:07.760 --> 0:17:10.560
Maybe you learned the count, dash count, right?

252
0:17:10.560 --> 0:17:15.560
So what it is is that it runs the same test a couple of times, six times, actually.

253
0:17:15.560 --> 0:17:18.500
And so one second, six times.

254
0:17:18.500 --> 0:17:24.320
And this is super important, because then you can use further tools, you will see, to check,

255
0:17:24.320 --> 0:17:27.040
you know, how reliable are your results.

256
0:17:27.040 --> 0:17:32.320
You will essentially calculate the variance between the timings, for example.

257
0:17:32.320 --> 0:17:37.400
So if the variance is too big, then your environment is not stable, right?

258
0:17:37.400 --> 0:17:39.040
And then I pin to one CPU.

259
0:17:39.040 --> 0:17:43.440
This is super important to generally pinning, not to one, right?

260
0:17:43.440 --> 0:17:47.480
Just pick something that works for you for concurrency, pick something that runs on production,

261
0:17:47.480 --> 0:17:49.000
maybe, or similar.

262
0:17:49.000 --> 0:17:51.240
But always between tests don't change that, right?

263
0:17:51.240 --> 0:17:52.640
So that's super important.

264
0:17:52.640 --> 0:17:56.960
And also I recommend to change less than numbers of CPU, because your operating system has

265
0:17:56.960 --> 0:17:58.240
to run on something, right?

266
0:17:58.240 --> 0:18:00.720
So those things matter.

267
0:18:00.720 --> 0:18:05.400
Also don't run this on laptop without power connected, because you will be CPU throttled.

268
0:18:05.400 --> 0:18:08.800
There are lots of kind of small things that you think, oh, it doesn't matter.

269
0:18:08.800 --> 0:18:11.880
No, it matters, because then you cannot rely on your results, right?

270
0:18:11.880 --> 0:18:17.840
So try to make this serious a little bit, and at least, you know, don't benchmark on

271
0:18:17.840 --> 0:18:20.800
your lap, you know, in the bed, you know, because they will be overheating.

272
0:18:20.800 --> 0:18:23.960
So yeah, small things, but it matters, right?

273
0:18:23.960 --> 0:18:26.920
I was doing that all the time, by the way, yeah.

274
0:18:26.920 --> 0:18:27.920
All right.

275
0:18:27.920 --> 0:18:31.360
So results, you know, result looks like this.

276
0:18:31.360 --> 0:18:33.640
You can see many of them.

277
0:18:33.640 --> 0:18:37.120
But this is not how I use it, or how we're supposed to use it, apparently.

278
0:18:37.120 --> 0:18:42.520
There is an amazing tool called Benchstat, and it just prints in a more human-readable

279
0:18:42.520 --> 0:18:43.960
way.

280
0:18:43.960 --> 0:18:50.120
And you can see it also aggregates and has some averages over those runs, and tells you

281
0:18:50.120 --> 0:18:56.080
within this percentage, for example, that the time latency, there is a variance of 1%,

282
0:18:56.080 --> 0:18:58.040
which is tolerable, for example, right?

283
0:18:58.040 --> 0:19:01.840
And you can kind of, like, customize what exact how it calculates this variance and

284
0:19:01.840 --> 0:19:02.840
so on.

285
0:19:02.840 --> 0:19:03.840
So we can trust it.

286
0:19:03.840 --> 0:19:09.360
Like, it's even 1% of, I guess, free, you could trust it, depend on what you do, but

287
0:19:09.360 --> 0:19:11.200
generally it's not too bad.

288
0:19:11.200 --> 0:19:13.920
Allocations, fortunately, are super stable, right?

289
0:19:13.920 --> 0:19:15.280
So, okay.

290
0:19:15.280 --> 0:19:18.640
So we benchmarked, we measured, okay, we know our function has these numbers.

291
0:19:18.640 --> 0:19:22.080
Like, I mean, what's next, right?

292
0:19:22.080 --> 0:19:24.160
Everybody was like, yeah, let's make it faster, let's make it faster.

293
0:19:24.160 --> 0:19:26.640
But wait, wait, wait, wait, why?

294
0:19:26.640 --> 0:19:27.640
Why should we make it faster?

295
0:19:27.640 --> 0:19:34.200
Maybe, okay, maybe that's a lot, 100 megabytes of every, you know, create invocation, but

296
0:19:34.200 --> 0:19:36.120
maybe that's fine, right?

297
0:19:36.120 --> 0:19:40.080
So this is where I think we are missing a lot of experience, usually.

298
0:19:40.080 --> 0:19:42.840
I mean, you have to set some expectations, right?

299
0:19:42.840 --> 0:19:47.120
Like, to what point you are optimizing?

300
0:19:47.120 --> 0:19:48.720
And you know, usually we don't have any expectations.

301
0:19:48.720 --> 0:19:53.960
Like, okay, yeah, I mean, even from product management, here we have maybe functionals,

302
0:19:53.960 --> 0:19:58.600
you know, requirements, but never really concrete performance requirements.

303
0:19:58.600 --> 0:19:59.800
So we don't know what to do.

304
0:19:59.800 --> 0:20:04.160
And honestly, if you don't just ignore those requirements, okay, I don't have, I just want

305
0:20:04.160 --> 0:20:08.080
to make it faster, then this premature optimization is always, right?

306
0:20:08.080 --> 0:20:13.800
Because it's always premature because it's a random, random goal you don't really understand,

307
0:20:13.800 --> 0:20:14.800
right?

308
0:20:14.800 --> 0:20:17.320
So maybe, maybe just make it fast, right?

309
0:20:17.320 --> 0:20:20.280
That's also like very fuzzy, obviously, that's not very helpful.

310
0:20:20.280 --> 0:20:21.760
So what is helpful?

311
0:20:21.760 --> 0:20:23.800
What I will look, and I know it's super hard.

312
0:20:23.800 --> 0:20:30.480
I know it's kind of uncomfortable, but I suggest doing some kind of efficiency requirements

313
0:20:30.480 --> 0:20:33.880
spec, super simple, as simple as possible.

314
0:20:33.880 --> 0:20:34.880
I call it rare.

315
0:20:34.880 --> 0:20:37.400
So, rare efficiency requirements.

316
0:20:37.400 --> 0:20:41.400
And what it means, essentially, try to find out some kind of function, right?

317
0:20:41.400 --> 0:20:46.440
Some kind of, you know, complexity, but not asymptotic complexity, just more concrete

318
0:20:46.440 --> 0:20:50.080
estimation of the complexity based on inputs, right?

319
0:20:50.080 --> 0:20:55.280
And for simple functions, like for example, our function, we can estimate, you know, what

320
0:20:55.280 --> 0:20:59.040
in our minds we think should happen, roughly, right?

321
0:20:59.040 --> 0:21:03.680
So for runtime, we know in one million time we do something.

322
0:21:03.680 --> 0:21:06.240
We don't know how many in our seconds, let's pick 30.

323
0:21:06.240 --> 0:21:11.640
This is actually pretty big for one iteration of just append, but just really pick some

324
0:21:11.640 --> 0:21:12.640
numbers.

325
0:21:12.640 --> 0:21:16.680
Sometimes it's good, it's just, you know, you can iterate over this number, but if you

326
0:21:16.680 --> 0:21:20.920
don't know where you go, then, you know, how you can make any decisions.

327
0:21:20.920 --> 0:21:28.640
In allocations, it's a little bit bitter, a little bit easier, because we expect a slice

328
0:21:28.640 --> 0:21:37.240
of six, of one million elements of strings, and as we learned from Matri talk, every string

329
0:21:37.240 --> 0:21:39.280
has these two parts.

330
0:21:39.280 --> 0:21:44.920
One part has 16 bits, which has length and capacity, or maybe capacity not, but then,

331
0:21:44.920 --> 0:21:51.560
yeah, length, capacity, and pointer, and then there's other parts which lies in the heap.

332
0:21:51.560 --> 0:21:56.240
But for this, you know, 16 bytes, we can assume that will be 16, right?

333
0:21:56.240 --> 0:21:59.800
So it's every element is 16 bytes, so now we just multiply.

334
0:21:59.800 --> 0:22:02.400
That's our function, that's what we all expect, right?

335
0:22:02.400 --> 0:22:09.000
And with this, we can, you know, kind of expect that every invocation of create should, you

336
0:22:09.000 --> 0:22:10.800
know, kind of allocate 15 megabytes.

337
0:22:10.800 --> 0:22:14.160
But what we see, we allocate 80 megabytes, right?

338
0:22:14.160 --> 0:22:19.120
So already we see that, oh, there might be like easy ways to do it, or something I don't

339
0:22:19.120 --> 0:22:20.680
understand about this program.

340
0:22:20.680 --> 0:22:27.480
And this is what leads us to better, to spotting maybe easy wins, and spotting, you know, if

341
0:22:27.480 --> 0:22:29.680
we need to do anything, right?

342
0:22:29.680 --> 0:22:34.240
In terms of time, latency, it's already kind of like more than we kind of expected, right?

343
0:22:34.240 --> 0:22:38.440
But this is more of a guessing, I just guessed it's 30 seconds, right?

344
0:22:38.440 --> 0:22:39.920
Okay, so what we do?

345
0:22:39.920 --> 0:22:48.800
Now we know we are, you know, not fast enough, not allocating, we are overallocated, right?

346
0:22:48.800 --> 0:22:52.000
So then we profile, then we check, okay, we have a problem.

347
0:22:52.000 --> 0:22:54.320
Now let's find what's going on.

348
0:22:54.320 --> 0:22:59.440
And this is where on micro level we can, you know, use profiling very easily by just adding

349
0:22:59.440 --> 0:23:01.280
those two flags.

350
0:23:01.280 --> 0:23:07.520
It will gather memory profiles and CPU profiles in a file, like v1.mempprof.

351
0:23:07.520 --> 0:23:12.880
On macro level, there are other ways of gathering profiles, but you can use the same format,

352
0:23:12.880 --> 0:23:19.520
the same tools, there are even continuous profiling tools in all the cells, like Parcadev,

353
0:23:19.520 --> 0:23:25.560
I really recommend them, and it's super easy then to gather those profiles over time.

354
0:23:25.560 --> 0:23:30.760
So this, what we want to really learn is that what causes this problem?

355
0:23:30.760 --> 0:23:36.520
And this is like a CPU profile, and we could spot, and the wider means it spends more CPU

356
0:23:36.520 --> 0:23:42.320
profiles, the depth doesn't matter, it's just how many functions we have, right?

357
0:23:42.320 --> 0:23:46.760
So you can see that create, of course, is one of the biggest contributors, but the growth

358
0:23:46.760 --> 0:23:47.760
lies, right?

359
0:23:47.760 --> 0:23:51.120
Like why we spend so many cycles growing slides?

360
0:23:51.120 --> 0:23:57.960
Ideally, I know how many elements I have, kind of why it doesn't grow me once, right?

361
0:23:57.960 --> 0:24:03.280
And then we can check, and by the way, you can use this go tool, pprof, dash, itp locally,

362
0:24:03.280 --> 0:24:08.360
I kind of use it a lot on this file to kind of expose this kind of interactive UI, you

363
0:24:08.360 --> 0:24:13.160
can do the same for memory, but honestly, this is not useful because Append is a standard

364
0:24:13.160 --> 0:24:16.160
library function, and they are not very well exposed, right?

365
0:24:16.160 --> 0:24:17.160
So they're hidden.

366
0:24:17.160 --> 0:24:21.280
So this is not very helpful, actually CPU profile was more helpful, because it pointed

367
0:24:21.280 --> 0:24:25.440
us to the growth lies, and if you just Google for that, you will notice this comes from

368
0:24:25.440 --> 0:24:30.600
Append, and then you can go to documentation of Append and learn what it actually does.

369
0:24:30.600 --> 0:24:35.080
And as you probably are familiar, because this is like, should be a trivial case, I

370
0:24:35.080 --> 0:24:41.600
mean, Append resizes the slides, or resizes the underlying RI whenever, you know, it's

371
0:24:41.600 --> 0:24:42.600
full, right?

372
0:24:42.600 --> 0:24:48.500
And resizing, it's not super simple, it has to kind of create a new, bigger RI, and copy

373
0:24:48.500 --> 0:24:55.760
things over, and, you know, garbage collection will kill the old one, but not fast enough

374
0:24:55.760 --> 0:24:57.540
because of the garbage collection.

375
0:24:57.540 --> 0:25:01.500
So we kind of aggregate that as another allocation, right?

376
0:25:01.500 --> 0:25:09.560
So this is what happens, and kind of the fix is to just pre-allocate, right?

377
0:25:09.560 --> 0:25:13.760
So to tell, you know, when you create the slides, okay, how much capacity you want to

378
0:25:13.760 --> 0:25:16.120
prepare for that, and thanks for that.

379
0:25:16.120 --> 0:25:22.880
So what we do now, okay, we did optimize in our TFBO, now we test before with the measuring,

380
0:25:22.880 --> 0:25:29.000
because if you are not testing if this, you know, this code is correct, then, you know,

381
0:25:29.000 --> 0:25:32.800
you might be, you know, yeah, we'll be happy that things are faster, but functionally broken.

382
0:25:32.800 --> 0:25:35.440
So always test, don't be, you know, lazy.

383
0:25:35.440 --> 0:25:37.040
Run those unit tests, easy.

384
0:25:37.040 --> 0:25:41.080
And then, you know, once they are passing, you can comfortably measure.

385
0:25:41.080 --> 0:25:46.920
Again, I just changed V2, just to specify another variable, right, on our file system,

386
0:25:46.920 --> 0:25:50.920
and then I can do bunch.v1.txt and then V2.txt.

387
0:25:50.920 --> 0:25:53.400
Again, I can put like 100 of those variables.

388
0:25:53.400 --> 0:25:59.240
It will compare all of them, but here we compare two, and not only we have absolute values

389
0:25:59.240 --> 0:26:01.840
of those measurements, but also a diff, right?

390
0:26:01.840 --> 0:26:08.240
So you can see we improved a lot, and if we check absolute value in regards to our efficiency

391
0:26:08.240 --> 0:26:14.720
requirements, you see that we met our threshold, roughly, but like we estimated it, so it's

392
0:26:14.720 --> 0:26:20.080
totally good, you know, 15 megabytes, we have 15 megabytes, and then it's faster than our

393
0:26:20.080 --> 0:26:21.080
goal.

394
0:26:21.080 --> 0:26:23.600
So now we are good to go and release it, right?

395
0:26:23.600 --> 0:26:27.840
So that's kind of the whole loop, and you kind of do it until you're happy with your

396
0:26:27.840 --> 0:26:28.840
results.

397
0:26:28.840 --> 0:26:33.640
So yeah, this is it, and learnings, again, five learnings.

398
0:26:33.640 --> 0:26:39.120
Follow TFBO, test, fix, benchmark, optimize, use benchmarks, they are built into Golang,

399
0:26:39.120 --> 0:26:42.320
they are super amazing, go test, slash, bench.

400
0:26:42.320 --> 0:26:46.120
Set the clear goals, goals are super important here, right?

401
0:26:46.120 --> 0:26:51.960
And then profile, and you can, I mean, Golang uses Pprof, which you can Google as well,

402
0:26:51.960 --> 0:26:57.760
it's like amazing kind of protocol, kind of set of tools integrated with other clouds

403
0:26:57.760 --> 0:27:02.920
and so on, and use it every day whenever I have to optimize something.

404
0:27:02.920 --> 0:27:09.080
And then finally, the key is to try to understand what happens, what I expected, and what's

405
0:27:09.080 --> 0:27:10.200
wrong.

406
0:27:10.200 --> 0:27:13.640
Reading documentation, reading code, this is what you have to do sometimes.

407
0:27:13.640 --> 0:27:19.680
And at General Tip, whenever you want to optimize something super, super carefully in some bottleneck

408
0:27:19.680 --> 0:27:25.600
part of your code, I mean, avoid standard library functions, because they are really

409
0:27:25.600 --> 0:27:27.080
built into generic functionality.

410
0:27:27.080 --> 0:27:31.240
It will test, I mean, it will do a lot of things with different edge cases that you

411
0:27:31.240 --> 0:27:32.360
might not have.

412
0:27:32.360 --> 0:27:37.600
So a lot of times I just implemented my own parsing integer function, it was much faster.

413
0:27:37.600 --> 0:27:42.400
So this is like General Tip that always works, but again, do it only when you need it, because

414
0:27:42.400 --> 0:27:45.080
you might have a box in the disk out, right?

415
0:27:45.080 --> 0:27:46.080
So that's it.

416
0:27:46.080 --> 0:27:47.080
Thank you.

417
0:27:47.080 --> 0:27:48.080
You have a link here, bwplot.dev.

418
0:27:48.080 --> 0:28:13.080
Thank you.

