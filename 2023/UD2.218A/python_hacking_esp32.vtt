WEBVTT

00:00.000 --> 00:26.400
I'm here to talk to you about graphics programming in Python on an embedded microcontroller,

00:26.400 --> 00:31.200
which is hilarious because I'm none of these things, I'm not a Python programmer, I'm

00:31.200 --> 00:36.840
not an embedded programmer, so we'll see how this goes.

00:36.840 --> 00:41.800
It's for that reason I just, you know, I can't emphasise enough this part of the talk description,

00:41.800 --> 00:48.760
this is not an instructional talk, this is just what I did.

00:48.760 --> 00:59.760
So there's some background, EMF camp is this weekend camping festival for hackers and makers,

00:59.760 --> 01:07.320
and it's in a similar vein to the Chaos Communication camp and the Dutch Hacker Festival, you know,

01:07.320 --> 01:11.040
there's robots and lasers and geodesic domes and things, it's great fun if you get the

01:11.040 --> 01:14.560
opportunity to go, I highly recommend it.

01:14.560 --> 01:23.200
And it's a bit of a tradition of these style of events to give the attendees electronic

01:23.200 --> 01:38.600
event badges, and the aim of these is to give attendees opportunity to play with some hardware

01:38.600 --> 01:42.480
that they might not have come across before.

01:42.480 --> 01:50.200
These are the two most recent badges from EMF camp, the one on the left here.

01:50.200 --> 01:57.600
If I told you they had, they put a SIM card on it and a GSM modem and then they set up

01:57.600 --> 02:03.280
an on-site cell phone network, you'll understand why it's made to look like a Nokia N-Gage,

02:03.280 --> 02:10.680
but it's got all of the usual peripherals and sensors and things on there as well like

02:10.680 --> 02:16.360
accelerometers and humidity and temperature and things, and because it runs micro Python

02:16.360 --> 02:23.040
it allows people to easily get started with experimenting with that kind of hardware.

02:23.040 --> 02:28.240
The one on the right there is the newest one, these photographs aren't to scale by the way,

02:28.240 --> 02:34.040
let me just hold them up for comparison, the newest one is much smaller.

02:34.040 --> 02:40.960
The reasons for that, you might guess, is because of the silicon shortage that's been

02:40.960 --> 02:49.120
caused by fire, flood and plague, as you might expect, but it's still a lovely device.

02:49.120 --> 02:53.080
The one on the left here you can see, you might recognise this as a version of the settlers

02:53.080 --> 03:02.160
of Kirtan, I spent a lot of time trying to isolate small parts of the screen to redraw

03:02.160 --> 03:08.760
because the update speed on that screen was so slow, it was almost unusable for anything

03:08.760 --> 03:14.040
in real time, so when I got my hands on the new one this year I obviously wanted to see

03:14.040 --> 03:18.960
what this one could do.

03:18.960 --> 03:24.360
And so the first thing I wanted to do was to just try and blitter full screen pixels

03:24.360 --> 03:31.040
to the device using the display driver directly, in this talk about 70 milliseconds, which

03:31.040 --> 03:35.960
is already orders of magnitude faster than the old badge.

03:35.960 --> 03:44.800
If I draw to an off screen buffer instead, that's way faster, but if you're doing that

03:44.800 --> 03:49.760
then you have to get into the business of implementing your own drawing functions for

03:49.760 --> 03:54.640
primitives and I didn't really want to do that.

03:54.640 --> 04:00.080
That is ominous foreshadowing by the way.

04:00.080 --> 04:05.280
But I did discover that micro Python has this frame buffer module which provides you with

04:05.280 --> 04:10.600
an off screen frame buffer and also some drawing functions, which is great.

04:10.600 --> 04:15.800
So 41 milliseconds, that was a fair compromise, that's a good start.

04:15.800 --> 04:21.160
Now I've got a baseline for how fast I can draw to the screen.

04:21.160 --> 04:28.400
So obviously what this is about is drawing 3D things to the screen of this device, and

04:28.400 --> 04:37.440
so this is just here to, in case you don't know, this is basically, I guess this is 3D

04:37.440 --> 04:42.880
rasterisation 101, this is like the minimum we have to do in order to get 3D points onto

04:42.880 --> 04:43.880
the screen.

04:43.880 --> 04:48.360
We start with our vertex coordinates and then that's multiplied by the model matrix to get

04:48.360 --> 04:53.520
into world space and then you multiply that by the view matrix to get the view space and

04:53.520 --> 05:00.120
then by the projection matrix to get the clip space and then the clip space allows you to

05:00.120 --> 05:04.440
see which vertices will be clipped by the edges of the screen or not.

05:04.440 --> 05:10.520
So then once we know we've got the list of vertices we want to render, then we can do

05:10.520 --> 05:16.640
the perspective division to bring that into normalised device coordinate space or NDC space.

05:16.640 --> 05:20.640
The perspective division is just the part that makes the further away points closer

05:20.640 --> 05:24.600
together, so it gives you that illusion of 3D.

05:24.600 --> 05:29.720
And then we've got to convert the normalised device coordinates which are between minus

05:29.720 --> 05:36.400
one and one to screen space which is like our pixel coordinates.

05:36.400 --> 05:48.360
And so when I was doing this, to render these eight points on the screen from a cube, it

05:48.360 --> 05:54.480
wasn't too bad, 53 seconds and then if you join those up to create your cube wireframe

05:54.480 --> 06:02.200
it's not that much slower, there's 12 triangles there obviously.

06:02.200 --> 06:07.880
The next step is to then start filling in these triangles, you want to draw solid shapes

06:07.880 --> 06:09.880
after all.

06:09.880 --> 06:18.960
Annoyingly there's no method or no function for doing that in the frame buff module for

06:18.960 --> 06:20.440
micro Python.

06:20.440 --> 06:29.520
There is in the display driver but as I mentioned, using the display driver directly is much

06:29.520 --> 06:34.080
slower because we're making many more calls to the hardware and we're setting pins high

06:34.080 --> 06:36.640
and low and stuff for every time we want to draw something.

06:36.640 --> 06:41.920
We just want to do that once when we blip the whole thing to the screen.

06:41.920 --> 06:51.560
And yeah, so frame buff doesn't provide a polygon or polygon fill method and so I do

06:51.560 --> 06:57.200
have to get into the business of writing these sort of functions myself after all.

06:57.200 --> 07:02.600
So yeah, the display driver itself does have these methods, so obviously that's the first

07:02.600 --> 07:09.680
place I looked for implementation clues, they have a polygon and a fill polygon method.

07:09.680 --> 07:18.000
Only obviously there are problems with it and it's a little bit rubbish.

07:18.000 --> 07:27.600
Here's the figure on the left there is just using the outline polygon method and then

07:27.600 --> 07:34.720
the second one here is where I've tried to draw a filled polygon over the top of the

07:34.720 --> 07:39.960
wire frame polygon and you can see it just doesn't quite match up.

07:39.960 --> 07:45.960
And so reading the code, there is, it seems to be implementing quite a well known or well

07:45.960 --> 07:52.480
documented fill polygon method and there's a link to the website where this algorithm

07:52.480 --> 07:54.480
is described.

07:54.480 --> 07:58.760
And that also supplies a reference implementation so I was able to copy the reference implementation

07:58.760 --> 08:04.600
to see if the display driver's implementation was different.

08:04.600 --> 08:08.280
And it isn't, it's exactly the same, it looks like the display driver has inherited the

08:08.280 --> 08:11.800
same problems that were in the reference implementation.

08:11.800 --> 08:17.240
You'll notice that it's not only incorrect on this side but the left edge here is completely

08:17.240 --> 08:21.280
different to this edge here, so it's over drawing on this side and not drawing enough

08:21.280 --> 08:23.800
on that side.

08:23.800 --> 08:31.200
A lot of the problems with it were sort of like rounding errors and like floating point

08:31.200 --> 08:38.120
to integer truncation and that sort of thing which I managed to mostly fix except for this

08:38.120 --> 08:42.840
really annoying pixel down here that I just couldn't get.

08:42.840 --> 08:48.880
And when I submitted, because I wanted to submit like this enhancement to the frame

08:48.880 --> 08:54.960
with module upstream to the project and so we spent a few days scratching our heads over

08:54.960 --> 08:58.840
this to try and figure out what we could do.

08:58.840 --> 09:05.160
Initially we proposed just drawing the outline again on top of the fill polygon just to sweep

09:05.160 --> 09:10.080
it under the rug but eventually we managed to figure out a much better way of doing it.

09:10.080 --> 09:18.460
We just tried to detect when these stray pixels were going to happen and then fill them in

09:18.460 --> 09:28.360
explicitly instead of letting the algorithm do it.

09:28.360 --> 09:42.120
Oh yeah, it's pretty obvious that the algorithm I think was developed by a physicist or a

09:42.120 --> 09:50.440
mathematician because in the article that describes the algorithm it says, and I'm quoting

09:50.440 --> 09:59.320
here, the detecting points on the polygon edge will deliver unpredictable results but

09:59.320 --> 10:06.960
that is quote, not generally a problem because quote, the edge of the polygon is infinitely

10:06.960 --> 10:11.400
thin.

10:11.400 --> 10:17.320
My polygons have an edge of one pixel so this is obviously why we had to fix the problems

10:17.320 --> 10:19.320
with it.

10:19.320 --> 10:23.760
Anyway, now we can draw arbitrary polygons to the screen.

10:23.760 --> 10:26.880
Let's see what that looks like.

10:26.880 --> 10:35.640
This is the cube here again which is basically the hello world of 3D graphics programming.

10:35.640 --> 10:45.040
And it seemed to work pretty well, 66 milliseconds there but you can see on the left hand screen

10:45.040 --> 10:49.400
shot there, that's not the inside, it looks like you're looking at the inside of the cube

10:49.400 --> 10:54.520
but it's just because we are drawing the back face of the back of the cube on top of the

10:54.520 --> 10:58.960
front face of the front of the cube.

10:58.960 --> 11:05.600
As part of this 3D rasterization process you've now got to do back face culling which is more

11:05.600 --> 11:12.760
maths added on to that pipeline.

11:12.760 --> 11:17.320
You've got to calculate the normal vector of the face which is the direction the face

11:17.320 --> 11:22.600
is facing and then compute the dot product of that with the direction you're looking

11:22.600 --> 11:27.880
so that you can know if the triangle is facing you or not and then just don't bother drawing

11:27.880 --> 11:30.200
the ones that aren't facing you.

11:30.200 --> 11:34.240
But yeah, it's just more maths so it adds more time.

11:34.240 --> 11:43.120
And yeah, I get the occasional really long frame and that coincides with a garbage collection.

11:43.120 --> 11:49.720
I guess we'll talk a bit more about that in a bit.

11:49.720 --> 11:54.040
So there's some really low hanging fruit things we can do to improve the performance initially

11:54.040 --> 11:59.520
which basically amounts to being smarter about the algorithms we use.

11:59.520 --> 12:05.000
We pre-calculate the normals instead of calculating them every frame which for a static model

12:05.000 --> 12:10.120
like this makes total sense.

12:10.120 --> 12:16.240
And yeah, avoid doing the perspective division if we can help it because it's like part of

12:16.240 --> 12:26.320
the matrix multiplication process and usually it's a no-op unless you're multiplying it

12:26.320 --> 12:32.280
by the perspective matrix and only then is it doing something.

12:32.280 --> 12:40.440
So we can just avoid doing those divisions at all on every vertex, in every face, in

12:40.440 --> 12:41.440
every frame.

12:41.440 --> 12:44.560
That's quite a lot of time saved.

12:44.560 --> 12:48.840
But it does mean I can add more things to it and make it do extra work like you add

12:48.840 --> 13:03.200
a rudimentary lighting model and make the cube nice looking by adding shading and whatnot.

13:03.200 --> 13:07.840
What I'm trying to do basically is to keep the rendering time below 100 milliseconds

13:07.840 --> 13:13.080
as well because that seems like a good target to have.

13:13.080 --> 13:19.000
If I can do that, then I get a reasonable performance of 10 frames per second.

13:19.000 --> 13:28.140
So this is, although this works well, that's within that target, it's close to that target.

13:28.140 --> 13:31.420
So I want to try something a bit more complex.

13:31.420 --> 13:38.800
So I download a model of the industry standard T-pop and try and render that.

13:38.800 --> 13:48.240
This is about 240 faces, 240 triangles, and this obviously completely destroyed my 100

13:48.240 --> 13:50.660
millisecond time limit.

13:50.660 --> 13:55.080
So I've got to think of more ways to make this faster.

13:55.080 --> 14:02.520
And the obvious way is to rewrite all of the hottest math functions in C as a micro Python

14:02.520 --> 14:05.080
native module.

14:05.080 --> 14:10.840
The two ones that are called the most often are the vector matrix multiplying method and

14:10.840 --> 14:14.200
the dot product method.

14:14.200 --> 14:20.400
And you can see that more than cuts the time in half.

14:20.400 --> 14:26.720
And with the success of that, it's pretty clear I should rewrite all of the maths in

14:26.720 --> 14:33.280
C. Because if I've got the bonnet up, I might as well.

14:33.280 --> 14:41.360
But that brings the time right down to a glorious, glorious, glorious six frames per second.

14:41.360 --> 14:48.840
But yeah, as a general strategy, if you find yourself calling a method, you know, 1200

14:48.840 --> 14:56.600
times a frame, it's probably a good target to be pushed down into the native layer.

14:56.600 --> 15:04.920
So yeah, a note on writing native code for micro Python.

15:04.920 --> 15:06.480
There's really two ways of doing it.

15:06.480 --> 15:15.200
There's the what is called the external C modules, which is basically C code that you

15:15.200 --> 15:21.600
write that is a module exposed to the Python runtime.

15:21.600 --> 15:31.520
Those are compiled directly into the firmware, which is a bit suboptimal because it would

15:31.520 --> 15:36.800
be nice if I didn't require other people who have these devices to reflash the firmware

15:36.800 --> 15:40.040
every time I change this program.

15:40.040 --> 15:45.640
So the other way of doing it is to write what they call a native module, which allows your

15:45.640 --> 15:53.840
application to supply native code as an MPY file, and then that can be dynamically loaded

15:53.840 --> 15:57.080
by your application at runtime, which is a much nicer way of doing it.

15:57.080 --> 15:58.880
Obviously that's what I wanted to do.

15:58.880 --> 16:02.600
But I did come across problems.

16:02.600 --> 16:07.560
When I tried to build the native code, because I used a floating point division in there

16:07.560 --> 16:14.720
for the perspective division step of the pipeline, I got this problem, which is a linker error

16:14.720 --> 16:21.880
from the Espressiv's tool chain for the ESP32.

16:21.880 --> 16:24.680
I'd love to know why this happens.

16:24.680 --> 16:28.280
And if anyone from Espressiv is here, I'd love to know if it's fixed in the newer version

16:28.280 --> 16:29.280
as well.

16:29.280 --> 16:33.880
But it seems like it can't link this software implementation of floating point division.

16:33.880 --> 16:41.600
So obviously what I did was I downloaded the source for the tool chain and found the assembly

16:41.600 --> 16:47.520
implementation of this method to add into my project, which also didn't work.

16:47.520 --> 16:52.440
The micro Python build system wasn't prepared to accept that, but that was an easy fix.

16:52.440 --> 16:55.400
And that was actually the first change I got accepted into micro Python.

16:55.400 --> 16:56.400
They were very good.

16:56.400 --> 17:01.120
They're very good at, in my experience, they're very good at accepting patches.

17:01.120 --> 17:07.920
And then once I got that building, it just caused my application to crash.

17:07.920 --> 17:15.720
I'm not sure why this happens, but there seems to be like a reference to the native stuff

17:15.720 --> 17:22.400
that gets collected erroneously by the garbage collection.

17:22.400 --> 17:28.360
And I spent a lot of time trying to reduce my object allocations in the frames, but all

17:28.360 --> 17:35.240
that did was just push out the crash to further in the future.

17:35.240 --> 17:43.600
So I had to settle for compiling my maths functions directly into the firmware.

17:43.600 --> 17:46.160
There are some other things I did to try and make it faster.

17:46.160 --> 17:49.400
The big one is trying to reduce object associations.

17:49.400 --> 17:54.800
This is super costly in Python.

17:54.800 --> 18:02.360
And wherever you can, preallocate lists and arrays and things and then just reuse them.

18:02.360 --> 18:11.840
I initially wanted to have a lot of my classes to be totally immutable, as a good programmer

18:11.840 --> 18:13.440
that I am.

18:13.440 --> 18:15.600
But they just totally wasn't feasible.

18:15.600 --> 18:21.760
So I just, you know, you just have to mutate when you do calculations on your vertices,

18:21.760 --> 18:29.000
just mutate one of the operands and send it back that way.

18:29.000 --> 18:33.720
You can also, the other thing I found that saved some time was reducing the amount of

18:33.720 --> 18:37.720
times that we cross from Python into native code and back again.

18:37.720 --> 18:43.680
I found I was doing lots of the same operation to vertices and matrices.

18:43.680 --> 18:48.840
So if I could just send them all as one batch in a single function call into the native

18:48.840 --> 18:51.680
side, then that made it perform a lot quicker.

18:51.680 --> 18:58.600
I think there's a lot of function and stack manipulation overhead there that you save.

18:58.600 --> 19:03.880
So pass arrays and not lists into the native functions as well.

19:03.880 --> 19:09.120
Especially for this kind of stuff where we know the data that we're passing are floats

19:09.120 --> 19:11.040
or whatever.

19:11.040 --> 19:15.480
You know ahead of time what type is in your array, which means you can make some assumptions

19:15.480 --> 19:17.320
that micro Python can't make.

19:17.320 --> 19:25.320
And when you manipulate the data objects in the native side, you can skip a bunch of type

19:25.320 --> 19:26.320
save stuff.

19:26.320 --> 19:32.000
You can just write directly to the data structure, which is useful.

19:32.000 --> 19:34.800
And also, this won't surprise me as well.

19:34.800 --> 19:37.680
Well, I don't know if it's surprising.

19:37.680 --> 19:46.780
Maybe it's obvious to people who are veteran Pythonistas, but I didn't expect the libc

19:46.780 --> 19:57.040
q sort function to be so much faster than the sort function in Python, but I was, if

19:57.040 --> 20:03.880
you look at this picture here, you can see that some parts of the teapot are drawn on

20:03.880 --> 20:08.280
top of, that should be occluded, drawn on top of the body of the teapot.

20:08.280 --> 20:16.640
So what I had to do was z sort the faces so that we draw the faces from back to front.

20:16.640 --> 20:21.960
And that's what I was doing, using the list sort method for here.

20:21.960 --> 20:29.160
But just like implementing this face sorting as a native function as well, as it says there,

20:29.160 --> 20:32.040
it's 100 times faster.

20:32.040 --> 20:38.240
And the other thing that made a measurable difference as well was locally caching object

20:38.240 --> 20:40.520
references in your functions as well.

20:40.520 --> 20:48.280
So instead of, if you're using an object value more than once, instead of doing self foo,

20:48.280 --> 20:55.160
self foo, self foo, just create a local reference, a local variable in the function and use that

20:55.160 --> 20:56.320
instead.

20:56.320 --> 21:04.840
So there's some dereferencing overhead there that is quite significant that we're saving.

21:04.840 --> 21:13.840
And so after applying all of this sort of stuff, this is the final result, or the result

21:13.840 --> 21:14.840
so far.

21:14.840 --> 21:17.880
I'm pretty happy with it.

21:17.880 --> 21:28.240
Getting the teapot model down to under 100 milliseconds per frame was really pleasing.

21:28.240 --> 21:38.520
And yeah, I'm pretty happy with the performance.

21:38.520 --> 21:43.960
So what can this be used for?

21:43.960 --> 21:53.400
Honestly, this was just a fun way to spend a few weekends after the festival had happened.

21:53.400 --> 22:01.120
But it seems to be performing enough where you could do some kind of small 3D game like

22:01.120 --> 22:07.520
a lunar lander or something like that, or make yourself a Jurassic Park style 3D user

22:07.520 --> 22:10.520
interface for your home automation.

22:10.520 --> 22:19.160
But really, the chief lesson for me, I think, was the best way to get involved with a project

22:19.160 --> 22:23.080
like MicroPython was to just start using it.

22:23.080 --> 22:31.320
And eventually, you come across some kind of limitation that probably your best place

22:31.320 --> 22:36.760
to overcome, because you're the one who's trying to solve the problem.

22:36.760 --> 22:38.680
You've got the vested interest in it.

22:38.680 --> 22:47.000
You have all of the information is currently paged into your brain.

22:47.000 --> 22:55.480
And then the MicroPython people were extremely helpful in helping me whip my contributions

22:55.480 --> 22:59.520
into shape.

22:59.520 --> 23:05.520
So yeah, thanks to them for helping me get involved in MicroPython.

23:05.520 --> 23:16.800
And thanks to you for listening.

23:16.800 --> 23:20.800
I can try and answer questions, but I'm not super expert on anything I've been talking

23:20.800 --> 23:23.800
about.

23:23.800 --> 23:30.320
Hi, and thanks for your talk.

23:30.320 --> 23:35.360
I had a question about the ESP2 that you were implementing on this.

23:35.360 --> 23:42.200
Did you ever look at using the dual core setup to try to accelerate any of the maths?

23:42.200 --> 23:44.720
That is a good question.

23:44.720 --> 23:48.760
And someone has mentioned this to me before, but when I was writing this, I was actually

23:48.760 --> 23:51.440
unaware that it had more than one core.

23:51.440 --> 23:59.880
So I haven't yet, but it's a great idea.

23:59.880 --> 24:00.880
Thanks very much for your talk.

24:00.880 --> 24:05.760
If you're interested in MicroPython in the building A, there is a stand about MicroPython

24:05.760 --> 24:17.200
and also a stand by pint64 who makes smartwatch that can run MicroPython and stuff.
