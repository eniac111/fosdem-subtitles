1
0:00:00.000 --> 0:00:07.800
Okay, thank you.

2
0:00:07.800 --> 0:00:09.080
So our next speaker is Jesus.

3
0:00:09.080 --> 0:00:13.360
We've been talking a few times in the Go Dev Room about everything that has to do deeply

4
0:00:13.360 --> 0:00:17.640
within the language and today he's going to talk to us about what's going on with functions.

5
0:00:17.640 --> 0:00:20.640
Round of applause.

6
0:00:20.640 --> 0:00:23.200
Hello, everybody.

7
0:00:23.200 --> 0:00:28.040
My name is Jesus.

8
0:00:28.040 --> 0:00:30.040
I'm software engineer at Marimos.

9
0:00:30.040 --> 0:00:32.240
I'm going to talk about squeezing and Go functions.

10
0:00:32.240 --> 0:00:34.880
So what is optimization?

11
0:00:34.880 --> 0:00:40.440
I think it's important to know that optimization is not being faster or consuming less memory.

12
0:00:40.440 --> 0:00:41.920
It depends on your needs.

13
0:00:41.920 --> 0:00:45.760
So it's better fresh, squeezed juice.

14
0:00:45.760 --> 0:00:50.280
Probably everybody will say yes, but it depends if you are looking for convenience or for

15
0:00:50.280 --> 0:00:52.480
something that lasts forever.

16
0:00:52.480 --> 0:00:55.480
So in that case, it's not the best option.

17
0:00:55.480 --> 0:01:01.320
Optimization is about what you need and trying to address that.

18
0:01:01.320 --> 0:01:03.300
It's important to optimize at the right level.

19
0:01:03.300 --> 0:01:04.920
You can buy the best car.

20
0:01:04.920 --> 0:01:11.420
You can get an F1 car and it's not going to be fast if this is the road.

21
0:01:11.420 --> 0:01:19.680
So try to optimize always at the upper level because these kind of optimizations that were

22
0:01:19.680 --> 0:01:25.160
the ones that we are going to see in the stock are micro optimizations that probably are

23
0:01:25.160 --> 0:01:31.080
not the first place that you should be starting.

24
0:01:31.080 --> 0:01:35.080
Optimize what you need and when you need it.

25
0:01:35.080 --> 0:01:41.920
It's not about taking a Go function and try to optimize forever and try to make that run

26
0:01:41.920 --> 0:01:48.960
superficially and scratch every single nanosecond because probably the bottleneck is no longer

27
0:01:48.960 --> 0:01:49.960
there.

28
0:01:49.960 --> 0:01:51.840
You have to search for the bottleneck.

29
0:01:51.840 --> 0:01:56.400
You have to optimize where the bottleneck is and then look again if the bottleneck is

30
0:01:56.400 --> 0:02:02.160
still there because if it's no longer there, you are over-optimizing that function without

31
0:02:02.160 --> 0:02:03.680
much gain.

32
0:02:03.680 --> 0:02:07.160
So just take that into consideration.

33
0:02:07.160 --> 0:02:15.680
Optimizing is an iterative cycle and you need to keep moving and keep searching for the

34
0:02:15.680 --> 0:02:18.360
bottleneck.

35
0:02:18.360 --> 0:02:19.760
Do not guess, please.

36
0:02:19.760 --> 0:02:26.200
I know everybody has things and all that stuff but guessing about performance is an awful

37
0:02:26.200 --> 0:02:32.400
thing because there's so many things that come into play that is just impossible.

38
0:02:32.400 --> 0:02:38.520
There's the operating system, the compiler, the optimizations of the compiler, if you

39
0:02:38.520 --> 0:02:42.200
are in the cloud, maybe a noisy neighbor.

40
0:02:42.200 --> 0:02:45.120
All that stuff comes into play with performance.

41
0:02:45.120 --> 0:02:52.240
You are not good at guessing almost for sure in performance.

42
0:02:52.240 --> 0:02:54.160
So just measure everything.

43
0:02:54.160 --> 0:02:58.640
The important thing here is try to measure everything and work with that data.

44
0:02:58.640 --> 0:03:05.720
Actually it's what probably the talk that is after the next one is about.

45
0:03:05.720 --> 0:03:11.640
I will suggest to go there also because it probably is a very interesting talk.

46
0:03:11.640 --> 0:03:13.560
So let's talk about benchmarks.

47
0:03:13.560 --> 0:03:22.440
The way that you measure performance in micro-optimizations or micro-benchmarks is through Go benchmarks.

48
0:03:22.440 --> 0:03:28.240
Go benchmark is a tool that comes with Go and is similar to the testing framework that

49
0:03:28.240 --> 0:03:31.360
comes in Go but very focused on benchmarking.

50
0:03:31.360 --> 0:03:38.280
In this case we can see here an example to have two benchmarks, one for MD5-SUM and one

51
0:03:38.280 --> 0:03:44.520
for SHA-256-SUM.

52
0:03:44.520 --> 0:03:45.520
That's it.

53
0:03:45.520 --> 0:03:49.920
It's just a function that starts with benchmarking and receives a testing.b argument and that's

54
0:03:49.920 --> 0:03:53.040
this four, half this four loop inside.

55
0:03:53.040 --> 0:03:58.040
And that is going to do all the job to give you the numbers.

56
0:03:58.040 --> 0:03:59.600
And I show you now the numbers.

57
0:03:59.600 --> 0:04:05.200
If I run this with GoBench, we go test dash bench dot.

58
0:04:05.200 --> 0:04:08.720
The dot is a regular expression that means everything.

59
0:04:08.720 --> 0:04:16.740
So you can use like the Go test run regular expression for only executing certain benchmarks.

60
0:04:16.740 --> 0:04:26.880
And here you can see that MD5-SUM is around twice time faster per operation than SHA.

61
0:04:26.880 --> 0:04:28.640
So well, just a number.

62
0:04:28.640 --> 0:04:29.640
It's that important.

63
0:04:29.640 --> 0:04:30.640
It depends.

64
0:04:30.640 --> 0:04:34.320
If you need more security, probably MD5 is not the best option.

65
0:04:34.320 --> 0:04:38.600
So it depends on your needs.

66
0:04:38.600 --> 0:04:42.560
Another interesting thing is the allocations.

67
0:04:42.560 --> 0:04:47.160
One thing that you maybe have heard is about counting allocations.

68
0:04:47.160 --> 0:04:49.520
Counting allocations, why is that important?

69
0:04:49.520 --> 0:04:54.000
Is because every time we allocate something, when we talk allocation, we are talking about

70
0:04:54.000 --> 0:04:55.880
allocation in the heap.

71
0:04:55.880 --> 0:05:00.760
If every time we allocate something in the heap, allocating that is going to introduce

72
0:05:00.760 --> 0:05:02.000
an overhead.

73
0:05:02.000 --> 0:05:06.080
And not only that, it's going to add more pressure to the garbage collector.

74
0:05:06.080 --> 0:05:11.040
That's why it's important to count the allocations when you are talking about performance.

75
0:05:11.040 --> 0:05:17.000
If you are not worried about performance at that point, don't count the allocation.

76
0:05:17.000 --> 0:05:18.320
It's not that important.

77
0:05:18.320 --> 0:05:25.320
And you are not going to gain a massive amount of performance from there if you are not in

78
0:05:25.320 --> 0:05:27.000
that point there.

79
0:05:27.000 --> 0:05:32.800
Okay, let's see an example here in MD5 and SHA sums.

80
0:05:32.800 --> 0:05:34.640
We have zero allocations.

81
0:05:34.640 --> 0:05:37.840
So well, this data is not very useful for us now.

82
0:05:37.840 --> 0:05:39.400
So let's use another thing.

83
0:05:39.400 --> 0:05:40.920
Let's open a file.

84
0:05:40.920 --> 0:05:47.640
Let's open a file thousands of times and see how it goes.

85
0:05:47.640 --> 0:05:52.280
Now I see that every single operation of opening a file, just opening the file, is going to

86
0:05:52.280 --> 0:05:58.080
generate three allocations, and it's going to consume 120 bytes per operation.

87
0:05:58.080 --> 0:05:59.480
Interesting.

88
0:05:59.480 --> 0:06:01.920
So now you are measuring things.

89
0:06:01.920 --> 0:06:08.240
You are measuring how much time it takes, how much time is gone in processing something,

90
0:06:08.240 --> 0:06:14.800
is gone in allocating things, how much memory is gone there.

91
0:06:14.800 --> 0:06:17.000
So let's talk about profiling.

92
0:06:17.000 --> 0:06:24.680
Because once you, well, actually, normally you do the profiling first to find your bottleneck,

93
0:06:24.680 --> 0:06:29.920
and then you do the benchmark to tune that bottleneck.

94
0:06:29.920 --> 0:06:35.880
But I'm playing with the fact that I already have the benchmark, and I'm going to do the

95
0:06:35.880 --> 0:06:38.000
profiling on top of the benchmark.

96
0:06:38.000 --> 0:06:40.000
So I'm going to execute the gobench.

97
0:06:40.000 --> 0:06:41.760
I'm going to pass the mem profile.

98
0:06:41.760 --> 0:06:43.720
I'm going to generate the mem profile.

99
0:06:43.720 --> 0:06:46.000
And I'm going to use the piproftool.

100
0:06:46.000 --> 0:06:49.240
The piproftool is going to allow me to analyze that profile.

101
0:06:49.240 --> 0:06:53.280
In this case, I'm just asking for a text output.

102
0:06:53.280 --> 0:06:59.560
And that text output is going to show me the top consumers of memory in this case.

103
0:06:59.560 --> 0:07:06.560
And I can see there that 84% of the memory is gone in OS new file.

104
0:07:06.560 --> 0:07:07.560
Okay.

105
0:07:07.560 --> 0:07:09.160
Let's see what happened.

106
0:07:09.160 --> 0:07:10.160
Okay.

107
0:07:10.160 --> 0:07:12.680
It's that file, but I need more information.

108
0:07:12.680 --> 0:07:15.680
Well, it's that function, sorry.

109
0:07:15.680 --> 0:07:17.520
I need more information.

110
0:07:17.520 --> 0:07:20.160
Actually, I kind of like this output.

111
0:07:20.160 --> 0:07:25.680
But if you don't like this output, you can, for example, use SVG.

112
0:07:25.680 --> 0:07:28.920
And you are going to get something like this that is very visual.

113
0:07:28.920 --> 0:07:34.360
And actually, it's kind of obvious that where is the bottleneck there.

114
0:07:34.360 --> 0:07:39.120
And in this case, again, it's OS new file.

115
0:07:39.120 --> 0:07:45.640
If I go to the piproftool again, and instead of that, I use the list of a function, I'm

116
0:07:45.640 --> 0:07:50.360
seeing here where is the memory going by line.

117
0:07:50.360 --> 0:07:58.520
And here, I can see that in the line 127 of the file, fileunix.go, I'm consuming the

118
0:07:58.520 --> 0:07:59.520
memory.

119
0:07:59.520 --> 0:08:02.360
Actually, there you see 74 megabytes.

120
0:08:02.360 --> 0:08:06.200
That is because it's counting all the allocation and aggregating all the allocations.

121
0:08:06.200 --> 0:08:12.520
It's not, every operation here is consuming only 120 bytes.

122
0:08:12.520 --> 0:08:15.120
Okay.

123
0:08:15.120 --> 0:08:17.360
The same with CPU profile.

124
0:08:17.360 --> 0:08:25.640
In this case, this is generating most of the CPU consumption is in Cisco, Cisco6.

125
0:08:25.640 --> 0:08:29.920
I can see in SVG, this time it's more scattered.

126
0:08:29.920 --> 0:08:35.160
So the CPU is consuming in way more places.

127
0:08:35.160 --> 0:08:38.880
But still, the Cisco, Cisco6 is the biggest one.

128
0:08:38.880 --> 0:08:43.200
So I'm going to list that, and I see some assembly code.

129
0:08:43.200 --> 0:08:47.960
Only you are not going to optimize more this function.

130
0:08:47.960 --> 0:08:52.640
So probably this is not the place that you should be looking for optimizations.

131
0:08:52.640 --> 0:09:01.320
Anyway, this is an example of getting to the root cause during the profiling.

132
0:09:01.320 --> 0:09:04.000
Okay.

133
0:09:04.000 --> 0:09:06.800
This talk is going to be more by examples.

134
0:09:06.800 --> 0:09:12.240
I'm going to try to show you some examples of optimizations.

135
0:09:12.240 --> 0:09:16.480
It's just to show you the process more than the specific optimization.

136
0:09:16.480 --> 0:09:21.000
I expect you learned something in between, but it's more about the process.

137
0:09:21.000 --> 0:09:22.000
Okay.

138
0:09:22.000 --> 0:09:26.440
One of the things that you can do is reducing the CPU usage.

139
0:09:26.440 --> 0:09:28.480
This is a kind of silly example.

140
0:09:28.480 --> 0:09:33.560
You have a fine function that have a needle and a high stack, and just go through the

141
0:09:33.560 --> 0:09:40.160
high stack and search for that needle and give you the result.

142
0:09:40.160 --> 0:09:47.360
This is looping over the whole string, sorry, the whole slice.

143
0:09:47.360 --> 0:09:48.600
I'm going to do a benchmark.

144
0:09:48.600 --> 0:09:50.720
The first thing, I'm going to do the benchmark.

145
0:09:50.720 --> 0:09:55.560
I'm going to generate a lot of strings.

146
0:09:55.560 --> 0:10:01.840
And I'm going to do a benchmark looking for something around in the middle.

147
0:10:01.840 --> 0:10:05.800
It's not exactly in the middle, but it's around there.

148
0:10:05.800 --> 0:10:13.880
And the benchmark is saying that it's taking nearly 300 nanoseconds.

149
0:10:13.880 --> 0:10:18.440
If I just early return, that is just a kind of silly optimization.

150
0:10:18.440 --> 0:10:22.040
It's not super smart or something like that.

151
0:10:22.040 --> 0:10:27.360
I'm going to save basically almost the half of the performance.

152
0:10:27.360 --> 0:10:33.360
This is because the benchmark is doing something really silly, and it can vary depending on

153
0:10:33.360 --> 0:10:35.680
the data that it inputs.

154
0:10:35.680 --> 0:10:38.880
That is an optimization of just doing less.

155
0:10:38.880 --> 0:10:43.200
That is one of the best ways of optimizing things.

156
0:10:43.200 --> 0:10:46.160
Well, reducing allocations.

157
0:10:46.160 --> 0:10:51.000
One of the classic examples of reducing allocations is when you are dealing with slices.

158
0:10:51.000 --> 0:10:55.520
When you have a slice, for example, this is a common way of constructing a slice.

159
0:10:55.520 --> 0:11:01.080
I create a slice, I loop over this, generate a loop, and start appending things to that

160
0:11:01.080 --> 0:11:02.080
slice.

161
0:11:02.080 --> 0:11:03.080
Okay, fine.

162
0:11:03.080 --> 0:11:08.160
I'm going to do a benchmark for checking that.

163
0:11:08.160 --> 0:11:16.240
And it's taking 39 allocations and around 41 megabytes per operation.

164
0:11:16.240 --> 0:11:20.400
Okay, sounds like a lot.

165
0:11:20.400 --> 0:11:23.480
Okay, let's do it.

166
0:11:23.480 --> 0:11:25.720
Let's do this.

167
0:11:25.720 --> 0:11:32.760
Let's build the slice, but we are going to give an initial size of a million.

168
0:11:32.760 --> 0:11:35.360
And the time, I'm just setting that.

169
0:11:35.360 --> 0:11:40.040
The final result is exactly the same, but now we have one allocation and we have consumed

170
0:11:40.040 --> 0:11:41.040
only one megabyte.

171
0:11:41.040 --> 0:11:47.840
And actually, if you see there, it's around 800 microseconds.

172
0:11:47.840 --> 0:12:00.280
And here you have around, well, let me check, 10, oh, yeah, around 10 milliseconds.

173
0:12:00.280 --> 0:12:04.960
So it's a lot of time, actually, a lot of CPU time, too.

174
0:12:04.960 --> 0:12:06.320
But you can squeeze it more.

175
0:12:06.320 --> 0:12:11.560
If you know that at compile time, if you know exactly the size that you want to have at

176
0:12:11.560 --> 0:12:14.720
compile time, you can build an array.

177
0:12:14.720 --> 0:12:17.480
It's faster than any slice, actually.

178
0:12:17.480 --> 0:12:23.400
So if I build an array, I'm now doing zero allocations, zero heap allocations.

179
0:12:23.400 --> 0:12:29.800
It's going to go in the stack or in binary somehow or whatever, but it's not consuming

180
0:12:29.800 --> 0:12:33.280
my heap allocations.

181
0:12:33.280 --> 0:12:38.040
And this time is 300 microseconds, approximately.

182
0:12:38.040 --> 0:12:43.880
So an interesting thing if you know that information at compile time.

183
0:12:43.880 --> 0:12:44.880
OK.

184
0:12:44.880 --> 0:12:46.520
Another thing is packing.

185
0:12:46.520 --> 0:12:52.320
If you are concerned about memory, you can build this stack and say, OK, I have a Boolean,

186
0:12:52.320 --> 0:12:56.120
I have a float, I have an N32.

187
0:12:56.120 --> 0:13:04.440
And the Go compiler is going to align my stack to make it more efficient and work better

188
0:13:04.440 --> 0:13:06.360
with the CPU and all that stuff.

189
0:13:06.360 --> 0:13:11.440
And in this case, it's just adding seven bytes between the boot and the float and four bytes

190
0:13:11.440 --> 0:13:14.720
after the integer to get everything aligned.

191
0:13:14.720 --> 0:13:16.120
OK.

192
0:13:16.120 --> 0:13:20.040
I built down a slice and initialized that's a slice.

193
0:13:20.040 --> 0:13:24.920
And I'm allocating one time because that's what the slice is doing.

194
0:13:24.920 --> 0:13:29.920
And I'm consuming around 24 megabytes per operation.

195
0:13:29.920 --> 0:13:35.000
If I just reorganize the struct, in this case, I put the float at the beginning, then the

196
0:13:35.000 --> 0:13:39.360
N32 and then the Boolean, the compiler is only going to add three bytes.

197
0:13:39.360 --> 0:13:42.880
So the whole structure is going to be smaller in memory.

198
0:13:42.880 --> 0:13:47.600
And in this case, now, it's 16 megabytes per operation.

199
0:13:47.600 --> 0:13:52.440
So this kind of optimization is not going to save your day if you are just creating

200
0:13:52.440 --> 0:13:58.680
some structs, but if you are creating millions of instances of an struct, it can be a significant

201
0:13:58.680 --> 0:14:02.160
amount of memory.

202
0:14:02.160 --> 0:14:03.160
Functioning lining.

203
0:14:03.160 --> 0:14:06.880
Functioning lining is something that the Go compiler does for us.

204
0:14:06.880 --> 0:14:12.600
It's just taking a function and replacing any call to that function with the code that

205
0:14:12.600 --> 0:14:17.320
is generated by the function.

206
0:14:17.320 --> 0:14:20.240
I want to show you a very damaged sample.

207
0:14:20.240 --> 0:14:26.560
I'm not inlining this function explicitly, and I'm using the inline version that is going

208
0:14:26.560 --> 0:14:31.960
to be inlined by the compiler because it's simple enough.

209
0:14:31.960 --> 0:14:33.520
And then I'm going to execute that.

210
0:14:33.520 --> 0:14:38.080
I'm saving a whole nanosecond there.

211
0:14:38.080 --> 0:14:44.200
So yeah, it's not a great optimization, to be honest.

212
0:14:44.200 --> 0:14:47.720
Probably you don't care about that nanosecond.

213
0:14:47.720 --> 0:14:53.160
But we are going to see why that is important later, not because of nanosecond.

214
0:14:53.160 --> 0:14:55.320
I'm going to talk now about escape analysis.

215
0:14:55.320 --> 0:14:59.640
Escape analysis is another thing that the compiler does for us and basically analyzes

216
0:14:59.640 --> 0:15:06.760
our variables and decides when a variable escapes from the context of the stack.

217
0:15:06.760 --> 0:15:11.400
It's something that is no longer able to get the information from the stack or store the

218
0:15:11.400 --> 0:15:16.640
information from the stack and be accessible where it needs to be accessible so it needs

219
0:15:16.640 --> 0:15:18.400
to escape to the heap.

220
0:15:18.400 --> 0:15:22.520
So it would generate that allocations.

221
0:15:22.520 --> 0:15:26.600
And we have seen that allocations have certain implications.

222
0:15:26.600 --> 0:15:29.480
So let's see an example here.

223
0:15:29.480 --> 0:15:35.280
This is an inlined, an not inlined function that returns a pointer.

224
0:15:35.280 --> 0:15:38.080
That is going to generate an allocation.

225
0:15:38.080 --> 0:15:40.800
This is something that returns by value.

226
0:15:40.800 --> 0:15:46.440
A value is going to copy the value to the stack of the caller so it's not going to generate

227
0:15:46.440 --> 0:15:47.880
allocations.

228
0:15:47.880 --> 0:15:54.120
So we can see that in the benchmark that is saying the first version have one allocation

229
0:15:54.120 --> 0:15:58.520
and it's allocating eight bytes and the second one have zero allocations.

230
0:15:58.520 --> 0:16:08.280
And actually you can see there is one allocation and it's taking ten times more to do that.

231
0:16:08.280 --> 0:16:12.080
Ten times more is in this case is around 12 nanoseconds.

232
0:16:12.080 --> 0:16:18.800
That is not a lot but everything adds up at the end, especially when you are calling millions

233
0:16:18.800 --> 0:16:21.280
of times the things.

234
0:16:21.280 --> 0:16:22.280
Okay.

235
0:16:22.280 --> 0:16:26.240
And one interesting thing is escape analysis plus inlining.

236
0:16:26.240 --> 0:16:27.240
Why?

237
0:16:27.240 --> 0:16:29.160
Well, imagine this situation.

238
0:16:29.160 --> 0:16:37.240
You have a struct, a function that generates or instantiates that struct and the constructor

239
0:16:37.240 --> 0:16:38.240
of that extract.

240
0:16:38.240 --> 0:16:43.680
Okay, the constructor returns me a pointer and do all the stuff that it needs.

241
0:16:43.680 --> 0:16:45.680
Okay, great.

242
0:16:45.680 --> 0:16:52.320
It is generating three allocations and it's consuming 56 bytes per operation.

243
0:16:52.320 --> 0:16:53.880
Okay.

244
0:16:53.880 --> 0:17:01.280
What happens if I just move the logic of that initialization process into a different function?

245
0:17:01.280 --> 0:17:07.200
If we do that, suddenly the new document is simple enough to be inlined.

246
0:17:07.200 --> 0:17:11.240
And because it's inlined, it's no longer escaped.

247
0:17:11.240 --> 0:17:14.680
So it's no longer needed that allocation.

248
0:17:14.680 --> 0:17:20.480
Something that simple allows you to just reduce the number of allocation of certain types

249
0:17:20.480 --> 0:17:23.200
when you have a constructor.

250
0:17:23.200 --> 0:17:27.840
What I would suggest is just keep your constructor as simple as possible and if you have to do

251
0:17:27.840 --> 0:17:32.600
certain complex logic, do it in an initialization function.

252
0:17:32.600 --> 0:17:37.680
Well, if that doesn't hurt the readability.

253
0:17:37.680 --> 0:17:40.400
Okay, let's see here.

254
0:17:40.400 --> 0:17:41.760
We have less allocations.

255
0:17:41.760 --> 0:17:47.520
We have now two allocations and 32 bytes per operation and the time consumed is just taking

256
0:17:47.520 --> 0:17:51.320
50 nanoseconds every time you instantiate that.

257
0:17:51.320 --> 0:17:56.920
So this is a good chunk.

258
0:17:56.920 --> 0:17:58.480
Okay.

259
0:17:58.480 --> 0:18:08.040
Well, this is optimization sometimes is a matter of tradeoffs.

260
0:18:08.040 --> 0:18:09.440
Sometimes you just can't do less.

261
0:18:09.440 --> 0:18:14.080
Like less allocations, less you work, less garbage collector pressure.

262
0:18:14.080 --> 0:18:16.880
All that stuff is things that can be done.

263
0:18:16.880 --> 0:18:22.160
But sometimes it's not about doing less.

264
0:18:22.160 --> 0:18:24.400
It's about consuming different kind of resources.

265
0:18:24.400 --> 0:18:30.600
I care less about memory and I care more about CPU or all the way around.

266
0:18:30.600 --> 0:18:36.120
So concurrency is one of the cases where you need to decide what you want to consume because

267
0:18:36.120 --> 0:18:40.600
coroutines are really cheap but are not free at all.

268
0:18:40.600 --> 0:18:44.760
So let's see an example with IO.

269
0:18:44.760 --> 0:18:47.000
This is two functions that I created.

270
0:18:47.000 --> 0:18:57.960
One is fake IO that is going to generate some kind of IO, IO simulation by time asleep.

271
0:18:57.960 --> 0:19:01.560
And then you have the fake IO parallel that receive the number of coroutines.

272
0:19:01.560 --> 0:19:12.480
It's doing basically the same but distributing all that 100 cycles between different coroutines.

273
0:19:12.480 --> 0:19:21.640
Then I built a benchmark to do that using three different approaches.

274
0:19:21.640 --> 0:19:24.400
One is serial one, no concurrency.

275
0:19:24.400 --> 0:19:29.320
The other one is concurrency using the number of CPUs in my machine.

276
0:19:29.320 --> 0:19:37.200
And the other one is using the number of tasks that I have.

277
0:19:37.200 --> 0:19:45.400
And because this is the result, I'm going to see that if I create one coroutine project,

278
0:19:45.400 --> 0:19:50.640
the number of bytes per operation and the number of allocations is going to spike.

279
0:19:50.640 --> 0:19:57.600
But the time that is going to be consumed is going to be way lower.

280
0:19:57.600 --> 0:20:09.160
So I'm able to execute 100 times this function using this one coroutine per job approach

281
0:20:09.160 --> 0:20:12.800
and only 12 using one CPU per job.

282
0:20:12.800 --> 0:20:13.800
Because this is audio.

283
0:20:13.800 --> 0:20:17.760
So let's see what happens if I do that with CPU.

284
0:20:17.760 --> 0:20:23.560
Using the CPU, this is to simulate some CPU load and using MD5-SUM.

285
0:20:23.560 --> 0:20:29.640
And it's more or less the same approach as we saw in the fake IELP.

286
0:20:29.640 --> 0:20:31.760
The benchmark is exactly the same approach.

287
0:20:31.760 --> 0:20:39.160
We are using the number of jobs and the number of CPUs and using no coroutines.

288
0:20:39.160 --> 0:20:46.080
And here is interesting because if you use the number of CPUs and this is a CPU workload,

289
0:20:46.080 --> 0:20:49.640
that is what is going to do the best efficiency.

290
0:20:49.640 --> 0:20:56.640
You can see here that executing one coroutine per job is going to be even slower than executing

291
0:20:56.640 --> 0:21:00.120
that in serial.

292
0:21:00.120 --> 0:21:04.760
And actually you have the worst of both worlds.

293
0:21:04.760 --> 0:21:10.480
You have plenty of allocations, plenty of memory consumption, plenty of time consumption,

294
0:21:10.480 --> 0:21:13.120
and you are not gaining anything.

295
0:21:13.120 --> 0:21:19.400
In the case of CPU, you are consuming more memory, you are consuming more memory, and

296
0:21:19.400 --> 0:21:25.040
you are getting better CPU performance because you are basically spreading the job all over

297
0:21:25.040 --> 0:21:28.560
your physical CPUs.

298
0:21:28.560 --> 0:21:34.640
And the serial one is just doing everything and it's using only one core of your CPU.

299
0:21:34.640 --> 0:21:39.120
So whenever you want to optimize using concurrency, you have to take into consideration what the

300
0:21:39.120 --> 0:21:44.560
kind of workload that you are using is the CPU workload, is it your workload, do you

301
0:21:44.560 --> 0:21:48.400
care about memory, do you care about CPU, what do you care about?

302
0:21:48.400 --> 0:21:54.480
So well, that's the whole idea.

303
0:21:54.480 --> 0:22:04.480
I just want to explain that all this is about measuring everything, doing all these benchmarks,

304
0:22:04.480 --> 0:22:12.280
doing all these kind of experiments to see if you are getting improvement on the performance

305
0:22:12.280 --> 0:22:14.960
and iterate over that.

306
0:22:14.960 --> 0:22:20.080
Just the main idea, I showed some examples of how you can improve things and some of

307
0:22:20.080 --> 0:22:25.760
them can be applied in general basics like using the...

308
0:22:25.760 --> 0:22:30.680
Try to keep constructors small or using the constructors for slices when you know the

309
0:22:30.680 --> 0:22:34.400
size and things like that.

310
0:22:34.400 --> 0:22:35.400
Some references.

311
0:22:35.400 --> 0:22:42.760
Efficient Go is a really book that is really, really interesting if you are really interested

312
0:22:42.760 --> 0:22:44.640
into efficiency.

313
0:22:44.640 --> 0:22:50.280
Dr. Tolomei Prok wrote that book and actually is going to give a talk after the next one.

314
0:22:50.280 --> 0:22:55.000
So I'm not sure it's going to be super interesting.

315
0:22:55.000 --> 0:22:57.240
High-Performance Workshop from Dave Cheney.

316
0:22:57.240 --> 0:23:01.560
There's a lot of documentation about that workshop that Dave Cheney did and it's really

317
0:23:01.560 --> 0:23:02.560
interesting also.

318
0:23:02.560 --> 0:23:05.280
The GoPerf book is a good lecture also.

319
0:23:05.280 --> 0:23:11.520
And Ultimate Go course from Aragon Labs is also an interesting course because it's giving

320
0:23:11.520 --> 0:23:16.840
you a lot of foundation and the course takes a lot of...

321
0:23:16.840 --> 0:23:21.560
Cares a lot about hardware sympathy and all that stuff.

322
0:23:21.560 --> 0:23:23.000
Well some creative common...

323
0:23:23.000 --> 0:23:30.560
All the images are creative common so I put the reference here because it's creative common.

324
0:23:30.560 --> 0:23:31.560
And thank you.

325
0:23:31.560 --> 0:23:32.560
That's it.

326
0:23:32.560 --> 0:23:41.920
Thank you.

