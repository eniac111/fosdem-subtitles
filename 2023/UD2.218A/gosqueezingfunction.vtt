WEBVTT

00:00.000 --> 00:07.800
Okay, thank you.

00:07.800 --> 00:09.080
So our next speaker is Jesus.

00:09.080 --> 00:13.360
We've been talking a few times in the Go Dev Room about everything that has to do deeply

00:13.360 --> 00:17.640
within the language and today he's going to talk to us about what's going on with functions.

00:17.640 --> 00:20.640
Round of applause.

00:20.640 --> 00:23.200
Hello, everybody.

00:23.200 --> 00:28.040
My name is Jesus.

00:28.040 --> 00:30.040
I'm software engineer at Marimos.

00:30.040 --> 00:32.240
I'm going to talk about squeezing and Go functions.

00:32.240 --> 00:34.880
So what is optimization?

00:34.880 --> 00:40.440
I think it's important to know that optimization is not being faster or consuming less memory.

00:40.440 --> 00:41.920
It depends on your needs.

00:41.920 --> 00:45.760
So it's better fresh, squeezed juice.

00:45.760 --> 00:50.280
Probably everybody will say yes, but it depends if you are looking for convenience or for

00:50.280 --> 00:52.480
something that lasts forever.

00:52.480 --> 00:55.480
So in that case, it's not the best option.

00:55.480 --> 01:01.320
Optimization is about what you need and trying to address that.

01:01.320 --> 01:03.300
It's important to optimize at the right level.

01:03.300 --> 01:04.920
You can buy the best car.

01:04.920 --> 01:11.420
You can get an F1 car and it's not going to be fast if this is the road.

01:11.420 --> 01:19.680
So try to optimize always at the upper level because these kind of optimizations that were

01:19.680 --> 01:25.160
the ones that we are going to see in the stock are micro optimizations that probably are

01:25.160 --> 01:31.080
not the first place that you should be starting.

01:31.080 --> 01:35.080
Optimize what you need and when you need it.

01:35.080 --> 01:41.920
It's not about taking a Go function and try to optimize forever and try to make that run

01:41.920 --> 01:48.960
superficially and scratch every single nanosecond because probably the bottleneck is no longer

01:48.960 --> 01:49.960
there.

01:49.960 --> 01:51.840
You have to search for the bottleneck.

01:51.840 --> 01:56.400
You have to optimize where the bottleneck is and then look again if the bottleneck is

01:56.400 --> 02:02.160
still there because if it's no longer there, you are over-optimizing that function without

02:02.160 --> 02:03.680
much gain.

02:03.680 --> 02:07.160
So just take that into consideration.

02:07.160 --> 02:15.680
Optimizing is an iterative cycle and you need to keep moving and keep searching for the

02:15.680 --> 02:18.360
bottleneck.

02:18.360 --> 02:19.760
Do not guess, please.

02:19.760 --> 02:26.200
I know everybody has things and all that stuff but guessing about performance is an awful

02:26.200 --> 02:32.400
thing because there's so many things that come into play that is just impossible.

02:32.400 --> 02:38.520
There's the operating system, the compiler, the optimizations of the compiler, if you

02:38.520 --> 02:42.200
are in the cloud, maybe a noisy neighbor.

02:42.200 --> 02:45.120
All that stuff comes into play with performance.

02:45.120 --> 02:52.240
You are not good at guessing almost for sure in performance.

02:52.240 --> 02:54.160
So just measure everything.

02:54.160 --> 02:58.640
The important thing here is try to measure everything and work with that data.

02:58.640 --> 03:05.720
Actually it's what probably the talk that is after the next one is about.

03:05.720 --> 03:11.640
I will suggest to go there also because it probably is a very interesting talk.

03:11.640 --> 03:13.560
So let's talk about benchmarks.

03:13.560 --> 03:22.440
The way that you measure performance in micro-optimizations or micro-benchmarks is through Go benchmarks.

03:22.440 --> 03:28.240
Go benchmark is a tool that comes with Go and is similar to the testing framework that

03:28.240 --> 03:31.360
comes in Go but very focused on benchmarking.

03:31.360 --> 03:38.280
In this case we can see here an example to have two benchmarks, one for MD5-SUM and one

03:38.280 --> 03:44.520
for SHA-256-SUM.

03:44.520 --> 03:45.520
That's it.

03:45.520 --> 03:49.920
It's just a function that starts with benchmarking and receives a testing.b argument and that's

03:49.920 --> 03:53.040
this four, half this four loop inside.

03:53.040 --> 03:58.040
And that is going to do all the job to give you the numbers.

03:58.040 --> 03:59.600
And I show you now the numbers.

03:59.600 --> 04:05.200
If I run this with GoBench, we go test dash bench dot.

04:05.200 --> 04:08.720
The dot is a regular expression that means everything.

04:08.720 --> 04:16.740
So you can use like the Go test run regular expression for only executing certain benchmarks.

04:16.740 --> 04:26.880
And here you can see that MD5-SUM is around twice time faster per operation than SHA.

04:26.880 --> 04:28.640
So well, just a number.

04:28.640 --> 04:29.640
It's that important.

04:29.640 --> 04:30.640
It depends.

04:30.640 --> 04:34.320
If you need more security, probably MD5 is not the best option.

04:34.320 --> 04:38.600
So it depends on your needs.

04:38.600 --> 04:42.560
Another interesting thing is the allocations.

04:42.560 --> 04:47.160
One thing that you maybe have heard is about counting allocations.

04:47.160 --> 04:49.520
Counting allocations, why is that important?

04:49.520 --> 04:54.000
Is because every time we allocate something, when we talk allocation, we are talking about

04:54.000 --> 04:55.880
allocation in the heap.

04:55.880 --> 05:00.760
If every time we allocate something in the heap, allocating that is going to introduce

05:00.760 --> 05:02.000
an overhead.

05:02.000 --> 05:06.080
And not only that, it's going to add more pressure to the garbage collector.

05:06.080 --> 05:11.040
That's why it's important to count the allocations when you are talking about performance.

05:11.040 --> 05:17.000
If you are not worried about performance at that point, don't count the allocation.

05:17.000 --> 05:18.320
It's not that important.

05:18.320 --> 05:25.320
And you are not going to gain a massive amount of performance from there if you are not in

05:25.320 --> 05:27.000
that point there.

05:27.000 --> 05:32.800
Okay, let's see an example here in MD5 and SHA sums.

05:32.800 --> 05:34.640
We have zero allocations.

05:34.640 --> 05:37.840
So well, this data is not very useful for us now.

05:37.840 --> 05:39.400
So let's use another thing.

05:39.400 --> 05:40.920
Let's open a file.

05:40.920 --> 05:47.640
Let's open a file thousands of times and see how it goes.

05:47.640 --> 05:52.280
Now I see that every single operation of opening a file, just opening the file, is going to

05:52.280 --> 05:58.080
generate three allocations, and it's going to consume 120 bytes per operation.

05:58.080 --> 05:59.480
Interesting.

05:59.480 --> 06:01.920
So now you are measuring things.

06:01.920 --> 06:08.240
You are measuring how much time it takes, how much time is gone in processing something,

06:08.240 --> 06:14.800
is gone in allocating things, how much memory is gone there.

06:14.800 --> 06:17.000
So let's talk about profiling.

06:17.000 --> 06:24.680
Because once you, well, actually, normally you do the profiling first to find your bottleneck,

06:24.680 --> 06:29.920
and then you do the benchmark to tune that bottleneck.

06:29.920 --> 06:35.880
But I'm playing with the fact that I already have the benchmark, and I'm going to do the

06:35.880 --> 06:38.000
profiling on top of the benchmark.

06:38.000 --> 06:40.000
So I'm going to execute the gobench.

06:40.000 --> 06:41.760
I'm going to pass the mem profile.

06:41.760 --> 06:43.720
I'm going to generate the mem profile.

06:43.720 --> 06:46.000
And I'm going to use the piproftool.

06:46.000 --> 06:49.240
The piproftool is going to allow me to analyze that profile.

06:49.240 --> 06:53.280
In this case, I'm just asking for a text output.

06:53.280 --> 06:59.560
And that text output is going to show me the top consumers of memory in this case.

06:59.560 --> 07:06.560
And I can see there that 84% of the memory is gone in OS new file.

07:06.560 --> 07:07.560
Okay.

07:07.560 --> 07:09.160
Let's see what happened.

07:09.160 --> 07:10.160
Okay.

07:10.160 --> 07:12.680
It's that file, but I need more information.

07:12.680 --> 07:15.680
Well, it's that function, sorry.

07:15.680 --> 07:17.520
I need more information.

07:17.520 --> 07:20.160
Actually, I kind of like this output.

07:20.160 --> 07:25.680
But if you don't like this output, you can, for example, use SVG.

07:25.680 --> 07:28.920
And you are going to get something like this that is very visual.

07:28.920 --> 07:34.360
And actually, it's kind of obvious that where is the bottleneck there.

07:34.360 --> 07:39.120
And in this case, again, it's OS new file.

07:39.120 --> 07:45.640
If I go to the piproftool again, and instead of that, I use the list of a function, I'm

07:45.640 --> 07:50.360
seeing here where is the memory going by line.

07:50.360 --> 07:58.520
And here, I can see that in the line 127 of the file, fileunix.go, I'm consuming the

07:58.520 --> 07:59.520
memory.

07:59.520 --> 08:02.360
Actually, there you see 74 megabytes.

08:02.360 --> 08:06.200
That is because it's counting all the allocation and aggregating all the allocations.

08:06.200 --> 08:12.520
It's not, every operation here is consuming only 120 bytes.

08:12.520 --> 08:15.120
Okay.

08:15.120 --> 08:17.360
The same with CPU profile.

08:17.360 --> 08:25.640
In this case, this is generating most of the CPU consumption is in Cisco, Cisco6.

08:25.640 --> 08:29.920
I can see in SVG, this time it's more scattered.

08:29.920 --> 08:35.160
So the CPU is consuming in way more places.

08:35.160 --> 08:38.880
But still, the Cisco, Cisco6 is the biggest one.

08:38.880 --> 08:43.200
So I'm going to list that, and I see some assembly code.

08:43.200 --> 08:47.960
Only you are not going to optimize more this function.

08:47.960 --> 08:52.640
So probably this is not the place that you should be looking for optimizations.

08:52.640 --> 09:01.320
Anyway, this is an example of getting to the root cause during the profiling.

09:01.320 --> 09:04.000
Okay.

09:04.000 --> 09:06.800
This talk is going to be more by examples.

09:06.800 --> 09:12.240
I'm going to try to show you some examples of optimizations.

09:12.240 --> 09:16.480
It's just to show you the process more than the specific optimization.

09:16.480 --> 09:21.000
I expect you learned something in between, but it's more about the process.

09:21.000 --> 09:22.000
Okay.

09:22.000 --> 09:26.440
One of the things that you can do is reducing the CPU usage.

09:26.440 --> 09:28.480
This is a kind of silly example.

09:28.480 --> 09:33.560
You have a fine function that have a needle and a high stack, and just go through the

09:33.560 --> 09:40.160
high stack and search for that needle and give you the result.

09:40.160 --> 09:47.360
This is looping over the whole string, sorry, the whole slice.

09:47.360 --> 09:48.600
I'm going to do a benchmark.

09:48.600 --> 09:50.720
The first thing, I'm going to do the benchmark.

09:50.720 --> 09:55.560
I'm going to generate a lot of strings.

09:55.560 --> 10:01.840
And I'm going to do a benchmark looking for something around in the middle.

10:01.840 --> 10:05.800
It's not exactly in the middle, but it's around there.

10:05.800 --> 10:13.880
And the benchmark is saying that it's taking nearly 300 nanoseconds.

10:13.880 --> 10:18.440
If I just early return, that is just a kind of silly optimization.

10:18.440 --> 10:22.040
It's not super smart or something like that.

10:22.040 --> 10:27.360
I'm going to save basically almost the half of the performance.

10:27.360 --> 10:33.360
This is because the benchmark is doing something really silly, and it can vary depending on

10:33.360 --> 10:35.680
the data that it inputs.

10:35.680 --> 10:38.880
That is an optimization of just doing less.

10:38.880 --> 10:43.200
That is one of the best ways of optimizing things.

10:43.200 --> 10:46.160
Well, reducing allocations.

10:46.160 --> 10:51.000
One of the classic examples of reducing allocations is when you are dealing with slices.

10:51.000 --> 10:55.520
When you have a slice, for example, this is a common way of constructing a slice.

10:55.520 --> 11:01.080
I create a slice, I loop over this, generate a loop, and start appending things to that

11:01.080 --> 11:02.080
slice.

11:02.080 --> 11:03.080
Okay, fine.

11:03.080 --> 11:08.160
I'm going to do a benchmark for checking that.

11:08.160 --> 11:16.240
And it's taking 39 allocations and around 41 megabytes per operation.

11:16.240 --> 11:20.400
Okay, sounds like a lot.

11:20.400 --> 11:23.480
Okay, let's do it.

11:23.480 --> 11:25.720
Let's do this.

11:25.720 --> 11:32.760
Let's build the slice, but we are going to give an initial size of a million.

11:32.760 --> 11:35.360
And the time, I'm just setting that.

11:35.360 --> 11:40.040
The final result is exactly the same, but now we have one allocation and we have consumed

11:40.040 --> 11:41.040
only one megabyte.

11:41.040 --> 11:47.840
And actually, if you see there, it's around 800 microseconds.

11:47.840 --> 12:00.280
And here you have around, well, let me check, 10, oh, yeah, around 10 milliseconds.

12:00.280 --> 12:04.960
So it's a lot of time, actually, a lot of CPU time, too.

12:04.960 --> 12:06.320
But you can squeeze it more.

12:06.320 --> 12:11.560
If you know that at compile time, if you know exactly the size that you want to have at

12:11.560 --> 12:14.720
compile time, you can build an array.

12:14.720 --> 12:17.480
It's faster than any slice, actually.

12:17.480 --> 12:23.400
So if I build an array, I'm now doing zero allocations, zero heap allocations.

12:23.400 --> 12:29.800
It's going to go in the stack or in binary somehow or whatever, but it's not consuming

12:29.800 --> 12:33.280
my heap allocations.

12:33.280 --> 12:38.040
And this time is 300 microseconds, approximately.

12:38.040 --> 12:43.880
So an interesting thing if you know that information at compile time.

12:43.880 --> 12:44.880
OK.

12:44.880 --> 12:46.520
Another thing is packing.

12:46.520 --> 12:52.320
If you are concerned about memory, you can build this stack and say, OK, I have a Boolean,

12:52.320 --> 12:56.120
I have a float, I have an N32.

12:56.120 --> 13:04.440
And the Go compiler is going to align my stack to make it more efficient and work better

13:04.440 --> 13:06.360
with the CPU and all that stuff.

13:06.360 --> 13:11.440
And in this case, it's just adding seven bytes between the boot and the float and four bytes

13:11.440 --> 13:14.720
after the integer to get everything aligned.

13:14.720 --> 13:16.120
OK.

13:16.120 --> 13:20.040
I built down a slice and initialized that's a slice.

13:20.040 --> 13:24.920
And I'm allocating one time because that's what the slice is doing.

13:24.920 --> 13:29.920
And I'm consuming around 24 megabytes per operation.

13:29.920 --> 13:35.000
If I just reorganize the struct, in this case, I put the float at the beginning, then the

13:35.000 --> 13:39.360
N32 and then the Boolean, the compiler is only going to add three bytes.

13:39.360 --> 13:42.880
So the whole structure is going to be smaller in memory.

13:42.880 --> 13:47.600
And in this case, now, it's 16 megabytes per operation.

13:47.600 --> 13:52.440
So this kind of optimization is not going to save your day if you are just creating

13:52.440 --> 13:58.680
some structs, but if you are creating millions of instances of an struct, it can be a significant

13:58.680 --> 14:02.160
amount of memory.

14:02.160 --> 14:03.160
Functioning lining.

14:03.160 --> 14:06.880
Functioning lining is something that the Go compiler does for us.

14:06.880 --> 14:12.600
It's just taking a function and replacing any call to that function with the code that

14:12.600 --> 14:17.320
is generated by the function.

14:17.320 --> 14:20.240
I want to show you a very damaged sample.

14:20.240 --> 14:26.560
I'm not inlining this function explicitly, and I'm using the inline version that is going

14:26.560 --> 14:31.960
to be inlined by the compiler because it's simple enough.

14:31.960 --> 14:33.520
And then I'm going to execute that.

14:33.520 --> 14:38.080
I'm saving a whole nanosecond there.

14:38.080 --> 14:44.200
So yeah, it's not a great optimization, to be honest.

14:44.200 --> 14:47.720
Probably you don't care about that nanosecond.

14:47.720 --> 14:53.160
But we are going to see why that is important later, not because of nanosecond.

14:53.160 --> 14:55.320
I'm going to talk now about escape analysis.

14:55.320 --> 14:59.640
Escape analysis is another thing that the compiler does for us and basically analyzes

14:59.640 --> 15:06.760
our variables and decides when a variable escapes from the context of the stack.

15:06.760 --> 15:11.400
It's something that is no longer able to get the information from the stack or store the

15:11.400 --> 15:16.640
information from the stack and be accessible where it needs to be accessible so it needs

15:16.640 --> 15:18.400
to escape to the heap.

15:18.400 --> 15:22.520
So it would generate that allocations.

15:22.520 --> 15:26.600
And we have seen that allocations have certain implications.

15:26.600 --> 15:29.480
So let's see an example here.

15:29.480 --> 15:35.280
This is an inlined, an not inlined function that returns a pointer.

15:35.280 --> 15:38.080
That is going to generate an allocation.

15:38.080 --> 15:40.800
This is something that returns by value.

15:40.800 --> 15:46.440
A value is going to copy the value to the stack of the caller so it's not going to generate

15:46.440 --> 15:47.880
allocations.

15:47.880 --> 15:54.120
So we can see that in the benchmark that is saying the first version have one allocation

15:54.120 --> 15:58.520
and it's allocating eight bytes and the second one have zero allocations.

15:58.520 --> 16:08.280
And actually you can see there is one allocation and it's taking ten times more to do that.

16:08.280 --> 16:12.080
Ten times more is in this case is around 12 nanoseconds.

16:12.080 --> 16:18.800
That is not a lot but everything adds up at the end, especially when you are calling millions

16:18.800 --> 16:21.280
of times the things.

16:21.280 --> 16:22.280
Okay.

16:22.280 --> 16:26.240
And one interesting thing is escape analysis plus inlining.

16:26.240 --> 16:27.240
Why?

16:27.240 --> 16:29.160
Well, imagine this situation.

16:29.160 --> 16:37.240
You have a struct, a function that generates or instantiates that struct and the constructor

16:37.240 --> 16:38.240
of that extract.

16:38.240 --> 16:43.680
Okay, the constructor returns me a pointer and do all the stuff that it needs.

16:43.680 --> 16:45.680
Okay, great.

16:45.680 --> 16:52.320
It is generating three allocations and it's consuming 56 bytes per operation.

16:52.320 --> 16:53.880
Okay.

16:53.880 --> 17:01.280
What happens if I just move the logic of that initialization process into a different function?

17:01.280 --> 17:07.200
If we do that, suddenly the new document is simple enough to be inlined.

17:07.200 --> 17:11.240
And because it's inlined, it's no longer escaped.

17:11.240 --> 17:14.680
So it's no longer needed that allocation.

17:14.680 --> 17:20.480
Something that simple allows you to just reduce the number of allocation of certain types

17:20.480 --> 17:23.200
when you have a constructor.

17:23.200 --> 17:27.840
What I would suggest is just keep your constructor as simple as possible and if you have to do

17:27.840 --> 17:32.600
certain complex logic, do it in an initialization function.

17:32.600 --> 17:37.680
Well, if that doesn't hurt the readability.

17:37.680 --> 17:40.400
Okay, let's see here.

17:40.400 --> 17:41.760
We have less allocations.

17:41.760 --> 17:47.520
We have now two allocations and 32 bytes per operation and the time consumed is just taking

17:47.520 --> 17:51.320
50 nanoseconds every time you instantiate that.

17:51.320 --> 17:56.920
So this is a good chunk.

17:56.920 --> 17:58.480
Okay.

17:58.480 --> 18:08.040
Well, this is optimization sometimes is a matter of tradeoffs.

18:08.040 --> 18:09.440
Sometimes you just can't do less.

18:09.440 --> 18:14.080
Like less allocations, less you work, less garbage collector pressure.

18:14.080 --> 18:16.880
All that stuff is things that can be done.

18:16.880 --> 18:22.160
But sometimes it's not about doing less.

18:22.160 --> 18:24.400
It's about consuming different kind of resources.

18:24.400 --> 18:30.600
I care less about memory and I care more about CPU or all the way around.

18:30.600 --> 18:36.120
So concurrency is one of the cases where you need to decide what you want to consume because

18:36.120 --> 18:40.600
coroutines are really cheap but are not free at all.

18:40.600 --> 18:44.760
So let's see an example with IO.

18:44.760 --> 18:47.000
This is two functions that I created.

18:47.000 --> 18:57.960
One is fake IO that is going to generate some kind of IO, IO simulation by time asleep.

18:57.960 --> 19:01.560
And then you have the fake IO parallel that receive the number of coroutines.

19:01.560 --> 19:12.480
It's doing basically the same but distributing all that 100 cycles between different coroutines.

19:12.480 --> 19:21.640
Then I built a benchmark to do that using three different approaches.

19:21.640 --> 19:24.400
One is serial one, no concurrency.

19:24.400 --> 19:29.320
The other one is concurrency using the number of CPUs in my machine.

19:29.320 --> 19:37.200
And the other one is using the number of tasks that I have.

19:37.200 --> 19:45.400
And because this is the result, I'm going to see that if I create one coroutine project,

19:45.400 --> 19:50.640
the number of bytes per operation and the number of allocations is going to spike.

19:50.640 --> 19:57.600
But the time that is going to be consumed is going to be way lower.

19:57.600 --> 20:09.160
So I'm able to execute 100 times this function using this one coroutine per job approach

20:09.160 --> 20:12.800
and only 12 using one CPU per job.

20:12.800 --> 20:13.800
Because this is audio.

20:13.800 --> 20:17.760
So let's see what happens if I do that with CPU.

20:17.760 --> 20:23.560
Using the CPU, this is to simulate some CPU load and using MD5-SUM.

20:23.560 --> 20:29.640
And it's more or less the same approach as we saw in the fake IELP.

20:29.640 --> 20:31.760
The benchmark is exactly the same approach.

20:31.760 --> 20:39.160
We are using the number of jobs and the number of CPUs and using no coroutines.

20:39.160 --> 20:46.080
And here is interesting because if you use the number of CPUs and this is a CPU workload,

20:46.080 --> 20:49.640
that is what is going to do the best efficiency.

20:49.640 --> 20:56.640
You can see here that executing one coroutine per job is going to be even slower than executing

20:56.640 --> 21:00.120
that in serial.

21:00.120 --> 21:04.760
And actually you have the worst of both worlds.

21:04.760 --> 21:10.480
You have plenty of allocations, plenty of memory consumption, plenty of time consumption,

21:10.480 --> 21:13.120
and you are not gaining anything.

21:13.120 --> 21:19.400
In the case of CPU, you are consuming more memory, you are consuming more memory, and

21:19.400 --> 21:25.040
you are getting better CPU performance because you are basically spreading the job all over

21:25.040 --> 21:28.560
your physical CPUs.

21:28.560 --> 21:34.640
And the serial one is just doing everything and it's using only one core of your CPU.

21:34.640 --> 21:39.120
So whenever you want to optimize using concurrency, you have to take into consideration what the

21:39.120 --> 21:44.560
kind of workload that you are using is the CPU workload, is it your workload, do you

21:44.560 --> 21:48.400
care about memory, do you care about CPU, what do you care about?

21:48.400 --> 21:54.480
So well, that's the whole idea.

21:54.480 --> 22:04.480
I just want to explain that all this is about measuring everything, doing all these benchmarks,

22:04.480 --> 22:12.280
doing all these kind of experiments to see if you are getting improvement on the performance

22:12.280 --> 22:14.960
and iterate over that.

22:14.960 --> 22:20.080
Just the main idea, I showed some examples of how you can improve things and some of

22:20.080 --> 22:25.760
them can be applied in general basics like using the...

22:25.760 --> 22:30.680
Try to keep constructors small or using the constructors for slices when you know the

22:30.680 --> 22:34.400
size and things like that.

22:34.400 --> 22:35.400
Some references.

22:35.400 --> 22:42.760
Efficient Go is a really book that is really, really interesting if you are really interested

22:42.760 --> 22:44.640
into efficiency.

22:44.640 --> 22:50.280
Dr. Tolomei Prok wrote that book and actually is going to give a talk after the next one.

22:50.280 --> 22:55.000
So I'm not sure it's going to be super interesting.

22:55.000 --> 22:57.240
High-Performance Workshop from Dave Cheney.

22:57.240 --> 23:01.560
There's a lot of documentation about that workshop that Dave Cheney did and it's really

23:01.560 --> 23:02.560
interesting also.

23:02.560 --> 23:05.280
The GoPerf book is a good lecture also.

23:05.280 --> 23:11.520
And Ultimate Go course from Aragon Labs is also an interesting course because it's giving

23:11.520 --> 23:16.840
you a lot of foundation and the course takes a lot of...

23:16.840 --> 23:21.560
Cares a lot about hardware sympathy and all that stuff.

23:21.560 --> 23:23.000
Well some creative common...

23:23.000 --> 23:30.560
All the images are creative common so I put the reference here because it's creative common.

23:30.560 --> 23:31.560
And thank you.

23:31.560 --> 23:32.560
That's it.

23:32.560 --> 23:41.920
Thank you.
