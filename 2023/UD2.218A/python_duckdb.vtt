WEBVTT

00:00.000 --> 00:13.680
Let's welcome Pedro Halanda for his talk on DuckDB and Magnificent Snake Duck.

00:13.680 --> 00:17.400
Yeah, you guys can be surprised at all.

00:17.400 --> 00:20.200
Anything you can find as a rubber duck these days, you know.

00:20.200 --> 00:22.920
All right, so I'm Pedro Halanda.

00:22.920 --> 00:28.440
I am one of the main contributors of the DuckDB projects, which is an open source database

00:28.440 --> 00:29.440
system.

00:29.440 --> 00:32.640
I'm the COO of DuckDB Labs.

00:32.640 --> 00:36.640
And today I'm going to be talking a little bit about how DuckDB can bring analytical

00:36.640 --> 00:40.480
SQL power directly into your Python shell.

00:40.480 --> 00:44.920
So to give you a little bit of an idea of how these talks look like, I'm going to start

00:44.920 --> 00:46.640
with what is DuckDB?

00:46.640 --> 00:50.080
So I'm here talking about one more database system.

00:50.080 --> 00:54.360
I'm going to motivate you guys that we actually needed to do one more database system.

00:54.360 --> 00:57.560
The other ones didn't solve the problems we had.

00:57.560 --> 01:00.360
And then I'm going to go over the main characteristics of DuckDB.

01:00.360 --> 01:03.720
So what actually makes it special?

01:03.720 --> 01:11.120
Then I'm going to go over DuckDB Python line, so how DuckDB integrates in the Python ecosystem.

01:11.120 --> 01:12.760
I'm going to do a little demo.

01:12.760 --> 01:18.160
The basic idea is that we're going to use the infamous New York City taxi data sets.

01:18.160 --> 01:23.060
And we're going to try to do some estimation of fair costs.

01:23.060 --> 01:27.400
And we're going to use DuckDB pandas and PySpark just to see a couple of the differences.

01:27.400 --> 01:30.120
Of the things I'm going to be talking over.

01:30.120 --> 01:32.440
And then some summary of the talk.

01:32.440 --> 01:33.440
So what is DuckDB?

01:33.440 --> 01:39.260
Well, DuckDB was actually born at CWI, which is the Research Center of Mathematics and

01:39.260 --> 01:41.720
Computer Science in the Netherlands.

01:41.720 --> 01:45.760
And what we actually had there is that a lot of the projects, the PhDs in the projects,

01:45.760 --> 01:48.800
the master projects, they are very data sciencey.

01:48.800 --> 01:52.360
So usually you have a data science problem and you want to throw a database management

01:52.360 --> 01:56.160
system at a data science problem because you're handling data.

01:56.160 --> 02:02.080
So initially we're like, okay, we can probably use a database server, use a database connection,

02:02.080 --> 02:07.600
and then just transfer the data from the relational database to your Python terminal, for example,

02:07.600 --> 02:08.600
right?

02:08.600 --> 02:10.400
Like where your analytical tools are.

02:10.400 --> 02:14.920
And it turns out that's quite a bad idea because you are transferring a lot of data.

02:14.920 --> 02:16.880
So that's pretty costly.

02:16.880 --> 02:19.760
And then we're like, okay, this is really not solving our problem.

02:19.760 --> 02:22.200
Can we draw inspiration from somewhere else?

02:22.200 --> 02:25.800
And then of course there are SQLites, the most famous database out there, at least the

02:25.800 --> 02:28.120
most used one.

02:28.120 --> 02:33.600
And it has a quite a nice property, which is being an embedded database system.

02:33.600 --> 02:37.320
Being an embedded database system, it means it can run inside your Python process.

02:37.320 --> 02:41.400
So you can eliminate this data transfer cost.

02:41.400 --> 02:46.440
SQLite comes with one design decision that is a transactional database.

02:46.440 --> 02:50.920
So it's actually super optimized for small updates, but it's not really optimized for

02:50.920 --> 02:52.720
analytics.

02:52.720 --> 02:57.760
So we kind of wanted to use SQLites in terms of being easy to use and eliminating this

02:57.760 --> 03:03.160
data transfer cost, but focusing on analytical queries.

03:03.160 --> 03:07.480
And that's kind of how the code was born, and that's also why we frame it as a SQLite

03:07.480 --> 03:09.560
for analytics.

03:09.560 --> 03:11.240
It also has a very simple installation.

03:11.240 --> 03:14.760
So if you think about Python, it's just a big solve and you're good.

03:14.760 --> 03:16.840
Since it's embedded, there's no server management.

03:16.840 --> 03:21.080
So let's say you just want to, I don't know, query a per K file, two lines of code you

03:21.080 --> 03:22.080
can already query it.

03:22.080 --> 03:24.880
There's no starting of server.

03:24.880 --> 03:27.280
There's no schema creation.

03:27.280 --> 03:28.960
The schema is inferred from the object.

03:28.960 --> 03:31.240
So it's very easy, very fast.

03:31.240 --> 03:35.680
And we also really focus on this fast transfer between analytical languages.

03:35.680 --> 03:39.000
And there are tools like in Python and R to DuckDB.

03:39.000 --> 03:41.840
DuckDB is currently in pre-release.

03:41.840 --> 03:46.760
I think the last version we released was 0.6, 0.7 is coming up soon.

03:46.760 --> 03:52.400
In the web page, there's a little bit more details about all the things that are in each

03:52.400 --> 03:53.400
release.

03:53.400 --> 03:54.400
All right.

03:54.400 --> 04:01.760
So I'm going to go over some of the main characteristics of DuckDB, particularly the color data storage,

04:01.760 --> 04:02.760
a little bit about compression.

04:02.760 --> 04:05.600
I'm going to talk about vectorized execution.

04:05.600 --> 04:07.720
So these are all like core database stuff.

04:07.720 --> 04:12.840
Actually talking about vectorized execution engine, it's going to be difficult because

04:12.840 --> 04:15.640
Professor Bong is here and he actually created that.

04:15.640 --> 04:20.040
So I'll try to do it correctly.

04:20.040 --> 04:25.160
A little bit about intimate corruptimization, parallelism and beyond memory execution.

04:25.160 --> 04:26.760
So color data storage.

04:26.760 --> 04:30.000
Well, there's basically two ways that you can do it.

04:30.000 --> 04:32.320
One is a row store, a scan store.

04:32.320 --> 04:34.920
As an example, row store, we have SQLites.

04:34.920 --> 04:39.760
And the whole thing about the whole idea is that you're storing your data consecutively

04:39.760 --> 04:42.920
in memory per row.

04:42.920 --> 04:47.400
So that basically means that if you want to fetch a row, that's very cheap because it's

04:47.400 --> 04:48.920
contiguous in memory.

04:48.920 --> 04:52.200
However, you always have to fetch all the columns.

04:52.200 --> 04:58.160
So analytical queries usually you have very wide tables, but you just want to really get

04:58.160 --> 05:00.240
a couple of these columns.

05:00.240 --> 05:02.020
So what if you only want to use a field?

05:02.020 --> 05:08.120
So in this example, what if you just are interested in the price of a product, but not the stores

05:08.120 --> 05:11.440
they sold, right?

05:11.440 --> 05:19.200
In a column store, you actually have your layouts that the data of the column is consecutively

05:19.200 --> 05:20.200
in memory.

05:20.200 --> 05:24.440
So if you want to access just a couple columns, you can actually have immense savings on disk

05:24.440 --> 05:27.240
I.O. and memory bandwidth.

05:27.240 --> 05:31.320
So that's why this type of format is really optimized for analytics.

05:31.320 --> 05:35.840
So to give you a more concrete example, let's say that we have a one terabyte table with

05:35.840 --> 05:38.040
100 columns.

05:38.040 --> 05:40.720
For simplicity, let's say all the columns have the same size.

05:40.720 --> 05:44.540
And we just require five columns of the table in our analytical queries.

05:44.540 --> 05:49.760
So in a row store like SQLites, reading this whole table, if you have a disk with around

05:49.760 --> 05:52.600
100 megabytes per second, it will take you three hours.

05:52.600 --> 05:57.160
If you were using a column store model, which is what Pandas and Ducktivitas, for example,

05:57.160 --> 06:00.840
reading these five columns from disk takes you eight minutes.

06:00.840 --> 06:08.280
So there's a huge improvement by just setting up the correct storage formats for your workload.

06:08.280 --> 06:12.840
Compression, well, I'm not going to go into a lot of detail about the compression algorithms

06:12.840 --> 06:15.760
that we implement in Ducktivi.

06:15.760 --> 06:20.120
But what I can tell you is because of having a column store, you're going to have your

06:20.120 --> 06:26.560
data from your column continuously in memory, which gives you a very good advantage to compress

06:26.560 --> 06:31.160
units, because usually the data from the same column is somewhat similar.

06:31.160 --> 06:41.080
So you can apply cool things like RLE, FSSC, and CHIMP for floating point numbers, FSSC

06:41.080 --> 06:42.080
for strings.

06:42.080 --> 06:45.800
So you can start applying all these algorithms and really decrease the size of your data.

06:45.800 --> 06:51.480
So in this table here, we actually have, I think this is from one year ago, one year

06:51.480 --> 06:55.600
and a half, 0.2.8 from Ducktivi.

06:55.600 --> 06:57.040
We had no compression at that point.

06:57.040 --> 07:01.840
And then a year and a half later, we actually managed to implement all these things which

07:01.840 --> 07:09.680
got us five times better compression line 19, for example, 3.18 better compression in

07:09.680 --> 07:12.640
the taxi data set that I'm going to be using later.

07:12.640 --> 07:13.840
And why is compression so important?

07:13.840 --> 07:19.040
Well, if we go back again to the same example where we're reading our five columns and it

07:19.040 --> 07:23.880
was causing us to read them from disk eight minutes because of the storage formats, if

07:23.880 --> 07:29.040
we compress these columns, we suddenly don't have to read 50 gigabytes anymore, right?

07:29.040 --> 07:30.120
You read less.

07:30.120 --> 07:34.000
And then of course, you apply the best case from what I showed you from the last table,

07:34.000 --> 07:35.000
five times.

07:35.000 --> 07:40.600
The RLE increases the cost to one minute and 40 seconds.

07:40.600 --> 07:42.600
So execution.

07:42.600 --> 07:47.080
Well, there's three ways of doing query execution.

07:47.080 --> 07:49.720
There's actually one more, but it's not in the slides.

07:49.720 --> 07:53.480
But SQLite uses a top-order time processing, which means that you process one row at a

07:53.480 --> 07:54.480
time.

07:54.480 --> 07:59.480
Pandas uses column-at-a-time processing, which means that you process one column at a time.

07:59.480 --> 08:04.920
Inductive B uses kind of like a mix of the both, which is a technique developed by Peter,

08:04.920 --> 08:09.720
the vectorized processing where you process batches of a column at a time.

08:09.720 --> 08:13.760
So basically, the top-order time model from SQLite, it was optimized for a time where

08:13.760 --> 08:15.720
computers didn't have a lot of memory.

08:15.720 --> 08:20.760
There was low memory to be used because you only need to really keep one row in memory

08:20.760 --> 08:23.840
throughout your whole query plan.

08:23.840 --> 08:25.560
So the memory was expensive.

08:25.560 --> 08:26.880
That's what you could do.

08:26.880 --> 08:31.480
But this comes with a high CPU overhead per tuple because you constantly resetting your

08:31.480 --> 08:33.000
cache.

08:33.000 --> 08:38.120
You don't have any cache-conscious algorithm running that piece of data up to the production

08:38.120 --> 08:40.000
of the query results.

08:40.000 --> 08:43.840
If you go to the column at a time, which is what Pandas uses, this already brings better

08:43.840 --> 08:44.840
CPU utilization.

08:44.840 --> 08:47.320
It allows for SIMD.

08:47.320 --> 08:52.040
But it comes with the cost of materializing large intermediates in memory.

08:52.040 --> 08:56.040
It basically means that you need the whole column in memory at that point to process

08:56.040 --> 08:57.360
for that operator.

08:57.360 --> 09:00.240
And of course, the intermediates can be gigabytes each.

09:00.240 --> 09:03.080
So that's pretty problematic when the data sizes are large.

09:03.080 --> 09:06.800
And that's why you see, for example, that Pandas, if your data doesn't fit in memory,

09:06.800 --> 09:07.800
what does it happen?

09:07.800 --> 09:10.480
It crashes.

09:10.480 --> 09:15.680
And then if you go to the vectorized query processing, it's actually optimized for CPU

09:15.680 --> 09:17.000
cache locality.

09:17.000 --> 09:19.240
You can see the instructions, pipelining.

09:19.240 --> 09:25.000
And the whole idea is that your intermediates are actually going to fit here in a L1 cache.

09:25.000 --> 09:28.520
So basically, you're going to be paying this latency of 100 milliseconds to be accessing

09:28.520 --> 09:33.280
your data throughout your query plan instead of paying the latency of a memory, which is

09:33.280 --> 09:37.320
also the case of a column database, which is 100 milliseconds.

09:37.320 --> 09:38.880
It seems like a small difference.

09:38.880 --> 09:46.320
But when you're constantly executing this, this really becomes a bottleneck.

09:46.320 --> 09:49.240
And so end-to-end query optimization is, of course, something that we have in DuckDB.

09:49.240 --> 09:54.200
So we have stuff like expression rewriting, join ordering, sub-query flattening, filtering

09:54.200 --> 09:59.440
and projection pushdown, which is a bit more simple, but it's extremely important and brings

09:59.440 --> 10:02.380
a huge difference in the cost of a query.

10:02.380 --> 10:05.720
So here's an example of a projection pushdown.

10:05.720 --> 10:10.400
Let's say you have a table with five columns, A, B, C, D, E. And you want to run a query.

10:10.400 --> 10:11.400
That's pretty small.

10:11.400 --> 10:17.240
The query is like it selects minimum from column A, where there's a filtering column

10:17.240 --> 10:21.600
A saying the column A is bigger than zero, and you group by B. So the whole point of

10:21.600 --> 10:24.800
the query is that you're only using two columns off the table.

10:24.800 --> 10:29.400
So what the DuckDB optimizer will do is like, OK, in the scanner, I know I don't need all

10:29.400 --> 10:30.400
the columns.

10:30.400 --> 10:31.400
I just need NMB.

10:31.400 --> 10:34.320
And you just don't have to read the other ones.

10:34.320 --> 10:38.720
If you do the same one in pandas, for example, you can apply your filter, and then you have

10:38.720 --> 10:41.000
the filter to group by the aggregator.

10:41.000 --> 10:43.960
But at the time you're doing this filter, you're still filtering all the other columns

10:43.960 --> 10:45.880
you're not going to be using your query.

10:45.880 --> 10:49.360
Of course, you can manually make this optimization.

10:49.360 --> 10:54.160
But it's pretty nice that the database system can do that for you.

10:54.160 --> 10:58.400
Of course, DuckDB also has automatic parallelism and beyond memory execution.

10:58.400 --> 11:02.000
So DuckDB has parallel versions of most of its operators.

11:02.000 --> 11:08.280
I think all of our scanners, including with insertion, all your preservation of parallelized

11:08.280 --> 11:14.680
now, aggregations, joins, pandas, for example, only supports single-threaded execution.

11:14.680 --> 11:17.240
We all have pretty good laptops these days, right?

11:17.240 --> 11:21.640
So it's a shame if you cannot really take advantage of parallelism.

11:21.640 --> 11:26.400
And DuckDB, again, supports the execution of data that does not fit in memory.

11:26.400 --> 11:28.760
It's kind of the never-give-up, never-surrender approach.

11:28.760 --> 11:32.120
It's like, we're going to execute this query.

11:32.120 --> 11:35.960
We try to always have graceful degradation also, that it just doesn't suddenly crash

11:35.960 --> 11:36.960
the performance.

11:36.960 --> 11:41.720
And the whole goal is really to never crash and always execute the query.

11:41.720 --> 11:43.080
All right.

11:43.080 --> 11:46.960
So a little bit about DuckDB in the Python lens.

11:46.960 --> 11:48.960
Basically we have an API.

11:48.960 --> 11:51.960
It's a DB API, it's a.org compliant.

11:51.960 --> 11:57.080
So very similar to what SQLite has, for example, where you can create a connection and you

11:57.080 --> 11:58.400
can start executing queries.

11:58.400 --> 12:04.280
But we also wanted to have something similar to the DataFrame API that still could, people

12:04.280 --> 12:08.360
that come from pandas, for example, could still have something familiar to work on.

12:08.360 --> 12:11.440
So here in this example, you can also create a connection.

12:11.440 --> 12:14.240
You can create this relation, which kind of looks like a DataFrame.

12:14.240 --> 12:16.480
It's just pointing to a table.

12:16.480 --> 12:21.560
You can do a show to inspect what the table is inside, and you can apply, for example,

12:21.560 --> 12:22.680
these chaining operators, right?

12:22.680 --> 12:24.520
Like a filter, a projection.

12:24.520 --> 12:29.480
So in the end, this is all lazily executed.

12:29.480 --> 12:33.880
And this also allows you to take advantage of the optimizer of DuckDB, even if you do

12:33.880 --> 12:38.080
the chaining operations.

12:38.080 --> 12:42.920
Of course, I talked to you about memory transfer.

12:42.920 --> 12:48.520
So we were very careful as well into being very integrated with these very common libraries

12:48.520 --> 12:49.600
in Python.

12:49.600 --> 12:57.040
So with pandas and PyArrow, for example, what we actually do is that in the end, for pandas,

12:57.040 --> 13:02.160
the columns are usually not PyArrows, which turns out they are C, vectors, which turns

13:02.160 --> 13:04.280
out it's also kind of what we use.

13:04.280 --> 13:09.040
So with a little bit of makeup in the metadata, we can just directly read them.

13:09.040 --> 13:10.720
And they're all in the same process, right?

13:10.720 --> 13:16.520
So we have access to that piece of memory, which in the end means that you can actually

13:16.520 --> 13:21.720
access the data from pandas in DuckDB without paying any transfer costs, at least constant

13:21.720 --> 13:26.400
transfer costs just for doing the metadata makeup, let's say.

13:26.400 --> 13:28.120
And there's the same thing with PyArrow.

13:28.120 --> 13:30.280
We also have what we call zero copy.

13:30.280 --> 13:36.880
So we can read error objects and output error objects without any extra cost.

13:36.880 --> 13:42.960
With NumPy, we also support SQL, and in IBIS, we're actually the default backend from them,

13:42.960 --> 13:48.400
I think, since six months ago.

13:48.400 --> 13:49.640
A little bit of usage.

13:49.640 --> 13:55.680
So as you can see, this is our PyPy download count.

13:55.680 --> 13:58.440
The Python library is actually our most downloaded API.

13:58.440 --> 14:01.720
We have APIs for all sorts of languages.

14:01.720 --> 14:05.680
And you can see that in the last month, we had like $900,000.

14:05.680 --> 14:12.200
So there are a lot of people that are trying out and using DuckDB in their Python scripts.

14:12.200 --> 14:14.600
So now is the demo time.

14:14.600 --> 14:18.640
We got this.

14:18.640 --> 14:21.880
All right.

14:21.880 --> 14:23.080
This looks like you can see.

14:23.080 --> 14:29.280
So this is just installing DuckDB, PySpark, and getting our yellow trip data set.

14:29.280 --> 14:32.040
I executed this already.

14:32.040 --> 14:35.320
Just importing the stuff we're going to be using.

14:35.320 --> 14:41.040
And here is just getting a connection from DuckDB, creating a relation that's just, okay,

14:41.040 --> 14:45.080
we're going to, as a parquet file, DuckDB can read parquet files, and then you can just

14:45.080 --> 14:47.080
print to inspect what's out there.

14:47.080 --> 14:51.320
So if we run this, we can see, okay, these are the columns we have.

14:51.320 --> 14:56.200
We have vendor ID, we have pickup dates, time, passenger counts, you have the types of the

14:56.200 --> 14:57.200
columns.

14:57.200 --> 15:01.160
You can also have a little result preview to have an idea of what it looks like.

15:01.160 --> 15:05.040
So I think this data set has about like 20 columns maybe.

15:05.040 --> 15:12.200
And there's just information about the taxi rides in New York in 2016.

15:12.200 --> 15:15.400
And then you can also, for example, run a simple query here.

15:15.400 --> 15:19.080
I'm just doing like accounts to know how many tuples are there.

15:19.080 --> 15:22.680
And we have about 10 million tuples on this data set.

15:22.680 --> 15:23.680
All right.

15:23.680 --> 15:27.200
So this function here is just to do a little bit of benchmarking coming from academia.

15:27.200 --> 15:31.360
We do have to do something that's kind of fair, I guess.

15:31.360 --> 15:35.280
So I run just five times and take the median time of everything.

15:35.280 --> 15:37.160
And then this is actually where our demo starts.

15:37.160 --> 15:39.640
So we start off with a data frame.

15:39.640 --> 15:41.640
So Pandas can also read parquet files.

15:41.640 --> 15:46.400
And the whole thing about DuckDB, again, is that it's not here as a replacement for Pandas.

15:46.400 --> 15:50.560
This is not what I'm trying to sell, but it's something that can work together with Pandas.

15:50.560 --> 15:57.000
So the cool thing is that we can, again, read and output data frames without any extra cost.

15:57.000 --> 16:01.120
So let's say that in the query here, we're just getting the passenger counts.

16:01.120 --> 16:07.480
Then the average trip amounts of trips that had a short distance, right?

16:07.480 --> 16:10.680
And the group by passengers, by the number of passengers.

16:10.680 --> 16:18.400
So what we want to know is for short trips, does the amount of tip matters by the number

16:18.400 --> 16:22.000
of passengers in that rides?

16:22.000 --> 16:26.520
And what you can see here is that you can, again, read from the data frame.

16:26.520 --> 16:27.640
That's what we're doing.

16:27.640 --> 16:32.480
And we just have to use the data frame name in our SQL query.

16:32.480 --> 16:37.120
And if you call.df from the query results, you also output your data frame.

16:37.120 --> 16:42.360
And it's pretty cool because the data frames have these plot bars, they have plotting capabilities

16:42.360 --> 16:47.240
that DuckDB doesn't have, and you can get easily a very nice chart.

16:47.240 --> 16:52.960
So you see here, apparently, there's some dirty data because people are getting tips

16:52.960 --> 16:55.560
when they don't have anyone in their rides.

16:55.560 --> 16:56.760
Not sure what that is.

16:56.760 --> 17:01.640
But apparently, if you have more people 7 to 9, maybe the more expensive cars, you get

17:01.640 --> 17:03.480
a higher tip.

17:03.480 --> 17:05.720
And you can do the same thing in Pandas, of course, right?

17:05.720 --> 17:09.200
So in Pandas you don't have SQL, you're going to have to use their own language to do the

17:09.200 --> 17:13.520
group by, the average, and you can directly use the plots.

17:13.520 --> 17:17.600
And the whole point here is to show the different execution time.

17:17.600 --> 17:19.200
Like now we're waiting.

17:19.200 --> 17:22.160
Okay, so it took a second.

17:22.160 --> 17:25.920
And DuckDB took 0.2, so this is like a 5x, right?

17:25.920 --> 17:28.120
0.25, so like 4x.

17:28.120 --> 17:31.920
And you also have to consider that we're using not a very beefy machine, right?

17:31.920 --> 17:33.480
This is a Colab machine.

17:33.480 --> 17:37.720
If you had more cores, this difference would also be bigger.

17:37.720 --> 17:40.120
And then I added Spark for fun.

17:40.120 --> 17:46.000
So actually, Spark can also read data frames, but it crashes out of memory in my Colab machine,

17:46.000 --> 17:50.040
so I had to give up on that and read directly from Parquet files.

17:50.040 --> 17:54.880
But it does output it as a data frame.

17:54.880 --> 17:56.960
I think we're going to have to wait a little bit.

17:56.960 --> 18:00.840
But as me, it's best.

18:00.840 --> 18:06.720
So of course Spark is not designed for small data sets, but turns out there are a lot of

18:06.720 --> 18:08.720
use cases where you use small data sets.

18:08.720 --> 18:09.720
It's still going.

18:09.720 --> 18:14.120
It's warming up a little bit.

18:14.120 --> 18:15.120
It's good for the winter.

18:15.120 --> 18:18.320
It produces some energy, I think.

18:18.320 --> 18:20.320
All right.

18:20.320 --> 18:21.320
Okay.

18:21.320 --> 18:26.680
So it took two seconds, 2.2 seconds.

18:26.680 --> 18:31.240
The extra execution, and that's already like what?

18:31.240 --> 18:32.800
More than two times what Pandas was.

18:32.800 --> 18:33.800
So yeah.

18:33.800 --> 18:39.560
Anyway, for the demo, of course, I showed you something that's fairly simple.

18:39.560 --> 18:41.280
Can you do actually very complicated things?

18:41.280 --> 18:43.760
Maybe not very complicated, but more complicated.

18:43.760 --> 18:47.280
So here I'm not really going to go over the query, but the whole idea is that we can just

18:47.280 --> 18:50.840
like for example use DuckDB to run a linear regression, right?

18:50.840 --> 18:58.600
So can we predict, can we estimate the fare with the trip distance?

18:58.600 --> 19:04.460
And turns out you can just calculate the alpha and beta with not such a crazy query, right?

19:04.460 --> 19:08.680
And then you can again export it to Pandas, and you have a very nice figure there.

19:08.680 --> 19:14.520
So you can really combine these two to get the best out of both.

19:14.520 --> 19:17.280
All right.

19:17.280 --> 19:18.280
That was a demo.

19:18.280 --> 19:19.280
Summary.

19:19.280 --> 19:20.720
Oh, that's my last slide.

19:20.720 --> 19:21.720
Very good.

19:21.720 --> 19:24.640
So yeah, DuckDB is an embedded database system.

19:24.640 --> 19:26.040
Again it's completely open source.

19:26.040 --> 19:28.480
It is under the MIT license.

19:28.480 --> 19:31.360
Since it came from academia, this is something that we're always worried about.

19:31.360 --> 19:36.520
It's to also give it back to everyone because it was initially funded by taxpayers money

19:36.520 --> 19:37.960
so everyone can use it.

19:37.960 --> 19:40.640
100% of what we do is actually open source.

19:40.640 --> 19:43.880
There's nothing that's closed source.

19:43.880 --> 19:46.080
It's designed for analytical queries.

19:46.080 --> 19:51.360
So data analysis, data science has binding for many languages.

19:51.360 --> 19:53.680
So of course I'm at the Python dev room.

19:53.680 --> 19:55.160
I'm talking about Python.

19:55.160 --> 19:57.480
But we have R, Java.

19:57.480 --> 19:59.880
Turns out that Java is one of our most downloaded APIs.

19:59.880 --> 20:03.760
So I guess that's an interesting sign.

20:03.760 --> 20:06.120
JavaScript and a bunch of others.

20:06.120 --> 20:09.520
It has very tight integrations with the Python ecosystem.

20:09.520 --> 20:15.280
Again the whole idea is that you eliminate transfer costs, implement the database in

20:15.280 --> 20:16.280
relation to APIs.

20:16.280 --> 20:20.120
The relation to API again is this more data frame like.

20:20.120 --> 20:21.520
It has full SQL support.

20:21.520 --> 20:27.520
So anything you can imagine like window functions or whatnot, you can just express them using

20:27.520 --> 20:28.520
DuckDB.

20:28.520 --> 20:30.040
And that's it.

20:30.040 --> 20:35.520
Thank you very much for paying attention.

20:35.520 --> 20:52.680
Happy to answer questions.

20:52.680 --> 21:02.920
Thank you, we have five minutes for questions.

21:02.920 --> 21:13.520
You mentioned beyond memory execution and that it tries not to degrade as much.

21:13.520 --> 21:19.080
Can you shine a little bit more light on what happens under the hood and how much degradation

21:19.080 --> 21:20.960
happens?

21:20.960 --> 21:26.000
Of course I think there's only the ordering operator that actually does that.

21:26.000 --> 21:29.020
We have Lawrence that's doing his PTC.

21:29.020 --> 21:33.120
So there's a lot of operators that need to do research to be developed.

21:33.120 --> 21:36.040
That's more of a goal than something that actually happens now.

21:36.040 --> 21:40.400
But the whole goal is that you really don't have this sudden spike in the future.

21:40.400 --> 21:57.260
But there's research going on in the future that will be more to be shared for sure.

21:57.260 --> 22:03.600
Thank you very much for the talk and it's very exciting to see such a powerful tool.

22:03.600 --> 22:09.920
I'm working usually with data warehouses and I saw on the website that you do not recommend

22:09.920 --> 22:12.800
using this with data warehouses.

22:12.800 --> 22:15.320
I would like to know why.

22:15.320 --> 22:19.520
So of course there's no one solution for our problems.

22:19.520 --> 22:23.680
There are cases that data warehouses are very good fits.

22:23.680 --> 22:29.120
It turns out that for data science, for example, which is kind of what we preach the most,

22:29.120 --> 22:33.120
they're usually not good because then you fall back to the senior data outside your

22:33.120 --> 22:34.120
database system.

22:34.120 --> 22:38.000
You're not really going to be running your Python codes inside the system.

22:38.000 --> 22:40.960
You can do that for UDS, for example, but they are messy.

22:40.960 --> 22:42.000
They're a bit nasty.

22:42.000 --> 22:46.560
So you want really to have it embedded in your Python process so you completely eliminate

22:46.560 --> 22:48.200
the data transfer costs.

22:48.200 --> 22:51.640
Because usually what you do is like, okay, I have a table, 10 columns, I'm going over

22:51.640 --> 22:56.560
four columns but I'm really hitting, reading like huge chunks of it.

22:56.560 --> 23:04.920
So that's a bottleneck we try to eliminate.

23:04.920 --> 23:05.920
How do you handle updates?

23:05.920 --> 23:10.920
Yeah, although we are in the Lidical database system, we do do updates.

23:10.920 --> 23:14.320
So Mark, I don't know where he is, but he's there.

23:14.320 --> 23:18.120
He developed a MVCC algorithm for OLAF.

23:18.120 --> 23:23.120
So we have same asset transaction capabilities that you would expect from a transactional

23:23.120 --> 23:24.120
database.

23:24.120 --> 23:30.800
Of course, if you have a transaction workloads, you should still go for Postgres or SQLIs

23:30.800 --> 23:34.000
or a database that handles this type of transactions.

23:34.000 --> 23:39.280
But Mark has developed like a full-on algorithm to handle updates completely.

23:39.280 --> 23:40.280
Yeah?

23:40.280 --> 23:43.160
How do you compare to Vertica?

23:43.160 --> 23:44.640
How do you compare to Vertica?

23:44.640 --> 23:53.440
Good question, I think in terms of analytical queries, TPCA is probably similar performance.

23:53.440 --> 23:58.560
But then again, the whole point is that if you go again for the Python process, the data

23:58.560 --> 24:02.320
transfer costs will take most of the time there.

24:02.320 --> 24:11.320
And then it's really catered for this type of scenario, the embedded scenario.

24:11.320 --> 24:14.840
We have one minute left for one more question.

24:14.840 --> 24:19.800
Yeah, I actually have a repo somewhere for a bunch of examples as well.

24:19.800 --> 24:20.800
I'm very happy to share it.

24:20.800 --> 24:22.800
I don't know where I'll post it.

24:22.800 --> 24:25.800
Ah, there's only the first one thing, I guess.

24:25.800 --> 24:26.800
All right.

24:26.800 --> 24:27.800
All right, thank you a lot, Pedro.

24:27.800 --> 24:28.800
Thanks a lot.

24:28.800 --> 24:29.800
Thank you very much.

24:29.800 --> 24:46.800
Any other questions or comments?
