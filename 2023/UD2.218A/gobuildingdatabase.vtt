WEBVTT

00:00.000 --> 00:16.600
It's four o'clock, so let's look at our preview.

00:16.600 --> 00:18.400
Sorry, now next talk.

00:18.400 --> 00:24.280
I have been doing some matting in Go, but building a database I honestly have strong

00:24.280 --> 00:25.600
respect for.

00:25.600 --> 00:31.200
So next up is Etienne, who is going to tell us everything about crazy good journeys in

00:31.200 --> 00:32.200
Go.

00:32.200 --> 00:33.200
Thank you.

00:33.200 --> 00:34.200
Thank you.

00:34.200 --> 00:39.840
Yeah, welcome to our mad journey of building a database in Go.

00:39.840 --> 00:43.040
And yeah, it's pretty mad to build a database at all.

00:43.040 --> 00:51.600
It may be even worse or even a matter to build a database in Go when most are built in...

00:51.600 --> 00:52.600
Okay.

00:52.600 --> 00:53.600
Closer?

00:53.600 --> 00:55.600
Okay.

00:55.600 --> 00:57.600
Cool.

00:57.600 --> 00:59.400
Let me start over in case you didn't hear it.

00:59.400 --> 01:00.600
Hi, my name is Etienne.

01:00.600 --> 01:03.960
Welcome to our mad journey of building a vector database in Go.

01:03.960 --> 01:08.040
So building a database at all could already be pretty mad at doing it in Go when most

01:08.040 --> 01:10.440
are built in C or C++.

01:10.440 --> 01:13.480
Could be even even matter or even more exciting.

01:13.480 --> 01:18.160
And we definitely encountered a couple of unique problems that led us to create creative

01:18.160 --> 01:21.920
solutions and there's lots of shouts out in there and also a couple of wish lists that

01:21.920 --> 01:26.800
we just released Go window 20 and of course the occasional madness.

01:26.800 --> 01:29.760
So let's get one question out of the way right away.

01:29.760 --> 01:32.640
Why does the world even need yet another database?

01:32.640 --> 01:38.400
There's so many out there already but probably you've seen this thing called chat GPT because

01:38.400 --> 01:42.400
that was pretty much everywhere and it's kind of hard to hide from it.

01:42.400 --> 01:48.020
And chat GPT is a large language model and it's really good at putting text together

01:48.020 --> 01:54.880
that sounds really sophisticated and sounds nice and sometimes is completely wrong.

01:54.880 --> 01:59.040
And so in this case we're asking is it math to write a database and go I might disagree

01:59.040 --> 02:00.440
with that.

02:00.440 --> 02:04.400
But either way basically we're now in a situation where on the one hand we have these machine

02:04.400 --> 02:08.160
learning models that can do all the cool stuff and do this sort of interactively and on the

02:08.160 --> 02:12.480
fly and on the other side we have traditional databases and those traditional databases

02:12.480 --> 02:15.920
they have the facts because that's kind of what databases are for.

02:15.920 --> 02:19.600
So wouldn't it be cool if we could somehow combine those two.

02:19.600 --> 02:25.500
So for example on the query side if I ask Wikipedia why can airplanes fly then the kind

02:25.500 --> 02:29.800
of passage that I want that has the answer in it is titled the physics of flight.

02:29.800 --> 02:33.480
But that is difficult for a traditional search engine because if you look at keyword overlap

02:33.480 --> 02:35.960
there's almost none in there.

02:35.960 --> 02:40.400
But a vector search engine can use machine learning models basically that can tell you

02:40.400 --> 02:45.640
these two things are the same and searching through that at scale is a big problem.

02:45.640 --> 02:51.000
Then there's that sort of chat GPT side where you don't just want to search through it but

02:51.000 --> 02:55.400
maybe you also want to say like take those results summarize them and also translate

02:55.400 --> 02:56.400
them to German.

02:56.400 --> 03:00.640
So basically not just return exactly what's in the database but do something with it and

03:00.640 --> 03:03.520
basically generate more data from it.

03:03.520 --> 03:05.120
And that is exactly where VVA come in.

03:05.120 --> 03:09.720
So VVA does a vector search engine which basically helps us solve this kind of searching by meaning

03:09.720 --> 03:15.800
instead of keywords without sort of losing what we've done in 20 plus years of search

03:15.800 --> 03:17.440
engine research.

03:17.440 --> 03:22.520
And now most recently you can also interact with these models such as chat GPT GPT 3 and

03:22.520 --> 03:26.160
of course also the open source versions of it.

03:26.160 --> 03:28.700
So VVA is written in go.

03:28.700 --> 03:29.740
Is that a good idea?

03:29.740 --> 03:34.040
Is that a bad idea or have we just gone playing that?

03:34.040 --> 03:35.040
So we're not alone.

03:35.040 --> 03:36.040
That's good.

03:36.040 --> 03:38.960
So you probably probably recognize these things.

03:38.960 --> 03:43.760
They're all bigger brands at the moment than VVA but VVA is growing fast.

03:43.760 --> 03:48.320
And some of those vendors have really great blog posts where you see some of the like

03:48.320 --> 03:51.540
optimization topics and some of the crazy stuff that they have to do.

03:51.540 --> 03:55.720
So if you've contributed to one of those some of the things I'm going to say might sound

03:55.720 --> 03:56.720
familiar.

03:56.720 --> 03:59.320
If not then buckle up.

03:59.320 --> 04:00.920
It's going to get mad.

04:00.920 --> 04:05.760
So first stop on our mad journey memory allocation then that also brings us to our friend the

04:05.760 --> 04:07.080
garbage collector.

04:07.080 --> 04:13.000
So for any high performance go application sooner or later you're going to talk about

04:13.000 --> 04:14.000
memory allocations.

04:14.000 --> 04:17.280
And I definitely consider a database a high performance application at least I consider

04:17.280 --> 04:19.800
VVA a high performance application.

04:19.800 --> 04:24.400
And if you think of what databases do like in essence basically you have something on

04:24.400 --> 04:26.080
disk and you want to serve it to the user.

04:26.080 --> 04:30.040
That's like one of the most important user journeys in a database.

04:30.040 --> 04:32.040
And here this is represented by just a number.

04:32.040 --> 04:33.960
So it went for UN32.

04:33.960 --> 04:39.040
So that's just four bytes on disk and basically you can see these four bytes.

04:39.040 --> 04:43.960
If you parse them into go they would have the value of 16 in that UN32.

04:43.960 --> 04:48.200
And this is essentially something very much simplified that the database needs to do and

04:48.200 --> 04:50.960
it needs to do it over and over again.

04:50.960 --> 04:56.520
So the standard library gives us the encoding binary package and there we have this binary

04:56.520 --> 04:59.840
dot read method with which I think looks really cool.

04:59.840 --> 05:04.560
To me it looks like idiomatic go because it has the IODA reader interface like everyone's

05:04.560 --> 05:08.280
favorite interface and you can put all of that stuff in there.

05:08.280 --> 05:12.040
And if you run this code and there's no error then basically you get exactly what you want.

05:12.040 --> 05:17.080
You can turn those sort of four bytes that were somewhere on disk, turn them into our

05:17.080 --> 05:20.920
in memory representation of that UN32.

05:20.920 --> 05:27.520
So is this a good idea to do that exactly like well if you do it once or maybe twice

05:27.520 --> 05:29.180
could be a good idea.

05:29.180 --> 05:32.640
If you do it a billion times this is what happens.

05:32.640 --> 05:38.840
So for those of you who are new to CPU profiles in Go this is pretty bad.

05:38.840 --> 05:44.320
So first of all you see it in the center parsing those one billion numbers took 26 seconds

05:44.320 --> 05:49.240
and 26 seconds is not the kind of time that we ever have in the database.

05:49.240 --> 05:55.120
But worse than that if you look at that profile we have stuff like runtime, MLGc, runtime,

05:55.120 --> 05:57.500
memmove, runtime, and advise.

05:57.500 --> 06:01.840
So all these things they're related to memory allocations or to garbage collection.

06:01.840 --> 06:05.800
What they're not related to is parsing data which is what we wanted to do right.

06:05.800 --> 06:10.000
So how much time of that 20 seconds that we spend what we wanted to do.

06:10.000 --> 06:13.120
Don't know doesn't even show up in the profile.

06:13.120 --> 06:18.000
So and to understand why that is the case we need to quickly talk about the stack and

06:18.000 --> 06:19.600
the heap.

06:19.600 --> 06:24.160
So you can think of the stack as basically your function stack so you call one function

06:24.160 --> 06:28.240
that calls another function and then at some point basically you go back through the stack

06:28.240 --> 06:32.520
and this is very short-lived and this is cheap and fast to allocate and why is it cheap?

06:32.520 --> 06:37.280
Because you know exactly the runtime of your variables or the life cycle of your variables

06:37.280 --> 06:39.120
so you don't even need to involve the garbage collector.

06:39.120 --> 06:41.480
So no garbage collector cheap and fast.

06:41.480 --> 06:45.920
Then on the other side you have the heap and the heap is basically this sort of long-lived

06:45.920 --> 06:49.920
kind of memory and that's expensive and slow to allocate and why?

06:49.920 --> 06:51.440
And also to deallocate.

06:51.440 --> 06:54.060
Why because it involves the garbage collector.

06:54.060 --> 06:58.000
So if the stack is so much cheaper then we can just always allocate on the stack right.

06:58.000 --> 07:00.400
So warning this is not real go.

07:00.400 --> 07:02.100
Please do not do this.

07:02.100 --> 07:05.840
This is sort of a fictional example of allocating a buffer of size 8 and then we're going to

07:05.840 --> 07:09.600
say like yeah please put this on the stack and that is not how it works.

07:09.600 --> 07:12.800
And for most of you you probably say like this is pretty good that it's not that it

07:12.800 --> 07:15.000
works that way because why would you want to deal with that?

07:15.000 --> 07:19.160
But for me just trying to build a database and go sometimes like this something like

07:19.160 --> 07:21.440
this may be good or maybe not.

07:21.440 --> 07:22.920
So how does it work?

07:22.920 --> 07:25.620
Go does something that's called escape analysis.

07:25.620 --> 07:32.300
So if you compile your code with gcflax-m then go annotates your code basically and

07:32.300 --> 07:34.240
tells you sort of what's happening there.

07:34.240 --> 07:40.600
So here you can see in the second line that this num variable that we used was moved to

07:40.600 --> 07:45.680
the heap and then in the next point you see the bytes.reader which represents our iodo.reader

07:45.680 --> 07:46.880
escaped to the heap.

07:46.880 --> 07:49.880
So two times we see that something happened to the or went to the heap.

07:49.880 --> 07:54.360
We don't exactly know what happened yet but at least there's proof that we have this kind

07:54.360 --> 07:56.960
of allocation problem.

07:56.960 --> 07:58.360
So what can we do?

07:58.360 --> 07:59.880
Well we can simplify it a bit.

07:59.880 --> 08:05.220
Turns out that the binary or encoding binary package also has another method that looks

08:05.220 --> 08:11.000
like this which is just called view in 32 on the little nd package and it kind of does

08:11.000 --> 08:12.000
the same thing.

08:12.000 --> 08:15.240
You just put in the buffer on the one side so no reader this time you just put in the

08:15.240 --> 08:19.760
raw buffer basically with the position offset and on the other side you get the number out.

08:19.760 --> 08:24.000
And the crazy thing is this one line needs no memory allocations.

08:24.000 --> 08:30.320
So if we do that again our one billion numbers that took 26 seconds before now take 600 milliseconds.

08:30.320 --> 08:35.280
Now we're starting to get into a range where like this is acceptable for a data basis.

08:35.280 --> 08:39.880
And more importantly what we see on that profile the profile is so much simpler right now.

08:39.880 --> 08:45.000
There's basically just this one function there and that is that is yeah it's what we wanted

08:45.000 --> 08:46.000
to do.

08:46.000 --> 08:49.160
So admittedly we're not doing much other than parsing the data at the moment but at least

08:49.160 --> 08:54.240
we got sort of rid of all the noise and you can see the speed up.

08:54.240 --> 08:59.360
Okay so quickly to recap if we say a database is nothing but reading data and sort of parsing

08:59.360 --> 09:04.960
it to serve it to the user then we do that over and over again and we need to take care

09:04.960 --> 09:06.160
of memory allocations.

09:06.160 --> 09:10.520
And the fix in this case was super simple we changed two lines of code and reduced it

09:10.520 --> 09:13.600
from 26 seconds to 600 milliseconds.

09:13.600 --> 09:17.280
But why we had to do that wasn't very intuitive like that it wasn't very obvious.

09:17.280 --> 09:22.000
In fact I haven't even told you yet why this binary dot little nd dot read why that escaped

09:22.000 --> 09:23.000
to the heap.

09:23.000 --> 09:26.560
And in this case it's because we passed in pointer and we passed in an interface and

09:26.560 --> 09:30.600
that's kind of a hint basically that something might escape to the heap.

09:30.600 --> 09:36.160
So what I would wish is yes this is not a topic that you need every day you write go

09:36.160 --> 09:42.160
but maybe if you do need this would be cool if there was better education.

09:42.160 --> 09:45.680
Okay so second step delayed decoding.

09:45.680 --> 09:51.480
So this is kind of the idea that we wouldn't want to do the same work twice and we're sticking

09:51.480 --> 09:57.040
with our example of serving data from disk but now while the number example was a bit

09:57.040 --> 10:00.560
too too simple so let's make it make it slightly more complex.

10:00.560 --> 10:07.800
We have this nested array here basically a sort of slice off slice of view in 64 and

10:07.800 --> 10:12.080
that's representative now for a more complex object on your database.

10:12.080 --> 10:14.960
Of course in reality you'd have like string props and other kind of things but just sort

10:14.960 --> 10:18.560
of to show that there's more going on than a single number.

10:18.560 --> 10:22.600
And let's say we have 80 million of them so 10 million of the outer slice and then eight

10:22.600 --> 10:25.920
elements in each inner slice and our task is just to sum those up.

10:25.920 --> 10:28.800
So these are 80 million numbers and we want to know what is the sum of them.

10:28.800 --> 10:36.480
So that is actually kind of a realistic database task for an OLAP kind of database.

10:36.480 --> 10:41.640
We need to somehow represent that data on disk and we're looking at two ways to do this.

10:41.640 --> 10:45.840
The first one is JSON representation and then the second one would be sort of binary encoding

10:45.840 --> 10:48.200
and then there'll be more.

10:48.200 --> 10:51.160
So JSON is basically just here for completeness.

10:51.160 --> 10:53.000
We can basically rule it out immediately.

10:53.000 --> 10:58.240
When you're building a database you're probably not using JSON to store stuff on disk unless

10:58.240 --> 11:00.440
it's sort of a JSON database.

11:00.440 --> 11:05.160
Why because it's space inefficient so if you want to represent those numbers on disk like

11:05.160 --> 11:09.560
space and JSON basically uses strings for it and then you have all these control characters

11:09.560 --> 11:13.440
like your curly braces and your quotes and your columns and everything that takes up

11:13.440 --> 11:14.440
space.

11:14.440 --> 11:18.680
So in our fictional example that would take up 1.6 gigabyte and you'll see soon that we

11:18.680 --> 11:20.140
can do that more efficient.

11:20.140 --> 11:25.120
But also it's slow and part of why it's slow is again because we have these memory allocations

11:25.120 --> 11:27.640
but also the whole parsing just takes time.

11:27.640 --> 11:34.000
So in our example this took 14 seconds to sum up those 80 million numbers and yeah as

11:34.000 --> 11:39.840
I said before you just don't have double digit seconds in a database.

11:39.840 --> 11:44.820
So we can do something that's a bit smarter which is called length encoding.

11:44.820 --> 11:50.680
So we're encoding this basically as binary and we're spending one in this case one byte

11:50.680 --> 11:54.400
so that's basically a U and eight and we're using that as a length indicator.

11:54.400 --> 11:57.440
So basically that tells us that when we're reading this from disk that just tells us

11:57.440 --> 11:58.440
what's coming up.

11:58.440 --> 12:02.120
So in this case it says we have eight elements coming up and then we know that our elements

12:02.120 --> 12:05.520
in this example is U and 32 so that's four bytes each.

12:05.520 --> 12:09.820
So basically the next 32 bytes that we're reading are going to be our eight in arrays

12:09.820 --> 12:10.820
and then we just continue.

12:10.820 --> 12:17.280
Then we basically read the next length indicator and this way we can encode the stuff in one

12:17.280 --> 12:19.040
continuous thing.

12:19.040 --> 12:23.440
Then of course we have to decode it somehow and we can do that because we've learned from

12:23.440 --> 12:27.760
our previous example right so we're not going to use binary.littleendian.read but we're

12:27.760 --> 12:30.560
doing this in an allocation free way.

12:30.560 --> 12:32.880
You can see it in the length line basically.

12:32.880 --> 12:38.520
And yeah our goal is to take that data and put it into our nested sort of go slice of

12:38.520 --> 12:44.720
slice of U in 64 and the code here basically you see we're reading the length and then

12:44.720 --> 12:48.160
we're increasing our offsets and we know where to read from and then we're basically repeating

12:48.160 --> 12:54.760
this for the inner slice which is just hinted at here by the decode inner function.

12:54.760 --> 12:57.000
So what happens when we do this?

12:57.000 --> 13:03.200
First of all the good news 660 megabytes that's way less than our 1.6 gigabyte before so basically

13:03.200 --> 13:09.600
just by using a more space efficient way to represent data we've done exactly that.

13:09.600 --> 13:11.320
We've reduced our size.

13:11.320 --> 13:18.160
Also it's much much faster so we were at 14 seconds before and now it's down to 260 milliseconds

13:18.160 --> 13:23.400
but this is our mad journey of building a database so we're not done here yet because

13:23.400 --> 13:25.400
there's some hidden madness.

13:25.400 --> 13:31.040
And the hidden madness is that we actually spend 250 milliseconds decoding while we spend

13:31.040 --> 13:33.840
10 milliseconds summing up those 80 million numbers.

13:33.840 --> 13:37.240
So again we're kind of in that situation where we're doing something that we never really

13:37.240 --> 13:41.960
set out to do like we wanted to do something else but we're spending our time on yeah doing

13:41.960 --> 13:43.840
something that we didn't want to do.

13:43.840 --> 13:46.280
So where does that come from?

13:46.280 --> 13:51.240
And the first problem is basically that what we did what we set out to do was fought from

13:51.240 --> 13:55.480
the get go because we said we want to decode so we're basically thinking in the same way

13:55.480 --> 13:58.280
that we're thinking as we were with Jason.

13:58.280 --> 14:02.240
We said that we want to decode this entire thing into this go data structure but that

14:02.240 --> 14:06.560
means that you see we need to allocate this massive slice again and that also means that

14:06.560 --> 14:10.160
we need to in each inner slice we also need to allocate again so we're basically allocating

14:10.160 --> 14:14.360
and allocating over and over again where our task is not to allocate our task was to sum

14:14.360 --> 14:16.160
up numbers.

14:16.160 --> 14:21.440
So we can actually just simplify this a bit and we can basically just not decode it like

14:21.440 --> 14:25.520
while we're looping over that data anyway instead of storing it in an array we can just

14:25.520 --> 14:29.840
do with it what we plan to do and in this case this would be summing up the data.

14:29.840 --> 14:36.280
So basically getting rid of that decoding step helps us to make this way faster.

14:36.280 --> 14:41.280
So now we're at 46 milliseconds of course our footprint of the data on disk hasn't changed

14:41.280 --> 14:45.160
because it's the same data that we're reading which is reading it in a slightly more efficient

14:45.160 --> 14:49.360
way but yeah we don't have to allocate slices and also because we don't have these nested

14:49.360 --> 14:53.140
slices we don't have slices that basically have pointers to other slices so we have better

14:53.140 --> 15:00.560
memory locality and now we're at 46 milliseconds and that is cool so 46 milliseconds is basically

15:00.560 --> 15:05.320
the time frame that can be acceptable for a database.

15:05.320 --> 15:10.200
So quickly in recap we immediately ruled out JSON because it just wasn't space efficient

15:10.200 --> 15:13.820
and we knew that we needed something more space efficient and also way faster binary

15:13.820 --> 15:19.360
encoding already made it much faster which is great but if we decode it upfront then

15:19.360 --> 15:24.040
yeah we still lost a lot of time and it can be worth it in these kind of high performance

15:24.040 --> 15:28.720
situations if you either sort of delay the decoding as late as possible until you really

15:28.720 --> 15:33.080
need it or just don't do it at all or do it in sort of small parts where we need it.

15:33.080 --> 15:38.560
No wish list here but an honorary mention so Go 1.20 they've actually removed it from

15:38.560 --> 15:43.440
the from the release notes because it's so experimental but Go 1.20 has support for memory

15:43.440 --> 15:48.400
arenas the idea for memory arenas is basically that you can bypass the garbage collector

15:48.400 --> 15:52.200
and sort of manually free that data so if you have something that you know has the same

15:52.200 --> 15:56.440
sort of life cycle then you can say okay put it in the arena and basically in the end free

15:56.440 --> 16:01.440
the entire arena which would sort of bypass the garbage collector so that could also be

16:01.440 --> 16:05.400
a solution in this case if that ever makes it like right now it's super experimental

16:05.400 --> 16:09.840
and it basically tells you we might just remove it so don't use it.

16:09.840 --> 16:15.280
Point stop is something that when I first heard it almost sounded like too good to be

16:15.280 --> 16:17.680
true so something called SIMD.

16:17.680 --> 16:22.120
I'll get to what that is in a second but first question to the audience who here remembers

16:22.120 --> 16:28.680
this thing raise your hands okay cool so you're just as old as I am so this is the Intel

16:28.680 --> 16:36.000
Pentium 2 processor and this came out in late 90s I think 1997 and was sold for a couple

16:36.000 --> 16:40.760
of years and back then I did not build databases definitely not in Go because that also didn't

16:40.760 --> 16:45.260
exist yet but what I would do was sort of try to play 3D video games and I would urge

16:45.260 --> 16:49.720
my parents to get one of those new computers with an Intel Pentium 2 processor and one

16:49.720 --> 16:53.160
of the arguments that I could have used in that discussion was hey it comes with MMX

16:53.160 --> 16:59.440
technology and of course I had no idea what that is and it probably took me 10 or so more

16:59.440 --> 17:04.240
years to find out what MMX is but it's the first in a long list of SIMD instructions

17:04.240 --> 17:07.720
I haven't explained what SIMD is yet but I will in a second.

17:07.720 --> 17:12.320
Some of those especially the one in the top line they're not really used anymore these

17:12.320 --> 17:17.920
days but the bottom line like AVX2 and AVX512 you may have heard them in fact for many open

17:17.920 --> 17:22.640
source projects sometimes you sort of slap that label and read me like yeah yeah has

17:22.640 --> 17:27.440
AVX2 optimizations and that kind of signals you yeah we care about speed because it's

17:27.440 --> 17:32.020
like low level optimized and VVA does the exact same thing by the way.

17:32.020 --> 17:37.000
So to understand how we could make use of that I quickly need to talk about vector embeddings

17:37.000 --> 17:42.560
because I said before that VVA doesn't search through data by keywords but rather through

17:42.560 --> 17:47.600
its meaning and it uses vector embeddings as a tool for that so this is basically just

17:47.600 --> 17:52.120
a long list of numbers in this case floats and then a machine learning model comes in

17:52.120 --> 17:56.320
and basically it says do something with my input and then you get this vector out and

17:56.320 --> 17:59.880
if you do this on all the objects then you can compare your vectors so you basically

17:59.880 --> 18:04.480
can do a vector similarity comparison and that tells you if something is close to one

18:04.480 --> 18:08.840
another or not so for example the query and the object that we have before.

18:08.840 --> 18:14.800
So without any SIMD we can use something called the dot product the dot product is a simple

18:14.800 --> 18:20.440
calculation where basically you use you multiply each element of the first vector with the

18:20.440 --> 18:25.220
same corresponding element of the second vector and then you just sum up all of those elements

18:25.220 --> 18:31.000
and we can think of this like multiplication and summing as two instructions so if we look

18:31.000 --> 18:35.520
out first shout out here to the compiler explorer which is a super cool tool to see like what

18:35.520 --> 18:40.600
your go code compiles to we can see that this indeed turns into two instructions so this

18:40.600 --> 18:44.360
is a bit of a lie because there's more stuff going on because it's in a loop etc but let's

18:44.360 --> 18:50.600
just pretend that indeed we have these two instructions to multiply it and to add it.

18:50.600 --> 18:56.040
So how could we possibly optimize this even further if we're already at such a low level

18:56.040 --> 19:01.720
well we can because this is our mad journey so all we have to do is introduce some madness

19:01.720 --> 19:08.620
and what we're doing now is a practice that's called unrolling so the idea here is that

19:08.620 --> 19:13.120
instead of looping over one element at a time we're now looping over eight elements at a

19:13.120 --> 19:17.360
time but we've gained nothing like this is we're still doing the same kind of work like

19:17.360 --> 19:21.440
we're doing 16 instructions now in a single loop and we're just doing fewer iterations

19:21.440 --> 19:27.160
so by this point nothing gained but why would we do that well here comes the part where

19:27.160 --> 19:32.720
I thought it was too good to be true what if we could do those 16 operations for the

19:32.720 --> 19:38.800
cost of just two instructions sounds crazy right well no because Cindy I'm finally revealing

19:38.800 --> 19:44.000
what the acronym stands for it stands for single instructions multiple data and that

19:44.000 --> 19:48.000
is exactly what we're doing here so we want to do the same thing over and over again which

19:48.000 --> 19:54.040
is multiplication and then additions and this is exactly what these SIMD instructions provide

19:54.040 --> 19:58.520
so in this case we can multiply eight floats with other eight floats and then we can add

19:58.520 --> 20:06.320
them up so all this perfect here maybe not because there's a catch of course it's our

20:06.320 --> 20:13.960
mad journey how do you tell go to use these AVX two instructions you don't you write assembly

20:13.960 --> 20:19.000
code because go has no way to do that directly the good part is that assembly code integrates

20:19.000 --> 20:23.960
really nicely into go and in the in the standard library it's used over and over again so it's

20:23.960 --> 20:29.480
kind of a standard practice and there is tooling here so shout out to avo really cool too that

20:29.480 --> 20:35.040
helps you basically you're still writing assembly with with avo but you're writing it in go

20:35.040 --> 20:38.080
and then it generates the assembly so you still need to know what you're doing but it's

20:38.080 --> 20:47.400
like it protects you a bit so it definitely helped us a lot so SIMD recap using avx instructions

20:47.400 --> 20:52.960
or other SIMD instructions you can basically trick your CPU into doing more work for free

20:52.960 --> 20:58.000
but you need to sort of also trick go to use assembly and with this tooling such as avo

20:58.000 --> 21:02.520
it can be better but it would be even nicer if the language had some sort of support for

21:02.520 --> 21:06.080
it and you made my saying now okay this is this mad guy on stage that wants to build

21:06.080 --> 21:10.480
a database but no one else does needs that but we have this issue here that was opened

21:10.480 --> 21:14.960
recently and unfortunately also most recently because no consensus could be reached but

21:14.960 --> 21:19.800
it comes up back and back basically that go users are saying like hey we want something

21:19.800 --> 21:24.720
in the language such as intrinsic so intrinsics are basically the idea of having high level

21:24.720 --> 21:30.920
language instructions to do these these sort of avx or SIMD instructions and C or C++ has

21:30.920 --> 21:36.120
that for example one way to do that and maybe you're wondering like okay if you have such

21:36.120 --> 21:40.560
a performance hot path like why don't you just write that in C and you see go or write

21:40.560 --> 21:45.520
it in Rust or something like that sounds good in theory but the problem is that the call

21:45.520 --> 21:51.320
overhead to call C or C++ is so high that you actually have to outsource quite a bit

21:51.320 --> 21:56.240
of your code for that to to pay off again so if you do that you basically end up writing

21:56.240 --> 22:00.760
more and more and more in that language and then you're not writing go anymore so fortunately

22:00.760 --> 22:07.960
that's not or it can be in some ways but it's not always a great idea so demo time this

22:07.960 --> 22:12.480
was going to be a live demo and maybe it still is because I prepared this running nicely

22:12.480 --> 22:16.680
in a docker container and then my docker network just broke everything and it didn't work but

22:16.680 --> 22:21.320
I just rebuild it without docker and I think it might work if not I have screenshots basically

22:21.320 --> 22:28.560
that do a backup so example query here I'm a big wine nerd so what I did is I put wine

22:28.560 --> 22:33.440
reviews into vv8 and I want to search them now and one way to do it to show you basically

22:33.440 --> 22:38.080
that the keyword that you don't need a keyword match but can search by meaning is for example

22:38.080 --> 22:47.040
if I go for an affordable Italian wine let's see if the internet connection works it does

22:47.040 --> 22:52.960
so what we got back is basically this this a wine review that I wrote about a Barolo

22:52.960 --> 22:57.880
that I recently drank it and you can see it doesn't say Italy anywhere it doesn't say

22:57.880 --> 23:02.840
affordable what it says like without breaking the bank so this is a vector search that basically

23:02.840 --> 23:08.720
happened in the in the background we can take this one step further by using the generative

23:08.720 --> 23:15.040
side so this is basically the chat GPT part we can now ask for database based on the review

23:15.040 --> 23:19.080
which is what I wrote when is this wine going to be ready to drink so let's see you saw

23:19.080 --> 23:22.120
before that was the failed career when the internet didn't work now now it's actually

23:22.120 --> 23:27.000
working so that's nice and here in this case you can see that so this is using open AI

23:27.000 --> 23:31.560
but you can plug in other tools can plug in open source versions of it this is using open

23:31.560 --> 23:35.880
AI because that's nice to be hosted at a service I don't have to run the machine learning model

23:35.880 --> 23:39.680
on my laptop then you can see it tells you the wine is not ready to drink yet we will

23:39.680 --> 23:43.120
need at least five more years which is sort of a good summary of this and then you can

23:43.120 --> 23:47.240
see another one is ready to drink right now it's in the perfect drinking window so for

23:47.240 --> 23:52.880
the final demo let's combine those two let's do a semantic search to identify something

23:52.880 --> 23:59.000
and then do an AI generation basically so in this case we're saying find me an aged

23:59.000 --> 24:04.760
classic Riesling best best wine in the world Riesling and based on the review would you

24:04.760 --> 24:08.880
consider this wine to be a fruit bomb so let's have sort of an opinion from the machine learning

24:08.880 --> 24:16.440
model in it and here we got one of my favorite wines and the model says no I would not consider

24:16.440 --> 24:20.560
this a fruit bomb while it does have some fruity notes it is balanced by the minerality

24:20.560 --> 24:25.240
and acidity which keeps it from being overly sweet or fruity which is if you read the text

24:25.240 --> 24:30.480
like this is nowhere in there so this is kind of cool that the model was able to do this

24:30.480 --> 24:35.600
okay so let's go back now is the demo time by the way have a GitHub repo with like this

24:35.600 --> 24:43.600
example so you can run it yourself and yeah try it out yourself so this was our mad journey

24:43.600 --> 24:49.960
and are we mad at go are we mad to do this well I would pretty much say no because yes

24:49.960 --> 24:54.920
there were a couple of parts we have to give get really creative and have to do some some

24:54.920 --> 25:00.320
yeah rather unique stuff but that was also basically like the highlight reload building

25:00.320 --> 25:04.560
database and all the other parts like it didn't even show the parts that went great like concurrency

25:04.560 --> 25:09.560
handling and the powerful standard library and of course all of you basically representing

25:09.560 --> 25:15.160
the gopher community which is super helpful and yeah this was my way to basically give

25:15.160 --> 25:19.520
back to all of you so if you ever want to build a database or run into other kind of

25:19.520 --> 25:21.680
high performance problems then maybe some of those
