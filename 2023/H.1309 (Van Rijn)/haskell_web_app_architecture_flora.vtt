WEBVTT

00:00.000 --> 00:11.500
Hekate is a Haskell from the trenches with an interest in resilient systems and documentation.

00:11.500 --> 00:16.300
When not at work or in the Haskell community, they are a trombonist in various orchestras

00:16.300 --> 00:17.780
and brass bands.

00:17.780 --> 00:25.340
Hekate uses they and them pronouns and farcical amounts of caffeine to retain human form.

00:25.340 --> 00:30.740
They are going to present to us a presentation entitled Web Application Architecture in

00:30.740 --> 00:33.320
Haskell with Flora.pm.

00:33.320 --> 00:34.780
Thank you Hekate.

00:34.780 --> 00:35.780
Thank you very much.

00:35.780 --> 00:37.780
Can everyone hear me?

00:37.780 --> 00:38.780
Perfect.

00:38.780 --> 00:45.440
So, welcome to this talk entitled Web Application Architecture in Haskell, When the Domain Drives

00:45.440 --> 00:48.580
the Types and Types Drive the Program.

00:48.580 --> 00:53.940
So this talk is intended for a missed audience of software engineers who are acquainted with

00:53.940 --> 01:00.420
the practice of domain-driven design and Haskell programmers who are interested in crafting

01:00.420 --> 01:01.980
better systems.

01:01.980 --> 01:07.020
The goal is to create a bridge between the practitioners of domain-driven design and

01:07.020 --> 01:10.620
the users of Haskell.

01:10.620 --> 01:16.540
So my name is ThÃ©ophile Chautry, aka Hekate, to the community.

01:16.540 --> 01:19.020
I am a backend engineer at Scryve.

01:19.020 --> 01:26.060
We are a three-ish company and we have an e-signature platform for contracts and various

01:26.060 --> 01:32.420
documents as well as a digital identity hub where we aggregate various national identity

01:32.420 --> 01:38.260
providers like Itzme in Belgium, for example, or BankID, France Connect for the French people

01:38.260 --> 01:39.660
here.

01:39.660 --> 01:46.820
I am also privileged to be a board member of the Haskell Foundation.

01:46.820 --> 01:53.100
This is one of my numerous involvement in the community.

01:53.100 --> 01:58.940
So Haskell, the pure functional programming language, so these are words and words don't

01:58.940 --> 02:02.340
mean nothing until we can practically apply them.

02:02.340 --> 02:07.460
These words bring us concrete features like native supers for recursion, for example,

02:07.460 --> 02:12.700
without blowing your stack type system that doesn't hate you, high order functions that

02:12.700 --> 02:17.140
almost every language has today and many other features.

02:17.140 --> 02:22.500
There are two features especially that I want to talk about and it is going to be its ability

02:22.500 --> 02:28.860
of the language to adequately represent business domains and the ability to track side effects

02:28.860 --> 02:31.940
in a semantic way.

02:31.940 --> 02:37.580
For example, the adequate representation of business domain, we can use algebraic data

02:37.580 --> 02:45.060
types to allow us to model with more precision the real world and its nuances.

02:45.060 --> 02:50.500
Encoding rules by construction at the type level is something that we can easily do.

02:50.500 --> 02:56.020
In this example, for example, the members of the excess data type visitor and admin

02:56.020 --> 03:02.980
are promoted to the type level for this user type, which means that, for example, there

03:02.980 --> 03:10.500
can only be two values in the privileges parameter of this user type.

03:10.500 --> 03:18.380
And also it means that I am going to get rejected if I pass a visitor user to this function

03:18.380 --> 03:20.180
called view back office.

03:20.180 --> 03:23.780
But I'm going to get rejected at the compilation.

03:23.780 --> 03:27.060
This is something that is trivial to implement at the value level.

03:27.060 --> 03:32.260
I could have a check on a member of the object, of a property of the object that would be

03:32.260 --> 03:34.980
ease admin with a Boolean.

03:34.980 --> 03:40.180
But I have to write the check manually and possibly for every function that needs to

03:40.180 --> 03:41.380
have such a check.

03:41.380 --> 03:47.180
If I encode this property, this immutable property at the type system, at the level

03:47.180 --> 03:56.020
of the types, the compiler then is tasked with checking if I'm doing my job correctly.

03:56.020 --> 03:58.460
Checking side effects semantically.

03:58.460 --> 04:02.980
If we take the previous example, we can see that view back office has a result type of

04:02.980 --> 04:09.740
IOHTML, which more or less means I am doing observable side effects and I return you a

04:09.740 --> 04:11.820
value in HTML.

04:11.820 --> 04:18.020
But that's, you know, it doesn't adequately represent the reality of the function.

04:18.020 --> 04:25.700
And for example, here we have a way of tracking side effects that are being executed in a

04:25.700 --> 04:30.340
more human readable form and we can declare them at the type level.

04:30.340 --> 04:38.620
So here we have a function that's, so the signature is we get an int, which is an identifier,

04:38.620 --> 04:42.740
and we return a function that returns text.

04:42.740 --> 04:49.580
And this F, monad, this F return type has also a list of effects.

04:49.580 --> 04:52.580
So what are these effects?

04:52.580 --> 04:58.700
So, I declare that I'm going to perform database access and possibly mutation.

04:58.700 --> 05:03.460
I am going to access a ready server for the caching and I'm also going to ship my logs

05:03.460 --> 05:11.940
to an external platform or at least to perform the loading effects, whatever that means.

05:11.940 --> 05:18.860
And then we can have more useful breakdown of which functions have which effects.

05:18.860 --> 05:23.940
So get entity name is composed of first we get the entity from the cache if we have it

05:23.940 --> 05:29.500
and then if we don't we can switch to the database to perform a perhaps more costly

05:29.500 --> 05:33.780
access and then finally we log the effects.

05:33.780 --> 05:41.260
So logging, DB, Redis, all of them are then unified in this list of effects at the type

05:41.260 --> 05:44.220
level.

05:44.220 --> 05:52.460
So when a type system is both strong and expressive, we get a lot closer to TLS refactoring.

05:52.460 --> 05:53.620
And what does that mean?

05:53.620 --> 05:59.580
Because many languages today claim to provide such a thing.

05:59.580 --> 06:05.460
So TLS refactoring, you more or less get vibe checked by the compiler, which is pretty cool

06:05.460 --> 06:09.820
because then the compiler keeps you in check regarding the changes in your program and

06:09.820 --> 06:12.420
how they affect the program's behavior.

06:12.420 --> 06:16.580
We also took up the terms with the fact that your worst enemy is now yourself.

06:16.580 --> 06:23.940
You can't blame errors in production because undefined is not a function.

06:23.940 --> 06:31.020
So there are limits of course to correct by construction and I think it's especially important

06:31.020 --> 06:33.940
to be intellectually honest with that.

06:33.940 --> 06:35.460
Haskell is not a prover.

06:35.460 --> 06:40.860
You can prove lemmas and theorems with it.

06:40.860 --> 06:45.420
So you have to write tests and tests come in many shapes and forms.

06:45.420 --> 06:49.420
You write tests for your integrations, your properties and end-to-end tests.

06:49.420 --> 06:54.020
Tests are not optional, unlike maybe.

06:54.020 --> 06:57.260
So functional application architecture.

06:57.260 --> 07:02.340
So this that I showed you gives us a tool to focus now on the topic of functional application

07:02.340 --> 07:07.860
architecture and this time we arrive in the lens of domain-driven design.

07:07.860 --> 07:12.340
There will be a brief overview of the terminology and the techniques that were created in the

07:12.340 --> 07:15.900
discipline and how they apply to us.

07:15.900 --> 07:20.300
So we have, without surprise, the concept of a bounded context.

07:20.300 --> 07:21.300
It's a context.

07:21.300 --> 07:23.500
There are workflow in there.

07:23.500 --> 07:25.020
So what is it really?

07:25.020 --> 07:28.080
It's an autonomous subsystem.

07:28.080 --> 07:33.220
So it's responsible for one or multiple workflows and it has well-defined boundaries, which

07:33.220 --> 07:34.640
is extremely important.

07:34.640 --> 07:41.340
So we have to formalize, or at least be very explicit, how we talk to it, its inputs and

07:41.340 --> 07:42.940
its outputs.

07:42.940 --> 07:48.300
If we take the example of florida.pm, which is a community website, an alternative package

07:48.300 --> 07:52.820
index for the Haskell community, the schema is fairly simple.

07:52.820 --> 07:57.180
We have the web component that talks to the core component, which is tasked then to interface

07:57.180 --> 07:58.420
with the database.

07:58.420 --> 08:04.180
And we have a jobs worker for the jobs queue that also talks to the core components and

08:04.180 --> 08:05.300
to the database.

08:05.300 --> 08:11.400
But we know that it's not going to talk with the web components.

08:11.400 --> 08:18.260
So this is the kind of setting boundaries because they are in a healthy relationship.

08:18.260 --> 08:22.560
And we know that we will not talk to each other.

08:22.560 --> 08:26.980
One more step forward, fearless refactoring.

08:26.980 --> 08:34.540
Now something that the Java and C-sharp world gave us are the concepts of data transfer

08:34.540 --> 08:39.260
objects, data access objects, and business objects.

08:39.260 --> 08:40.940
Sometimes they are the same thing.

08:40.940 --> 08:47.020
Sometimes you are lucky that the JSON payload you receive is the same object upon which

08:47.020 --> 08:52.420
you will perform your business computations and which you will store in your database.

08:52.420 --> 08:53.940
But sometimes they are not.

08:53.940 --> 08:57.700
And really it is pure luck that sometimes these types align.

08:57.700 --> 09:03.820
An example of how this bit me when I was young and hopeful, obviously, without much practice.

09:03.820 --> 09:08.700
During meeting with other people, we were trying to define a JSON-based format for data

09:08.700 --> 09:10.940
exchange between several systems.

09:10.940 --> 09:14.980
And we had Elixir systems, PHP, Ruby, Python.

09:14.980 --> 09:23.060
And these all give you several slight differences in how you can have data types encoded in

09:23.060 --> 09:24.620
these languages.

09:24.620 --> 09:29.780
And for example, if you are dealing with a Rubyist or PHP users, they will try and push

09:29.780 --> 09:33.020
a heterogeneous list in the data format.

09:33.020 --> 09:36.900
So you can have a string and natively in Haskell we don't do that.

09:36.900 --> 09:41.180
So we would have to create some abstraction on top of it.

09:41.180 --> 09:48.060
And I was realizing that I was constraining myself with the capacity of each language

09:48.060 --> 09:52.420
to create this data format based on JSON.

09:52.420 --> 09:53.820
But you don't have to do it.

09:53.820 --> 10:01.620
You can have your fully external way to talk to your mates, your other systems, and have

10:01.620 --> 10:05.940
different representation inside your core components.

10:05.940 --> 10:12.740
For example, if we apply this to Flora, we can see that actually I have my business objects

10:12.740 --> 10:15.620
living inside my bounded context.

10:15.620 --> 10:22.140
When I need to store them, I serialize them to a data access object that will be compliant

10:22.140 --> 10:24.540
with what my database expects.

10:24.540 --> 10:29.220
So it means no fancy mutually recursive types, for example, or something like that.

10:29.220 --> 10:33.980
And when I need to send that on the wire, I will serialize that to a format that is

10:33.980 --> 10:39.820
easily representable by XML, JSON, and other various cursed binary representations that

10:39.820 --> 10:44.580
we may find, especially in the banking system.

10:44.580 --> 10:51.460
So in the end, if I was to summarize bounded context, I showed you a very simple diagram

10:51.460 --> 10:53.100
earlier of Flora.

10:53.100 --> 10:55.860
But how does it interface with each other?

10:55.860 --> 11:02.220
So we have DTOs between each component, and especially between the clients and us, DAOs

11:02.220 --> 11:09.700
for storage access, and inside each component we operate on our business objects.

11:09.700 --> 11:14.460
It so happens that the business object can be extremely similar between the web and the

11:14.460 --> 11:17.060
core components, but sometimes they are not.

11:17.060 --> 11:24.020
And I think it's very liberating to know that you don't have to keep to a single representation

11:24.020 --> 11:26.100
from A to Z all the way.

11:26.100 --> 11:31.980
You really can have conversion layers between your components, between your interfaces,

11:31.980 --> 11:33.620
and it's perfectly all right.

11:33.620 --> 11:37.580
For example, very trivial, but reading configuration.

11:37.580 --> 11:42.100
The 12-factor application model tells us to read configuration from the environment, from

11:42.100 --> 11:43.100
the shell.

11:43.100 --> 11:50.620
So what we have on the left is the config type, which models what I get from the environment

11:50.620 --> 11:53.780
with a twist, because I can force some types.

11:53.780 --> 11:55.420
It's not all text-based.

11:55.420 --> 12:02.580
I can force my parsing of HTTP ports to be a word 16, for example, because I'm not so

12:02.580 --> 12:10.260
interested in having port number of 1 million, unless not without overflow.

12:10.260 --> 12:14.980
So I've got my external configuration that describes, for example, the first member is

12:14.980 --> 12:17.340
DB config with a pool configuration.

12:17.340 --> 12:20.720
So it's all the information I need for the pool, the database pool.

12:20.720 --> 12:25.180
And then internal configuration, it's the pool itself.

12:25.180 --> 12:32.660
And it's very useful because then I have this very explicit conversion, and it's perfectly

12:32.660 --> 12:38.540
all right then to change something inside or outside my core components.

12:38.540 --> 12:43.900
Because then I only have this bottleneck that I can easily change and one more step towards

12:43.900 --> 12:47.260
TLS refactoring.

12:47.260 --> 12:49.980
Separating commands and queries.

12:49.980 --> 12:57.620
So this has practical effects in terms of operation infrastructure and also in terms

12:57.620 --> 13:02.940
of ergonomics for the people who read our code.

13:02.940 --> 13:07.860
In a practical way, if we know that we have a recurrent, fairly heavy processing query

13:07.860 --> 13:13.900
that runs and can take significant lock or CPUs on our server, we have the option to

13:13.900 --> 13:19.140
have these queries run on a read-only replica for our database.

13:19.140 --> 13:22.020
And put this replica on another machine.

13:22.020 --> 13:26.780
So PostgreSQL, for example, very specific example, but I can talk about that.

13:26.780 --> 13:31.980
You can have read-only replicas which take read-only queries and will be very angry at

13:31.980 --> 13:35.700
you if you ever try to mutate the state of this replica.

13:35.700 --> 13:42.020
So you have your primary server which upon which you perform mutating commands and then

13:42.020 --> 13:47.380
it will stream these changes to read-only replica and then the replica will provide

13:47.380 --> 13:49.560
you with a read-only interface.

13:49.560 --> 13:55.820
That is like not only enforced at the level of the types, for example, in your applications,

13:55.820 --> 13:58.860
but fundamentally on the protocol itself.

13:58.860 --> 14:02.900
You will get a runtime error if you try to mutate this state.

14:02.900 --> 14:10.980
So you can't unsafe perform on an unsafe course your way behind that.

14:10.980 --> 14:18.260
So something I learned at my current place of employment scribe is to have a physical

14:18.260 --> 14:22.660
separation in the code between types, the commands and the queries.

14:22.660 --> 14:32.100
So the dialect, the idiom that we have here is that we have these.query and.update modules

14:32.100 --> 14:37.500
in which we put the read-only and mutating queries.

14:37.500 --> 14:43.500
And then when we import them, we qualify them, for example, import qualified as query and

14:43.500 --> 14:45.820
then there is a visual indicator.

14:45.820 --> 14:49.500
So it's very bare-bones but it does work.

14:49.500 --> 14:51.980
This is a query that is going to be read-only.

14:51.980 --> 14:58.060
It's not going to increment a counter in a side table because you have performed something

14:58.060 --> 15:00.380
that is seemingly read-only.

15:00.380 --> 15:05.740
A good example, for example, it's LinkedIn.

15:05.740 --> 15:10.260
When you view someone's page on LinkedIn, they have a notification.

15:10.260 --> 15:16.300
So you would think that viewing something, it's fundamentally read-only, even the terms

15:16.300 --> 15:19.260
reading, viewing, you know, you would think it's read-only.

15:19.260 --> 15:24.860
But perhaps there is a counter that is increased with user tracking so that you can later report

15:24.860 --> 15:27.020
who has viewed your page.

15:27.020 --> 15:35.740
But if you can bring one step more into separating the queries and the commands, then it's much

15:35.740 --> 15:43.260
easier to know which kind of operation you're performing at which place in the code.

15:43.260 --> 15:51.180
So we could go further even and declare queries and commands as effects with their own connection

15:51.180 --> 15:52.180
pulls.

15:52.180 --> 15:57.060
For example, I don't only have the DB effects in my stack.

15:57.060 --> 16:06.060
I'm declaring that I'm performing a read-only operation on the read-only replica of my PostgreSQL

16:06.060 --> 16:07.220
database.

16:07.220 --> 16:11.220
So one more step towards, you know, more...

16:11.220 --> 16:16.180
So of course it can be a technical detail but also I think it's very important to be

16:16.180 --> 16:22.380
able to say to the readers of your code what are you performing, which side effect does

16:22.380 --> 16:28.500
it have, especially in the system that you have ownership of.

16:28.500 --> 16:29.500
Now that's my...

16:29.500 --> 16:33.660
An orchist tendency is coming up.

16:33.660 --> 16:35.740
Let's keep our distance from the state.

16:35.740 --> 16:40.060
The state is best contained.

16:40.060 --> 16:45.420
So the cache of our application is actually a bounded context in its own.

16:45.420 --> 16:48.900
It has its own lifecycle, data storage and API.

16:48.900 --> 16:54.540
And by decoupling our application monolith from its state, we have worked a significant

16:54.540 --> 17:00.580
portion of the path to having a setup where we can have multiple instances of our application

17:00.580 --> 17:03.620
and serving data from the same database in cache.

17:03.620 --> 17:10.860
So at this point, by ensuring that the database server keeps operations in sync, we've got

17:10.860 --> 17:15.020
higher consistency of the application.

17:15.020 --> 17:18.260
So that's the CAP theorem for the disability systems.

17:18.260 --> 17:20.300
You've got CAP.

17:20.300 --> 17:22.100
Is your application consistent?

17:22.100 --> 17:26.100
Is it available or is it tolerant to partition?

17:26.100 --> 17:32.980
And in some industries where you work in very sensitive... with very sensitive data, if you

17:32.980 --> 17:40.860
have a production incident, you can't risk having inconsistent data or having an inconsistent

17:40.860 --> 17:45.500
state where people can read someone else's private folder.

17:45.500 --> 17:48.780
So it's better to shut things down for a bit.

17:48.780 --> 17:52.980
We keep our count, we take a long, deep, long breath and then we restart the system.

17:52.980 --> 18:02.100
But it's because availability has to take one for the team in order to keep consistent.

18:02.100 --> 18:06.660
And partition, tolerance, sorry, to partition, can go out the window.

18:06.660 --> 18:13.060
So for Flora, for example, very simple, we can have our clients that talk to our NGINX

18:13.060 --> 18:18.180
gateway and then the multiple instances of Flora that still speak to the same database

18:18.180 --> 18:24.300
server for mutating operations and the same replica for read-only commands.

18:24.300 --> 18:30.780
I'm not selling you microservice architectures and scale to the moon type of stuff.

18:30.780 --> 18:36.500
But I think it's a very decent way to start a monolith.

18:36.500 --> 18:42.420
We all know that a good distributed system has to start as a monolith and then split

18:42.420 --> 18:44.020
it further and further.

18:44.020 --> 18:51.700
If you start with a microservice-based architecture, you might end up with a distributed monolith.

18:51.700 --> 18:57.060
But the whole thing of a microservice-based application is to have independent contexts

18:57.060 --> 18:58.620
that can still run.

18:58.620 --> 19:04.300
So here we don't take the bet that every component is fully independent.

19:04.300 --> 19:09.260
We acknowledge the boundaries that we have some boundaries between the web, the core,

19:09.260 --> 19:10.980
and the job worker's components.

19:10.980 --> 19:15.540
And then themselves, they have their own context.

19:15.540 --> 19:18.740
So it's also about realism.

19:18.740 --> 19:22.220
Do you want to scale to the moon and raise like hundreds of thousands of dollars from

19:22.220 --> 19:23.220
venture capitalists?

19:23.220 --> 19:28.220
Or do you want to create a nice community website that indexes packages for the Haskell

19:28.220 --> 19:30.860
environment?

19:30.860 --> 19:36.820
I'm going to make a short detour here, and it's directing our workflows with types.

19:36.820 --> 19:42.460
So it's a technique that brings together type safety and ergonomics, which is one of my

19:42.460 --> 19:45.900
favorite subjects, to create type-directed state machines.

19:45.900 --> 19:47.900
Very fancy word.

19:47.900 --> 19:56.420
Basically, it's really the way that your operations are composed together, and you will be driven

19:56.420 --> 19:58.820
to compose these operations via their types.

19:58.820 --> 20:03.500
It can be a bit scary sometimes to think of your business operations as a state machine,

20:03.500 --> 20:10.300
but it gives us a terminology and a literature to take from and to think of how we organize

20:10.300 --> 20:12.180
and compose our operations.

20:12.180 --> 20:19.700
So for example, here we have a workflow state, which can have three values, arrival, processed,

20:19.700 --> 20:25.380
and departure, and a workflow that has this state type parameter.

20:25.380 --> 20:32.660
So we have a new workflow value that creates a workflow w1, and then the process workflow

20:32.660 --> 20:35.860
function takes a workflow, but not any kind of workflow.

20:35.860 --> 20:38.820
It has to be set to arrival.

20:38.820 --> 20:45.220
It can only take newly arrived workflows and then sets them as processed.

20:45.220 --> 20:52.820
Again, this could be in-codes trivially implemented at the value level with properties of the

20:52.820 --> 21:01.380
workflow object, and we could very easily verify, check these properties in the value

21:01.380 --> 21:02.860
level in the code.

21:02.860 --> 21:08.740
But here I factorize all these checks, and I put them really at a place where the compiler

21:08.740 --> 21:13.580
can guide my hand and tell me where I went wrong with that.

21:13.580 --> 21:19.980
And finally, the send back workflow can only take processed workflows by the laws of the

21:19.980 --> 21:25.300
use, and then sets the workflow as deported.

21:25.300 --> 21:30.140
So if I compose the functions in the good order, so new workflow and then a pipeline

21:30.140 --> 21:35.580
of functions, and then I pipe it into process workflow and send back workflow, everything

21:35.580 --> 21:36.580
is good.

21:36.580 --> 21:45.180
If I try to skip a step, I will get a compiler error that says you wanted me to take a processed

21:45.180 --> 21:46.460
workflow.

21:46.460 --> 21:52.420
But actually, I need, sorry, you wanted me to take an arrival workflow, but actually

21:52.420 --> 21:56.340
I need the processed workflow.

21:56.340 --> 22:00.300
And this code, you're not sending that code to production because you cannot compile this

22:00.300 --> 22:02.980
code.

22:02.980 --> 22:10.260
In terms of web application development, there are also some, for us, haskalers, we like

22:10.260 --> 22:14.940
to put everything at the level of types and think of our code as being formally proven

22:14.940 --> 22:16.820
or code by construction.

22:16.820 --> 22:22.180
But sometimes we must not drink all the cool aid or all the clemate.

22:22.180 --> 22:26.540
For example, database layers that promise type safe SQL.

22:26.540 --> 22:31.980
If you ever find a database layer that promise type safety, either it's the kind of type

22:31.980 --> 22:37.980
safety that is trivial to implement, and it's totally expected of the tool to have it, or

22:37.980 --> 22:43.020
it has encoded the semantics of SQL at the type level, and we've either found the golden

22:43.020 --> 22:51.340
goose or someone who has clearly underestimated the difficulties of SQL semantics.

22:51.340 --> 22:55.700
Also SQLite for development and PostgreSQL for production, that's something that the Python

22:55.700 --> 23:01.900
community has popularized in the 2010s.

23:01.900 --> 23:06.060
So we can accomplish great things by lying to the universe, but we scarcely accomplish

23:06.060 --> 23:07.980
anything by lying to ourselves.

23:07.980 --> 23:15.460
And SQLite is its own system, and unless you somehow perfectly code in this common subset

23:15.460 --> 23:21.180
of SQL supported by both implementations, you will be maintaining two sets of database

23:21.180 --> 23:24.340
migrations and sometimes of code.

23:24.340 --> 23:30.940
So PostgreSQL has very good features, SQLite has different but also good features, not

23:30.940 --> 23:32.420
its type system, of course.

23:32.420 --> 23:37.860
But if you get used to one locally and then discover the second one once you're deployed,

23:37.860 --> 23:42.660
you're going to have a bad time and also the muscle memory, because the brain is a muscle,

23:42.660 --> 23:49.340
that you will have accumulated with SQLite would be fairly useless with PostgreSQL.

23:49.340 --> 23:50.700
So where to go from here?

23:50.700 --> 23:51.700
Documentation.

23:51.700 --> 23:53.500
You produce documentation.

23:53.500 --> 24:00.060
We have many ways of producing documentation, and we hold also tremendous power in the types.

24:00.060 --> 24:04.580
And coupled with introspection, it means that the algebraic data types, like the sum types

24:04.580 --> 24:10.780
and the product types, so the enums and the records, they can serve as the backbone for

24:10.780 --> 24:12.700
further even documentation.

24:12.700 --> 24:18.460
The types themselves are not documentation, but they can be used to guide the reader.

24:18.460 --> 24:21.060
And you remember how I told you to write the tests.

24:21.060 --> 24:26.740
So the best tests are those that can describe real world behaviors.

24:26.740 --> 24:33.700
And if you can even produce a summary web page that shows the behaviors and the high

24:33.700 --> 24:40.380
level paths taken by your program according to some input, this is very particularly helpful

24:40.380 --> 24:45.740
for less technical people, like product managers, who want to know the behavior of your program.

24:45.740 --> 24:50.740
If you can present a nice interface of how the code is executed according to some high

24:50.740 --> 24:56.780
level business operation, it's even better.

24:56.780 --> 25:00.380
So I have a couple of sources for what I'm saying.

25:00.380 --> 25:02.940
I'm not pulling that out of my ass.

25:02.940 --> 25:07.060
The first one is Dominant Modeling Made Functional by Scott Vlachin.

25:07.060 --> 25:13.300
It's an excellent book written in F-sharp for the functional and DDD practitioners.

25:13.300 --> 25:14.300
It's excellent.

25:14.300 --> 25:15.620
I encourage you to read it.

25:15.620 --> 25:19.660
As well as Living Documentation by Cyril Marcher.

25:19.660 --> 25:21.900
And that one is also excellent.

25:21.900 --> 25:28.220
Really it puts documentation as its own living system for which you will have real clients,

25:28.220 --> 25:34.420
because PMs and other engineers in your organization or consumers of documentation, and of course

25:34.420 --> 25:38.140
hideous amounts of caffeine, as Fraser told earlier.

25:38.140 --> 25:48.660
So that would be the end of my talk.

25:48.660 --> 25:50.900
Do we have a couple of minutes for questions perhaps?

25:50.900 --> 25:52.660
Yes, thank you, Akate.

25:52.660 --> 25:55.900
And we do have about 10 minutes for questions.

25:55.900 --> 26:00.300
Yes, young man there.

26:00.300 --> 26:03.380
This is on.

26:03.380 --> 26:08.180
This is more of a comment than a question.

26:08.180 --> 26:13.700
So one little detail that I think that sort of you could have sold also, right, is the

26:13.700 --> 26:21.060
fact that when you do this, when you do the data kind annotation on your workflow, instead

26:21.060 --> 26:26.900
of checking that during runtime, we do the type annotation and that's actually more efficient,

26:26.900 --> 26:32.820
right, because of type erasure that there's no runtime data or check that has to happen.

26:32.820 --> 26:38.900
Yes, so what Bjorn says is that indeed there is a matter of efficiency, because the data

26:38.900 --> 26:46.980
kinds when we encode the nature of parameters in our workflow, this all goes away at code

26:46.980 --> 26:48.140
generation.

26:48.140 --> 26:55.820
So if you are in a setup where you need some very minimal code that is being generated,

26:55.820 --> 27:01.260
if you are in tight loop for example, this code is completely raised at the time of code

27:01.260 --> 27:06.860
generation and indeed you spare some CPU cycles.

27:06.860 --> 27:07.860
Any other question?

27:07.860 --> 27:09.260
You can also call me up on my bullshit.

27:09.260 --> 27:11.260
I won't be offended.

27:11.260 --> 27:12.260
Yes?

27:12.260 --> 27:14.540
Do I need my code?

27:14.540 --> 27:15.540
I can repeat your question.

27:15.540 --> 27:16.540
Oh yes, that's not quite quickly.

27:16.540 --> 27:26.300
The libraries that offer type safe database access that are, I'm sure, hideously incomplete

27:26.300 --> 27:33.460
also offer abstraction of a different database back ends, which is one of the problems you

27:33.460 --> 27:37.340
were talking about, like why you're using Postgres to develop locally.

27:37.340 --> 27:44.100
So my question is, are there really situations for a 4PM where those libraries didn't provide

27:44.100 --> 27:45.580
a feature that you needed?

27:45.580 --> 27:53.020
Yes, so the question is, those libraries that encode all of the semantics of SQL at the

27:53.020 --> 27:57.860
type level, are there situations where they don't provide features that I would need for

27:57.860 --> 27:58.860
a 4PM?

27:58.860 --> 27:59.860
Yes.

27:59.860 --> 28:00.860
Yes.

28:00.860 --> 28:05.100
So as I told earlier, I'm very preoccupied by ergonomics.

28:05.100 --> 28:14.140
20 minutes of compilation time and 20 gigabytes of half the interface files on disk, I would

28:14.140 --> 28:19.700
consider that a problem in terms of feedback loop for contributors.

28:19.700 --> 28:26.100
My previous place of employment, we used the toolkit Squeal for type level-encoded SQL

28:26.100 --> 28:29.500
queries because they were business critical.

28:29.500 --> 28:35.300
So we wanted to invest in something very much type safe because of the critical aspect of

28:35.300 --> 28:37.200
these queries.

28:37.200 --> 28:38.200
It was hell.

28:38.200 --> 28:39.200
It was horrendous.

28:39.200 --> 28:45.900
It was not only to view and to review, but also because it took so much time to compile,

28:45.900 --> 28:52.820
like unironically 20 minutes, and we had some problems with stack because the interface

28:52.820 --> 28:56.860
files on disk were taking way too much space.

28:56.860 --> 29:04.460
Type families in Haskell are best consumed responsibly.

29:04.460 --> 29:10.420
I'm a servant user, so I can't shit too much on type families, but in some cases, very

29:10.420 --> 29:14.820
specific cases, it's best to rely on the expertise of outside systems.

29:14.820 --> 29:21.220
For example, my best friend, who's here actually in FOSDEM, is my database administrator at

29:21.220 --> 29:22.220
work.

29:22.220 --> 29:24.220
I keep him close.

29:24.220 --> 29:32.940
Do you ever have experience to need to onboard like a newer developer to maintain or even

29:32.940 --> 29:35.060
do new feature to the project?

29:35.060 --> 29:38.340
If so, what's the experience, especially if they don't have any Haskell experience or

29:38.340 --> 29:39.340
especially this kind of experience?

29:39.340 --> 29:40.340
Yes, very good question.

29:40.340 --> 29:43.500
Do we have any experience onboarding new developers on the project?

29:43.500 --> 29:48.300
Actually this talk was supposed to be the continuation of the different onboarding sessions

29:48.300 --> 29:50.060
rather than on Flora PM.

29:50.060 --> 29:56.060
So sometimes if you find me on Discord or Matrix, I will share my screen and introduce

29:56.060 --> 29:57.580
people to the code base.

29:57.580 --> 30:02.100
I think that's one of the most important aspects of Flora as a project.

30:02.100 --> 30:09.340
Not only it is a community tool that aims to satisfy the users, but also it's a vessel

30:09.340 --> 30:10.340
for teaching.

30:10.340 --> 30:15.940
So I have got many techniques that I explained in this talk implemented in Flora, and Flora

30:15.940 --> 30:20.060
is the factor code base to teach these techniques.

30:20.060 --> 30:27.300
I had very bad experience with community tools that have badly aged, and the code is only

30:27.300 --> 30:36.220
known by the 10% of maintainers that stick around, even if the vast majority of contributors

30:36.220 --> 30:43.220
of a project are the 90% of people who just make one pull request and then go away forever.

30:43.220 --> 30:48.540
So it's very hard to retain institutional knowledge, and also it's very hard not to

30:48.540 --> 30:56.300
aim to please the 10% of people who stick around and submit patches on the regular.

30:56.300 --> 31:01.960
So yes, I would think that, and that's the goal of Flora, onboarding new contributors

31:01.960 --> 31:09.860
easily is actually a feature, and if it can't be done anymore, it's a bug.

31:09.860 --> 31:10.860
Any other question?

31:10.860 --> 31:11.860
Nope.

31:11.860 --> 31:22.300
With such a representation, is it possible to write a function that, say, generates a

31:22.300 --> 31:23.300
diagram?

31:23.300 --> 31:24.620
It technically is.

31:24.620 --> 31:26.660
I have references for you.

31:26.660 --> 31:31.980
So the question is, can we generate diagrams from such representations?

31:31.980 --> 31:36.540
Because indeed we have the possible values that we have at a type level, and we can do

31:36.540 --> 31:39.860
many things with our types, including inspecting them.

31:39.860 --> 31:46.220
So yes, I believe there are several libraries on Hackage that aim to, for example, provenance.

31:46.220 --> 31:52.180
It's a library that gives you the path that the data takes and the provenance of your

31:52.180 --> 31:55.460
data throughout the code.

31:55.460 --> 32:02.300
I would say it's one of the greatest things to be able to do, is to represent your code

32:02.300 --> 32:11.020
and to extract facts and movements from your code in a higher level representation.

32:11.020 --> 32:12.860
So yeah, I believe we can do it today.

32:12.860 --> 32:14.100
I don't do it personally.

32:14.100 --> 32:19.700
I think it's possible.

32:19.700 --> 32:22.380
There's time for more questions.

32:22.380 --> 32:26.860
Or you can like, duel me if you want to challenge my beliefs.

32:26.860 --> 32:30.460
Okay.

32:30.460 --> 32:31.460
That seems like it.

32:31.460 --> 32:32.460
So thank you again, Ekate.

32:32.460 --> 32:50.420
Thank you very much.
