WEBVTT

00:00.000 --> 00:08.120
Thank you for coming to my talk.

00:08.120 --> 00:12.360
It's not a TED talk, but it's just my talk.

00:12.360 --> 00:14.560
Continuous delivery to many coronavirus clusters.

00:14.560 --> 00:20.520
My name is Carlos Sanchez, and I'm here to talk to you about our life experience, real

00:20.520 --> 00:22.020
world.

00:22.020 --> 00:24.800
I'm not here to solve you anything.

00:24.800 --> 00:31.280
I'll try to tell you if I have time for some of the mistakes we made, she's not all beautiful

00:31.280 --> 00:33.520
and wonderful.

00:33.520 --> 00:40.120
I'm a principal scientist at Adobe Experience Manager Cloud Service.

00:40.120 --> 00:43.320
I'll talk a little bit about the product.

00:43.320 --> 00:48.400
On the open source side, I started the Jenkins Kubernetes plugin.

00:48.400 --> 00:49.800
Anybody heard about Jenkins?

00:49.800 --> 00:52.760
Yes, some people probably.

00:52.760 --> 00:55.880
I'm a Kubernetes.

00:55.880 --> 00:59.080
Anybody heard about Kubernetes?

00:59.080 --> 01:04.160
Anybody using Kubernetes in production?

01:04.160 --> 01:10.280
I'm a long time contributor to open source, multiple projects on Jenkins, Apache Foundation,

01:10.280 --> 01:11.280
and all that.

01:11.280 --> 01:18.480
A quick intro to what Adobe Experience Manager is, because every time I say Adobe, people

01:18.480 --> 01:28.280
say Photoshop and PDF and Flash.

01:28.280 --> 01:31.160
That's not any of those.

01:31.160 --> 01:37.280
This is a content management system that you probably never heard of, but it's powering

01:37.280 --> 01:43.480
80% of the 4100, and it's very enterprise-y, so I'm not expecting people in front of them

01:43.480 --> 01:46.840
to know.

01:46.840 --> 01:51.560
This is widely used because it's based in a lot of open source.

01:51.560 --> 01:56.920
It's a distributed OSDI application that was started many years ago, and uses a lot of

01:56.920 --> 02:01.440
components of open source from the Apache Foundation, and we contribute back to those

02:01.440 --> 02:09.160
components like Felix, Apache Felix, Sling, and a few things about content management

02:09.160 --> 02:10.160
there.

02:10.160 --> 02:16.000
It has a huge market of extension developers, people that are writing their own Java code

02:16.000 --> 02:23.200
and then runs on Adobe Experience Manager in AM.

02:23.200 --> 02:28.520
When I'm doing Adobe, the goal was let's move this into a cloud service, and this is running

02:28.520 --> 02:31.720
AM on Kubernetes.

02:31.720 --> 02:38.920
We're running currently on Azure, and we have 35 clusters and growing very quickly.

02:38.920 --> 02:42.840
Because this is a content management, we run it in multiple regions.

02:42.840 --> 02:48.640
Right now, 11, so multiple ones in the US, Europe, Australia, Singapore, Japan, whatever,

02:48.640 --> 02:55.640
because people want to have low latency between their users and the content.

02:55.640 --> 03:03.080
Then, another interesting fight is that we have on the Kubernetes clusters, we don't

03:03.080 --> 03:07.400
run them directly, we build stuff on top of them, and we have a different team at Adobe

03:07.400 --> 03:12.240
that manages Kubernetes for us.

03:12.240 --> 03:17.400
Some curiosity is that customers can run their own code, so we are running this for them,

03:17.400 --> 03:22.800
and we take their code and run it inside our processes.

03:22.800 --> 03:28.880
We have to limit clusters' permissions for security, and we have several security concerns,

03:28.880 --> 03:34.960
because this is a very multi-tenant setup.

03:34.960 --> 03:42.760
Each customer can have multiple environments, multiple copies, and they can self-service,

03:42.760 --> 03:47.800
so they can deploy new environments whenever they want, they can update them and do a few

03:47.800 --> 03:52.480
things, so it's not just us controlling what is running, it's also the customers.

03:52.480 --> 03:58.640
Each customer can have three or more Kubernetes namespaces where these environments run, and

03:58.640 --> 04:05.800
I like to call this a micromanolith, so we don't run a big service that expands like

04:05.800 --> 04:13.480
thousands of instances, we run slightly different versions of the same service over a thousand,

04:13.480 --> 04:16.880
ten thousand times.

04:16.880 --> 04:23.560
Micromanolith defines it very well, and then we use namespace, Kubernetes namespaces, to

04:23.560 --> 04:28.840
provide the scope of network isolation, quotas, permissions, and so on.

04:28.840 --> 04:36.920
Now internally, we have multiple teams building services, so different services have different

04:36.920 --> 04:43.000
requirements, people can use different languages, and we are more in a philosophy of you build

04:43.000 --> 04:51.760
it, you run it, and we are basically doing each service as an API, or we follow the Kubernetes

04:51.760 --> 04:54.440
operator patterns.

04:54.440 --> 05:01.280
We also use, to split the monolith, we use a lot of init containers and sidecars, if

05:01.280 --> 05:07.280
you know in Kubernetes, you can run multiple containers at the same time, so the main application

05:07.280 --> 05:12.880
runs in one container, and then we have to apply division of concern, many sidecars that

05:12.880 --> 05:20.680
do different things, and it's an easy way to split separate concerns without having

05:20.680 --> 05:28.480
to rewrite your whole architecture to go to a fully network-based maker service oriented

05:28.480 --> 05:32.240
architecture.

05:32.240 --> 05:37.520
On the continuous delivery side, which is probably what you are interested in here,

05:37.520 --> 05:45.000
we are running, we are moving from a generally released to, I mean we are pushing changes

05:45.000 --> 05:52.840
daily multiple times, not only, not just the application, that the application is maybe

05:52.840 --> 05:59.840
slower to move, but on the operational side, and all the services, and operators, all these

05:59.840 --> 06:06.560
things, all of them together, any of them at any point in time, any day can receive

06:06.560 --> 06:09.640
changes.

06:09.640 --> 06:17.640
So we use Jenkins for CI CD in some places, we have Tekton, you heard about that in one

06:17.640 --> 06:24.440
of the talks before, it's another open source project to do workflows on Kubernetes to

06:24.440 --> 06:32.840
orchestrate some pipelines, and we also started using Argo CD for some new micro services.

06:32.840 --> 06:39.600
We follow a GitOps process, so where most of the configuration is a storing Git, and

06:39.600 --> 06:41.640
it's reconciling each commit, right?

06:41.640 --> 06:50.740
And we use a pull versus push model to scale, and I'll go through this in a bit.

06:50.740 --> 06:57.680
We have a combination of multiple things being deployed to the clusters, we have the AM

06:57.680 --> 07:04.120
version that is deployed with a Helm chart, we have operation services that are operators

07:04.120 --> 07:10.040
and services and all the other things that are not the application, these are deployed

07:10.040 --> 07:17.080
using Kubernetes files but templatized, and we are also using customized Argo CD for some

07:17.080 --> 07:20.800
new micro services.

07:20.800 --> 07:28.680
On the Helm side, we use the Helm operator, so in each namespace we use the Helm operator

07:28.680 --> 07:37.480
CRDs to do a more state-based installation of Helm.

07:37.480 --> 07:42.220
So we create the CRD and the Helm operator is going to install the application based

07:42.220 --> 07:45.040
on the parameters on the CRD.

07:45.040 --> 07:53.600
A word of advice is don't mix application and infrastructural configuration on the same

07:53.600 --> 08:00.800
package because if you cannot enforce the same Helm chart for all tenants, for example,

08:00.800 --> 08:06.080
as I said mentioned before, customers can decide when to update things, right?

08:06.080 --> 08:11.440
So we have some customers in older releases and some ones in newer releases, this is something

08:11.440 --> 08:17.760
that we want to change, but in the meantime, if we want to update a specific version of

08:17.760 --> 08:24.440
something in an old release, it's hard when this is already packaged on Helm.

08:24.440 --> 08:29.600
So we built a solution for this.

08:29.600 --> 08:35.020
So from the platform level, we can go and manipulate this Helm chart.

08:35.020 --> 08:42.200
So we can have overrides and this is easy to do when you have the Helm operator.

08:42.200 --> 08:50.160
So you can inject, whenever there's a request to install a new Helm chart, we change parameters.

08:50.160 --> 08:56.160
So we change both Helm values, this is easy, instead of passing some values, we pass different

08:56.160 --> 08:57.600
ones.

08:57.600 --> 09:02.880
Or you can use customized patches and this is also support from the Helm operator, this

09:02.880 --> 09:05.200
is also support for customized patches.

09:05.200 --> 09:09.760
And customized patches are very interesting because they allow you to patch any Kubernetes

09:09.760 --> 09:16.440
resource so even if there was no previous Helm value defined for it.

09:16.440 --> 09:23.360
So if we want to change a sidecar container image version across the Helm fleet, we just

09:23.360 --> 09:25.440
have to change the patch.

09:25.440 --> 09:32.800
And this patch is going to be applied to all the clusters, all the namespaces, and all

09:32.800 --> 09:37.280
the Helm charts that were installed are going to get reinstalled with the right version

09:37.280 --> 09:38.640
that we want.

09:38.640 --> 09:48.520
So we do this combination of both Helm chart and then operational values on the other hand.

09:48.520 --> 09:52.880
Very important for us was the shift left mentality, right?

09:52.880 --> 09:58.960
Detecting problems as soon as possible, not waiting for developers to push things to production

09:58.960 --> 10:02.380
because the cost increases.

10:02.380 --> 10:09.280
So we run checks as soon as we can on pull requests so it is still fresh in your memory

10:09.280 --> 10:14.880
when you make a change and something is broken, you want to catch it as soon as possible.

10:14.880 --> 10:22.400
And we do this by generating all these templates, we have some tests that generate these templates

10:22.400 --> 10:29.780
and then apply tests, multiple tests on them.

10:29.780 --> 10:36.360
The most basic check that you can run is the apply kubectl apply drive run.

10:36.360 --> 10:43.320
This will tell you if the manifest is wrong in some very obvious way.

10:43.320 --> 10:47.760
So if it's valid or it's not valid.

10:47.760 --> 10:52.540
KubeConform is a tool that will allow you to validate the Kubernetes schemas.

10:52.540 --> 10:55.840
So this is the successor of kubefall.

10:55.840 --> 10:58.440
Anybody heard about kubefall or kubefirm?

10:58.440 --> 10:59.440
Yeah?

10:59.440 --> 11:08.520
So this is very useful for if you have custom CRDs or yeah, just to make sure typical problems,

11:08.520 --> 11:14.600
you miss the JAML indentation and now it's not valid anymore and then you catch this

11:14.600 --> 11:19.800
on a PR to just run this and it will tell you this property is missing or this property

11:19.800 --> 11:26.080
is in the wrong place because everybody loves JAML, right?

11:26.080 --> 11:30.840
Most tests is another tool for open policy agents.

11:30.840 --> 11:34.960
Any people familiar with open policy agents?

11:34.960 --> 11:35.960
Open policy?

11:35.960 --> 11:38.520
OPA, yeah.

11:38.520 --> 11:50.440
So OPA allows you to write policies where you can go and pretty much check anything in any

11:50.440 --> 11:52.400
structure file.

11:52.400 --> 12:01.320
In the case of Kubernetes, you could say, I don't know, don't run the pod as root, make

12:01.320 --> 12:07.040
sure you don't mount secrets as environment variables or with files, make sure and force

12:07.040 --> 12:14.880
that all the pods have some labels, any random thing that you can think of, you can do it.

12:14.880 --> 12:21.800
And like don't pull from Docker Hub, pull from the internal registry, you can do that

12:21.800 --> 12:24.720
with contest and OPA policies.

12:24.720 --> 12:29.600
The only problem is that uses the regular language that if you haven't heard of is very

12:29.600 --> 12:39.680
painful to work with, but it works great once you try to figure out.

12:39.680 --> 12:42.680
We added another tool which is called Pluto.

12:42.680 --> 12:48.520
Pluto is just a CLI that will tell you what API versions have been deprecated or removed.

12:48.520 --> 12:55.120
So if you are thinking about upgrading Kubernetes, you run Pluto and it will tell you, you know,

12:55.120 --> 12:59.280
this is deprecated, it's going to be removed in this version and so on.

12:59.280 --> 13:04.080
So you can enforce that.

13:04.080 --> 13:10.540
We built a tool that we call Git init, which is our own version of a GitHub spool.

13:10.540 --> 13:15.000
So we have the Kubernetes definitions store in Git and we deploy these to blob stores

13:15.000 --> 13:17.360
across regions.

13:17.360 --> 13:21.000
So they are pulled in each cluster.

13:21.000 --> 13:24.440
And Git init is a deployment that runs continuously on each namespace.

13:24.440 --> 13:29.720
We have around 10,000 namespaces in our fleet.

13:29.720 --> 13:36.640
So it basically pulls the blob, applies the changes and does this thing every so often.

13:36.640 --> 13:42.720
And an example of why we do pull versus pull, because pushing to all the clusters, we have

13:42.720 --> 13:49.320
a job that does this and it runs in parallel, like in 20 threads or something, and still

13:49.320 --> 13:52.240
takes like five hours to run.

13:52.240 --> 13:56.120
So we cannot push things when we want.

13:56.120 --> 14:08.320
On Argo CD, we have a new CAS platform that allows you to do Argo CD based microservices.

14:08.320 --> 14:14.320
Argo CD basically this would create a new Git repo, it would come with some templates

14:14.320 --> 14:17.720
and that would get deployed with Argo CD to the cluster.

14:17.720 --> 14:24.560
And this is for us, we're thinking about moving this way and each team will have their own

14:24.560 --> 14:29.080
Git repo, because right now we have mostly centralized operators and everything.

14:29.080 --> 14:33.480
And this is good for the, okay, you go on your own direction, you do whatever you want,

14:33.480 --> 14:35.000
you build it, you run it.

14:35.000 --> 14:39.560
On the other hand, this will be tricky because when we decide or figure out something is

14:39.560 --> 14:45.440
problematic, we cannot just centrally say, you know, and this Git repo tell me who is

14:45.440 --> 14:48.360
doing this and let's change it.

14:48.360 --> 14:53.920
But we're moving towards that direction.

14:53.920 --> 15:00.000
Let me skip this and talk a bit about progressive delivery.

15:00.000 --> 15:06.000
So progressive delivery is a way, well, it's something that is a name for something that

15:06.000 --> 15:14.200
you probably heard of, which is Canary rollouts and doing percentage based rollouts, feature

15:14.200 --> 15:15.840
flags, blue ring.

15:15.840 --> 15:22.920
So basically don't update everybody at the same time, because you can break everybody.

15:22.920 --> 15:28.000
So we can do rollouts to different customer groups in separate waves and we can also do

15:28.000 --> 15:32.160
rollouts to percentage of customers.

15:32.160 --> 15:38.160
By default we have a time based rollout that goes from dev to stage to prod candidate after

15:38.160 --> 15:39.440
a period of time.

15:39.440 --> 15:44.280
And this is running on Jenkins and ensures that things have been running on dev and stage

15:44.280 --> 15:46.080
before we merge them to prod.

15:46.080 --> 15:49.160
I mean, this is very basic.

15:49.160 --> 15:53.680
What we built was feature flags at the namespace level.

15:53.680 --> 15:59.760
We have 10,000 namespaces and the in the Kubernetes definition templates.

15:59.760 --> 16:07.920
So what we allow developers to do is for each namespace, they can decide I want to roll

16:07.920 --> 16:14.560
out this change to this environment, dev, stage or prod, or I want to deploy this change

16:14.560 --> 16:23.640
to a specific cluster or by template namespace type of namespace or a percentage.

16:23.640 --> 16:26.360
And this is just using templates on Kubernetes objects.

16:26.360 --> 16:35.800
So an example is in this case, a Kubernetes definition where you can have a template that

16:35.800 --> 16:44.120
is says full version or bar version, or you can enable a site card container or disable

16:44.120 --> 16:45.120
it.

16:45.120 --> 16:47.200
And then at the bottom you can see the rules.

16:47.200 --> 16:51.120
So by default we want full version to be 1.0.

16:51.120 --> 16:55.800
But for the namespace, all the namespaces on the dev environment, we want that to be

16:55.800 --> 16:57.080
1.1.

16:57.080 --> 17:02.560
So this allows us to quickly roll out changes, but progressively.

17:02.560 --> 17:04.720
We can also do it for percentile.

17:04.720 --> 17:09.800
So in this case we could say, I want all the namespaces in dev and all the namespaces in

17:09.800 --> 17:16.320
a stage to have this full version 1.1 and enable my rule true, but for prod I only want

17:16.320 --> 17:17.400
5%.

17:17.400 --> 17:25.720
I roll out the change to 5% of prod and then I can continue after that.

17:25.720 --> 17:32.040
So this has proven really useful for developers to test things safely, increase development

17:32.040 --> 17:39.520
speed, PRs are much faster, so it's all great.

17:39.520 --> 17:45.400
And we are thinking about, well, we are thinking, we are working on getting our good rollouts,

17:45.400 --> 17:51.880
but at the deployment level, article rollouts allows you to do blue, green and canary rollouts

17:51.880 --> 17:59.920
where you can say progress the number of pods over a period of time.

17:59.920 --> 18:04.720
So instead of changing, I don't know, 10 pods at the same time, it goes one by one.

18:04.720 --> 18:09.960
And if you have a service mesh, you can go even more fine grained and say, I want 5%

18:09.960 --> 18:14.000
of the traffic to go to the old version, to the new version, everything else to the old

18:14.000 --> 18:20.000
version and keep progressing that and do automatic rollbacks.

18:20.000 --> 18:29.680
So yeah, with the service mesh you can fine tune the traffic percentages, but with Kubernetes

18:29.680 --> 18:35.040
services you can still do it, it's just that we are limited with the number of pods.

18:35.040 --> 18:42.160
So to sum up, shift left on guard rails, so keeping people safe on what they are doing,

18:42.160 --> 18:48.120
this increases development speed, it reduces the issues that you are going to have in production,

18:48.120 --> 18:56.320
and you're never going to prevent having issues in production, what you can prevent is how

18:56.320 --> 19:01.280
many customers are affected and how fast you can fix them, right?

19:01.280 --> 19:07.800
So for us, what was very useful is the process delivery techniques like canaries, percent

19:07.800 --> 19:15.720
of rollouts or automated rollbacks, and the automation to do these control and progressive

19:15.720 --> 19:19.320
rollout pays off over time.

19:19.320 --> 19:22.480
So I think we have one minute for questions.

19:22.480 --> 19:30.560
Or you can find me afterwards.

19:30.560 --> 19:38.520
Thank you.
