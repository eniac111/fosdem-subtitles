1
0:00:00.000 --> 0:00:16.140
Hello, everyone. Do you hear me well? So thanks. Pretty large audience. If I may ask a quick

2
0:00:16.140 --> 0:00:22.240
show of hands, who among you have some experience, just any level of experience with machine

3
0:00:22.240 --> 0:00:33.200
learning? Okay, cool. Awesome. So I'll be talking today about how to run testing on machine

4
0:00:33.200 --> 0:00:39.240
learning systems. So there are different keywords, CICD, quality assurance. A few words about

5
0:00:39.240 --> 0:00:46.800
us. So I'm one of the founders of GeeseGuard. We are building a collaborative and open source

6
0:00:46.800 --> 0:00:52.760
software platform to precisely ensure the quality of AI models. And I'll be explaining

7
0:00:52.760 --> 0:01:01.160
in this presentation a bit how it works. In terms of agenda, I prepared two sections on

8
0:01:01.160 --> 0:01:08.240
the why. Why a project on testing machine learning systems is needed. Why I personally

9
0:01:08.240 --> 0:01:17.880
decided to work on that problem. Some of the risks and why classical software testing methods

10
0:01:17.880 --> 0:01:25.960
don't quite work on AI. And then I'll do some more concrete examples on two important quality

11
0:01:25.960 --> 0:01:32.380
criteria that you might want to test for machine learning. One is robustness and the other

12
0:01:32.380 --> 0:01:38.400
is fairness. And if we have the time, it's just 30 minutes, I hope that we can do a quick

13
0:01:38.400 --> 0:01:48.400
demo of an example use case where we run the full CICD pipeline on a machine learning model.

14
0:01:48.400 --> 0:01:56.040
So to kind of start off easy, I put together a series of memes to explain my personal story

15
0:01:56.040 --> 0:02:05.640
of why I came to create a company and a project around this machine learning testing thing.

16
0:02:05.640 --> 0:02:13.360
So about ten years ago, I started in machine learning, statistics, data science. And you

17
0:02:13.360 --> 0:02:19.640
know, you had this, you start using the Scikit-learn API and you're like, yeah, it's super easy,

18
0:02:19.640 --> 0:02:27.080
right? Anybody can be a data scientist, you just dot fit, dot predict, and that's it.

19
0:02:27.080 --> 0:02:33.760
You're a data scientist. And probably if you're here today, you're like, yeah, have you tested

20
0:02:33.760 --> 0:02:42.560
your model? Yeah, sure. Train test, split. Reality, if you've deployed in production,

21
0:02:42.560 --> 0:02:54.560
it's quite different. So if you've deployed to production, often you'll have this painful

22
0:02:54.560 --> 0:03:02.320
discovery where you have your product manager, business stakeholders to whom you said, look,

23
0:03:02.320 --> 0:03:09.480
I worked really hard on the fine tuning and the grid search to get to 85% accuracy and

24
0:03:09.480 --> 0:03:14.880
you push your first version to production and things don't quite work out. You don't

25
0:03:14.880 --> 0:03:26.520
reproduce these good accuracy numbers. So, well, this was me. I hope it's not you. It

26
0:03:26.520 --> 0:03:33.320
was one of these, my first experience deploying machine learning for production was on a fraud

27
0:03:33.320 --> 0:03:40.720
detection system. Frauds are notoriously difficult as a use case for machine learning because

28
0:03:40.720 --> 0:03:46.320
what you're trying to detect doesn't quite want to be detected. There are people behind

29
0:03:46.320 --> 0:03:53.960
it who have a vested interest not to have your machine learning system detect them.

30
0:03:53.960 --> 0:04:00.880
So often in terms of performance, that's at least what I ended up doing a lot of hot fixes

31
0:04:00.880 --> 0:04:12.360
in production. It's bad. So kind of like five years ago, this was my stance on machine learning

32
0:04:12.360 --> 0:04:21.080
in production. A very painful grueling experience where you just never know when you're going

33
0:04:21.080 --> 0:04:29.720
to be a complaint, where you're going to be on call to solve something in production.

34
0:04:29.720 --> 0:04:39.920
So that's when I decided to buff up and switch roles to join a software engineering team.

35
0:04:39.920 --> 0:04:50.840
I was at DataEcoo back then, so I moved internally from data science to the product team. And

36
0:04:50.840 --> 0:04:58.960
here are some of the things to summarize that as someone with a machine learning background

37
0:04:58.960 --> 0:05:04.360
but no real software engineering experience that these were kind of like what I was told.

38
0:05:04.360 --> 0:05:11.400
You must learn the ways of the CI CD. Otherwise your project will not come to production.

39
0:05:11.400 --> 0:05:19.240
And for context, so I was specifically at that time in charge of creating like an open

40
0:05:19.240 --> 0:05:30.320
source layer to federate all the NLP and computer vision APIs that vendors in the cloud provide.

41
0:05:30.320 --> 0:05:37.360
And then to do the same for pre-trained NLP and time series models. So what was difficult

42
0:05:37.360 --> 0:05:43.240
in this context is I was not even the one in charge of the models. And the models will

43
0:05:43.240 --> 0:05:49.920
be retrained fine-tuned. So the guarantees into the properties of that systems as an

44
0:05:49.920 --> 0:05:56.920
engineer, that's more difficult. There are some elements in the stack that you don't

45
0:05:56.920 --> 0:06:03.960
have control of. So yeah, this is a bit of a repeat of a previous

46
0:06:03.960 --> 0:06:12.520
meme and I really wanted to say one does not simply ship an ML product with our tests.

47
0:06:12.520 --> 0:06:21.400
The challenge I had then is that from an engineering management standpoint, I was told, yeah, but

48
0:06:21.400 --> 0:06:27.040
you know, it's easy. No engineers, they all write their test cases. So you do machine

49
0:06:27.040 --> 0:06:33.120
learning, just write them all. Just write all the test cases.

50
0:06:33.120 --> 0:06:39.920
So this was me being kind of square one. It's like, okay, so you're telling me I just need

51
0:06:39.920 --> 0:06:48.840
to write unit tests? Okay. That will not really solve the issue. And that's kind of the beginning

52
0:06:48.840 --> 0:06:56.240
of a quest that set me on to build something to solve that gap between, okay, I want to

53
0:06:56.240 --> 0:07:02.800
test my models, I need to test my models, and how can I do that? Because clearly, and

54
0:07:02.800 --> 0:07:11.200
I'll explain why, unit testing your model is really not enough.

55
0:07:11.200 --> 0:07:17.360
So a different angle on the why. I'll try to take a step back and talk about quality

56
0:07:17.360 --> 0:07:23.720
in general. I think in this track, we all agree that quality matters. And if you look

57
0:07:23.720 --> 0:07:32.640
at AI, it's an industry that's an engineering practice that is far younger than software

58
0:07:32.640 --> 0:07:40.200
engineering or civil engineering. And it's just riddled with incidents. I encourage you

59
0:07:40.200 --> 0:07:47.520
if you don't know that resource already, it's an open source database. It's incidentdatabase.ai.

60
0:07:47.520 --> 0:07:56.600
And it's a public collection of reports, mostly in the media, of AI models that have not worked

61
0:07:56.600 --> 0:08:02.320
properly. And it's a really great work that has been going on for about two years and

62
0:08:02.320 --> 0:08:11.320
a half. It's a global initiative. And just in this time, they collected more than 2,000

63
0:08:11.320 --> 0:08:18.360
incidents. Since these are public reports, think of it as the tip of the iceberg, of

64
0:08:18.360 --> 0:08:24.600
course. There are a lot of incidents internal to companies that are not necessarily spoken

65
0:08:24.600 --> 0:08:31.840
out in the media. The incidentdatabase has a very interesting taxonomy of the different

66
0:08:31.840 --> 0:08:42.280
types of incidents. It's very multifaceted. I took the liberty to simplify it in three

67
0:08:42.280 --> 0:08:49.360
big categories of incidents. One is FX. The other is business, economic impacts. And the

68
0:08:49.360 --> 0:08:57.720
third one is on security. And we are talking about really if they happen at a global company

69
0:08:57.720 --> 0:09:05.520
scale, incidents that are very, very severe. In FX, you can have a sexist credit scoring

70
0:09:05.520 --> 0:09:12.800
algorithm that exposes the company to lawsuits, to brand image damages, et cetera. And these

71
0:09:12.800 --> 0:09:21.000
are notoriously hard to identify. Because, I mean, in a way, machine learning is precisely

72
0:09:21.000 --> 0:09:26.000
about discrimination. So it's hard to tell a machine that is learning to discriminate,

73
0:09:26.000 --> 0:09:30.960
not to discriminate against certain sensitive groups. I'll speak on some methods that can

74
0:09:30.960 --> 0:09:36.720
be used precisely on this problem. Apple was working with, at the time, Goldman Sachs on

75
0:09:36.720 --> 0:09:44.400
deploying this algorithm. And probably some tests and safeguards were unfortunately skipped.

76
0:09:44.400 --> 0:09:51.600
So it was actually discovered on Twitter that, in a simple case, a male loan applicant would

77
0:09:51.600 --> 0:10:01.120
get 10x their loan limit compared to it was his wife. That sparked a huge controversy.

78
0:10:01.120 --> 0:10:10.000
That probably exposed Apple to some lawsuits. In another area that is not with sensitive

79
0:10:10.000 --> 0:10:20.480
features such as gender, there was a huge catastrophe a year and a half ago that happened

80
0:10:20.480 --> 0:10:30.640
to Zillow, a real estate company, where there was a small bias that was overestimating the

81
0:10:30.640 --> 0:10:41.040
prices of homes. And they decided to put this algorithm live to buy and sell houses. In

82
0:10:41.040 --> 0:10:48.240
turn out that this tiny bias, which was left unchecked, was exploited by the real estate

83
0:10:48.240 --> 0:10:58.000
agents in the US. And literally, this created a loss of nearly a half a billion dollars.

84
0:10:58.000 --> 0:11:06.040
And again, probably going back to testing, this could have been anticipated and avoided.

85
0:11:06.040 --> 0:11:12.160
Now on a more cyber security spectrum, there's a lot of good research from cyber security

86
0:11:12.160 --> 0:11:18.000
labs showing that you can hack, for example, a computer vision system in an autonomous

87
0:11:18.000 --> 0:11:26.120
driving context. So here you put a special tape on the road and you can crash a tester.

88
0:11:26.120 --> 0:11:31.000
We don't quite know if these types of vulnerabilities have been exploited in real life yet. But

89
0:11:31.000 --> 0:11:36.560
as AI becomes super ubiquitous, and obviously there are some bad actors out there that might

90
0:11:36.560 --> 0:11:43.200
want to hack the systems and introduces a new type of attack vectors. So that's also

91
0:11:43.200 --> 0:11:53.440
something we need to care about. So both from practitioners of AI and from a regulatory

92
0:11:53.440 --> 0:12:01.160
standpoint, testing just makes sense. Yann LeCun, chief AI scientist at Metar, was actually

93
0:12:01.160 --> 0:12:06.760
taking a stance at the beginning of last year on Twitter saying that if you want trust in

94
0:12:06.760 --> 0:12:14.080
the system, you need tests. And also making a slight criticism towards some of the explainability

95
0:12:14.080 --> 0:12:19.640
methods. Because two years ago, if you've followed that realm, people were saying, oh,

96
0:12:19.640 --> 0:12:24.240
you just need explainability and then your problems will go away. Well, that's just part

97
0:12:24.240 --> 0:12:31.800
of the answer. Lastly, and this was covered in some of the talks this morning on the big

98
0:12:31.800 --> 0:12:38.920
auditorium, there's a growing regulatory requirements to put some checks and balances

99
0:12:38.920 --> 0:12:44.680
in place. And that also says that you need specifically in case your AI system is high

100
0:12:44.680 --> 0:12:51.040
risk, you need to put quality measures in place. And the definition of high risk AI

101
0:12:51.040 --> 0:12:56.280
systems is pretty large. Obviously, you have anything related to infrastructure like critical

102
0:12:56.280 --> 0:13:03.360
infrastructure, defense, et cetera. But you also have all AI systems that are involved

103
0:13:03.360 --> 0:13:09.160
in human resources and public service and financial services because these are considered

104
0:13:09.160 --> 0:13:19.120
obviously critical components of society. Right. But now that we kind of agreed that

105
0:13:19.120 --> 0:13:28.200
it's an important problem, these are some of the challenges because if you've encountered

106
0:13:28.200 --> 0:13:35.360
some of this issue, you probably looked at some easy solutions, right? Taking some analogies

107
0:13:35.360 --> 0:13:42.200
on what you might do to do this. And there are three points that make this problem of

108
0:13:42.200 --> 0:13:50.400
testing machine learning a bit special, meaning it's still a big work in progress. Point one

109
0:13:50.400 --> 0:13:57.240
is that it is really not enough to check the data quality to guarantee the quality of a

110
0:13:57.240 --> 0:14:04.760
machine learning system. One of my co-founders doing his PhD proved it's a mountain. You

111
0:14:04.760 --> 0:14:11.600
can run experiments that you can have really clean data in a bad model. Right. So you cannot

112
0:14:11.600 --> 0:14:19.680
just say it's an upstream problem. It's technically like systems engineering. You have to take

113
0:14:19.680 --> 0:14:28.080
the data, the machine learning model and the user into context to analyze its properties.

114
0:14:28.080 --> 0:14:35.880
Moreover, the errors of a machine learning systems are often caused by pieces of data

115
0:14:35.880 --> 0:14:47.880
that did not exist when the model was created. They were clean, but they did not exist. Second

116
0:14:47.880 --> 0:14:54.600
point, it's pretty hard just to copy paste some of the testing methods from software

117
0:14:54.600 --> 0:15:02.840
into AI. One is like, yes, you can do some unit tests on machine learning models, but

118
0:15:02.840 --> 0:15:10.280
they won't prove much. Because the principle is that it's a transactional systems and things

119
0:15:10.280 --> 0:15:15.600
are moving quite a lot. So that's a good baseline. If you have a machine learning system and

120
0:15:15.600 --> 0:15:20.440
you have some unit tests, that's really like step one. It's better to have that than to

121
0:15:20.440 --> 0:15:29.040
have nothing. But you have to embrace the fact that there has got to be a large number

122
0:15:29.040 --> 0:15:35.480
of test cases. So you cannot just test on a free 500, even a thousand cases will not

123
0:15:35.480 --> 0:15:42.200
be enough. The models themselves are probabilistic. So you have to take into account statistical

124
0:15:42.200 --> 0:15:49.680
methods of tests. And lastly, and I think this is specific to, because there has been

125
0:15:49.680 --> 0:15:57.200
some systems that were heavily dependent on data, but with AI, AI also came with a fact

126
0:15:57.200 --> 0:16:03.680
that you increase the number of data inputs compared to traditional systems. So you very

127
0:16:03.680 --> 0:16:12.120
quickly come into issues of combinatorial problem and it's factually impossible to generate

128
0:16:12.120 --> 0:16:23.720
all the combinations. Very simple example of that. How can you test an NLP system? Lastly,

129
0:16:23.720 --> 0:16:29.240
AI touching a lot of different points. If you want to have a complete test coverage,

130
0:16:29.240 --> 0:16:35.000
you really need to take into account multiple criteria. So performance of a system, but

131
0:16:35.000 --> 0:16:43.480
also robustness, robustness to errors. Fairness, privacy, security, reliability. And also,

132
0:16:43.480 --> 0:16:48.720
and that's becoming an increasingly important topic with green AI, it's like what is the

133
0:16:48.720 --> 0:16:54.720
carbon impact of this AI? Do you really need that many GPUs? Can you make your system a

134
0:16:54.720 --> 0:17:02.400
bit more energy efficient? So today I'll focus on the, because I see we have 10 more minutes,

135
0:17:02.400 --> 0:17:14.440
I'll focus on two aspects, the robustness and the ethics. So I'll start with robustness.

136
0:17:14.440 --> 0:17:24.520
Who has read or heard about this paper? Quick show of hands. Okay. Ah, one. So who has heard

137
0:17:24.520 --> 0:17:31.280
of behavioral testing? Because that's not machine learning specific. Yeah, cool. So

138
0:17:31.280 --> 0:17:40.880
Ribero three years ago, along with other co-writers of this paper, did I think a fantastic job

139
0:17:40.880 --> 0:17:50.800
to see how to adapt behavioral testing, which is a really good practice from software engineering,

140
0:17:50.800 --> 0:18:02.200
to the context of machine learning and specifically wrote something for NLP models. The main problem

141
0:18:02.200 --> 0:18:10.720
that this research paper aim to solve was test case generation. Because really NLP is

142
0:18:10.720 --> 0:18:18.880
by a sense a problem, NLP, a natural language processing, so you have an input text, right?

143
0:18:18.880 --> 0:18:27.120
It just wrote text, right? So you cannot, you need test this, really. But what you can

144
0:18:27.120 --> 0:18:41.560
do is to generate test cases that rely on mapping the input and the input changes in

145
0:18:41.560 --> 0:18:50.280
the text to expectations. I'll give three examples from very, very simple to a bit more

146
0:18:50.280 --> 0:18:58.640
complex. One is like the principle of minimum functionality. For example, if you are building

147
0:18:58.640 --> 0:19:05.720
a sentiment prediction machine learning system, you could just have a test that says if you

148
0:19:05.720 --> 0:19:13.960
have extra ordinary in the sentence, you should always predict that the model will say it's

149
0:19:13.960 --> 0:19:22.280
a positive message. Now you will probably tell me, yeah, but what about if the user

150
0:19:22.280 --> 0:19:30.680
has written it's not extraordinary or absolutely not extraordinary? And that actually brings

151
0:19:30.680 --> 0:19:38.360
me to the concept of test template. And the fact that probably for NLP, what you need

152
0:19:38.360 --> 0:19:43.340
to do, and this is obviously language specific, is start to have templates where you change

153
0:19:43.340 --> 0:19:51.040
the text by, for example, adding negations. And then, so you might want to test if your

154
0:19:51.040 --> 0:19:56.720
system, if you're adding negation, if you have a certain direction, because normally,

155
0:19:56.720 --> 0:20:03.080
if a machine learning model has understood, it should, if it's about sentiment, understand

156
0:20:03.080 --> 0:20:10.800
that putting not an extraordinary or not good, you have then synonyms, will not affect the

157
0:20:10.800 --> 0:20:19.800
system too much. Or actually, either your system, you want it to move to a certain direction,

158
0:20:19.800 --> 0:20:26.160
or there are cases where you want actually the opposite behavior. You want robustness.

159
0:20:26.160 --> 0:20:35.120
So that's called invariance. So for instance, you will want a system that is robust to typos,

160
0:20:35.120 --> 0:20:46.120
to just changing a location name, just putting synonyms, et cetera, et cetera.

161
0:20:46.120 --> 0:20:54.280
So we've created this diagram to explain it, and it's a really thriving field in the research.

162
0:20:54.280 --> 0:20:59.640
There is a lot of research going on these days about testing machine learning systems,

163
0:20:59.640 --> 0:21:06.040
and metamorphic testing is one of the leading methods to do that.

164
0:21:06.040 --> 0:21:13.640
The principle is, if I take an analogy, is very similar to if you've worked in finance

165
0:21:13.640 --> 0:21:20.560
or if you have some friends who work there, the principle of backtesting, an investment

166
0:21:20.560 --> 0:21:28.440
strategy. You simulate different changes in the market conditions, and you see how your

167
0:21:28.440 --> 0:21:36.360
strategy, your algorithm behaves, and what is the variance of that strategy.

168
0:21:36.360 --> 0:21:46.600
This concept applies very well to machine learning. So you need two things. You need

169
0:21:46.600 --> 0:21:57.680
one to define a perturbation. So what I was explaining earlier in NLP, perturbation might

170
0:21:57.680 --> 0:22:05.040
be adding typos, adding negation. In another context, like let's say it's more in an industrial

171
0:22:05.040 --> 0:22:16.120
case, it might be about doubling the values of some sensors or adding noise to an image.

172
0:22:16.120 --> 0:22:22.800
And then, pretty simply, you define an expectation, a test expectation, in terms of the metamorphic

173
0:22:22.800 --> 0:22:31.160
relation between the output of a machine learning model and the distribution of the output after

174
0:22:31.160 --> 0:22:36.600
perturbation. And once you have that, and if you have enough data, then you can actually

175
0:22:36.600 --> 0:22:42.240
have a, like you can do actual statistical tests, see there's a difference in distribution,

176
0:22:42.240 --> 0:22:47.160
et cetera. So I won't have too much time to dive into

177
0:22:47.160 --> 0:22:52.920
all the details of this, but we have wrote a technical guide on this topic, and you have

178
0:22:52.920 --> 0:23:01.040
a link in QR code up there. Next, I'll talk a bit about a really tricky

179
0:23:01.040 --> 0:23:07.720
topic, which is AI fairness. And I want to emphasize that it's, at least our recommendation

180
0:23:07.720 --> 0:23:15.400
is not to come at the problem of AI ethics with a closed mind or a top-down definition

181
0:23:15.400 --> 0:23:25.040
of this is an ethical system, or no, this is an unethical system. My co-founder did

182
0:23:25.040 --> 0:23:34.000
his PhD on precisely on this topic and wrote a full paper on this, looking at the philosophical

183
0:23:34.000 --> 0:23:43.240
and sociological implications of this. And the gist of it is that, yes, to a certain

184
0:23:43.240 --> 0:23:52.200
extent you can adopt a top-down approach to AI fairness, saying, well, for instance, as

185
0:23:52.200 --> 0:24:00.240
an organization, we want to test the fairness on explicitly free sensitive categories. You

186
0:24:00.240 --> 0:24:08.000
can say, well, we want to check for gender balance, we want to check for race balance.

187
0:24:08.000 --> 0:24:14.440
That means if the country where you deploy a machine learning allows to collect this

188
0:24:14.440 --> 0:24:23.720
data, this is not always the case. But the challenge with these approaches is that, A,

189
0:24:23.720 --> 0:24:31.000
you might not have the data to measure this, and B, you may miss out, because often when

190
0:24:31.000 --> 0:24:39.520
this exercise of defining the quality criteria for fairness and for balance are done, you

191
0:24:39.520 --> 0:24:50.680
only have a limited sample. So it's, and taking some sociological analogies, it's really important

192
0:24:50.680 --> 0:24:58.560
to have this kind of top-down definition of AI ethics, meet the reality on the ground,

193
0:24:58.560 --> 0:25:07.080
and confront the actual users and the makers of the systems to get them to define the definition

194
0:25:07.080 --> 0:25:14.440
of ethics, rather than a big organization, if I put a bit of a caricature that says,

195
0:25:14.440 --> 0:25:20.440
AI ethics, yeah, we wrote a charter about this, you follow, you read this, you sign,

196
0:25:20.440 --> 0:25:32.400
and then, oops, you're ethical. Having said that, so there are some good top-down metrics

197
0:25:32.400 --> 0:25:37.960
to adopt that are kind of a baseline, and I'll explain one of them, which is disparate

198
0:25:37.960 --> 0:25:47.200
impact. Disparate impact is actually a metric from the human resources management industry

199
0:25:47.200 --> 0:25:57.560
from at least 40 years ago, so it's not new, that says, so it's probabilities, but essentially

200
0:25:57.560 --> 0:26:06.920
it's about setting a rule of 80%, where you measure the probability of, you define a positive

201
0:26:06.920 --> 0:26:17.280
outcome with respect to a given protected population, and you say, well, I want to the

202
0:26:17.280 --> 0:26:22.600
proportions of a probability of a positive outcome relative to a probability of a positive

203
0:26:22.600 --> 0:26:35.040
outcome in the unprotected context to be above 80%. So, for instance, so if you want to apply

204
0:26:35.040 --> 0:26:45.800
that to a, oops, to put more concrete, yeah, so if, say, you are building a model to predict

205
0:26:45.800 --> 0:26:53.040
the churn of customers, and you want to check whether your model is biased or not for each

206
0:26:53.040 --> 0:27:03.200
class, this formula allows you to really define this metric and write a concrete test case.

207
0:27:03.200 --> 0:27:14.640
Right, so I just have three minutes, so I'll highlight what one of the features of our

208
0:27:14.640 --> 0:27:21.240
project enables is putting human feedback, so really having an interface where users

209
0:27:21.240 --> 0:27:27.840
and not only data scientists can change the parameters, so there's a link to metamorphic

210
0:27:27.840 --> 0:27:34.160
testing and actually give human feedback to a point out where the biases may be, and the

211
0:27:34.160 --> 0:27:40.120
benefit of this approach is that it allows for the community to precisely define what

212
0:27:40.120 --> 0:27:53.440
they think are the risks. So, sadly, we won't have time to do a demo, but this phase, in

213
0:27:53.440 --> 0:28:01.040
our project, we call that the inspection phase, and it's about before you test, and this is

214
0:28:01.040 --> 0:28:05.880
super important, and again, one of the things where it's different from traditional software

215
0:28:05.880 --> 0:28:12.360
testing, before you even test, you need to confront yourself with the data and the model,

216
0:28:12.360 --> 0:28:19.120
so that's where actually we think explainability methods really shine, because they allow to

217
0:28:19.120 --> 0:28:24.760
debug and to identify the zones of risks, and this is precisely what helps once you

218
0:28:24.760 --> 0:28:29.480
have qualified feedback to know where you should put your effort in test.

219
0:28:29.480 --> 0:28:36.160
So, in a nutshell, what I'm saying for testing machine learning systems is it's not a matter

220
0:28:36.160 --> 0:28:41.200
of creating hundreds of tests, of automating everything, but rather to have a good idea

221
0:28:41.200 --> 0:28:50.000
of from a fairness standpoint and for performance standpoint of what are the 10, 15, maybe max

222
0:28:50.000 --> 0:28:55.360
20 tests that you want in your platform. If you want to get started actually on it, this

223
0:28:55.360 --> 0:29:04.040
is up our GitHub, and yeah, if you have a machine learning system to test, we're interested

224
0:29:04.040 --> 0:29:11.920
in your feedback.

