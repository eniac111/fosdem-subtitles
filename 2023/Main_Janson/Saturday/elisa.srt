1
0:00:00.000 --> 0:00:10.600
Okay, there's next talk.

2
0:00:10.600 --> 0:00:13.840
Please be silent.

3
0:00:13.840 --> 0:00:19.560
Okay, this will get an interesting thing.

4
0:00:19.560 --> 0:00:23.860
Normally I'm used to move my arms a lot while I'm talking, so I try to get the microphone

5
0:00:23.860 --> 0:00:27.320
always close to my body now.

6
0:00:27.320 --> 0:00:31.280
I will give you some information about the ELISA project.

7
0:00:31.280 --> 0:00:35.400
ELISA stands for Enabling Linux and Safety Applications.

8
0:00:35.400 --> 0:00:41.000
Maybe a quick question up front, who is aware of safety-critical software?

9
0:00:41.000 --> 0:00:43.200
So shall we raise the hand?

10
0:00:43.200 --> 0:00:47.560
Hi, that's good, maybe 25-30%.

11
0:00:47.560 --> 0:00:52.960
I hope you will also learn something new then.

12
0:00:52.960 --> 0:00:59.640
See before we start fully, I just give you a short view on which project context I'm

13
0:00:59.640 --> 0:01:00.640
working.

14
0:01:00.640 --> 0:01:04.880
So as you can see, my project is mainly focusing on embedded IoT Linux at Bosch.

15
0:01:04.880 --> 0:01:09.900
And what you try to do is utilizing a lot of open source projects, see how they fit

16
0:01:09.900 --> 0:01:16.320
into a landscape, and can be of value for very, very different device classes because

17
0:01:16.320 --> 0:01:22.720
normally you don't believe it, but in all of these kind of products you will find Linux

18
0:01:22.720 --> 0:01:27.520
in there are also embedded real-time OS and so on.

19
0:01:27.520 --> 0:01:31.760
So that's all about this part, shortly about myself.

20
0:01:31.760 --> 0:01:33.640
Who am I?

21
0:01:33.640 --> 0:01:36.120
I'm a technical business development manager.

22
0:01:36.120 --> 0:01:41.000
I'm focusing on embedded open source mainly, doing this for the Bosch.

23
0:01:41.000 --> 0:01:42.960
And in parallel, that's also why I'm speaking here.

24
0:01:42.960 --> 0:01:48.120
I'm the technical steering committee chair and working group lead for the Linux Foundation

25
0:01:48.120 --> 0:01:49.880
ELISA project.

26
0:01:49.880 --> 0:01:54.880
I bring a past history of 15 years plus, I guess I started in, we'll see, Ubuntu 6.

27
0:01:54.880 --> 0:02:00.440
10 more or less to set it up on old PCs sharing it to exchange students, so like a distributed

28
0:02:00.440 --> 0:02:02.560
hub of PCs.

29
0:02:02.560 --> 0:02:06.340
And since 10 years I'm more or less in the automotive space with Linux.

30
0:02:06.340 --> 0:02:09.920
We had our first product with 2.6 kernel out.

31
0:02:09.920 --> 0:02:14.000
And yeah, I guess now we can start on the real things.

32
0:02:14.000 --> 0:02:20.760
So if we talk about Linux in safety critical systems, we first need to get an understanding

33
0:02:20.760 --> 0:02:23.120
what a system really means.

34
0:02:23.120 --> 0:02:29.260
And a critical system maybe, first you say assessing whether the system is safe requires

35
0:02:29.260 --> 0:02:31.560
understanding sufficiently.

36
0:02:31.560 --> 0:02:35.920
And you can see here's nothing about Linux in there because a system always goes beyond

37
0:02:35.920 --> 0:02:42.280
the scope of a pure operating system, beyond maybe a single component.

38
0:02:42.280 --> 0:02:48.700
And in this one you have a system context in which Linux plays a role.

39
0:02:48.700 --> 0:02:53.160
And you need to understand the system context and how this is used because if you don't

40
0:02:53.160 --> 0:02:57.800
get the understanding how Linux operates, you cannot see in which components you're

41
0:02:57.800 --> 0:03:01.720
interested and which features you may need or which not.

42
0:03:01.720 --> 0:03:08.240
And then you can evaluate what kind of these features are really relevant for safety.

43
0:03:08.240 --> 0:03:16.080
And while you're doing so, we'll most likely identify gaps that exist and you'll definitely

44
0:03:16.080 --> 0:03:19.300
need more and more work to get this done.

45
0:03:19.300 --> 0:03:27.440
So if you look into the Linux ecosystem, which we have already, there's a good reason also

46
0:03:27.440 --> 0:03:33.680
to take Linux because there is a large variety of devices.

47
0:03:33.680 --> 0:03:34.880
The ecosystem is strong.

48
0:03:34.880 --> 0:03:39.680
You have good tools around this, an incredible amount of hardware support.

49
0:03:39.680 --> 0:03:42.600
It runs on many, many devices.

50
0:03:42.600 --> 0:03:47.600
And also very important, you have a broad set of experts in there.

51
0:03:47.600 --> 0:03:55.320
If you see what's sometimes taken as the benefit of a certified safety critical OS, often it

52
0:03:55.320 --> 0:03:58.040
comes with hard real-time requirements and capabilities.

53
0:03:58.040 --> 0:04:02.120
We know that the pre-empt RT patches are in good shape and in the kernel.

54
0:04:02.120 --> 0:04:08.440
But hard real-time maybe goes even further down the road and then there is a development

55
0:04:08.440 --> 0:04:10.040
process.

56
0:04:10.040 --> 0:04:15.080
And if you see these two sites, if you come and want to address very complex products

57
0:04:15.080 --> 0:04:20.000
like in the automotive field or maybe you can even call your robot vacuum cleaner a

58
0:04:20.000 --> 0:04:24.740
more complex product, then you come from two perspectives.

59
0:04:24.740 --> 0:04:29.480
On the one side, you could go with a traditional small component driven RTOS and you have to

60
0:04:29.480 --> 0:04:34.720
handle all the complexity so you need to have more hardware involved, you have more multi-core

61
0:04:34.720 --> 0:04:37.640
support, not everything works out there.

62
0:04:37.640 --> 0:04:40.560
Or you go the other way around and you come with a Linux where you all have these kind

63
0:04:40.560 --> 0:04:44.760
of things but you need to improve and see what do you do about the development process,

64
0:04:44.760 --> 0:04:47.520
what do you do about the real-time capabilities and so on.

65
0:04:47.520 --> 0:04:52.680
So anyway, when you build a more complex product, you need to find a way to tackle these kind

66
0:04:52.680 --> 0:04:54.680
of challenges.

67
0:04:54.680 --> 0:04:59.000
And also bring the difference closer to each other.

68
0:04:59.000 --> 0:05:03.600
While we were looking at Linux, I'll take the part in the beginning, it's a little bit

69
0:05:03.600 --> 0:05:06.560
like a disclaimer in a little more text.

70
0:05:06.560 --> 0:05:11.000
In this collaboration of Elisa, we said we cannot engineer your system to be safe.

71
0:05:11.000 --> 0:05:16.240
We're talking about functional safety, not about cyber security, but if we just take

72
0:05:16.240 --> 0:05:20.520
this example, there's always a strong risk also that you have security breaches in your

73
0:05:20.520 --> 0:05:21.520
system.

74
0:05:21.520 --> 0:05:23.200
So it's similar here also with safety.

75
0:05:23.200 --> 0:05:28.960
If you build a system, it's still your responsibility for it and just because we provide a little

76
0:05:28.960 --> 0:05:34.440
bit of guidance or engineering principles and so on, it's still in your responsibility

77
0:05:34.440 --> 0:05:37.680
as someone producing a product to make things safe.

78
0:05:37.680 --> 0:05:44.200
And also that way to make sure you really have the prescribed processes in use, use

79
0:05:44.200 --> 0:05:49.680
the methodologies and one of the core questions which typically could come is like, oh, so

80
0:05:49.680 --> 0:05:54.880
you're from Elisa, you make a safe Linux, will you certify a kernel version?

81
0:05:54.880 --> 0:05:58.920
And that's not what will work because we all know you have to move forward.

82
0:05:58.920 --> 0:06:02.320
There's continuous improvement there, sync and vulnerabilities fixed so you need to go

83
0:06:02.320 --> 0:06:06.480
on and this gives an additional challenge which is continuous certification.

84
0:06:06.480 --> 0:06:11.240
So we will definitely not have a version and we will also not certify Linux in this project.

85
0:06:11.240 --> 0:06:14.480
We just give the tools and other elements in there.

86
0:06:14.480 --> 0:06:16.160
So here are the last part of it.

87
0:06:16.160 --> 0:06:20.440
There's still responsibility, legal obligations, liability and so on which is also in your

88
0:06:20.440 --> 0:06:21.440
role.

89
0:06:21.440 --> 0:06:25.800
Nevertheless, we find a good set of partners already which are willing to support this

90
0:06:25.800 --> 0:06:31.960
mission and they subscribe and say we would like to bring the whole thing forward and

91
0:06:31.960 --> 0:06:37.080
seeing this there is the mission statement which we have drawn.

92
0:06:37.080 --> 0:06:38.760
It's lengthy.

93
0:06:38.760 --> 0:06:42.040
Basically you can read that there's set of elements, processes, tools.

94
0:06:42.040 --> 0:06:45.000
It should be amenable to safety certification.

95
0:06:45.000 --> 0:06:49.600
We look into software and documentation development and in the end that we aid the development,

96
0:06:49.600 --> 0:06:59.400
deployment, operation or the adoption of a project into another project.

97
0:06:59.400 --> 0:07:04.400
If you look at this mission, you see basically four key parts which we will also talk later

98
0:07:04.400 --> 0:07:05.400
about.

99
0:07:05.400 --> 0:07:09.960
You have elements and software which is concrete implementation of what we're doing and you

100
0:07:09.960 --> 0:07:12.680
also have the processes.

101
0:07:12.680 --> 0:07:16.480
A development process always falls into safety critical, into security system wherever you

102
0:07:16.480 --> 0:07:18.120
look at.

103
0:07:18.120 --> 0:07:23.240
And if you start to automate things, if you would like to analyze, there's always a strong

104
0:07:23.240 --> 0:07:25.840
involvement of tools in there.

105
0:07:25.840 --> 0:07:32.160
And the last thing is when you do all this kind of work, you need to document it.

106
0:07:32.160 --> 0:07:37.480
And actually there's a lot of documentation work needed in any place.

107
0:07:37.480 --> 0:07:40.880
So how we do all this kind of things?

108
0:07:40.880 --> 0:07:46.120
We take it in our ELISA working groups.

109
0:07:46.120 --> 0:07:50.800
We split this depending on different topics, on different contexts, they're growing depending

110
0:07:50.800 --> 0:07:53.440
on demands of certain sizes reached.

111
0:07:53.440 --> 0:07:55.040
We're extending this.

112
0:07:55.040 --> 0:08:01.960
And if we take a first look, we have a safety architecture work group.

113
0:08:01.960 --> 0:08:08.800
This is a group which actively looks inside the kernel and takes, for example, a watchdog

114
0:08:08.800 --> 0:08:13.620
subsystem because watchdog is one of the crucial elements which we have in use.

115
0:08:13.620 --> 0:08:16.480
It looks what are potential safety related functionality.

116
0:08:16.480 --> 0:08:19.280
Is there something in the kernel which is non-safety related?

117
0:08:19.280 --> 0:08:21.600
How would these kind of things interfere?

118
0:08:21.600 --> 0:08:27.900
And by this, the safety architecture work group does a lot of analysis, try to improve

119
0:08:27.900 --> 0:08:30.820
documentation in the kernel, provide new tools.

120
0:08:30.820 --> 0:08:34.020
So that's a strong set in there.

121
0:08:34.020 --> 0:08:39.120
Basically driven by use cases and demands of products.

122
0:08:39.120 --> 0:08:45.840
And a little more broader approach is brought in by the Linux features.

123
0:08:45.840 --> 0:08:49.580
And actually the full name is Linux features for safety critical systems.

124
0:08:49.580 --> 0:08:54.520
So it's not about generic features, it's about the safety criticality part in there.

125
0:08:54.520 --> 0:08:59.120
You can imagine this a little bit like if you're familiar with security measures like

126
0:08:59.120 --> 0:09:07.360
namespaces or other parts, that we're looking for elements in here which could improve safety.

127
0:09:07.360 --> 0:09:12.720
So which means if you take this special kernel configuration, a feature, turn it off on whatever

128
0:09:12.720 --> 0:09:17.640
you do and say, okay, this will come up as a blueprint, this is something how you better

129
0:09:17.640 --> 0:09:23.120
work with memory, how you not work with memory, all these kind of things are tackled in the

130
0:09:23.120 --> 0:09:24.580
Linux features.

131
0:09:24.580 --> 0:09:30.400
And then it's a nice group because with the results which are in there, if you're already

132
0:09:30.400 --> 0:09:34.600
in a process of enhancing Linux and don't want to wait for all the results of the use

133
0:09:34.600 --> 0:09:39.800
work group and so on, you can have incremental steps here, just take some part of it and

134
0:09:39.800 --> 0:09:44.420
make your system more robust, more dependable and you can also judge it against how does

135
0:09:44.420 --> 0:09:47.600
it compare to security things which you're doing.

136
0:09:47.600 --> 0:09:51.120
And so here, that's the big value of this group.

137
0:09:51.120 --> 0:09:58.800
It's more on a direct use base and serving a long-term safety argumentation, but not

138
0:09:58.800 --> 0:10:00.600
that it's something which develops for years or so.

139
0:10:00.600 --> 0:10:04.400
It's basically assess what's there.

140
0:10:04.400 --> 0:10:07.920
This also the improvement of code quality is very important.

141
0:10:07.920 --> 0:10:11.680
We have tools investigation code improvement work group.

142
0:10:11.680 --> 0:10:16.320
The code improvements could be, for example, done with doing fuzzy testing on the kernel

143
0:10:16.320 --> 0:10:24.400
using tools like CodeChecker or Syscaller and then bring them also into a set up where

144
0:10:24.400 --> 0:10:30.600
we have a server kind of a CI which runs on Linux Next or whatever kernel configuration

145
0:10:30.600 --> 0:10:36.080
to identify issues to get the kernel more robust, more dependable, reliable and serve

146
0:10:36.080 --> 0:10:40.520
also the argumentation about the quality of the kernel.

147
0:10:40.520 --> 0:10:45.280
And what was also on the right side in some of the challenges part was on the engineering

148
0:10:45.280 --> 0:10:49.720
process and as you know, there are rigorous methods within the kernel development.

149
0:10:49.720 --> 0:10:55.600
So there are a lot of reviews, patches are rejected and you see that there's strong demand

150
0:10:55.600 --> 0:11:02.320
from traditional project management when it comes to safety products and not every process

151
0:11:02.320 --> 0:11:04.240
complies with it directly.

152
0:11:04.240 --> 0:11:10.440
So we need to find an argumentation how is there an equivalence to the open source development

153
0:11:10.440 --> 0:11:18.360
process compared to what, for example, an ISO 26262 requests for automotive products.

154
0:11:18.360 --> 0:11:23.560
On top, what is very interesting to understand here is also that if we look into open source,

155
0:11:23.560 --> 0:11:30.000
you basically cannot easily buy a maintainer or developer there.

156
0:11:30.000 --> 0:11:38.680
So you cannot buy features directly or so you get more an unbiased view or maybe a personal

157
0:11:38.680 --> 0:11:44.720
view but a maintainer who is really committed for the component for this power subsystem

158
0:11:44.720 --> 0:11:46.040
of the kernel and so on.

159
0:11:46.040 --> 0:11:50.600
And with this strong commitment, for example, you already fulfilled a little bit of independent

160
0:11:50.600 --> 0:11:56.040
view because in safety systems, whenever it comes later on, the developer needs to commit

161
0:11:56.040 --> 0:11:59.400
to what has been done but of course it's not written down.

162
0:11:59.400 --> 0:12:00.400
It's not written down.

163
0:12:00.400 --> 0:12:02.360
The maintainer fully commits to whatever it does.

164
0:12:02.360 --> 0:12:08.380
So this is some part, for example, where you can start argumenting on it.

165
0:12:08.380 --> 0:12:12.840
And as the different elements need to get somewhere and need to be visible, we figured

166
0:12:12.840 --> 0:12:17.120
this out because we were running quite in parallel with different streams on this but

167
0:12:17.120 --> 0:12:21.320
never brought this forward, we came up with the systems work group and the system work

168
0:12:21.320 --> 0:12:26.700
group actually should take all these different elements, bring them together, works cross-functional

169
0:12:26.700 --> 0:12:31.600
and maybe even cross-project and combine the elements.

170
0:12:31.600 --> 0:12:35.560
In order to tailor the system properly, we have vertical use cases.

171
0:12:35.560 --> 0:12:41.320
A newly created one, so there's not much information in this presentation about the aerospace work

172
0:12:41.320 --> 0:12:42.760
group yet.

173
0:12:42.760 --> 0:12:46.280
The overall idea is it should address everything which flies and you know that in aerospace

174
0:12:46.280 --> 0:12:51.720
there are many safety standards, safety integrity standards, various levels in there.

175
0:12:51.720 --> 0:12:55.840
What you may not know and that's at least what we have heard so far was there is already

176
0:12:55.840 --> 0:13:01.240
Linux in use and also in certified product there's Linux use but it's only on a very

177
0:13:01.240 --> 0:13:09.520
low safety level so it's not on a very higher upper level of safety certification.

178
0:13:09.520 --> 0:13:13.800
What's an obvious thing if you see the member there is like 50 to 60 percent is from the

179
0:13:13.800 --> 0:13:19.620
field of automotive and therefore we have an automotive use case in there.

180
0:13:19.620 --> 0:13:24.620
If you drive a car, if you have a scooter or whatever you may see sometimes that there

181
0:13:24.620 --> 0:13:29.960
is an oil pressure sign, oil temperature sign, check engine, whatever basically when you

182
0:13:29.960 --> 0:13:34.200
put on the ignition you can see all these little LEDs and this is also the use case

183
0:13:34.200 --> 0:13:39.400
which we are using in the automotive work group.

184
0:13:39.400 --> 0:13:43.800
Basically what we said digital or cluster, instrument cluster, the speedometer, everything

185
0:13:43.800 --> 0:13:48.720
becomes digital, everyone has a display in your cars and that gives a good chance because

186
0:13:48.720 --> 0:13:55.040
there are more complex system in there, a lot of rendering, graphics rendering involved

187
0:13:55.040 --> 0:13:57.320
and this is actually safety critical function.

188
0:13:57.320 --> 0:14:02.600
Even if you are in driving or in rear gear mode this has to be properly displayed and

189
0:14:02.600 --> 0:14:05.000
it has a safety criticality assigned.

190
0:14:05.000 --> 0:14:09.960
Because showing the check engine part is a safety criticality.

191
0:14:09.960 --> 0:14:18.000
The third group which we have is from the medical devices and here this is something

192
0:14:18.000 --> 0:14:22.360
from a completely different perspective while automotive has the commercial element in mind

193
0:14:22.360 --> 0:14:27.960
maybe want to have cost savings driving topics forward with the open APS, APS the artificial

194
0:14:27.960 --> 0:14:30.840
pancreas system.

195
0:14:30.840 --> 0:14:35.840
It's driven by open source so there were open standards, chances to interact with your insulin

196
0:14:35.840 --> 0:14:39.680
pump and you see that this can become very uncomfortable.

197
0:14:39.680 --> 0:14:43.760
There's a nice TED talk from Dan M. Lewis, I recommend this, I put the link also in the

198
0:14:43.760 --> 0:14:46.400
slide that you can download them and check it.

199
0:14:46.400 --> 0:14:52.480
You can see that you basically need to track your glucose level and certain dose of your

200
0:14:52.480 --> 0:14:58.040
insulin depending on your glucose level and this is also with warnings and so on and it's

201
0:14:58.040 --> 0:15:05.240
very basically event triggered so you see the blood pressure goes up so you set the

202
0:15:05.240 --> 0:15:10.680
dose it has a certain delay until it reacts and what came in here was to add the raspberry

203
0:15:10.680 --> 0:15:17.360
pie in the middle writing some scripting around it getting it stabilized and create a product

204
0:15:17.360 --> 0:15:18.520
out of it.

205
0:15:18.520 --> 0:15:24.560
And why I want to stress this is this is not to any IEC ISO certification done, it was

206
0:15:24.560 --> 0:15:30.520
done by an open source engineer, started this project and if you download this, if you use

207
0:15:30.520 --> 0:15:37.080
it, you use it on your own risk and therefore the work of Elisa was basically also the first

208
0:15:37.080 --> 0:15:40.840
use case we put directly in the beginning of the workshop to say let's take a deeper

209
0:15:40.840 --> 0:15:43.080
look let's analyze what's in there.

210
0:15:43.080 --> 0:15:47.680
It's running for thousands of people, it has never been certified, they are very happy

211
0:15:47.680 --> 0:15:52.740
and you see it's increasing quality of their life but it's not certified.

212
0:15:52.740 --> 0:15:56.760
This is safety critical product not certified and we are not targeting to do the direct

213
0:15:56.760 --> 0:15:58.440
certification of it.

214
0:15:58.440 --> 0:16:02.520
In the first step we are looking into the different levels of the analysis, see what

215
0:16:02.520 --> 0:16:06.640
is involved, what workloads are in there, is there something which could make this fail,

216
0:16:06.640 --> 0:16:11.720
is there a risk in there, what potentially could go wrong here and this is basically

217
0:16:11.720 --> 0:16:17.600
the completion of the use cases and I've drawn this basically together as you can see an

218
0:16:17.600 --> 0:16:23.400
inner part which is very common for almost all the different projects which get fed then

219
0:16:23.400 --> 0:16:28.480
by the use cases feeding and say this is how you need to configure, how you need to specialize

220
0:16:28.480 --> 0:16:34.320
because you cannot create a full safety critical item completely out of the context, you cannot

221
0:16:34.320 --> 0:16:39.880
have this generic safety argumentation, you always need to judge it towards assumed context

222
0:16:39.880 --> 0:16:44.160
and this then turns into Elisa deliverables.

223
0:16:44.160 --> 0:16:49.840
A little bit on another view here you can see also an exemplary system architecture

224
0:16:49.840 --> 0:16:54.320
mainly how we triggered it in the systems work group.

225
0:16:54.320 --> 0:16:59.760
It's not only Linux involved in these latest products so if you come and you of course

226
0:16:59.760 --> 0:17:04.840
in the medical devices open APS system it's a pure raspberry on it.

227
0:17:04.840 --> 0:17:08.840
There is not the direct archers involved if you don't treat the sensor or the insulin

228
0:17:08.840 --> 0:17:15.080
pump as the archers next to it but if you come to more complex products you always need

229
0:17:15.080 --> 0:17:20.360
to face that there are archers involved, there are microcontroller, microprocesses, container

230
0:17:20.360 --> 0:17:25.240
technology come into picture, everybody talks about containers and embedded these days and

231
0:17:25.240 --> 0:17:31.160
also virtualization technologies like Big Zen or the KVM so this is something which

232
0:17:31.160 --> 0:17:39.040
gets in there easily and for this part if you see on the working group site this Linux

233
0:17:39.040 --> 0:17:44.080
features, architecture, code improvement this directly go into the Linux work.

234
0:17:44.080 --> 0:17:50.280
So the main outcome of this is for the Linux ecosystem, the Linux kernel and a lot of this

235
0:17:50.280 --> 0:17:58.080
work is also not directly related to the hypervisor or the archers but there are things also

236
0:17:58.080 --> 0:18:01.540
which going a little bit further like the tools and the engineering process things which

237
0:18:01.540 --> 0:18:07.320
are coming out there may also have a good value for other products which you build on

238
0:18:07.320 --> 0:18:14.120
so if you have a Yachta involved in there you can build a file also with a meter layer

239
0:18:14.120 --> 0:18:19.040
and then it may be good to have this tooling parts in there or also code improvements

240
0:18:19.040 --> 0:18:25.280
can come into picture there, certain tools which we make use of in your CI for testing,

241
0:18:25.280 --> 0:18:30.040
QA system or others this is an element to be considered here and lastly the use cases

242
0:18:30.040 --> 0:18:35.400
to further completeness they basically tailor down this system to whatever you need so for

243
0:18:35.400 --> 0:18:40.320
example in the automotive work we for now tailor the system down for getting a better

244
0:18:40.320 --> 0:18:44.600
Linux kernel understanding and we get rid of the endear originally from the container

245
0:18:44.600 --> 0:18:50.600
the virtualization the archers but we know once we have solved some parts of our work

246
0:18:50.600 --> 0:18:56.880
we need to get the system context and the system context involve all these kind of things

247
0:18:56.880 --> 0:19:03.480
right and saying this we also do a certain outreach to other projects so I put in the

248
0:19:03.480 --> 0:19:09.560
Zephyr community we have the automotive Linux which is already in there there could be other

249
0:19:09.560 --> 0:19:15.120
Linux versions and also strong involvement of the Yachta project and said I didn't know

250
0:19:15.120 --> 0:19:20.960
where to put the SPDX properly on this picture but we see it later on how we interact so

251
0:19:20.960 --> 0:19:27.600
far we already are in discussions with Zephyr and Xen we have weekly meetings also where

252
0:19:27.600 --> 0:19:33.800
Xen members pop up where Zephyr is present with some representative and we saw that these

253
0:19:33.800 --> 0:19:38.920
are safety critical open source projects so they basically save the share the same burden

254
0:19:38.920 --> 0:19:43.400
they need to show how the development process is done how do we guarantee certain quality

255
0:19:43.400 --> 0:19:47.120
levels where is the testing done where are the requirements management and the trace

256
0:19:47.120 --> 0:19:53.640
abilities to everything this is something which pops in there quite good if we take

257
0:19:53.640 --> 0:19:58.760
this architecture and as I'm coming from the automotive part we have different projects

258
0:19:58.760 --> 0:20:04.080
which share these architectural sorts and there is a large group on the Eclipse SDV

259
0:20:04.080 --> 0:20:07.920
project there is a SOF initiative from ARM basically having similar members like the

260
0:20:07.920 --> 0:20:14.240
SDV and then we have a large automotive grade Linux which also is so nice to provide us

261
0:20:14.240 --> 0:20:18.520
with the reference implementation for the automotive use case so they share very similar

262
0:20:18.520 --> 0:20:25.760
architectures lastly not directly related to safety but having safety considerations

263
0:20:25.760 --> 0:20:30.800
in there and being part of the system is the Yachta project for some building part to get

264
0:20:30.800 --> 0:20:36.240
this into a CI reproducible here for example the S-bomb generation suddenly plays into

265
0:20:36.240 --> 0:20:41.160
the game which you can do with the Yachta project and while we were discussing we figured

266
0:20:41.160 --> 0:20:48.440
out that there is also like data needed into system S-bomb and for this we reached out

267
0:20:48.440 --> 0:20:55.480
to the SPDX and there is actually SPDX special interest group on FUSA meeting weekly to extend

268
0:20:55.480 --> 0:21:02.680
this scope there is also later on talk where parts of it get presented why do we do all

269
0:21:02.680 --> 0:21:07.200
this I like this statement from George Bernard Shaw he said if I have an apple and you have

270
0:21:07.200 --> 0:21:11.000
an apple if we exchange the apple we have still one apple but if I have an idea and

271
0:21:11.000 --> 0:21:15.680
you have an idea and we exchange these ideas and we have two ideas and that is basically

272
0:21:15.680 --> 0:21:18.960
where it goes about we need to get a good understanding we need to bring the things

273
0:21:18.960 --> 0:21:26.320
together and by this we of course need to look into certain activities so now we come

274
0:21:26.320 --> 0:21:34.720
into the part what the different work groups do and if we check for example the elements

275
0:21:34.720 --> 0:21:39.640
process tools documentation not every work group acts in the same amount as the others

276
0:21:39.640 --> 0:21:46.200
do so just put some bubbles in here to see where our mainly our work is going so we have

277
0:21:46.200 --> 0:21:49.320
a lot of things of course on the software part that people are interested in the Linux

278
0:21:49.320 --> 0:21:55.200
kernel and the process part is maybe not so strong because it needs to be centralized

279
0:21:55.200 --> 0:22:00.240
and the usage of this process goes into the other work groups so the OSAP the medical

280
0:22:00.240 --> 0:22:04.800
part architecture a little bit also they work on these kind of processes and bring this

281
0:22:04.800 --> 0:22:11.240
into the other work groups tools seem to be pop out on multiple work groups because here

282
0:22:11.240 --> 0:22:15.800
tools are handy tools pop up we bring it into the into repo you might tell about it get

283
0:22:15.800 --> 0:22:21.160
it used and if we want to go into continuous certification at some point of time there

284
0:22:21.160 --> 0:22:25.600
will be a need of having a lot of tool support in there and basically every work group does

285
0:22:25.600 --> 0:22:31.320
documentation I want to give you some examples on this from the process perspective there

286
0:22:31.320 --> 0:22:36.040
is a system theoretic process analysis that the first topic I will tell a little bit more

287
0:22:36.040 --> 0:22:43.040
about so it's the dry stuff about the systems architecture it's not the code level on this

288
0:22:43.040 --> 0:22:47.400
but we figured out when you do this kind of STPA analysis at some point of time you reach

289
0:22:47.400 --> 0:22:51.960
also a level where you need to understand more about the kernel so I'll tell you something

290
0:22:51.960 --> 0:22:57.480
a little bit about the workload tracing which we have done and also here supporting from

291
0:22:57.480 --> 0:23:01.880
the another work group here of a call tree tool that's self not in basically utilizing

292
0:23:01.880 --> 0:23:08.000
tools and approving things but writing something also from scratch and this all then later

293
0:23:08.000 --> 0:23:14.760
on fits into the meta ELISA which is basically the Yachtel layer for the automotive use case

294
0:23:14.760 --> 0:23:19.920
enhancing the automotive grade Linux demo alright we also did something without modification

295
0:23:19.920 --> 0:23:24.080
like the code checker implementation syscall I will not tell that much about it but just

296
0:23:24.080 --> 0:23:30.600
to give some examples of our work on and all our information is public so we are quite

297
0:23:30.600 --> 0:23:35.960
spread it up to GitHub part there's some parts on G drive we do regular blog posts and have

298
0:23:35.960 --> 0:23:39.960
some white papers published so it always depends on whom do you want to have as audience or

299
0:23:39.960 --> 0:23:47.280
readers so we share this is also YouTube channel but I don't judge this as documentation okay

300
0:23:47.280 --> 0:23:56.920
as at first we look into STPA so STPA stands for system theoretic process analysis what's

301
0:23:56.920 --> 0:24:01.640
interesting to see is if you're coming from safety criticality maybe automotive you know

302
0:24:01.640 --> 0:24:09.040
hazard analysis risk assessment FMAs you may grow with watch spreadsheets drawing cases

303
0:24:09.040 --> 0:24:13.640
checking your API interfaces and all these kind of things and the nice thing about the

304
0:24:13.640 --> 0:24:18.520
STPA is you go a little more in a graphical approach like on the left part of the picture

305
0:24:18.520 --> 0:24:25.480
some basics here it's still relatively new I say this because the old analysis part come

306
0:24:25.480 --> 0:24:31.240
from microcontroller worlds up down to the 60 70 I guess 70s is more or less so there

307
0:24:31.240 --> 0:24:35.080
was a long time where a lot of these analysis techniques came in and they haven't been much

308
0:24:35.080 --> 0:24:40.400
improved but the systems which have been a lot of analyzed have increased complexity

309
0:24:40.400 --> 0:24:47.640
and this is something which need to be considered and this system theoretic process on STPA is

310
0:24:47.640 --> 0:24:54.800
able to handle very complex systems the reason for this is that you can start from a quite

311
0:24:54.800 --> 0:24:59.240
broad view and maybe you don't know all the elements so you have something you just get

312
0:24:59.240 --> 0:25:02.880
a name for it you don't know how it really looks like and you have another blob where

313
0:25:02.880 --> 0:25:09.080
you have more details so you can connect all these different blocks and these analysis

314
0:25:09.080 --> 0:25:14.640
will still survive even if you know not the whole block of some specific part yet and

315
0:25:14.640 --> 0:25:19.280
then you will go in a very iterative approach and just go there step by step you figure

316
0:25:19.280 --> 0:25:22.960
something out you go to one level down going deeper into the system figure out that your

317
0:25:22.960 --> 0:25:28.600
assumption didn't hold true so you do these kind of things for the analysis and what's

318
0:25:28.600 --> 0:25:32.920
also good if you have certain analysis it basically looks on an API level it looks under

319
0:25:32.920 --> 0:25:37.880
definitions or so but this one explicitly goes on the system context and it includes

320
0:25:37.880 --> 0:25:45.880
human interaction the human operation and this is also what's not there for other parts

321
0:25:45.880 --> 0:25:48.880
in parallel you directly get a good while you do the analysis you already improve your

322
0:25:48.880 --> 0:25:53.520
documentation you get a good standard standing of the system and you can even if you are

323
0:25:53.520 --> 0:25:58.960
in a QA department so you can even integrate it properly with existing systems model based

324
0:25:58.960 --> 0:26:06.160
approaches the principles of it to get the very very high level it's quite easy there

325
0:26:06.160 --> 0:26:14.520
are four key elements there is the controller on top this one sends a control action to

326
0:26:14.520 --> 0:26:21.680
a controlled process and this provides typically a feedback well that's not enough in the end

327
0:26:21.680 --> 0:26:26.800
there's also important to know that the controlled process as such may also control something

328
0:26:26.800 --> 0:26:33.120
else so that's how things get more grown up and the question now in the end is what could

329
0:26:33.120 --> 0:26:39.680
go wrong what are unsafe control actions you can use these methodology for maybe understanding

330
0:26:39.680 --> 0:26:45.480
how your water pipes flow in a building or how people walk through certain so you can

331
0:26:45.480 --> 0:26:50.560
always attach this to whatever use case you like you always the same approach but for

332
0:26:50.560 --> 0:26:56.600
our case and the main idea of it was for safety criticality for risk assessments and that's

333
0:26:56.600 --> 0:27:01.920
why we say let's look under unsafe control action a little bit of warning and the next

334
0:27:01.920 --> 0:27:08.440
slide is in a way that you will not read it's level one analysis of this open APS use case

335
0:27:08.440 --> 0:27:13.320
and well yeah that's how it looks like in the middle there's the open APS system you

336
0:27:13.320 --> 0:27:19.400
have a view from the top level so it's a developer view it's not the full user view here so you

337
0:27:19.400 --> 0:27:24.120
have infrastructure people if algorithm developer you release the software then they come to

338
0:27:24.120 --> 0:27:30.480
human operator who uses the software installs it further on this goes in the system we don't

339
0:27:30.480 --> 0:27:34.320
know yet what the system is this is what I mean with the very first level you don't care if

340
0:27:34.320 --> 0:27:39.720
it's the Linux system or whatever is an easy just so this is my open APS system and when you

341
0:27:39.720 --> 0:27:45.040
have understood what is your critical part in there how the system context looks like you

342
0:27:45.040 --> 0:27:51.920
may go into the next level and now we zoom in into this open APS system and go on the next level

343
0:27:51.920 --> 0:27:55.880
and in this you see there is an actually a Raspberry Pi involved we know this from the

344
0:27:55.880 --> 0:28:01.960
hardware part and the OS and there's a Raspbian you have an open APS toolkit involved the

345
0:28:01.960 --> 0:28:07.720
actual algorithm this may control the insulin pump the night scout part is also an edge

346
0:28:07.720 --> 0:28:12.960
external command you see all these kind of things and the work group has been on this

347
0:28:12.960 --> 0:28:18.760
level for some time and then try to write down the next level going deeper and then

348
0:28:18.760 --> 0:28:25.960
actually needed support so that's where workload tracing came into picture we used the

349
0:28:25.960 --> 0:28:30.160
mentorship project here and had support so someone fully concentrating on the activity

350
0:28:30.160 --> 0:28:34.960
of workload tracing that's another little table which you can only read therefore the

351
0:28:34.960 --> 0:28:41.960
main things to be known is we use S trace and C scope as the main tools for the analysis

352
0:28:41.960 --> 0:28:46.320
but there are stressors in there like stress and G, Paks, TES and other parts this may depend

353
0:28:46.320 --> 0:28:50.960
on your workload which you use once you are challenged with the system and in this one

354
0:28:50.960 --> 0:28:56.000
the information which is coming in there now our system calls how often are these system

355
0:28:56.000 --> 0:29:00.200
calls coming in the frequency of it which subsystem does they belong to that you know

356
0:29:00.200 --> 0:29:04.720
okay where is my critical parts where is the system call entry point and by this you can

357
0:29:04.720 --> 0:29:09.600
more deep dive into the different system and this causes a lot of refinement into the upper

358
0:29:09.600 --> 0:29:13.200
layers again because now you have iteration and see maybe you have a wrong assumption

359
0:29:13.200 --> 0:29:18.880
but still before everything was correct as you understood now you just improve it related

360
0:29:18.880 --> 0:29:25.240
to this cause of the call tree tool that's something basically rewritten and own part

361
0:29:25.240 --> 0:29:31.920
so the idea was to see here is a system call what else of course what are the ways how

362
0:29:31.920 --> 0:29:36.280
to interact there how to visualize things because if you just see something and grow

363
0:29:36.280 --> 0:29:41.840
through the code you cannot really grab the complexity and this was just the first shots

364
0:29:41.840 --> 0:29:46.520
also here it's not worth the reason but you can see there is a file system part and the

365
0:29:46.520 --> 0:29:52.480
very interesting part is this is quite a static thing so you will see all the potential options

366
0:29:52.480 --> 0:29:56.880
while in the previous view if you have a call if you have the workload tracing you basically

367
0:29:56.880 --> 0:30:01.960
see where has the pass gone but you don't directly uncover the untraced passes and here

368
0:30:01.960 --> 0:30:05.440
you see all the passes but you have the part chance that you meet something completely

369
0:30:05.440 --> 0:30:09.760
irrelevant because you're not on this with your workload and this is a compromise or

370
0:30:09.760 --> 0:30:15.120
complementing element of this and well you get a good insights on the kernel construction

371
0:30:15.120 --> 0:30:22.840
and it can help you to analyze more workload in there right we bring all these things together

372
0:30:22.840 --> 0:30:29.840
in the meta-aliasing instrument cluster it looks like the AGL instrument cluster we saw

373
0:30:29.840 --> 0:30:35.840
this picture before I highlighted the change which we did we write danger in there and

374
0:30:35.840 --> 0:30:41.200
this made us the whole thing safe which well is of course not the full story the full story

375
0:30:41.200 --> 0:30:47.920
is that we just needed a use case to which we can analyze which has safety relevance and

376
0:30:47.920 --> 0:30:52.960
it was a good QT based demo so we could make use of it it was running on QM0.

377
0:30:52.960 --> 0:30:58.760
The kernel has a little drawbacks on this I'll come to this very soon but with this

378
0:30:58.760 --> 0:31:05.760
you can start analysis tracing workloads and also add a watchtruck mechanism yeah watchtruck

379
0:31:06.640 --> 0:31:13.400
would be the next part of it basically what we like use of in a lot of concept is an external

380
0:31:13.400 --> 0:31:18.200
watchtruck even if you don't see it directly in the open APS system for example there's

381
0:31:18.200 --> 0:31:24.480
still an external monitoring involved which gives emergency data if the Raspberry Pi would

382
0:31:24.480 --> 0:31:28.000
do something wrong in the one or the other direction not that it happens but there is

383
0:31:28.000 --> 0:31:33.600
a monitor there which controls which will give a beep or so and inform the user similar

384
0:31:33.600 --> 0:31:40.160
you do it in the automotive case where you have this telltale environment and you want

385
0:31:40.160 --> 0:31:47.160
to have something which is traced in your workload so yeah this challenge response watchtruck

386
0:31:47.840 --> 0:31:52.480
challenge response make basically it's not simply looking for something but it gives

387
0:31:52.480 --> 0:31:58.120
a little challenge to the workload while the workload process process other parts and it

388
0:31:58.120 --> 0:32:03.200
gets a response in there so that you know okay yeah that's really alive and it's not

389
0:32:03.200 --> 0:32:09.600
just replying and it's the demand here comes basically that we for a lot of use cases cannot

390
0:32:09.600 --> 0:32:15.200
fully guarantee that the workload comes in the proper time that a process doesn't hang

391
0:32:15.200 --> 0:32:19.440
and this release a lot of responsibility from you by checking this was an external

392
0:32:19.440 --> 0:32:24.120
workouts and it's mainly looking into the safety critical workload I know there are

393
0:32:24.120 --> 0:32:29.000
ideas to say well let's put this watchtruck thing and let's watch everything in there

394
0:32:29.000 --> 0:32:32.560
this typically doesn't work out so you really concentrate on the things and say this is

395
0:32:32.560 --> 0:32:39.560
safety critical and all the other parts are related to user experience so if you're drawing

396
0:32:40.680 --> 0:32:45.440
rendering engine it and God's lucky and you see a lot of delay and touch screen or whatever

397
0:32:45.440 --> 0:32:50.160
that's nothing which you want to experience from a user perspective but as long as the

398
0:32:50.160 --> 0:32:55.080
warning signs come in time and improper from safety perspective this is all fine so it's

399
0:32:55.080 --> 0:32:59.560
good to split up here between what is the intended functionality what is the safety

400
0:32:59.560 --> 0:33:05.040
criticality of it what do I need to monitor and what not and for this this is just the

401
0:33:05.040 --> 0:33:09.680
safety net in there here I said this is used widely in automotive there are other industries

402
0:33:09.680 --> 0:33:15.080
basically always have your safety net somewhere around which monitor sinks and what we try

403
0:33:15.080 --> 0:33:21.800
to do is we want to get more responsibility to Linux and by this you can start where with

404
0:33:21.800 --> 0:33:28.800
a lot of elements in this safety critical part and yeah so that's the main thing on

405
0:33:29.120 --> 0:33:36.120
this part and the last message is very important for me it's not that you consider your watchtruck

406
0:33:37.960 --> 0:33:44.400
in this design as being there or need to be there you basically start creating your system

407
0:33:44.400 --> 0:33:48.240
that you never need to trigger the watchtruck because you don't want this this is just your

408
0:33:48.240 --> 0:33:54.120
system functionality and it has to work and in best case this gets not triggered into

409
0:33:54.120 --> 0:34:01.120
a safe state for a total use case for example this could mean that the screen is turned

410
0:34:01.920 --> 0:34:05.840
off or that you do a restart basically you would maybe make a black screen or so that

411
0:34:05.840 --> 0:34:10.600
people directly recognize the driver oh it's not going right here that could be also be

412
0:34:10.600 --> 0:34:14.640
be a warning message or what else but depending on what's your safety passes you need to make

413
0:34:14.640 --> 0:34:18.840
sure that this is really also triggered so their safety criticality comes in picture

414
0:34:18.840 --> 0:34:25.840
again I prepared a one minute video but I never know how these kind of things properly

415
0:34:26.240 --> 0:34:33.240
work if you do a demonstration so I just put the YouTube link on the material and if you

416
0:34:33.280 --> 0:34:38.680
are brave enough or even not I guess it's nice a straightforward thing we have a good

417
0:34:38.680 --> 0:34:45.680
documentation how to experience this demo because when we started with the Elisa work

418
0:34:45.680 --> 0:34:52.080
we saw that we basically start building our topics from scratch we documented everything

419
0:34:52.080 --> 0:34:56.840
right good as best on our standing and then someone came and said well but I'm not using

420
0:34:56.840 --> 0:35:02.640
Ubuntu I'm using an open sousa tumbleweed and we figure oh we need a little bit more

421
0:35:02.640 --> 0:35:05.880
maybe that we have more environment set up that people can reproduce things so we came

422
0:35:05.880 --> 0:35:11.440
up with a docker container which basically gets the things packages installed which you

423
0:35:11.440 --> 0:35:15.720
need the right version of it to make it easier for people then the next thing we observed

424
0:35:15.720 --> 0:35:21.720
was oh okay the people do a yukto build yukto consumes a lot of space and a lot of compilation

425
0:35:21.720 --> 0:35:27.280
maybe the cache binaries would be a good option and so we also enabled the estate in there

426
0:35:27.280 --> 0:35:32.240
so that you cannot can now build like in the parts which are still buildable or needed

427
0:35:32.240 --> 0:35:37.760
to be built in a roughly 40 minutes on a poor laptop it's basically depends on your download

428
0:35:37.760 --> 0:35:43.120
speed also right it's quite amount of download which you typically have with the yukto build

429
0:35:43.120 --> 0:35:46.840
on long one we also see if we can extend the two other systems that we are maybe also Debian

430
0:35:46.840 --> 0:35:51.160
version of it or so but for now it's the yukto star the last thing which we figured out there

431
0:35:51.160 --> 0:35:55.800
are also use cases maybe where you want to deep dive into the system and this would be

432
0:35:55.800 --> 0:35:58.920
the complementing part to this demo if you don't want to see the video and you want to

433
0:35:58.920 --> 0:36:06.040
just try it out directly if you have qm on your system installed just download the binaries

434
0:36:06.040 --> 0:36:11.680
directly they get built nightly so really nightly so every night you get a new one it

435
0:36:11.680 --> 0:36:16.000
always goes to the latest version of the AGL with a little bit of problems last week but

436
0:36:16.000 --> 0:36:20.320
it's up and running again does a build check does a boot check so that you can really experience

437
0:36:20.320 --> 0:36:25.240
it and it basically uses the instructions which are written down in the github readme

438
0:36:25.240 --> 0:36:34.960
markdown file right yeah this is about this some next steps the stpa is continued so we're

439
0:36:34.960 --> 0:36:40.960
getting into deeper levels of it we need to see that we get the workload tracing properly

440
0:36:40.960 --> 0:36:45.280
reflected in the different diagrams this was heavily driven by the medical devices work

441
0:36:45.280 --> 0:36:50.560
with the automotive has not used the workload tracing that much but we bring this in there

442
0:36:50.560 --> 0:36:58.560
the call tree also got extended with another tool which was uh ks called ks naf does certain

443
0:36:58.560 --> 0:37:03.600
kernel static navigation tool so to get a better analysis on better view on this um

444
0:37:03.600 --> 0:37:07.440
therefore the meta eliza as i was talking about qm where everybody wants to see real

445
0:37:07.440 --> 0:37:13.240
hardware so we also are on a pass on bringing this on an arm based hardware for now so we

446
0:37:13.240 --> 0:37:18.200
have the x86 in qm with simulation and an arm underneath is mainly driven by systems

447
0:37:18.200 --> 0:37:23.720
work group and what is very important so far this display checking in there so we are not

448
0:37:23.720 --> 0:37:27.080
normally you would check what the rendering of a telltale but there's so many different

449
0:37:27.080 --> 0:37:31.760
kind of implementation so that we mock a lot of things there and we want to improve this

450
0:37:31.760 --> 0:37:37.560
so that we have proper display checks and also a lot of monitoring this is basically

451
0:37:37.560 --> 0:37:45.080
on the four topics which we have seen uh additionally we work on the system as bomb we enabled the

452
0:37:45.080 --> 0:37:51.520
s bomb part for generating material in the demo we want to improve kernel configuration

453
0:37:51.520 --> 0:37:58.480
trimmed on the size of the image then uh have the rt documentation updated have more complex

454
0:37:58.480 --> 0:38:06.080
cluster demo involved and that's mainly it so summarizing what you have seen uh we talked

455
0:38:06.080 --> 0:38:10.720
about the challenges in the beginning basically what a difference between the traditional

456
0:38:10.720 --> 0:38:14.600
safety critical arthas and the new one what's this is what the collaboration can and what

457
0:38:14.600 --> 0:38:21.080
cannot achieve you heard about the goals and the way of the strategy which tools we analyzed

458
0:38:21.080 --> 0:38:26.560
or which which elements we looked into and also then uh you could see how the different

459
0:38:26.560 --> 0:38:30.980
work groups interacted how they put into a system how we outreach to wider community

460
0:38:30.980 --> 0:38:36.600
parts i talk about the contributions of the different work groups what is shared with

461
0:38:36.600 --> 0:38:43.600
the community also in form of usable use case downloadable then you could see methodologies

462
0:38:43.600 --> 0:38:50.960
of our stpa workload tracing and lastly uh we got a little bit of view on what's coming

463
0:38:50.960 --> 0:39:08.080
next and i guess we're good from the time from the questioning part

464
0:39:08.080 --> 0:39:21.280
does anyone have a question there's one above coming down you have a question okay

465
0:39:21.280 --> 0:39:27.520
thanks for the interesting talk uh you mentioned certification uh as one big problem so uh

466
0:39:27.520 --> 0:39:35.720
where uh can we improve things so that certification processes become more open source friendly

467
0:39:35.720 --> 0:39:41.520
and open source software becomes more certification friendly so what has to be done or can be

468
0:39:41.520 --> 0:39:47.400
done there yeah i i guess some part from the certificate so you're asking how can uh open

469
0:39:47.400 --> 0:39:52.720
source and certification come closer to each other from both sides right and um one thing

470
0:39:52.720 --> 0:39:59.400
could be for example done in the documentation in improving tracing down having tools supporting

471
0:39:59.400 --> 0:40:04.280
how do certain features get from the mailing list into the system if there's a test around

472
0:40:04.280 --> 0:40:10.440
it so this gives a lot of confidence and trust in what it's doing um from another perspective

473
0:40:10.440 --> 0:40:15.680
there's not much in the safety integrity standards which allow the usage of pre-existing software

474
0:40:15.680 --> 0:40:20.960
and so for this there's also an iso pass currently which allows more usage i mean depends on

475
0:40:20.960 --> 0:40:25.800
the safety standard which you're in if you're some relaxed medical standards it's less requirements

476
0:40:25.800 --> 0:40:31.800
on this but for automotive it's very strong and prohibitive on this um so i would say

477
0:40:31.800 --> 0:40:37.400
doing careful work and explaining design decisions and so on making this visible and more structured

478
0:40:37.400 --> 0:40:41.840
having maybe centralized bug tracking and so on this this can help a lot from this perspective

479
0:40:41.840 --> 0:40:53.160
it will be good for the certification authorities and we do a lot of clearance also yeah if i

480
0:40:53.160 --> 0:40:58.240
heard you correctly said from supporting the um assessments and authorities in there we

481
0:40:58.240 --> 0:41:04.600
also have uh company support where we really are in the working groups and get from certification

482
0:41:04.600 --> 0:41:08.160
authorities input in the continuous work which we are doing so they are directly working

483
0:41:08.160 --> 0:41:17.720
within the work groups as well yeah chin as well thank you very much for your talk um

484
0:41:17.720 --> 0:41:21.480
i had a i had a just uh quick question i want to get a feel for what your opinion on on

485
0:41:21.480 --> 0:41:29.200
this is um do you think there's space as um a certification for for something like linux

486
0:41:29.200 --> 0:41:33.240
improves can you move the mic a little closer because it's for me i hear the people louder

487
0:41:33.240 --> 0:41:41.320
leaving so just a little sorry yeah oh oh wow yeah see the difference um as as um as

488
0:41:41.320 --> 0:41:47.600
uh process for certification and for validation of uh linux kind of improve and and change

489
0:41:47.600 --> 0:41:52.040
over time do you think there's ever going to be space for for linux to be used in in

490
0:41:52.040 --> 0:41:57.440
kind of a critical component on vehicles or do you think that space is completely reserved

491
0:41:57.440 --> 0:42:04.120
for for something that's actually using real time uh the main part which i heard was if

492
0:42:04.120 --> 0:42:10.200
there's i got the real time part in the end yeah like do you think there is it's already

493
0:42:10.200 --> 0:42:27.960
there fair thank you okay was there someone else have a question you have a question yeah

494
0:42:27.960 --> 0:42:34.320
so what is the place for linux itself in um let's say what does safety integrate and to

495
0:42:34.320 --> 0:42:39.200
integrate the level of linux itself in this model because if i take let's say i saw two

496
0:42:39.200 --> 0:42:45.960
six two six two um there's a v model requirements for development this but linux already has

497
0:42:45.960 --> 0:42:52.960
source code there are no you know there is no coverage this test with all these memcdc

498
0:42:52.960 --> 0:43:00.960
coverage etc etc so what's the place of linux and how to keep it maintain it without forking

499
0:43:00.960 --> 0:43:06.160
yeah so um you say where's the space and the place of linux if you see the v model for

500
0:43:06.160 --> 0:43:09.800
example the iso two six two six two where does things fit in there a lot of demands

501
0:43:09.800 --> 0:43:19.080
like uh car coverage parts tracing and so on so what you can see is that um first of

502
0:43:19.080 --> 0:43:23.520
all speaking about a level you will not directly go to an azl d level which puts much more

503
0:43:23.520 --> 0:43:28.480
requirement on the tools that's for sure so you should start on the lower azl ab level

504
0:43:28.480 --> 0:43:33.320
that's also what we did uh we relaxed some part also for automotive cases let's don't

505
0:43:33.320 --> 0:43:37.800
start with two complex parts maybe get a real-time criticality out there because then you have

506
0:43:37.800 --> 0:43:48.080
to review much more parts and um so the space which i see is that you should argue equivalence

507
0:43:48.080 --> 0:43:53.160
for certain things that you are in close collaboration with successors and explain how things are

508
0:43:53.160 --> 0:43:59.880
done because when the iso was originally prepared it was not considering a complex system as

509
0:43:59.880 --> 0:44:04.680
linux being in use and the large amount of pre-existing software so from this if you

510
0:44:04.680 --> 0:44:09.400
are in an assessment if you are there if you can show and show the credibility by requirements

511
0:44:09.400 --> 0:44:16.440
work by good concepts you may in the first and come up which to system which is arguably

512
0:44:16.440 --> 0:44:24.280
safe but not directly certifiable to your iso two six two part but uh this already showed

513
0:44:24.280 --> 0:44:29.440
you the perfect discussion room also right because then you see well you cannot tell

514
0:44:29.440 --> 0:44:34.640
me this is not working but you still say it's not certifiable and then you see also the

515
0:44:34.640 --> 0:44:39.440
glitch of the standard and if you reach this point you have a lot of good support when

516
0:44:39.440 --> 0:44:43.480
you go with certification authorities early if you have internal assessments and you can

517
0:44:43.480 --> 0:44:48.840
judge it and in the end it's also your responsibility where you say oh i argue foreign equivalence

518
0:44:48.840 --> 0:44:53.720
because it's not saying in this book you have to it says recommended highly recommended

519
0:44:53.720 --> 0:44:59.000
leaving you also trace for showing equivalence to this model i'm using this and on top i'm

520
0:44:59.000 --> 0:45:03.120
adding this and by this you can get an iron and of course getting feedback from your developers

521
0:45:03.120 --> 0:45:10.040
that the work which you're doing also into kernel mainline and so on so maybe also it's

522
0:45:10.040 --> 0:45:17.920
possible to somehow affect how iso two six two six two is developed because it's a bit

523
0:45:17.920 --> 0:45:26.280
outdated in some way some of the members in elisa have people in these iso committees

524
0:45:26.280 --> 0:45:30.920
that are basically taking it back into that direction for the future revs of the standards

525
0:45:30.920 --> 0:45:34.240
i we don't have visibility at least i don't because i'm not in those committees but we

526
0:45:34.240 --> 0:45:39.560
do know that um some of those member companies you saw up there um are there and they are

527
0:45:39.560 --> 0:45:49.820
advocating for things to work a little bit better in future revs okay is there anyone

528
0:45:49.820 --> 0:45:57.760
else who has a question um okay well thank you for your talk

529
0:45:57.760 --> 0:46:24.160
uh

