1
0:00:00.000 --> 0:00:10.800
So, it's nice to see a nice crowd after two years of pandemic.

2
0:00:10.800 --> 0:00:11.800
You're beautiful.

3
0:00:11.800 --> 0:00:20.680
So, today we're going to talk about similarity detection and how we use it in integrity.

4
0:00:20.680 --> 0:00:29.680
Sustainability as a way to ensure that the website is a safe place, that people just

5
0:00:29.680 --> 0:00:33.840
maintain an integral place.

6
0:00:33.840 --> 0:00:36.120
The outline of the presentation is as follows.

7
0:00:36.120 --> 0:00:41.620
We're going to outline the problem, then how we use automation and similarity detection

8
0:00:41.620 --> 0:00:45.120
in order to achieve what we want.

9
0:00:45.120 --> 0:00:50.120
The current technology that we use for images, which is the vector search, then we're going

10
0:00:50.120 --> 0:00:58.280
to discuss in depth what is the actual technology, the vector embedding makes possible to transform

11
0:00:58.280 --> 0:01:01.360
a picture into an element of search.

12
0:01:01.360 --> 0:01:08.600
The current platform offering the metaput is a disposal to allow other people to crowdsource

13
0:01:08.600 --> 0:01:12.840
all of their findings into a centralized place.

14
0:01:12.840 --> 0:01:20.080
And last but not least, what we have of OpenM3 that you can install, you can deploy in your

15
0:01:20.080 --> 0:01:28.320
own site to benefit all these technological findings.

16
0:01:28.320 --> 0:01:34.260
The problem is that any big platform bears the responsibility to ensure it's a safe place

17
0:01:34.260 --> 0:01:35.260
to serve.

18
0:01:35.260 --> 0:01:40.660
No matter what also law says that you have to make sure whatever the user posts, you

19
0:01:40.660 --> 0:01:47.460
are ultimately responsible to make sure that everybody is just not exposed to things that

20
0:01:47.460 --> 0:01:52.080
would violate your community guidelines.

21
0:01:52.080 --> 0:01:57.520
Meta has almost 3 billion users, it's like the last word population.

22
0:01:57.520 --> 0:02:03.240
And although the vast majority of our users follow rules, some fringe bad actors will

23
0:02:03.240 --> 0:02:04.520
always be present.

24
0:02:04.520 --> 0:02:13.800
And at that scale, fringe means tens of millions of bad person creating a lot of problems.

25
0:02:13.800 --> 0:02:21.760
And when I mean issues, problems, I mean child exploitation, imageries, non-consensual, intimate

26
0:02:21.760 --> 0:02:27.440
imagery, which is a way to say revenge porn, all sexual exploitation, people forced to

27
0:02:27.440 --> 0:02:36.400
perform sexual acts in front of camera against the real, terrorism, violence, whatever.

28
0:02:36.400 --> 0:02:43.220
And just to give you a couple of numbers, Meta publishes a transparency report quarterly

29
0:02:43.220 --> 0:02:48.880
about what we do to ensure that the platform stays safe.

30
0:02:48.880 --> 0:02:57.560
And on the second quarter of 2022, we removed 38 million of adult sexual exploitation pieces

31
0:02:57.560 --> 0:02:59.320
of content taken down.

32
0:02:59.320 --> 0:03:03.800
And it's just for this category, child exploitation is not so huge, thank God, but also there

33
0:03:03.800 --> 0:03:07.240
are other like violence, terrorism and stuff.

34
0:03:07.240 --> 0:03:12.720
That accounted for the 0.04% of youth content worldwide.

35
0:03:12.720 --> 0:03:20.680
And in case you were asking, 97% of this content was proactively taken off, even before people

36
0:03:20.680 --> 0:03:23.880
could even see it.

37
0:03:23.880 --> 0:03:29.320
The remaining 2.8% is user reports, like AI found this.

38
0:03:29.320 --> 0:03:34.440
And we take that down also, and we also add to the data banks just to make sure that we

39
0:03:34.440 --> 0:03:36.880
are not forgetting about that.

40
0:03:36.880 --> 0:03:40.320
Sometimes there are false positives because it's just unavoidable.

41
0:03:40.320 --> 0:03:48.360
And half million was restored upon user appeal, and we restore accounts and mostly accounts

42
0:03:48.360 --> 0:03:51.080
and the pictures that we are banned for.

43
0:03:51.080 --> 0:03:56.000
It goes by itself that the sheer volume of content, the huge scale, the problem we are

44
0:03:56.000 --> 0:04:04.840
facing requires both automation and also human review to ensure either accuracy, both accuracy

45
0:04:04.840 --> 0:04:06.560
and also consistency.

46
0:04:06.560 --> 0:04:12.120
Because it will be a problem if we had 1 million people clicking and making decisions, and

47
0:04:12.120 --> 0:04:16.400
what is valid for one is not for the other.

48
0:04:16.400 --> 0:04:22.440
And we cannot just employ automation, because otherwise we will have this very powerful

49
0:04:22.440 --> 0:04:27.800
site decapitating everybody, also innocent users.

50
0:04:27.800 --> 0:04:33.960
So the role of automation and similarity detection, the thing is that a lot of the

51
0:04:33.960 --> 0:04:38.360
things that happen online are things that are being repeated.

52
0:04:38.360 --> 0:04:43.720
So are things that already occurred in the past, like people posting a picture of some

53
0:04:43.720 --> 0:04:50.240
shooting, some mass shooting, for example, like the buffalo or the Christchurch, gets

54
0:04:50.240 --> 0:04:54.800
taken down, and 10 more accounts spawn and post the same things.

55
0:04:54.800 --> 0:05:04.240
So it's very efficient to reason in terms of let's just redo the things that we already

56
0:05:04.240 --> 0:05:06.600
found that have worked.

57
0:05:06.600 --> 0:05:11.680
We employ automation to scale, of course, handle the scale of the problem, and to consistently

58
0:05:11.680 --> 0:05:17.280
repeat a decision that a human reviewer has already vatted in the past.

59
0:05:17.280 --> 0:05:24.080
So we tie a content to a decision, a violating content to a decision, let's act upon this.

60
0:05:24.080 --> 0:05:27.840
And we tie the decision to the actions.

61
0:05:27.840 --> 0:05:33.200
Let's just repeat this action every time we meet a piece of content that triggered this

62
0:05:33.200 --> 0:05:34.680
same decision.

63
0:05:34.680 --> 0:05:38.720
We do that for videos, for pictures, and also for text.

64
0:05:38.720 --> 0:05:44.400
Today we'll be mostly talking about images because the techniques for video and pictures

65
0:05:44.400 --> 0:05:46.360
are somewhat very similar.

66
0:05:46.360 --> 0:05:53.320
Text has a completely different array of techniques that will not be presented today.

67
0:05:53.320 --> 0:05:59.480
So a way to, if you want to achieve a similarity at action, you have to come up with a way

68
0:05:59.480 --> 0:06:02.080
to achieve similarity first.

69
0:06:02.080 --> 0:06:05.280
So how do we compare to pictures?

70
0:06:05.280 --> 0:06:09.440
Of course, we are not doing pixel by pixel comparison.

71
0:06:09.440 --> 0:06:11.600
We want to be much faster.

72
0:06:11.600 --> 0:06:17.040
A way to do that is just, okay, let's just MD5 hash all the pictures, or SHA1 all the

73
0:06:17.040 --> 0:06:22.480
pictures, and then we store them somewhere in an indexing system.

74
0:06:22.480 --> 0:06:29.760
And whenever a new picture comes in, we just recreate the hash, and if it matches, we just

75
0:06:29.760 --> 0:06:30.760
ban, right?

76
0:06:30.760 --> 0:06:36.400
Well, that doesn't work very well because the cryptographic hashes are not resistant

77
0:06:36.400 --> 0:06:44.600
to resizing rotation, one pixel alteration, all the hash changes altogether.

78
0:06:44.600 --> 0:06:51.720
Instead we can really benefit from local hashing because it allows for similarity measurement.

79
0:06:51.720 --> 0:06:59.040
Like you change slightly one piece, one portion of the image, and the hash changes a little,

80
0:06:59.040 --> 0:07:00.040
but not completely.

81
0:07:00.040 --> 0:07:06.240
Then you can reason in terms of distance between two hashes.

82
0:07:06.240 --> 0:07:11.360
So you have to find a way to turn an image into a vector, and then you perform a vector

83
0:07:11.360 --> 0:07:12.360
search.

84
0:07:12.360 --> 0:07:18.480
Whenever two vectors are very, very close beyond a certain threshold, then it's probably

85
0:07:18.480 --> 0:07:19.760
a match.

86
0:07:19.760 --> 0:07:24.020
And just in case if you're asking, this is your base as the architecture.

87
0:07:24.020 --> 0:07:29.000
You have more or less all the architectures shared these four stages.

88
0:07:29.000 --> 0:07:35.440
Observation, an image has been generated, usually push-shaven-like, user uploaded something.

89
0:07:35.440 --> 0:07:42.360
Then you have the representation phase in which you hash the image to a compact representation.

90
0:07:42.360 --> 0:07:46.400
If you're indexing, you store data into your index.

91
0:07:46.400 --> 0:07:52.000
Instead if you are at inference time, like an event, someone uploaded something, you

92
0:07:52.000 --> 0:07:55.200
search the index that you've built with representation.

93
0:07:55.200 --> 0:08:01.320
In case you have a match, you action upon what you decide what to do with the match

94
0:08:01.320 --> 0:08:03.120
you got.

95
0:08:03.120 --> 0:08:07.120
Usually the idea is that this is very close to an image that I already see in the past

96
0:08:07.120 --> 0:08:10.200
that was abandoned and also the account was taken down.

97
0:08:10.200 --> 0:08:14.480
Do the same to this user.

98
0:08:14.480 --> 0:08:19.240
So first free piece of content.

99
0:08:19.240 --> 0:08:26.800
Facebook has released a library, which is FICE, the Facebook I similarity search library.

100
0:08:26.800 --> 0:08:34.040
It's a library to do similarity search over a vector of dense vectors, so vector floats

101
0:08:34.040 --> 0:08:36.400
or integers, for example.

102
0:08:36.400 --> 0:08:39.760
You can think about it like a C++ version of Lucene.

103
0:08:39.760 --> 0:08:45.240
So you index stuff, puts that in a very big space, and you can search in this space very

104
0:08:45.240 --> 0:08:46.240
fast.

105
0:08:46.240 --> 0:08:51.720
It supports CUDA, so you can use your GPUs to search.

106
0:08:51.720 --> 0:08:54.680
It's basically an index on steroids.

107
0:08:54.680 --> 0:08:57.960
And it's C++, but it has Python bindings available.

108
0:08:57.960 --> 0:09:00.120
And it scales almost nearly.

109
0:09:00.120 --> 0:09:08.720
You can really index 100 millions of pieces on a single machine and it just handles them

110
0:09:08.720 --> 0:09:09.720
really.

111
0:09:09.720 --> 0:09:11.560
It doesn't need to saturate all the memory.

112
0:09:11.560 --> 0:09:16.660
So it has a very good optimization properties that makes it very good too.

113
0:09:16.660 --> 0:09:21.440
And you can go and download that on GitHub.

114
0:09:21.440 --> 0:09:27.920
Today we are also mostly referring to with the perceptual hashing.

115
0:09:27.920 --> 0:09:31.520
This means that we are reasoning in terms of colors.

116
0:09:31.520 --> 0:09:33.220
Colors and images, shapes.

117
0:09:33.220 --> 0:09:36.720
We are not reasoning about what's happening inside the image.

118
0:09:36.720 --> 0:09:43.240
That sees the semantic hashing, which we are not going to talk about today.

119
0:09:43.240 --> 0:09:46.600
Perceptible hashing just captures visual similarities.

120
0:09:46.600 --> 0:09:52.160
It's very nice for a use case because it exactly does its job.

121
0:09:52.160 --> 0:09:58.960
So you might think that we are all talking about machine learning systems that come up

122
0:09:58.960 --> 0:10:03.160
with very clever representations about our pictures.

123
0:10:03.160 --> 0:10:07.200
And I'm asking, do we really need a convnet for that?

124
0:10:07.200 --> 0:10:09.440
Do we really need to employ GPUs?

125
0:10:09.440 --> 0:10:13.260
You already said that it's some CUDA, so perhaps that's a nice hint.

126
0:10:13.260 --> 0:10:15.120
But absolutely not.

127
0:10:15.120 --> 0:10:18.400
Most of this technology is like hashing technology.

128
0:10:18.400 --> 0:10:23.600
So they just compute and represent a mathematical transformation over the image.

129
0:10:23.600 --> 0:10:24.600
It's really fast.

130
0:10:24.600 --> 0:10:25.600
And it's really cheap.

131
0:10:25.600 --> 0:10:29.100
And it can be executed almost everywhere.

132
0:10:29.100 --> 0:10:35.240
So a little bit of history, the first very notable example.

133
0:10:35.240 --> 0:10:39.440
It comes from a source that nobody would have thought about.

134
0:10:39.440 --> 0:10:42.400
It's Microsoft in 2009.

135
0:10:42.400 --> 0:10:45.400
Microsoft invents photo DNA.

136
0:10:45.400 --> 0:10:50.640
Photo DNA is the first algorithm employed in fight against exploitive images of children.

137
0:10:50.640 --> 0:10:54.520
So pedagornography.

138
0:10:54.520 --> 0:11:04.040
It transforms a picture into an ash of 144 unsigned integers on 8-bit representation.

139
0:11:04.040 --> 0:11:05.680
It's proprietary.

140
0:11:05.680 --> 0:11:15.760
So Microsoft licenses this to any nonprofit or an organization that wants to fight exploitive

141
0:11:15.760 --> 0:11:16.760
images of children.

142
0:11:16.760 --> 0:11:19.520
It gives you a license you can use for that and nothing else.

143
0:11:19.520 --> 0:11:23.040
But I cannot disclose the details of how that works.

144
0:11:23.040 --> 0:11:25.640
It can be used only for that.

145
0:11:25.640 --> 0:11:30.440
But Microsoft donated the photo DNA to the national center for the missing and exploited

146
0:11:30.440 --> 0:11:31.440
children, the NACMAQ.

147
0:11:31.440 --> 0:11:39.200
It's this American nonprofit that basically acts as a coordination center in the global

148
0:11:39.200 --> 0:11:42.040
fight against this phenomenon.

149
0:11:42.040 --> 0:11:47.880
And shares this library with anyone that wants to integrate.

150
0:11:47.880 --> 0:11:50.040
This I cannot talk about how this works.

151
0:11:50.040 --> 0:11:54.520
This is the only moment in which I will say something like that.

152
0:11:54.520 --> 0:12:00.120
But we can talk about a resource counterpart that almost ten years later Facebook releases

153
0:12:00.120 --> 0:12:01.120
PDQ.

154
0:12:01.120 --> 0:12:08.240
PDQ stands for Perceptual Algorithm using Discrete Cosine Transform and gives a quality

155
0:12:08.240 --> 0:12:09.240
metric.

156
0:12:09.240 --> 0:12:10.240
It's a very, very bad acronym.

157
0:12:10.240 --> 0:12:13.440
But we need a three-letter acronym.

158
0:12:13.440 --> 0:12:17.960
It creates a 256-bit hash.

159
0:12:17.960 --> 0:12:20.680
It's hamming distance to compute distance.

160
0:12:20.680 --> 0:12:21.960
It's really fast.

161
0:12:21.960 --> 0:12:27.640
The compute overhead is negligible compared to discrete.

162
0:12:27.640 --> 0:12:30.940
Can tolerate some level of adversity.

163
0:12:30.940 --> 0:12:35.800
This means that you change the image because you want to fool the systems in that this

164
0:12:35.800 --> 0:12:38.600
image is not something which is well known.

165
0:12:38.600 --> 0:12:43.880
PDQ can resist a little to this manipulation but not to all of them.

166
0:12:43.880 --> 0:12:48.880
It's used in stopncii.org.

167
0:12:48.880 --> 0:12:54.120
It's a website where people in case you have a fight with your experience and is threatening

168
0:12:54.120 --> 0:12:59.080
to publish your intimate imagery, you go to stopncii.org.

169
0:12:59.080 --> 0:13:02.960
You upload your intimate imagery.

170
0:13:02.960 --> 0:13:05.160
Fingerprints get taken.

171
0:13:05.160 --> 0:13:08.280
Original images get deleted right away, of course.

172
0:13:08.280 --> 0:13:15.920
And these fingerprints are shared with partners that, okay, if I am going to see these fingerprints

173
0:13:15.920 --> 0:13:19.640
in my website, my platform, I'm going to take them down.

174
0:13:19.640 --> 0:13:24.600
So it's a crowdsource effort and uses PDQ for images.

175
0:13:24.600 --> 0:13:26.400
How does that work?

176
0:13:26.400 --> 0:13:33.720
So PDQ hashing is optionally scaled down to a square image.

177
0:13:33.720 --> 0:13:35.840
Then you compute the luminance.

178
0:13:35.840 --> 0:13:42.240
Luminance is the idea that you take the pixel that contributes most in the RGB channel.

179
0:13:42.240 --> 0:13:45.080
Instead of putting black and white, you use the luminance.

180
0:13:45.080 --> 0:13:47.360
It's just another procedure.

181
0:13:47.360 --> 0:13:51.400
And the idea is that the luminance gives you better information about what was the channel

182
0:13:51.400 --> 0:13:58.240
that was contributing most to the color or to the light in that place.

183
0:13:58.240 --> 0:14:03.960
Then you down sample to 64 times 64 using a blur filter.

184
0:14:03.960 --> 0:14:09.200
And the idea of the blur filter, the tent filter, is that it gets the most significant

185
0:14:09.200 --> 0:14:12.000
value in that region.

186
0:14:12.000 --> 0:14:16.400
Because if you keep convulating a pixel with your neighborhood, what you will have in the

187
0:14:16.400 --> 0:14:19.960
end will be the highest value.

188
0:14:19.960 --> 0:14:26.600
So you obtain a representation which is compact and retains the most significant information.

189
0:14:26.600 --> 0:14:31.360
Then you divide the images in 16 times 16 boxes, each one by 4 pixels.

190
0:14:31.360 --> 0:14:35.200
You calculate a discrete causing transform of each box.

191
0:14:35.200 --> 0:14:40.720
The discrete causing transform, so the box is the 4-bar color there.

192
0:14:40.720 --> 0:14:47.120
You see that the grid with a lot of wobbly images, that is a discrete causing transform.

193
0:14:47.120 --> 0:14:54.660
The idea is that any image, any signal can be represented as a sum of causing signals.

194
0:14:54.660 --> 0:14:57.520
You only take the signal, the most significant one.

195
0:14:57.520 --> 0:14:59.920
So it's a form of compression, actually.

196
0:14:59.920 --> 0:15:09.120
And you take the most significant coefficient for the biggest causing you have.

197
0:15:09.120 --> 0:15:12.040
And then you calculate if the median is above a certain value.

198
0:15:12.040 --> 0:15:13.820
Then it's 1, otherwise it's 0.

199
0:15:13.820 --> 0:15:20.880
So you get this 256 in array of 010101 in case this pixel were a high luminance or a

200
0:15:20.880 --> 0:15:23.240
low luminance.

201
0:15:23.240 --> 0:15:27.600
The DCT provides a spectral hashing property.

202
0:15:27.600 --> 0:15:31.720
What is the point in the images that contributes more or less?

203
0:15:31.720 --> 0:15:36.840
You have an hashing space which is 2 to the power of 1 to 28 because it's half the hashes,

204
0:15:36.840 --> 0:15:41.200
because half is always 0, half is always 1.

205
0:15:41.200 --> 0:15:47.800
To search, you just do a vector search against what you've just created.

206
0:15:47.800 --> 0:15:53.120
In case we want, we can use partially the same technology to do video hashing.

207
0:15:53.120 --> 0:15:57.880
And this is another, it comes in almost the same paper.

208
0:15:57.880 --> 0:16:00.440
The TNK is a temporary matching kernel.

209
0:16:00.440 --> 0:16:10.600
It's a way to use the PDQ creation to do a video similarity detection algorithm.

210
0:16:10.600 --> 0:16:12.920
It produces fixed length video hashes.

211
0:16:12.920 --> 0:16:20.560
So your hash stays the same length, which is like a 256 kilobyte, if I'm wrong.

212
0:16:20.560 --> 0:16:25.720
Even if your video lasts for 3 hours or 30 seconds, it just produces a fixed length.

213
0:16:25.720 --> 0:16:27.560
So it's really nice.

214
0:16:27.560 --> 0:16:30.560
What you do is that you resample a video to 15 frames.

215
0:16:30.560 --> 0:16:35.280
Then you compute the PDQ without the 01 quantization.

216
0:16:35.280 --> 0:16:36.880
So you keep the float numbers.

217
0:16:36.880 --> 0:16:40.320
That's why it's called PDQF, PDQ float.

218
0:16:40.320 --> 0:16:47.920
And then you compute the average of the old descriptors that you have within various periods

219
0:16:47.920 --> 0:16:49.560
of the cuisine and scene.

220
0:16:49.560 --> 0:16:52.360
Why we add the cuisine curves?

221
0:16:52.360 --> 0:17:00.000
Because a cuisine or a scene adds this wobbly movement that tells you whether a frame is

222
0:17:00.000 --> 0:17:06.480
before or later in the near surroundings, the near neighborhood of the frames.

223
0:17:06.480 --> 0:17:10.760
So in case you have like 10 pictures, you add this cuisine signal.

224
0:17:10.760 --> 0:17:16.460
You know this picture is before this one because you see the cuisine curve which is going up

225
0:17:16.460 --> 0:17:17.600
and going down.

226
0:17:17.600 --> 0:17:25.360
And it's a nice uniqueness fingerprinting time signature algorithm to add a cuisine.

227
0:17:25.360 --> 0:17:31.080
So you compute the average of all the frames, the PDQF for all the frames, with various

228
0:17:31.080 --> 0:17:33.160
periods, various scene and cuisine.

229
0:17:33.160 --> 0:17:35.040
And then you pack them all together.

230
0:17:35.040 --> 0:17:38.380
And you have these like five or six averages.

231
0:17:38.380 --> 0:17:42.800
And that's your PDQF embedding.

232
0:17:42.800 --> 0:17:49.000
Everything is just you compare first the vector zero, which is the average of all the frames

233
0:17:49.000 --> 0:17:51.880
and doesn't retain the temporal signature.

234
0:17:51.880 --> 0:17:57.280
Then if there is a match, you compare also all the other vectors at different periods,

235
0:17:57.280 --> 0:18:01.160
which are level two action as the time signature.

236
0:18:01.160 --> 0:18:04.560
And so you can really be sure that the videos are really the same.

237
0:18:04.560 --> 0:18:09.600
Because if you find the same averages with the same periods, it must be the same video.

238
0:18:09.600 --> 0:18:13.720
It's nice that it's resistant to resampling because you always resample.

239
0:18:13.720 --> 0:18:18.000
So in some way, if you vary the frame rate, the video will change.

240
0:18:18.000 --> 0:18:19.760
And MD5 hash will change.

241
0:18:19.760 --> 0:18:22.120
But this one is not full.

242
0:18:22.120 --> 0:18:28.680
Hashing is really slow because you have to do a transcoding of all the videos first.

243
0:18:28.680 --> 0:18:33.440
And then you have to read all the frames and compute the PDQ for every frame.

244
0:18:33.440 --> 0:18:36.840
But search is actually very fast.

245
0:18:36.840 --> 0:18:39.920
Another nice hashing technique that we have is the video MD5.

246
0:18:39.920 --> 0:18:43.600
I said that we will not be using a crypto-ashers highlight.

247
0:18:43.600 --> 0:18:46.440
We use crypto-ashers, but just for videos.

248
0:18:46.440 --> 0:18:50.960
This because if you take MD5 of video and find exact copies, it's really cheap in this

249
0:18:50.960 --> 0:18:51.960
way.

250
0:18:51.960 --> 0:18:58.400
A lot of actors just post unmodified, repost unmodified content.

251
0:18:58.400 --> 0:19:04.320
They're not going really through the hassle of the do-or-encoding just to try to fool

252
0:19:04.320 --> 0:19:05.320
the systems.

253
0:19:05.320 --> 0:19:07.280
Just try to repost again.

254
0:19:07.280 --> 0:19:09.760
So the MD5 actually works.

255
0:19:09.760 --> 0:19:14.600
And it can be done with vector search if we use the bytes for the MD5 algorithm.

256
0:19:14.600 --> 0:19:20.880
And it's used widely in stopnci.org also.

257
0:19:20.880 --> 0:19:27.440
In 2022, Facebook has released the video PDQ, which is a different algorithm from the former

258
0:19:27.440 --> 0:19:28.840
one.

259
0:19:28.840 --> 0:19:33.880
Hashing is we hash every frame to a PDQ asher, and we just pack the list.

260
0:19:33.880 --> 0:19:36.120
It's much bigger.

261
0:19:36.120 --> 0:19:39.680
It's not slower than the other one.

262
0:19:39.680 --> 0:19:46.180
But it has a nice property that we just have to search for individual frames.

263
0:19:46.180 --> 0:19:50.140
So we treat the problem as a back-of-word approach.

264
0:19:50.140 --> 0:19:55.320
So we just put all these frames inside an index library.

265
0:19:55.320 --> 0:20:00.360
Then we search, and we take all the candidates, and we do a pairwise comparison.

266
0:20:00.360 --> 0:20:05.480
If the pairwise comparison is successful beyond a certain threshold, then it's a match.

267
0:20:05.480 --> 0:20:12.080
And also this you get for free, and it's released along with the PDQ, along with the TMK, PDQF.

268
0:20:12.080 --> 0:20:20.080
All this is available inside the Facebook research GitHub repository.

269
0:20:20.080 --> 0:20:22.840
What do you do once you have these hashes?

270
0:20:22.840 --> 0:20:25.800
So your platform is computing the hashes.

271
0:20:25.800 --> 0:20:30.500
Well, it's the first time that you see this content, but perhaps older actors have already

272
0:20:30.500 --> 0:20:32.680
seen this content too.

273
0:20:32.680 --> 0:20:36.520
Well, you upload them to the threat exchange platform.

274
0:20:36.520 --> 0:20:42.400
NACMAQ shares the PDNA hashes I told you with all companies that are asking for them.

275
0:20:42.400 --> 0:20:48.160
So can you please tell me where this picture that someone uploaded is a matching NACMAQ?

276
0:20:48.160 --> 0:20:52.440
So I already know that this is something I should call the law enforcement.

277
0:20:52.440 --> 0:20:59.400
Data does the equivalent, but for the PDQ, because it has much less friction to adopt

278
0:20:59.400 --> 0:21:02.640
PDQ compared to the PDNA.

279
0:21:02.640 --> 0:21:06.280
There's a team, the Internet Safety Engineering, that builds and operates all these services

280
0:21:06.280 --> 0:21:15.960
where anyone can upload fingerprints, and so you can crowdsource a big graph of matches.

281
0:21:15.960 --> 0:21:23.680
The rest API to access and post new data has multi-language clients, uses PDQ, and users

282
0:21:23.680 --> 0:21:25.040
can also download the data.

283
0:21:25.040 --> 0:21:27.800
You're not forced to stay online, stay connected.

284
0:21:27.800 --> 0:21:33.080
You can just request for a dump of the database, and you can search it.

285
0:21:33.080 --> 0:21:40.040
And you find all the data and all the APIs at the GitHub page.

286
0:21:40.040 --> 0:21:49.720
In 2020, Facebook also has released its most advanced algorithm to spot similar images,

287
0:21:49.720 --> 0:21:52.880
the SIM SearchNet++.

288
0:21:52.880 --> 0:22:01.960
This is an error network, and it is capable of facing adversarial manipulation that the

289
0:22:01.960 --> 0:22:05.920
other embeddings just are not able to.

290
0:22:05.920 --> 0:22:12.760
Unfortunately, SIM SearchNet is proprietary, so I cannot really talk about that.

291
0:22:12.760 --> 0:22:21.800
But we have a cousin product, SSCD, the SIM Search Copy Detection, or Simulagty Search

292
0:22:21.800 --> 0:22:25.720
Copy Detection, which is open source and free.

293
0:22:25.720 --> 0:22:27.600
So I can really talk about that.

294
0:22:27.600 --> 0:22:34.680
They are somewhat related in some technological principles, so I can really talk about this.

295
0:22:34.680 --> 0:22:39.520
So this is a PyTorch-based model.

296
0:22:39.520 --> 0:22:46.200
So the problem that this state-of-the-art product is trying to solve is what happens

297
0:22:46.200 --> 0:22:54.880
if I take a picture and I put a caption on it, alternating so many pixels everywhere.

298
0:22:54.880 --> 0:23:00.720
A PDQ or a PDNA hash will be altered dramatically.

299
0:23:00.720 --> 0:23:07.920
Is there anything we can do to teach a computer to just ignore all the captions, all the rotations,

300
0:23:07.920 --> 0:23:11.640
all the jitters, all the cropping of the image?

301
0:23:11.640 --> 0:23:12.640
Yes, there is.

302
0:23:12.640 --> 0:23:17.280
A person is able to do that, so we can teach a computer to do that too.

303
0:23:17.280 --> 0:23:19.200
So models and code are available.

304
0:23:19.200 --> 0:23:24.040
What is now available is the training data that we use to create a model, of course.

305
0:23:24.040 --> 0:23:31.720
For those which are into deep learning, it's a ResNet-50 convolative neural network.

306
0:23:31.720 --> 0:23:37.320
And the novelty of the approach is that it's based on our MAC vocabularies.

307
0:23:37.320 --> 0:23:44.120
A regional MAC, for those of you who know how a convolative network works, raise your

308
0:23:44.120 --> 0:23:45.120
hand.

309
0:23:45.120 --> 0:23:47.620
Okay, fine, very good.

310
0:23:47.620 --> 0:23:53.800
So it's a network for the others that looks at the image, looks at portions of the image.

311
0:23:53.800 --> 0:23:59.680
Each neuron looks at a different portion, and then they pass what they have understood

312
0:23:59.680 --> 0:24:04.800
to a higher level series of neurons, the higher and the higher and the higher, until the last

313
0:24:04.800 --> 0:24:10.640
layer of neurons has a very wide overview of the whole picture.

314
0:24:10.640 --> 0:24:17.240
In this case, we are using the maximum activation of all the channels that we have.

315
0:24:17.240 --> 0:24:24.960
So we take note which are the regions of our Carnaug maps for every different channel,

316
0:24:24.960 --> 0:24:28.840
which across all channels have the maximum activation.

317
0:24:28.840 --> 0:24:34.800
If you have 10 channels, and that region across all the different channels, all of them, you

318
0:24:34.800 --> 0:24:36.480
have a maximum activation.

319
0:24:36.480 --> 0:24:39.320
That means that that area is an area of interest.

320
0:24:39.320 --> 0:24:45.660
So we use these areas of interest as a word in a vocabulary.

321
0:24:45.660 --> 0:24:52.220
So exactly when you do the cosine similarity search for documents, you take all the words,

322
0:24:52.220 --> 0:24:57.760
you index all the words, you say these documents as these words, so it's like a vector of words.

323
0:24:57.760 --> 0:25:05.080
And then we try to see which are the vectors that have the most words in common, and put

324
0:25:05.080 --> 0:25:07.160
in the same place.

325
0:25:07.160 --> 0:25:10.700
We do the same things, but for portions of the image.

326
0:25:10.700 --> 0:25:13.120
So we use the R-MAX.

327
0:25:13.120 --> 0:25:17.280
The idea is that it's a self-supervised system also.

328
0:25:17.280 --> 0:25:25.240
So it means that it's trained to recognize augmented input, and it's trained to match

329
0:25:25.240 --> 0:25:28.320
an input to its augmented version.

330
0:25:28.320 --> 0:25:30.720
So what we do is that we take the training set.

331
0:25:30.720 --> 0:25:32.240
We repeat a lot of augmentation.

332
0:25:32.240 --> 0:25:37.880
We add the captions, the random, we rotate, we flip, we alter the colors.

333
0:25:37.880 --> 0:25:46.560
For example, if you do one degree of whitening, you make the image brighter, which is you

334
0:25:46.560 --> 0:25:49.440
add plus one to all the pixels in the image.

335
0:25:49.440 --> 0:25:51.520
You are altering all the pixels.

336
0:25:51.520 --> 0:25:56.760
But in this case, the PDQ ash is capable of understanding the difference.

337
0:25:56.760 --> 0:26:01.240
That's a very weak form of adversarial attack, because the PDQ just computes the difference

338
0:26:01.240 --> 0:26:03.520
between regions, so it's not going to be fooled.

339
0:26:03.520 --> 0:26:09.280
But you can be much more violent and put just a spot color somewhere, and PDQ is going to

340
0:26:09.280 --> 0:26:10.280
be fooled by that.

341
0:26:10.280 --> 0:26:12.440
Then you do through the CNN.

342
0:26:12.440 --> 0:26:19.480
You do a thing called gem pool, which means that you do a generative mean pooling, a generalization

343
0:26:19.480 --> 0:26:22.840
of the average pooling, in case you were wondering.

344
0:26:22.840 --> 0:26:30.400
Then you go, and at the end, you use entropy-oriented loss function.

345
0:26:30.400 --> 0:26:39.360
This means that we want to encourage the network to spread the representation of training data

346
0:26:39.360 --> 0:26:46.880
along all different places, because we want to maximize the distance between all the training

347
0:26:46.880 --> 0:26:48.140
examples in the training set.

348
0:26:48.140 --> 0:26:52.440
So you get a nice uniform search space.

349
0:26:52.440 --> 0:26:57.520
When you add in Frankenstein, you do the same with CNN, and then you obtain a vector, which

350
0:26:57.520 --> 0:26:59.760
is a representation of an image.

351
0:26:59.760 --> 0:27:06.680
And the idea is that there is a distance that you can compute between the data set of the

352
0:27:06.680 --> 0:27:08.440
reference images.

353
0:27:08.440 --> 0:27:14.960
Of course, you can subtract a background data set that was used generally to augment the

354
0:27:14.960 --> 0:27:15.960
images.

355
0:27:15.960 --> 0:27:21.120
But in this case, what you obtain in the end is that the score of the augmented image is

356
0:27:21.120 --> 0:27:27.400
almost the same of the non-augmented version, because it just learns to ignore the places

357
0:27:27.400 --> 0:27:30.480
which are not organic in the image.

358
0:27:30.480 --> 0:27:34.400
And SSCD is freely available.

359
0:27:34.400 --> 0:27:37.120
You can download that and start playing.

360
0:27:37.120 --> 0:27:42.600
You find both code and models, as I already said, but not the training data.

361
0:27:42.600 --> 0:27:47.160
And by the way, Facebook has also announced an image similarity challenge.

362
0:27:47.160 --> 0:27:52.320
You have to determine whether a query image is a modified copy of any image in a reference

363
0:27:52.320 --> 0:27:53.840
corpus of one million.

364
0:27:53.840 --> 0:28:01.160
This is very similar to the Netflix recommendation challenge, where you had to recommend the

365
0:28:01.160 --> 0:28:05.520
movies and you had to beat Netflix algorithm.

366
0:28:05.520 --> 0:28:10.640
And this image similarity challenge and also the meta-IE video similarity challenge, which

367
0:28:10.640 --> 0:28:14.520
is two tracks.

368
0:28:14.520 --> 0:28:18.880
Generate a useful vector representation for a video.

369
0:28:18.880 --> 0:28:24.000
And also try to find a reference video into this very big corpus.

370
0:28:24.000 --> 0:28:26.480
And you don't have to only find a video.

371
0:28:26.480 --> 0:28:36.240
You have to find a clip, so a super portion of a video, into a very big corpus.

372
0:28:36.240 --> 0:28:43.480
And last but not least, since the last part of a donor is the TASTEAR-1, we have your

373
0:28:43.480 --> 0:28:49.520
turnkey open source solution that you can install in your own premise.

374
0:28:49.520 --> 0:28:51.680
The HASHR-MATCHR actioner.

375
0:28:51.680 --> 0:28:57.720
HMA is an open source turnkey safety solution.

376
0:28:57.720 --> 0:29:03.520
So you just download it, install it, and it starts working right away.

377
0:29:03.520 --> 0:29:09.920
What it does is that it scans the images that you want to push towards it.

378
0:29:09.920 --> 0:29:16.320
It has an index that is updated with all the hashes coming from thread exchange, but also

379
0:29:16.320 --> 0:29:18.400
from yours.

380
0:29:18.400 --> 0:29:26.200
And it's able to say, to bind banks, verticals of violations.

381
0:29:26.200 --> 0:29:29.700
You might have non-severe violation or very severe violation.

382
0:29:29.700 --> 0:29:34.720
You might decide that for non-severe violation, you just delete the content and send a warning.

383
0:29:34.720 --> 0:29:41.120
Or for high severity violation, you just immediately delete the content, shut down the account

384
0:29:41.120 --> 0:29:45.960
of the poster, and you also signal it to the law enforcement.

385
0:29:45.960 --> 0:29:47.960
You can do that also.

386
0:29:47.960 --> 0:29:53.680
And you can configure actions in a backend that are tied to the content that you want

387
0:29:53.680 --> 0:29:58.440
to bank into your HMA platform.

388
0:29:58.440 --> 0:30:03.480
Can pull violating seats from Facebook thread exchange API and works on AWS only, because

389
0:30:03.480 --> 0:30:12.300
we wanted to make a very easy to use thing and also something that doesn't really mix

390
0:30:12.300 --> 0:30:13.800
your bill higher.

391
0:30:13.800 --> 0:30:17.640
So we built it on AWS lambda.

392
0:30:17.640 --> 0:30:19.800
So it doesn't cost anything until it runs.

393
0:30:19.800 --> 0:30:24.720
It runs, spawns lambda instance, and then goes down, and you only pay for the seconds

394
0:30:24.720 --> 0:30:27.240
that it actually runs.

395
0:30:27.240 --> 0:30:28.720
But it's very fast.

396
0:30:28.720 --> 0:30:33.600
And there's a Terraform module available, thanks to the lovely folks of Internet Safety

397
0:30:33.600 --> 0:30:34.600
Engineering.

398
0:30:34.600 --> 0:30:37.520
This is how you deploy data.

399
0:30:37.520 --> 0:30:43.320
Your infra, you co-locate HMA to your platform.

400
0:30:43.320 --> 0:30:49.200
For example, you might own a platform where people have a chat or people post pictures.

401
0:30:49.200 --> 0:30:55.160
Whenever new content comes, the web server asks the Azure, have you seen this?

402
0:30:55.160 --> 0:30:57.480
Then the Azure goes to Matcher.

403
0:30:57.480 --> 0:31:01.000
Azure goes to the index and says, do I know this?

404
0:31:01.000 --> 0:31:09.200
And in case there's a match, the actioner module will just tell your, you have to define

405
0:31:09.200 --> 0:31:11.800
a callback API in your own platform.

406
0:31:11.800 --> 0:31:17.960
Like whenever the actioner calls, you are killing this content in your own backend.

407
0:31:17.960 --> 0:31:24.360
Of course, you can fetch from external API new content from a fact exchange platform.

408
0:31:24.360 --> 0:31:30.180
So wrapping up, automation is necessary to be effective.

409
0:31:30.180 --> 0:31:35.440
But you will lose precision, of course, because automation doesn't really think.

410
0:31:35.440 --> 0:31:38.960
It just does whatever you have configured blindly.

411
0:31:38.960 --> 0:31:44.400
Human support is always needed for appeals and also to establish the ground through.

412
0:31:44.400 --> 0:31:47.480
So what is actually violating, what is not.

413
0:31:47.480 --> 0:31:52.360
Do expect false positive, because they will happen.

414
0:31:52.360 --> 0:31:59.240
You should put in place an appeal process to allow your users to restore the content.

415
0:31:59.240 --> 0:32:07.160
PDQ, VWQ, MT5 and SSCD will provide you with a way to obtain compact representation of

416
0:32:07.160 --> 0:32:11.440
high dimensional content like pictures and videos.

417
0:32:11.440 --> 0:32:18.800
HMA provides you with a turnkey solution you can install on your premise and search and

418
0:32:18.800 --> 0:32:24.280
enforce your integrity policies at your platform.

419
0:32:24.280 --> 0:32:29.080
And third exchange provides you with a platform for exchanging representation with other

420
0:32:29.080 --> 0:32:34.720
big actors like Meta itself, for example.

421
0:32:34.720 --> 0:32:35.800
That was all from me.

422
0:32:35.800 --> 0:32:46.160
Thank you very much for listening.

423
0:32:46.160 --> 0:32:55.120
Any question?

424
0:32:55.120 --> 0:33:00.440
You mentioned that for the challenge, I think.

425
0:33:00.440 --> 0:33:05.280
So you mentioned that for the challenge, finding a clip of a video.

426
0:33:05.280 --> 0:33:08.600
Can PDQ do that, actually?

427
0:33:08.600 --> 0:33:11.240
You can hear me.

428
0:33:11.240 --> 0:33:19.240
So can PDQ find clips of videos of...

429
0:33:19.240 --> 0:33:20.880
That's my question, actually.

430
0:33:20.880 --> 0:33:23.880
So you should...

431
0:33:23.880 --> 0:33:29.460
You say perhaps I heard about YouTube whether it is something that already does.

432
0:33:29.460 --> 0:33:33.200
Like if the challenge is to find the clips of videos...

433
0:33:33.200 --> 0:33:34.200
Yeah.

434
0:33:34.200 --> 0:33:46.920
In general, it's possible, of course, and the video PDQ algorithms will ask every frame.

435
0:33:46.920 --> 0:33:54.560
So in case you send a very small sub portion of a video, you will have like 100 frames,

436
0:33:54.560 --> 0:33:58.600
for example, then these 100 frames will be treated as a bag of words.

437
0:33:58.600 --> 0:34:00.280
You search the index.

438
0:34:00.280 --> 0:34:03.920
You find the video that contains all of these words.

439
0:34:03.920 --> 0:34:11.200
So you have a match of all your query frames inside the index at the very long video that

440
0:34:11.200 --> 0:34:12.200
has it.

441
0:34:12.200 --> 0:34:13.780
And so it's a match.

442
0:34:13.780 --> 0:34:14.780
That's how we do.

443
0:34:14.780 --> 0:34:19.360
Of course, there are more clever ways to do that.

444
0:34:19.360 --> 0:34:20.880
Thanks.

445
0:34:20.880 --> 0:34:23.000
Hello.

446
0:34:23.000 --> 0:34:28.200
Not a technical question, but let's see.

447
0:34:28.200 --> 0:34:35.720
I was thinking that if you're using such a system to try to prevent digital crimes and

448
0:34:35.720 --> 0:34:42.720
such things like that, from an ethical perspective, I was just wondering that you...

449
0:34:42.720 --> 0:34:47.640
I suppose you have such images to compare them.

450
0:34:47.640 --> 0:34:49.640
And how do you process those?

451
0:34:49.640 --> 0:34:53.120
How do you make the decisions whether...

452
0:34:53.120 --> 0:34:57.480
So I repeat the question.

453
0:34:57.480 --> 0:35:02.320
From the ethical perspective, the idea is that, of course, we have to see the images

454
0:35:02.320 --> 0:35:06.000
in order to be able to know what's happening, right?

455
0:35:06.000 --> 0:35:07.000
Was it the question?

456
0:35:07.000 --> 0:35:08.000
Yeah, see.

457
0:35:08.000 --> 0:35:12.080
And of course, you have to save them and, I don't know, process them.

458
0:35:12.080 --> 0:35:14.540
And how do you handle this?

459
0:35:14.540 --> 0:35:19.960
So this is not the kind of question that I really can answer because it is related to

460
0:35:19.960 --> 0:35:22.440
internal procedures.

461
0:35:22.440 --> 0:35:30.320
But if we have to compute the fingerprint of an image, there must be one second in which

462
0:35:30.320 --> 0:35:35.160
the image is on our servers.

463
0:35:35.160 --> 0:35:41.720
Since the agencies like NACMEC, they share ashes.

464
0:35:41.720 --> 0:35:45.480
So you might have an ash for which you don't have a picture.

465
0:35:45.480 --> 0:35:50.840
And you have to trust that these ashes coming from a trusted source that has already vetted

466
0:35:50.840 --> 0:35:54.560
whether this ash is a nasty stuff or not.

467
0:35:54.560 --> 0:36:01.400
That's how we actually avoid sanctioning heavily innocent people.

468
0:36:01.400 --> 0:36:05.600
So there is a collaboration with the trusted entities for this.

469
0:36:05.600 --> 0:36:10.600
When you receive those from an external agent, if those images are on your platform, you

470
0:36:10.600 --> 0:36:14.560
already know what you've seen.

471
0:36:14.560 --> 0:36:16.220
Thank you.

472
0:36:16.220 --> 0:36:19.100
Can you hear me despite the mask?

473
0:36:19.100 --> 0:36:20.100
Can you hear me?

474
0:36:20.100 --> 0:36:22.240
Thank you.

475
0:36:22.240 --> 0:36:27.640
So I have a question, but first I have a thanks because I have worked in this kind of thing

476
0:36:27.640 --> 0:36:31.960
and NACMEC doesn't share any useful data.

477
0:36:31.960 --> 0:36:36.080
IWF doesn't share any useful data.

478
0:36:36.080 --> 0:36:38.960
FAROS doesn't share any useful data.

479
0:36:38.960 --> 0:36:42.920
So I will definitely take a look at the threat exchange platform and hope that it's much

480
0:36:42.920 --> 0:36:44.280
more useful.

481
0:36:44.280 --> 0:36:45.520
And thanks for that.

482
0:36:45.520 --> 0:36:49.480
No, I have a question anyway.

483
0:36:49.480 --> 0:36:56.620
If I was an attacker, I could download data from the threat exchange platform and try

484
0:36:56.620 --> 0:37:02.880
and run as many filters automatically until I find something that is not matched by PDQ,

485
0:37:02.880 --> 0:37:06.080
video PDQ, et cetera.

486
0:37:06.080 --> 0:37:07.560
What's the way to counter that?

487
0:37:07.560 --> 0:37:12.920
Oh, you're asking whether adversarial attacks are possible on PDQ.

488
0:37:12.920 --> 0:37:14.680
Yeah, of course.

489
0:37:14.680 --> 0:37:19.520
PDQ is a very naive algorithm that just detects the patches of colors.

490
0:37:19.520 --> 0:37:24.440
It is actually possible to create adversarial attacks.

491
0:37:24.440 --> 0:37:34.400
Just if you think that you alter many pixels in the image and perceptually for us doesn't

492
0:37:34.400 --> 0:37:43.560
change anything, but you might end up changing the most relevant pictures for the DCT algorithm.

493
0:37:43.560 --> 0:37:51.160
It will create a completely different hashing in the end.

494
0:37:51.160 --> 0:37:58.280
Also someone has demonstrated an attack, a reverse engineering attack on photo DNA, like

495
0:37:58.280 --> 0:38:05.080
from the project is called the ribosome.

496
0:38:05.080 --> 0:38:13.000
And it's a neural network that from hash reconstructs a very blurry picture.

497
0:38:13.000 --> 0:38:16.240
So it is actually possible to do that.

498
0:38:16.240 --> 0:38:21.520
But PDQ is a very simple and fast algorithm.

499
0:38:21.520 --> 0:38:29.080
If you really want to combat the seriously adversarial engineering, the things that are

500
0:38:29.080 --> 0:38:35.400
in the neural networks like SSCD because it contains so many relations to different parts

501
0:38:35.400 --> 0:38:38.000
of the images, it's much harder to fool.

502
0:38:38.000 --> 0:38:41.560
I'm not saying it's not impossible because of course it's possible.

503
0:38:41.560 --> 0:38:43.560
One or later someone will find a way.

504
0:38:43.560 --> 0:38:49.280
But it's the usual arms race between attackers and defenders.

505
0:38:49.280 --> 0:38:51.000
And it's no exception.

506
0:38:51.000 --> 0:38:54.000
Thank you for your question.

507
0:38:54.000 --> 0:38:55.000
Hello.

508
0:38:55.000 --> 0:38:57.920
First, thank you for the presentation.

509
0:38:57.920 --> 0:39:00.520
I think it's a very interesting topic.

510
0:39:00.520 --> 0:39:06.880
I wanted to link it because it's been a bit of a buzz the past few weeks.

511
0:39:06.880 --> 0:39:11.760
And the generative AI, especially chat GPT, was wondering if when you use that kind of

512
0:39:11.760 --> 0:39:17.360
algorithm and you scan an image, detect something, is there a level of confidence attached to

513
0:39:17.360 --> 0:39:18.360
the result?

514
0:39:18.360 --> 0:39:21.680
And can you detect when an image is potentially a fake?

515
0:39:21.680 --> 0:39:22.680
Or?

516
0:39:22.680 --> 0:39:27.800
There is a hard time because there's an echo, so I cannot really.

517
0:39:27.800 --> 0:39:30.000
Can you do it louder, please?

518
0:39:30.000 --> 0:39:31.600
It's hard to understand from here.

519
0:39:31.600 --> 0:39:32.600
Hello.

520
0:39:32.600 --> 0:39:33.600
Is it better?

521
0:39:33.600 --> 0:39:34.600
Okay.

522
0:39:34.600 --> 0:39:37.120
So thank you.

523
0:39:37.120 --> 0:39:41.080
I wanted to link to generative AI.

524
0:39:41.080 --> 0:39:47.160
And I was asking, so when you run that kind of algorithm to detect violence or child abuse

525
0:39:47.160 --> 0:39:53.760
or anything else, can you also attach a level of confidence in the response to explain whether

526
0:39:53.760 --> 0:39:58.320
it's, well, to define whether it's potentially fake picture?

527
0:39:58.320 --> 0:40:03.200
Or is there an extension to the algorithm where you can link with the generative AI?

528
0:40:03.200 --> 0:40:08.400
I'm not sure about the answer.

529
0:40:08.400 --> 0:40:14.280
So we can go for a beer and can explain more details.

530
0:40:14.280 --> 0:40:16.720
Let's see.

531
0:40:16.720 --> 0:40:20.400
Yeah, you have a question.

532
0:40:20.400 --> 0:40:21.400
Hi.

533
0:40:21.400 --> 0:40:22.520
Thank you for the talk.

534
0:40:22.520 --> 0:40:24.440
It was very interesting.

535
0:40:24.440 --> 0:40:25.560
One more question also.

536
0:40:25.560 --> 0:40:28.880
Do you run SSCD in production as well?

537
0:40:28.880 --> 0:40:31.240
The deep learning network?

538
0:40:31.240 --> 0:40:33.080
If we're using SSCD in production.

539
0:40:33.080 --> 0:40:34.080
Yes.

540
0:40:34.080 --> 0:40:35.520
Can I reply to this question?

541
0:40:35.520 --> 0:40:36.520
Okay.

542
0:40:36.520 --> 0:40:38.080
We use SimSearch.

543
0:40:38.080 --> 0:40:40.120
We use SimSearch Net++.

544
0:40:40.120 --> 0:40:41.120
Yes.

545
0:40:41.120 --> 0:40:46.480
We use this other one because we have written a blog post about this.

546
0:40:46.480 --> 0:40:49.680
So I can confirm that we use SimSearch Net++.

547
0:40:49.680 --> 0:40:52.960
We cannot confirm or deny about SSCD.

548
0:40:52.960 --> 0:40:53.960
That's okay.

549
0:40:53.960 --> 0:40:55.600
Those are related technologies.

550
0:40:55.600 --> 0:40:56.600
So I can...

551
0:40:56.600 --> 0:40:58.280
That's okay.

552
0:40:58.280 --> 0:41:02.200
What does the production stack for SimSearch Net++ look like?

553
0:41:02.200 --> 0:41:03.200
How do you serve it?

554
0:41:03.200 --> 0:41:05.840
It must be pretty hard to deal with the GPS.

555
0:41:05.840 --> 0:41:07.480
This is not a question that unfortunately...

556
0:41:07.480 --> 0:41:08.480
Okay.

557
0:41:08.480 --> 0:41:09.480
I'm sorry.

558
0:41:09.480 --> 0:41:10.480
I cannot talk about production setups.

559
0:41:10.480 --> 0:41:12.480
I'm sorry.

560
0:41:12.480 --> 0:41:14.560
Okay.

561
0:41:14.560 --> 0:41:15.560
Any question nearby?

562
0:41:15.560 --> 0:41:16.560
Thank you.

563
0:41:16.560 --> 0:41:21.560
But of course, you can imagine that we do not operate in the vacuum.

564
0:41:21.560 --> 0:41:30.320
So if you can think about how we serve results from an error network, it is something perhaps

565
0:41:30.320 --> 0:41:42.360
similar to what would you do if you would have to put behind an API a model.

566
0:41:42.360 --> 0:41:44.680
So I kind of have two questions.

567
0:41:44.680 --> 0:41:50.000
The first question is to what extent do...

568
0:41:50.000 --> 0:41:53.880
So I think there are potentially two problems.

569
0:41:53.880 --> 0:41:59.500
Intentional mismatches and unintentional mismatches.

570
0:41:59.500 --> 0:42:05.280
So situations where perhaps an image has been recompressed or has been cropped or is perhaps

571
0:42:05.280 --> 0:42:10.720
another image of the same situation versus situations where people have deliberately

572
0:42:10.720 --> 0:42:14.880
deformed the image to try and get around these kind of systems.

573
0:42:14.880 --> 0:42:15.880
So...

574
0:42:15.880 --> 0:42:21.380
Do you have any idea of how performant it is against the two scenarios of either accidental

575
0:42:21.380 --> 0:42:26.040
or unintentional mismatches versus intentionally trying to avoid it?

576
0:42:26.040 --> 0:42:32.240
So it is of course possible to have unintentional mismatches.

577
0:42:32.240 --> 0:42:42.440
And I've seen images that were adversarial engineered to give the same embedding.

578
0:42:42.440 --> 0:42:44.400
Those are absolutely possible.

579
0:42:44.400 --> 0:42:50.840
Again, in PDQ, PDNA and all the perceptual hashing, which is just a mathematical transformation.

580
0:42:50.840 --> 0:42:57.300
You just have to find a way where the input seems the same to the algorithm.

581
0:42:57.300 --> 0:43:02.560
For the neural network things, it depends.

582
0:43:02.560 --> 0:43:03.920
You can study the code.

583
0:43:03.920 --> 0:43:08.000
You can study how it's done.

584
0:43:08.000 --> 0:43:15.320
It is absolutely possible sooner or later because the adversarial attack on convnets

585
0:43:15.320 --> 0:43:16.760
are a reality.

586
0:43:16.760 --> 0:43:18.240
So it's absolutely possible.

587
0:43:18.240 --> 0:43:26.360
I've seen some mismatches, but usually two perceptual hashes.

588
0:43:26.360 --> 0:43:31.840
Usually the more I find a technique, the harder it is to attack, of course.

589
0:43:31.840 --> 0:43:36.520
Otherwise we just will stay with MD5 because it will be enough.

590
0:43:36.520 --> 0:43:37.520
Crops.

591
0:43:37.520 --> 0:43:40.400
PDQ is resistant to crops.

592
0:43:40.400 --> 0:43:46.360
SACD is very resistant to crops.

593
0:43:46.360 --> 0:43:51.680
If you have rotations, I believe also PDQ is resistant to rotations, like flips.

594
0:43:51.680 --> 0:43:58.320
But you cannot ask much more than that.

595
0:43:58.320 --> 0:43:59.320
Other questions?

596
0:43:59.320 --> 0:44:00.320
Yeah.

597
0:44:00.320 --> 0:44:06.680
Do you have any information about speed difference between SACD and PDQ?

598
0:44:06.680 --> 0:44:08.680
Yeah.

599
0:44:08.680 --> 0:44:17.160
So the question is whether I have some speed benchmarks for difference of performance between

600
0:44:17.160 --> 0:44:21.680
PDQ and SACD at inference time.

601
0:44:21.680 --> 0:44:28.640
PDQ is faster than your time to read the image from disk.

602
0:44:28.640 --> 0:44:30.520
So it's negligible.

603
0:44:30.520 --> 0:44:31.520
It will just compute.

604
0:44:31.520 --> 0:44:33.920
It's a mathematical transformation on the pixel.

605
0:44:33.920 --> 0:44:37.080
The neural network requires dedicated hardware.

606
0:44:37.080 --> 0:44:41.120
If you do that on CPU, it will take seconds.

607
0:44:41.120 --> 0:44:43.960
Also because the model, I think, is big enough.

608
0:44:43.960 --> 0:44:49.720
It's not as big as GPT, but it's a 50-level CNET.

609
0:44:49.720 --> 0:44:54.680
So it's, of course, lower and requires dedicated hardware.

610
0:44:54.680 --> 0:44:55.880
But it's more precise.

611
0:44:55.880 --> 0:45:02.760
It just finds, SACD finds anything that PDQ is able to find and much more.

612
0:45:02.760 --> 0:45:11.680
So in case if you are very curious about, sorry, if you are very conscious about I have

613
0:45:11.680 --> 0:45:17.480
to scan this stuff just to make sure they don't come from a ill source, you might want

614
0:45:17.480 --> 0:45:22.280
to set up an async process that will take more, but will just batch process all your

615
0:45:22.280 --> 0:45:23.280
stuff.

616
0:45:23.280 --> 0:45:30.240
If you need a super fast thing, PDQ will not really wait over your server.

617
0:45:30.240 --> 0:45:31.240
Thank you.

618
0:45:31.240 --> 0:45:33.240
Any other questions?

619
0:45:33.240 --> 0:45:34.240
Hi.

620
0:45:34.240 --> 0:45:44.880
First of all, great question from my former colleague, David, I think, down there.

621
0:45:44.880 --> 0:45:47.280
Not even looking this way.

622
0:45:47.280 --> 0:45:50.240
But what happens if you get a false positive match?

623
0:45:50.240 --> 0:45:53.040
What if you get a false positive match?

624
0:45:53.040 --> 0:45:58.560
How do you disregard that in the future without potentially disregarding a real match?

625
0:45:58.560 --> 0:46:05.240
So if we get a false positive match, how do we do to restore?

626
0:46:05.240 --> 0:46:06.240
How do you restore it?

627
0:46:06.240 --> 0:46:10.000
You mean in meta?

628
0:46:10.000 --> 0:46:11.000
Just anywhere.

629
0:46:11.000 --> 0:46:12.000
As a concert, what would you do?

630
0:46:12.000 --> 0:46:14.360
In meta, I cannot really say.

631
0:46:14.360 --> 0:46:21.160
With the Azure Matcher Actioner, you have the, you should provide a capability to your

632
0:46:21.160 --> 0:46:25.160
own platform for which you are soft-deleting the image.

633
0:46:25.160 --> 0:46:31.800
Because you have to provide a way, an API in your platform that HMA will call on, where

634
0:46:31.800 --> 0:46:34.560
you say, soft-delete this picture.

635
0:46:34.560 --> 0:46:39.560
So make it unavailable, but do not really delete it in case you want to appeal.

636
0:46:39.560 --> 0:46:46.360
So you need to provide, like, undelete and soft-delete and soft-delete.

637
0:46:46.360 --> 0:46:54.600
This is the simplest way, most effective way to deal with false positive in case, oops,

638
0:46:54.600 --> 0:46:57.520
I did a mistake, I want to restore the content.

639
0:46:57.520 --> 0:47:02.680
Sure, but if you have an image that someone wants to upload, say it's a popular image

640
0:47:02.680 --> 0:47:09.200
that a lot of people are going to upload, but it matches a pattern of another bad image,

641
0:47:09.200 --> 0:47:15.680
can you auto, is there a good way to make a more precise hash and exclude that and say

642
0:47:15.680 --> 0:47:18.880
this one is a false positive, it doesn't match what you think it does?

643
0:47:18.880 --> 0:47:20.960
So you don't have to keep undoing.

644
0:47:20.960 --> 0:47:21.960
Okay, fine.

645
0:47:21.960 --> 0:47:28.040
So, partly, if the image is popular, so we have many examples and we have many examples

646
0:47:28.040 --> 0:47:32.400
of an image which is not bad, then comes a bad image.

647
0:47:32.400 --> 0:47:36.200
Whether we can use the fact that it's very widespread to augment our position, is this

648
0:47:36.200 --> 0:47:38.200
the question?

649
0:47:38.200 --> 0:47:40.200
Okay.

650
0:47:40.200 --> 0:47:48.040
Well, really, there's nothing in this presentation that says these, because once you train the

651
0:47:48.040 --> 0:47:53.560
network is trained, you start serving and the network will give you the same answers

652
0:47:53.560 --> 0:47:56.360
to the same question, to the same query.

653
0:47:56.360 --> 0:48:02.880
PDQ or other mathematical, perceptual algorithm is just a mathematical function so will not

654
0:48:02.880 --> 0:48:05.040
change, there's nothing to train.

655
0:48:05.040 --> 0:48:12.960
So to change a deficiency of your model, you have to retrain.

656
0:48:12.960 --> 0:48:18.760
You can do a better retraining and sometimes models are retrained as anything which is

657
0:48:18.760 --> 0:48:20.800
still under maintenance.

658
0:48:20.800 --> 0:48:25.600
For example, we get new data, for example, and we might want to retrain as any other

659
0:48:25.600 --> 0:48:27.560
model for the spam filters.

660
0:48:27.560 --> 0:48:31.440
It's the same.

661
0:48:31.440 --> 0:48:36.960
Do we have more room for questions?

662
0:48:36.960 --> 0:48:37.960
I think it's done.

663
0:48:37.960 --> 0:48:38.960
Thank you so much.

664
0:48:38.960 --> 0:48:45.960
It would be a wonderful audience.

