WEBVTT

00:00.000 --> 00:12.000
The next work is on graph aggregation, graph grouping, especially on streaming graphs.

00:12.000 --> 00:18.000
Graph grouping is a really interesting challenge because we've all seen individualizations,

00:18.000 --> 00:21.000
hairballs, and complex graphs.

00:21.000 --> 00:25.000
When graph grouping allows you to split into these graphs, group them by certain attributes,

00:25.000 --> 00:31.000
and you have these better nodes that then can be selectively expanded, but you can also

00:31.000 --> 00:35.000
then, on a group graph, which is mostly a molar-type graph in many cases, you can also

00:35.000 --> 00:38.000
run graph analytics, which is a really interesting problem.

00:38.000 --> 00:45.000
I'm really excited to have Christopher here because both working on the streaming graphs

00:45.000 --> 00:50.000
as well as on temporal graphs, with graph grouping is a really challenging and interesting

00:50.000 --> 00:55.000
aspect, so I'm really looking forward to the talk.

00:55.000 --> 00:59.000
With our full schedule, welcome to the graph bedroom.

00:59.000 --> 01:00.000
Thanks.

01:00.000 --> 01:07.000
Here's a thanks for that introduction and also thanks for accepting my abstract for

01:07.000 --> 01:08.000
this talk.

01:08.000 --> 01:10.000
My name is Christopher Grost.

01:10.000 --> 01:13.000
I'm a PhD student of the University of Leipzig.

01:13.000 --> 01:19.000
I'm currently writing my thesis, so I'm glad that I bring some free time to doing this

01:19.000 --> 01:20.000
talk.

01:20.000 --> 01:28.000
Yeah, so about us or our team, so that's me and I have also a master's student working

01:28.000 --> 01:37.000
on this project we called Graphstream Zuma, and this project is a result of two master's

01:37.000 --> 01:43.840
thesis of our university from Elias Daman and Radha Nuridin, and I think it's also nice

01:43.840 --> 01:49.640
to show the result of a master's thesis here at the FOSTAIM.

01:49.640 --> 01:54.480
At the top is our professor of the database department, just to say that.

01:54.480 --> 01:55.480
Okay.

01:55.480 --> 02:01.320
I want you to take away from this talk, so you will see what the property Graphstream

02:01.320 --> 02:11.640
is and why it's important to have the streaming idea inside the graph topic, and second why

02:11.640 --> 02:17.400
you should or should not group a Graphstream, and then you will learn what the Graphstream

02:17.400 --> 02:44.800
Zuma's specific project is and the main idea behind it and which specific project you want.

02:44.800 --> 02:49.080
First we provide the Java API to do that.

02:49.080 --> 02:51.760
Okay.

02:51.760 --> 02:54.080
Let's just start with an event stream.

02:54.080 --> 03:01.360
I think I can skip that, so we say that anything that happens at a specific time and that can

03:01.360 --> 03:10.680
be recorded is an event, and if an event stream is now a stream of these events, so a sequence

03:10.680 --> 03:13.040
that is ordered by time.

03:13.040 --> 03:18.880
I think everyone knows why we need event processing, so we cannot store everything into a database

03:18.880 --> 03:25.000
or whatever to analyze it, so I want to identify these meaningful events and respond to them

03:25.000 --> 03:26.800
as quickly as possible.

03:26.800 --> 03:27.800
Okay.

03:27.800 --> 03:29.720
What is now a Graphstream?

03:29.720 --> 03:36.720
A Graphstream is an event stream where each event is a graph element or some graph update.

03:36.720 --> 03:41.760
Yeah, that could be edges, could be vertices, triples, or whatever.

03:41.760 --> 03:44.280
And a graph update could be a modification of this.

03:44.280 --> 03:49.800
For example, the addition of an edge, the removal of an edge, the addition of a property

03:49.800 --> 03:51.400
or to an edge or whatever.

03:51.400 --> 03:53.960
Yeah, so this is just an overview.

03:53.960 --> 03:58.280
Okay, why should I use now a Graphstream?

03:58.280 --> 04:07.040
Because I can execute on this Graphstream all algorithms and also all mathematical stuff

04:07.040 --> 04:10.840
from GraphTeary on this stream of graph data.

04:10.840 --> 04:17.160
For example, calculate patch strength concurrently with the evolving graph of the Graphstream.

04:17.160 --> 04:22.800
Okay, I can update my analysis results with a low latency if I combine that in a stream

04:22.800 --> 04:30.000
processing engine and my goal is to monitor the changes or monitor the changes in the

04:30.000 --> 04:34.360
graph or to create some notification or some reactivity.

04:34.360 --> 04:41.040
For example, if something, some average goes over a threshold, then I create a notification.

04:41.040 --> 04:46.760
Okay, by now the Graphstream could be very heterogeneous.

04:46.760 --> 04:53.760
That means consists of many different types and it can also occur in a high frequency.

04:53.760 --> 04:59.360
So it is advisable to summarize the graph elements in a specific way.

04:59.360 --> 05:05.360
And we can summarize graph elements by three criterias, for example, by time, that means

05:05.360 --> 05:10.680
graph elements that belong together, for example, the time window, we group them together.

05:10.680 --> 05:15.840
By structure, that means, for example, edges that share the same source or target vertex

05:15.840 --> 05:17.480
can be grouped together.

05:17.480 --> 05:22.360
And by content, that means vertices and edges that share the same label or a specific value

05:22.360 --> 05:24.400
of a property.

05:24.400 --> 05:31.160
So and we introduced for our algorithm, so-called grouping key functions, that means it is a

05:31.160 --> 05:35.080
function that maps a vertex or an edge to a grouping key.

05:35.080 --> 05:38.560
And that could be everything that is inside this vertex or edge.

05:38.560 --> 05:42.840
It could be labeled temporal information, some kind of property or whatever.

05:42.840 --> 05:47.960
So you can map everything that is represented by vertex or edge to a key function or to

05:47.960 --> 05:51.760
key and on this key, we group the graph.

05:51.760 --> 05:58.840
So that means at the end, the result is again a graph stream, but the grouped representation

05:58.840 --> 06:01.960
of that.

06:01.960 --> 06:04.040
Now you can question, OK, why I need this?

06:04.040 --> 06:06.360
So what are the applications of that?

06:06.360 --> 06:10.800
You can think about it as a preprocessing step.

06:10.800 --> 06:16.840
For example, before you calculate the patch frames, you just group the vertices to the

06:16.840 --> 06:19.520
city attribute of users together or something like this.

06:19.520 --> 06:23.680
You also use it as a preprocessing step.

06:23.680 --> 06:27.000
Second application could be as a postprocessing step.

06:27.000 --> 06:33.040
For example, after you apply the cross stream analysis, for example, community detection,

06:33.040 --> 06:38.480
you can now group on this cluster ID with our grouping algorithm to summarize the different

06:38.480 --> 06:42.300
communities together.

06:42.300 --> 06:46.000
You can also use it to understand the cross stream in more detail.

06:46.000 --> 06:52.600
For example, OK, just to know which vertex or edge types exist in my graph stream, how

06:52.600 --> 06:59.600
frequent these different types arrive or how vertices and edges with different characteristics

06:59.600 --> 07:01.200
are connected together.

07:01.200 --> 07:08.200
So just to get deeper insights or if you use our aggregation functions or aggregation,

07:08.200 --> 07:13.240
for example, counting or calculating an average on that, you can also get or reveal some hidden

07:13.240 --> 07:17.240
information that you would not see in the graph stream itself.

07:17.240 --> 07:21.280
OK, so this is an introduction.

07:21.280 --> 07:29.160
Now I explain the ideas behind this graph stream zoomer application just by an example.

07:29.160 --> 07:31.320
And then afterwards I summarize this.

07:31.320 --> 07:42.280
OK, for example, we're using bike rental data that can have two different graph schemas.

07:42.280 --> 07:50.240
I named them A and B. So the first graph schema A is that a bike rental is an edge between

07:50.240 --> 07:51.240
two station nodes.

07:51.240 --> 07:53.880
Yeah, you see it on the left side.

07:53.880 --> 07:59.720
So a station has several properties like the name, the number of bikes, latitude, longitude,

07:59.720 --> 08:01.160
and so on.

08:01.160 --> 08:07.000
And the trip edge has properties like the user ID, so who rented the bike, which bike

08:07.000 --> 08:10.640
was used by the bike ID, and from and to.

08:10.640 --> 08:15.520
So when this trip happens, until when, and the duration, for example, in seconds.

08:15.520 --> 08:18.080
So this is schema A on the left side.

08:18.080 --> 08:21.240
On the right side we have a more heterogeneous schema.

08:21.240 --> 08:25.480
So we have also stations and trips as vertices here.

08:25.480 --> 08:31.320
And we have also bike nodes and user nodes with several properties.

08:31.320 --> 08:40.520
And I just divided into this because I can explain the examples that follow a bit better

08:40.520 --> 08:46.160
compared to just using a simple schema like here on the left side.

08:46.160 --> 08:52.400
Okay, so how a graph stream of these schemas could look like this.

08:52.400 --> 08:59.000
Of schema A I have just these trip edges between two stations and all information inside.

08:59.000 --> 09:05.520
And from schema B I have here the trip nodes connected with all the other vertex types.

09:05.520 --> 09:10.800
This is just an excellent review of how a stream of this graph data could look like.

09:10.800 --> 09:21.080
Okay, so now begin with a very simple example of our grouping algorithm.

09:21.080 --> 09:27.260
So the input of the grouping is the graph stream, I think it's clear, and we need a

09:27.260 --> 09:29.300
grouping configuration.

09:29.300 --> 09:34.680
And the grouping configuration consists of five attributes.

09:34.680 --> 09:39.760
The first is the window because we are doing windowing on our graph stream.

09:39.760 --> 09:45.720
So I can define a window size, which is here, for example, 10 minutes.

09:45.720 --> 09:52.320
And then VgKey are the vertex grouping keys, that means the key functions that leads to

09:52.320 --> 09:53.800
the grouping of vertices.

09:53.800 --> 10:01.040
VgKey are the edge grouping keys, that means the key functions that are needed to group

10:01.040 --> 10:02.640
the edges together.

10:02.640 --> 10:10.240
And we also have a collection of vertex aggregate functions, this is VagG and EagG are the group

10:10.240 --> 10:12.840
of edge aggregate functions.

10:12.840 --> 10:20.880
So the four on the bottom are like just an array of several key functions and aggregate

10:20.880 --> 10:21.880
functions.

10:21.880 --> 10:29.640
Okay, and now having the input stream and applying that grouping, we get a result.

10:29.640 --> 10:35.000
And just looking like this, because we defined for the vertex grouping keys a function that

10:35.000 --> 10:38.120
maps every vertex to an integer value.

10:38.120 --> 10:43.960
And that results in, it doesn't matter which vertex exists in our graph stream, we group

10:43.960 --> 10:46.720
everything together to one vertex.

10:46.720 --> 10:50.340
And that's the white one with an empty label.

10:50.340 --> 10:54.320
And the same for edges, that means every edge that exists in our graph stream, we group

10:54.320 --> 10:58.920
them together to a super edge, we call them super vertex and super edge.

10:58.920 --> 11:03.360
With that is displayed here in gray.

11:03.360 --> 11:08.040
And because of the count aggregate functions, we add a new property to the super vertex

11:08.040 --> 11:13.240
with the count of all the vertices that are grouped together, and a new property to the

11:13.240 --> 11:16.720
super edge with also the count value.

11:16.720 --> 11:21.320
And this is now our result for every window that we defined on the graph stream, for example

11:21.320 --> 11:24.040
here the first window, second window and so on.

11:24.040 --> 11:31.080
That means we are creating a stream of grouped graphs here.

11:31.080 --> 11:35.520
Okay that is for schema A, and that's the most zoomed out view, so I group everything

11:35.520 --> 11:39.320
together that exists in the graph stream.

11:39.320 --> 11:45.560
For schema B and same grouping configuration, it looks the same, because it doesn't matter

11:45.560 --> 11:48.240
which type label exists, we group everything together.

11:48.240 --> 11:52.840
So we have just different counts, because we have a bit more vertices and edges, and

11:52.840 --> 11:56.240
also for the second window it looks the same.

11:56.240 --> 12:00.880
Okay so this is my first example, so the most zoomed out way.

12:00.880 --> 12:06.360
The second example are called a graph stream schema.

12:06.360 --> 12:12.320
So that means now we are using as a vertex grouping key a function that maps a vertex

12:12.320 --> 12:17.080
to its label, and a function that maps the edge to its label.

12:17.080 --> 12:24.000
And what's the result here, now our node has a label station and the count, because the

12:24.000 --> 12:29.760
count aggregate function stays the same, and our edge has a label trip.

12:29.760 --> 12:33.600
So it's more or less the same, because our graph streamer has just one node type and

12:33.600 --> 12:37.600
one edge type, and that's it also for the second window.

12:37.600 --> 12:43.600
Now it gets a bit more interesting when we are using schema B.

12:43.600 --> 12:49.400
The result here with the same grouping configuration as before is now this.

12:49.400 --> 12:54.440
That means every vertex is grouped by their label and every edge is grouped by their label,

12:54.440 --> 13:00.200
and now I have a schema representation of my graph stream.

13:00.200 --> 13:05.000
And again with all counts, because we are just using count aggregation.

13:05.000 --> 13:07.600
Okay and that's for the second window and so on and so on.

13:07.600 --> 13:14.280
I just left here the properties.

13:14.280 --> 13:20.000
Okay the next example, we stay with the vertex grouping keys and edge grouping keys, but

13:20.000 --> 13:23.920
now I added several aggregate functions to vertices and edges.

13:23.920 --> 13:29.000
For example I say okay for the vertices I want to calculate the average of all available

13:29.000 --> 13:34.160
bikes for these stations, and for the edge aggregate functions I want to have the minimum,

13:34.160 --> 13:41.160
maximum and average duration that a trip between two vertices has.

13:41.160 --> 13:48.200
And the result would be this, so same grouped graph, but now I have three additional properties

13:48.200 --> 13:54.320
on the edges, minimum duration, it's in seconds here, maximum duration and average duration,

13:54.320 --> 14:00.880
and the average bikes available on this station also as a new property.

14:00.880 --> 14:03.600
Same for the second window.

14:03.600 --> 14:11.640
And now my last example, it is, I called it, well not the last example, there is one more.

14:11.640 --> 14:18.040
So I added now here a second vertex grouping key function, and that's an important thing

14:18.040 --> 14:22.200
of the graph stream grouper, you can also implement your own grouping key function.

14:22.200 --> 14:27.520
For example this one called getdistrict consumes the latitude and longitude property of the

14:27.520 --> 14:34.880
vertices and then calculates like a district identifier, for example here of Prusall whatever,

14:34.880 --> 14:37.600
so in which district that belongs to.

14:37.600 --> 14:44.800
And then the graph is grouped on this representing district identifier.

14:44.800 --> 14:48.960
And we also say that for the edges we want to group the edges on the user type, that

14:48.960 --> 14:54.440
means for every edge, so in this data set we have a user type subscriber and something

14:54.440 --> 14:57.000
else we will see it shortly.

14:57.000 --> 15:02.800
That means for every edge I get new now two edges, one for this one user type, one for

15:02.800 --> 15:07.840
the other one, and also here some aggregate functions added.

15:07.840 --> 15:11.520
And the result is something like this.

15:11.520 --> 15:17.840
So here exemplified for three stations, so you see here the district ID one, two and

15:17.840 --> 15:22.480
three, and the average latitude and longitude, for example for visualization, proposes to

15:22.480 --> 15:31.760
place it on the map, and for the trips between two stations or between two district representers,

15:31.760 --> 15:36.720
we calculated also the minimum, maximum and average duration and counted them.

15:36.720 --> 15:43.720
And you see here the green edges are for the user type customer and the red edges are for

15:43.720 --> 15:48.080
user type subscriber.

15:48.080 --> 15:55.720
So the last example is then this one here, if I say okay, as vertex grouping key functions

15:55.720 --> 16:00.160
I say please extract me the identifier of that, that means every vertex that exists

16:00.160 --> 16:05.560
in the graph stream is placed here, and also for the edge identifier, that means since

16:05.560 --> 16:10.440
we all have unique identifiers, every vertex and edge is placed here, and this is more

16:10.440 --> 16:16.440
or less like a snapshot of the current state of this graph stream for the specific window.

16:16.440 --> 16:21.960
So therefore I call that zoomed in, it's the most zoomed in configuration that you could

16:21.960 --> 16:23.920
use.

16:23.920 --> 16:31.800
Okay, you could imagine implementing this is not that easy, so the master students found

16:31.800 --> 16:38.480
a way using Apache Flink and its table API, so everything works distributed since we are

16:38.480 --> 16:46.000
using just the API functions of Apache Flink, but we also figured out several implementation

16:46.000 --> 16:47.000
challenges.

16:47.000 --> 16:55.520
So first was to find a good graph representation, second one is since we are creating a workflow

16:55.520 --> 17:00.040
of this graph stream we have to ensure the chronological ordering of every step in this

17:00.040 --> 17:03.160
workflow.

17:03.160 --> 17:09.160
As a third point is also do you want to ensure the scalability, since if we scale out this

17:09.160 --> 17:19.240
algorithm the scalability should be also high, and also keep the state as minimum as possible,

17:19.240 --> 17:21.240
and provide a low latency and high throughput.

17:21.240 --> 17:29.160
So these were several challenges the master student solved quite well, and at the end

17:29.160 --> 17:34.960
we created a grouping operator looking like this.

17:34.960 --> 17:40.440
I don't want to get into details, it's just an architectural overview of every Flink

17:40.440 --> 17:43.400
steps we used.

17:43.400 --> 17:48.800
What is quite interesting is that we created an operator encapsulation of this, that means

17:48.800 --> 17:54.800
the operator consumes a graph stream at input and has a graph stream as output, that means

17:54.800 --> 18:00.240
you can combine several of these grouping operators, or if you define another graph

18:00.240 --> 18:06.320
stream algorithm that produces a graph stream as output you can just put them before.

18:06.320 --> 18:11.120
So you can like chaining these grouping operators together.

18:11.120 --> 18:17.760
And like I said so this consists of the mapping of the input data, the deduplication of vertices,

18:17.760 --> 18:26.400
grouping of vertices and edges and then mapping it to an output graph stream.

18:26.400 --> 18:32.760
How an API would look like, so it looks a bit messy but I think it's quite fast here,

18:32.760 --> 18:33.960
what's happening here.

18:33.960 --> 18:39.040
So first you have to define, I don't know if I see my...

18:39.040 --> 18:45.040
First you have to define the execution environment of Flink, then we read the data from some

18:45.040 --> 18:50.160
streaming source, for example a socket source or some Kafka stream or whatever you want,

18:50.160 --> 18:57.360
our Flink supports in our case, then we map it to a graph stream object which is the internal

18:57.360 --> 19:02.360
representation of our stream and here's the interesting part here, you define the grouping

19:02.360 --> 19:03.360
operator.

19:03.360 --> 19:07.680
So in the middle, that's the grouping config I showed you in the examples, you can define

19:07.680 --> 19:13.560
it here by an API, you set the window size, you set the vertex and edge grouping keys,

19:13.560 --> 19:19.120
you set the aggregate functions and so on and at the end you just execute this operator

19:19.120 --> 19:24.600
on this graph stream and then you can define the thing or just bring it to the console

19:24.600 --> 19:28.200
or whatever you want.

19:28.200 --> 19:37.520
So that's the operator call, how you define it in the API and current state, so we are

19:37.520 --> 19:43.200
about, all the students are about at 90% of the complete implementation of that.

19:43.200 --> 19:49.840
There were some, we figured out some bugs at the SQL or at the table API of Flink that

19:49.840 --> 19:57.280
were not fixed yet, so we had to define some workarounds that cost us time but yeah, like

19:57.280 --> 20:01.760
I said, we found a workaround and the next steps are that we plan an evaluation, so how

20:01.760 --> 20:06.800
is the latency and throughput of this complete system and we want to test it on real world

20:06.800 --> 20:12.280
and synthetic graph streams and maybe then publish some results, so let's see.

20:12.280 --> 20:20.160
And also the user defined key and aggregate functions are still under development.

20:20.160 --> 20:24.680
Okay then that's it, that's all folks.

20:24.680 --> 20:28.400
Please check out our GitHub repository or maybe you want to contribute, so we are open

20:28.400 --> 20:30.320
for this.

20:30.320 --> 20:36.480
The two links here at the bottom are also the icons are two other projects, the one

20:36.480 --> 20:43.560
is Gradube, this is a big temporary graph processing engine also based on Apache Flink,

20:43.560 --> 20:50.800
so there were also a main contributor to that project which was initially created by Marty

20:50.800 --> 20:57.920
Nungen, who is now a worker at Neo4j and yeah, also the temporary graph explorer is a user

20:57.920 --> 21:05.680
interface for that system where you can play around with the evolution of a graph but in

21:05.680 --> 21:07.480
a historical dataset.

21:07.480 --> 21:11.280
Okay, so that's it and please ask questions.

21:11.280 --> 21:12.280
Thanks.

21:12.280 --> 21:13.280
Yeah, please.

21:13.280 --> 21:30.280
On one slide you said a problem was to decide on the 520 I think is all, the optimal graph

21:30.280 --> 21:33.280
representation in the streaming model?

21:33.280 --> 21:34.280
Yeah.

21:34.280 --> 21:37.120
What's the answer?

21:37.120 --> 21:42.920
So the question was, so we had this challenge to find the optimal graph representation and

21:42.920 --> 21:44.640
what was the answer?

21:44.640 --> 21:52.960
The answer was a triple stream but a rich triple stream we called it since two property

21:52.960 --> 21:58.480
graph vertices are connected with an edge, that means every vertex consists of the label

21:58.480 --> 22:06.520
and possibly a big set of key value pairs as properties and the same for the edges.

22:06.520 --> 22:13.400
And this was our optimal because you can then model everything with this model but the counterpart

22:13.400 --> 22:18.400
of this was in here that we have to do a vertex dedeplication.

22:18.400 --> 22:22.840
For example, if you have a self-loop, so from one vertex to another one, we have a duplicate

22:22.840 --> 22:26.920
of that vertex, so we have to dedeplicate it afterwards for this model, so this was

22:26.920 --> 22:33.600
one counterpart, but we figured out that using every concept of the property graph model

22:33.600 --> 22:41.480
there as a triple is now the best choice for the students.

22:41.480 --> 22:42.480
So another?

22:42.480 --> 22:43.480
Yeah?

22:43.480 --> 22:49.480
Could you comment a bit more on the scalability, like what's the graph size, the distance,

22:49.480 --> 22:50.480
all in one?

22:50.480 --> 22:55.360
Yeah, so the question was some words about the scalability.

22:55.360 --> 23:01.960
The scalability is an open point of future work, so we don't have concrete results of

23:01.960 --> 23:09.680
that, so we tested it with some city bike data that we interpreted as a stream, so some

23:09.680 --> 23:18.640
historical data and we could process, I think it was about 600,000 edges in a few seconds,

23:18.640 --> 23:26.080
but this is just some first results and we have not tried it on big and high-frequent

23:26.080 --> 23:31.760
graph streams on a cluster, because we have huge fling clusters at our university, so

23:31.760 --> 23:37.200
we can benchmark the scalability of that in a later step.

23:37.200 --> 23:38.200
Yeah, thanks.

23:38.200 --> 23:39.200
Yeah?

23:39.200 --> 23:44.360
These aggregate functions, they part of, you know, do you, like a Java API and how do you

23:44.360 --> 23:45.360
define them exactly?

23:45.360 --> 23:50.760
Yeah, so the question was how we define the aggregate functions, so we have a set of predefined

23:50.760 --> 23:55.160
aggregate functions like the count, average min max, and then you have an interface you

23:55.160 --> 23:59.440
can implement against, so there's an interface called aggregate function and then you have

23:59.440 --> 24:06.040
to implement, I think, two or three functions and then you can define your own and use it

24:06.040 --> 24:12.760
then here on, yeah, there, where you say, where you give the classes of count and average

24:12.760 --> 24:18.000
probably you can give your own class and then it will, is used, yeah.

24:18.000 --> 24:19.000
Yeah?

24:19.000 --> 24:20.000
I have a question.

24:20.000 --> 24:27.720
Could you elaborate more on the real-life use cases or real-life applications?

24:27.720 --> 24:33.480
So the question is if we elaborate more real-life use cases and real-life questions.

24:33.480 --> 24:38.120
So applications.

24:38.120 --> 24:44.280
So since, yeah, so we are in, so I'm at a university that means we are missing real-world

24:44.280 --> 24:50.000
data a lot and we need also some input from companies to provide us with real-world data

24:50.000 --> 24:51.440
that we can use.

24:51.440 --> 24:59.960
So use cases could be, we also have to, we only have this bike sharing stuff or Twitter

24:59.960 --> 25:05.480
data and whatever and I think if you have something like this aggregated function like

25:05.480 --> 25:10.600
here an average property you can use because at the end it's a time series of changing

25:10.600 --> 25:15.960
values for example of the average property and of like defining a threshold and get the

25:15.960 --> 25:17.360
notification afterwards.

25:17.360 --> 25:20.760
I think this is maybe one good application afterwards.

25:20.760 --> 25:26.080
So for example if you have network traffic you see, okay, now the average, I don't know,

25:26.080 --> 25:30.600
packet size is increasing, now I get notified for example like this.

25:30.600 --> 25:32.600
This could be an application, yeah.

25:32.600 --> 25:42.640
I was thinking about something like, I would say, like they can be made in a video, something

25:42.640 --> 25:50.640
happened in a video because it's a frame could be considered as a graph and it could be taken

25:50.640 --> 25:54.600
like an event happening in the video stream of video.

25:54.600 --> 25:55.600
Yeah.

25:55.600 --> 25:56.600
Yeah, interesting application.

25:56.600 --> 26:01.560
Yeah, so the idea was to use like a video stream for that but then the question is how

26:01.560 --> 26:07.480
much graphy that is, could that maybe not be done just in a regular stream processing

26:07.480 --> 26:08.980
way.

26:08.980 --> 26:15.160
So I think this is just advisable to use that if you have some quite complex relationships

26:15.160 --> 26:21.800
between entities then you can use this system besides just an ordinary stream processing

26:21.800 --> 26:25.360
engine or complex event processing engine.

26:25.360 --> 26:33.960
So I think the unique point of this is to have the graph aspects into the streaming world.

26:33.960 --> 26:34.960
So any further questions?

26:34.960 --> 26:35.960
Yeah.

26:35.960 --> 26:36.960
So currently events are already edited, right?

26:36.960 --> 26:42.280
So you can only add to the graph but not delete from the graph by history.

26:42.280 --> 26:49.160
Yeah, so at the moment everything is interpreted as an insult only but since Flink supports

26:49.160 --> 26:56.640
everything like also updates and also deletions it is thinkable about some future work that

26:56.640 --> 27:02.400
we also can support this because at the end the result of us is also since we are using

27:02.400 --> 27:08.200
windowing it's an also an insult only stream at the end but if we maybe think about to

27:08.200 --> 27:13.840
remove the windowing aspect so they have something like a continuous aggregation or whatever

27:13.840 --> 27:21.960
then we need to support like a continuous addition on the end so to update already existing

27:21.960 --> 27:24.280
aggregating results.

27:24.280 --> 27:27.360
Okay, any further questions?

27:27.360 --> 27:29.480
Go out of line then.

27:29.480 --> 27:30.480
Thank you again.

27:30.480 --> 27:48.960
I appreciate it.
