1
0:00:00.000 --> 0:00:07.480
We have Magnus here,

2
0:00:07.480 --> 0:00:10.760
he's going to talk about like the current zone.

3
0:00:10.920 --> 0:00:13.540
Magnus works for Microsoft,

4
0:00:13.540 --> 0:00:15.720
and please welcome him.

5
0:00:15.720 --> 0:00:17.720
Thanks a lot.

6
0:00:19.880 --> 0:00:24.840
So we're probably shifting gears a bit from all this deeply technical,

7
0:00:24.840 --> 0:00:32.520
intimidating talks and connect to the first talk of this track.

8
0:00:32.520 --> 0:00:37.080
Basically, I found this really interesting idea of emulating

9
0:00:37.080 --> 0:00:40.920
the Let's Encrypt moment for confidential computing,

10
0:00:40.920 --> 0:00:44.040
because it is more or less what we're doing.

11
0:00:44.040 --> 0:00:47.640
So I'm a software engineer at Microsoft Skinfolk team.

12
0:00:47.640 --> 0:00:49.480
We do a lot of exciting stuff,

13
0:00:49.480 --> 0:00:56.560
in terms of open source, Linux, EVPF, and containers and Kubernetes.

14
0:00:56.560 --> 0:01:01.560
My team is also involved in exploring ways to integrate

15
0:01:01.560 --> 0:01:04.240
existing confidential computing technology,

16
0:01:04.240 --> 0:01:07.200
all that we heard in the talks before,

17
0:01:07.200 --> 0:01:13.040
with existing ways to deploy containers.

18
0:01:13.040 --> 0:01:20.040
So this talk is kind of, we're on a tight schedule,

19
0:01:20.040 --> 0:01:27.800
so I have to really do cut a few and short and probably will be a bit hand wavy.

20
0:01:27.800 --> 0:01:29.800
So sorry about that.

21
0:01:29.800 --> 0:01:34.520
Please reach out if you want to get details about something.

22
0:01:34.520 --> 0:01:41.640
So obviously, it's also not a comprehensive coverage of all the confidential containers realm.

23
0:01:41.640 --> 0:01:46.000
It's quite wide and covers a lot of areas.

24
0:01:46.000 --> 0:01:52.120
So I'm focusing on a few things that we are looking at at the moment.

25
0:01:52.120 --> 0:01:56.640
And also maybe the point in the talks will not age very well,

26
0:01:56.640 --> 0:02:04.200
because things are very much evolving and whatever is mentioned later could be,

27
0:02:04.200 --> 0:02:08.720
in a few months, could be already like in another stage.

28
0:02:08.720 --> 0:02:15.320
But the idea basically is to provide pointers for people who want to get involved in this,

29
0:02:15.320 --> 0:02:17.320
because from my perspective, it's very exciting.

30
0:02:17.320 --> 0:02:20.480
It's something that's a very practical problem you can solve.

31
0:02:20.480 --> 0:02:26.600
And there's a lot of open questions that are really accessible,

32
0:02:26.600 --> 0:02:33.400
I think also for people without a very deep technical background in confidential computing.

33
0:02:33.400 --> 0:02:39.400
And eventually, we have to define cloud native to establish some terminology,

34
0:02:39.400 --> 0:02:41.200
but we cannot go very deep here.

35
0:02:41.200 --> 0:02:44.440
So essentially, it's a bit of a buzzword.

36
0:02:44.440 --> 0:02:51.760
But you will find that it's more or less an ecosystem of practices, tools, interfaces,

37
0:02:51.760 --> 0:03:00.240
APIs that more or less aim to ease the deployment and management of applications on cloud platforms.

38
0:03:00.240 --> 0:03:03.040
And that can be infrastructure as a service.

39
0:03:03.040 --> 0:03:05.480
It can be functions as a service.

40
0:03:05.480 --> 0:03:08.760
But in most cases, containers are very prominent in this space.

41
0:03:08.760 --> 0:03:12.560
And it's also what you're focusing on.

42
0:03:12.560 --> 0:03:18.960
And Kubernetes in this space, among other competitors,

43
0:03:18.960 --> 0:03:26.040
has been adopted as a go-to solution really for container orchestration and management.

44
0:03:26.040 --> 0:03:30.280
And quickly, introduce Kubernetes in two lines.

45
0:03:30.280 --> 0:03:38.880
So Kubernetes is a container orchestrator, management, API abstraction layer.

46
0:03:38.880 --> 0:03:44.200
I would say that it's not trivial to host and operate.

47
0:03:44.200 --> 0:03:51.560
So it's a very popular offering by cloud service providers

48
0:03:51.560 --> 0:04:03.000
to offer some hosted solution for Kubernetes and provide the developers, engineers with some API layers.

49
0:04:03.000 --> 0:04:07.440
And in Kubernetes, what we have is maybe a bit unique.

50
0:04:07.440 --> 0:04:11.680
You have this notion of pods, which define a logical environment.

51
0:04:11.680 --> 0:04:14.360
It's like they're isolated resource constraint.

52
0:04:14.360 --> 0:04:17.720
They also share namespaces, C groups.

53
0:04:17.720 --> 0:04:21.200
And they are composed of individual containers.

54
0:04:21.200 --> 0:04:23.760
So a node has pods.

55
0:04:23.760 --> 0:04:28.240
And in the pods, it can be very co-located containers if you want.

56
0:04:28.240 --> 0:04:36.560
And this is an abstraction that is quite useful in confidential computing.

57
0:04:36.560 --> 0:04:42.600
So in general, I think this is also for people who work with confidential computing quite common.

58
0:04:42.600 --> 0:04:47.560
There's kind of this trade-off scale where you say, you have a small TCP surface.

59
0:04:47.560 --> 0:04:50.000
This means you run in enclaves.

60
0:04:50.000 --> 0:04:53.120
You have SDKs.

61
0:04:53.120 --> 0:04:56.760
And your workloads have to be customized for this.

62
0:04:56.760 --> 0:04:59.680
But this also means you don't have to care for a lot of stuff,

63
0:04:59.680 --> 0:05:02.400
because the TCP surface is quite small.

64
0:05:02.400 --> 0:05:05.000
On the other end, you have a bigger TCP surface.

65
0:05:05.000 --> 0:05:10.240
It's like bare-metal VMs, Kubernetes clusters, for example.

66
0:05:10.240 --> 0:05:19.000
And those have the convenience of running unmodified workloads, this kind of lift and shift idea.

67
0:05:19.000 --> 0:05:25.640
And if you want, the Kubernetes pods could kind of fit in there somewhere on this scale.

68
0:05:25.640 --> 0:05:28.240
Probably not exactly in the middle, but somewhere.

69
0:05:28.240 --> 0:05:37.720
So because the idea is that we only have minimal adjustments to existing workloads that are already running in Kubernetes.

70
0:05:37.720 --> 0:05:48.720
And as mentioned before, some workloads are simply locked out of cloud-native and public clouds due to compliance issues.

71
0:05:48.720 --> 0:06:00.160
And so our approach is basically to ease the adoption of confidential computing by enabling confidentiality with minimal upfront investments.

72
0:06:00.160 --> 0:06:12.160
Because really only big corporations are able to invest in self-hosting environments that provide confidentiality.

73
0:06:12.160 --> 0:06:16.240
And as I said, it has been widely adopted by the industry, Kubernetes.

74
0:06:16.240 --> 0:06:19.160
And Kubernetes provides some abstractions and technologies.

75
0:06:19.160 --> 0:06:28.120
I think you could also even argue that the main value of Kubernetes is not really the tech, but the kind of API abstractions that are there.

76
0:06:28.120 --> 0:06:35.360
That really makes you, if you move from one place to the other, you will be able to adopt quite easily,

77
0:06:35.360 --> 0:06:43.280
because there's a lot of shared solutions these days.

78
0:06:43.280 --> 0:06:47.640
And we can use those abstractions.

79
0:06:47.640 --> 0:06:50.960
For example, we look at SEV and TDX.

80
0:06:50.960 --> 0:06:55.080
They leverage VMs for confidentiality.

81
0:06:55.080 --> 0:07:06.120
And we have already an established solution with Kata containers that run those pod units that we've seen before in virtual machines.

82
0:07:06.120 --> 0:07:12.320
So this is something that we don't have to start from scratch, but we can kind of see, OK, there's something that's been working so far.

83
0:07:12.320 --> 0:07:16.440
And maybe we can leverage this.

84
0:07:16.440 --> 0:07:27.600
And so ideally, probably you don't see this, but ideally, the result is that you have a Kubernetes spec where you just add a property.

85
0:07:27.600 --> 0:07:29.040
Something is confidential.

86
0:07:29.040 --> 0:07:30.000
Confidential true.

87
0:07:30.000 --> 0:07:43.000
So this is a kind of lift and shift idea that we have very low friction enabled customers to deploy confidential workloads.

88
0:07:43.000 --> 0:07:49.640
And the problem starts when we look at the Kubernetes privileges.

89
0:07:49.640 --> 0:07:53.480
Like they're formed in a permit usually.

90
0:07:53.480 --> 0:08:06.680
So the squiggly lines here indicate that it's not clear cut, but it's more or less like some parts are owned maybe by the platform engineers, by the so-called orchestrators.

91
0:08:06.680 --> 0:08:11.000
Some parts are taken over by cloud service providers.

92
0:08:11.000 --> 0:08:15.280
But the idea is that the hierarchy is pretty clear.

93
0:08:15.280 --> 0:08:25.400
So the closer you get to the app users, eventually to the service users, the less privileged you are.

94
0:08:25.400 --> 0:08:42.360
And the platform engineers are really also an in-between the layer that exists like classical operators, but on Kubernetes API level.

95
0:08:42.360 --> 0:08:46.400
And the confidentiality permit drops.

96
0:08:46.400 --> 0:08:55.760
And it's a bit messy because now we basically have the system that's been built for years and everything's figured out nicely.

97
0:08:55.760 --> 0:08:58.560
And now our privileged model doesn't fit anymore.

98
0:08:58.560 --> 0:09:04.760
So from the confidentiality perspective, you want to lock out the cloud service provider.

99
0:09:04.760 --> 0:09:14.560
And we also don't want the cluster administrators to mess with the workloads, possibly not even the developers.

100
0:09:14.560 --> 0:09:23.520
So and it might be really only the app users who have access to confidential data compute.

101
0:09:23.520 --> 0:09:27.200
And this is something we have to deal with.

102
0:09:27.200 --> 0:09:34.840
And this is also basically the challenges I think in this model we have to overcome.

103
0:09:34.840 --> 0:09:44.000
I think it's definitely not an exhaustive list, but it's like three topics I picked, which in recent months I followed the discussions.

104
0:09:44.000 --> 0:09:46.240
But I also don't think they're insurmountable.

105
0:09:46.240 --> 0:09:53.880
So it's definitely stuff that we can solve and it's partially nice engineering problems.

106
0:09:53.880 --> 0:09:57.920
Starting with image management.

107
0:09:57.920 --> 0:10:07.880
So the container images usually are managed by the infrastructure, by the cubelet, container D, by those layers.

108
0:10:07.880 --> 0:10:13.800
And this makes a lot of sense because there's also a lot of shared resources.

109
0:10:13.800 --> 0:10:18.120
So containers images are organized in layers.

110
0:10:18.120 --> 0:10:25.480
So instead of pulling an image 10 times, it can be cached on the node.

111
0:10:25.480 --> 0:10:31.840
It can be shared through replications of a single service, for example.

112
0:10:31.840 --> 0:10:39.720
And in a trusted execution environment, we need verified or encrypted images for workloads.

113
0:10:39.720 --> 0:10:46.560
And actually, there are already OCI facilities for those aspects.

114
0:10:46.560 --> 0:10:48.480
But the problem is they're running on the wrong layer.

115
0:10:48.480 --> 0:10:49.560
So they're not in the TE.

116
0:10:49.560 --> 0:11:06.960
So if basically the infrastructure part is not part of the TE, then we kind of need to drill a hole or move stuff to a trusted execution environment.

117
0:11:06.960 --> 0:11:10.160
And so there's basically some approaches.

118
0:11:10.160 --> 0:11:21.440
So I think the pragmatic band aid to start with, which made a lot of sense, is just pull everything in encrypted memory in a confidential pod VM.

119
0:11:21.440 --> 0:11:30.960
And this has, I think, practical merits in the first, because you can start with a solution.

120
0:11:30.960 --> 0:11:39.680
But it's very clear that there's downsides to this because you need to provision potentially a big chunks of memory for this.

121
0:11:39.680 --> 0:11:48.160
And also, each pod needs to pull individually their image layers.

122
0:11:48.160 --> 0:11:55.280
And some of those workloads, they run PyTorch images, for example.

123
0:11:55.280 --> 0:11:58.520
They're really like gigabytes big.

124
0:11:58.520 --> 0:12:04.120
And it's not something that you want to pull individually all the time.

125
0:12:04.120 --> 0:12:08.520
We can, of course, create encrypted scratch spaces to do this.

126
0:12:08.520 --> 0:12:11.720
So we get rid of the over provisioning of the memory.

127
0:12:11.720 --> 0:12:19.080
But the unshared images will still yield pretty bad startup time.

128
0:12:19.080 --> 0:12:23.160
The good news is that there's approaches that we can use today.

129
0:12:23.160 --> 0:12:31.160
Like we can stream encrypted image layers or otherwise chunked up logs.

130
0:12:31.160 --> 0:12:41.040
So there's different ideas from the host to the confidential pod. And the technology to do this is also not something that we need to invent from scratch.

131
0:12:41.040 --> 0:12:47.960
It's already there in recent versions of ContainerD.

132
0:12:47.960 --> 0:12:51.760
It's called remote snapshotters that basically do this.

133
0:12:51.760 --> 0:12:53.720
But it's not 100% there.

134
0:12:53.720 --> 0:12:57.200
But it doesn't meet our requirements fully.

135
0:12:57.200 --> 0:12:58.840
But I think it's pretty close.

136
0:12:58.840 --> 0:13:06.240
So about the whole image topic, I think we can be pretty optimistic.

137
0:13:06.240 --> 0:13:11.440
There's another problem that's about metadata.

138
0:13:11.440 --> 0:13:15.920
Kubernetes, and that's maybe a bit counterintuitive to people who don't know this.

139
0:13:15.920 --> 0:13:21.240
But Kubernetes will freely transform your specified workloads in multiple ways.

140
0:13:21.240 --> 0:13:24.480
So it will inject mount points, environment variables.

141
0:13:24.480 --> 0:13:25.920
It will change your image definitions.

142
0:13:25.920 --> 0:13:27.680
And this is all by design.

143
0:13:27.680 --> 0:13:35.560
So because the cluster operators that I mentioned before, they basically make sure that all the workloads are compliant.

144
0:13:35.560 --> 0:13:42.520
That if some engineer deploys some Redis image, usually like Redis latest or something,

145
0:13:42.520 --> 0:13:49.680
they would basically on the fly have some admission controller that just rewrites this stuff.

146
0:13:49.680 --> 0:13:55.320
And from our perspective, this is not what we want from the confidentiality perspective.

147
0:13:55.320 --> 0:14:07.760
We specify what we want to run, and we cannot have the orchestrator or the platform engineers rewriting our specs.

148
0:14:07.760 --> 0:14:12.320
So we want to run exactly what we specified.

149
0:14:12.320 --> 0:14:16.920
And also in terms of, for example, environment variable injection.

150
0:14:16.920 --> 0:14:19.160
So Kubernetes does this for very good reasons.

151
0:14:19.160 --> 0:14:28.400
For example, for service discovery, all kinds of information, the pod receives from the orchestrator.

152
0:14:28.400 --> 0:14:31.720
But it's very problematic if you think of the pod image before.

153
0:14:31.720 --> 0:14:38.240
You have a small batch thing that does some caching on Redis locally.

154
0:14:38.240 --> 0:14:47.240
And if the control plane would just inject the Redis host environment variable and forward this to whatever Redis instance,

155
0:14:47.240 --> 0:14:50.040
this would obviously top a confidentiality.

156
0:14:50.040 --> 0:14:53.040
So this is something we need to deal with.

157
0:14:53.040 --> 0:14:59.320
And this is a bit, I think from my perspective, a bit messier than the image part.

158
0:14:59.320 --> 0:15:17.400
Because we basically have to, or at least I don't see another way than saying, OK, we have to review the delta between what the user eventually specified and what is eventually being provisioned on the container D site.

159
0:15:17.400 --> 0:15:28.600
And I think we can then validate whether we're fine with the applied changes in the user specified policies.

160
0:15:28.600 --> 0:15:32.400
But it's very hard, I think, in some cases, to write those policies.

161
0:15:32.400 --> 0:15:33.280
They're very complex.

162
0:15:33.280 --> 0:15:46.120
So the whole idea that we have a nice UX that says, like, just confidential true is not working anymore because the users have to write those policies.

163
0:15:46.120 --> 0:15:51.600
And there's some dynamisms in Kubernetes that are very hard to model.

164
0:15:51.600 --> 0:15:59.720
So we either have to find a way to kind of make this really convenient or we find some other way to tackle this.

165
0:15:59.720 --> 0:16:14.080
And I think another idea I've recently read about is basically that we have a variation of this idea, as more or less, we kind of log the changes that have been performed and yeah,

166
0:16:14.080 --> 0:16:30.240
toppled or approaches from this perspective that we see, OK, we don't model what could happen, but we more or less look at the log file of the changes that are there and see whether we are fine with those changes.

167
0:16:30.240 --> 0:16:38.640
But that's, I think, the same idea, but it's only a variation.

168
0:16:38.640 --> 0:16:45.200
Eventually, I think, and this is also something that is a bit more challenging.

169
0:16:45.200 --> 0:16:53.000
We have to address the problem that the control plane, APIs and the host components, they interact with the Kubernetes parts.

170
0:16:53.000 --> 0:17:04.760
And it's really like a crucial part of all the tooling of all the of how developers interact with Kubernetes, say, with, I know, XSAC into their containers for debugging stuff.

171
0:17:04.760 --> 0:17:11.360
They will get locks and all this is currently going through the control plane.

172
0:17:11.360 --> 0:17:20.560
And we basically our task is to cut out this kind of middleman from the user to the workload.

173
0:17:20.560 --> 0:17:25.480
And there's a lot of aspects like observability.

174
0:17:25.480 --> 0:17:40.800
This is a key concept of like cloud native workloads and it's obviously almost a contradiction like confidentiality and observability. It's very hard to reconcile, but it's something that I think is we have to address at some point.

175
0:17:40.800 --> 0:17:46.840
So we kind of need to obscure the logs, traces, metrics from non trusted parties.

176
0:17:46.840 --> 0:17:58.400
And those metrics are, for example, sometimes also used in by the orchestrator directly so they would use garbage collection metrics to perform out of scaling of pots.

177
0:17:58.400 --> 0:18:04.040
So it's really a tricky question how to deal with this.

178
0:18:04.040 --> 0:18:19.160
So we need to deprivialage basically the non trusted parties and prevent them from doing those or from retrieving those metrics, but also from executing commands and the scope of a confidential pot.

179
0:18:19.160 --> 0:18:27.160
And I think there's, yeah, as we see images, I think the pragmatic bandaid is probably at the moment locking down those problematic parts.

180
0:18:27.160 --> 0:18:34.840
It's like you cannot execute the moment, maybe retrieving logs is not that easy as it was before.

181
0:18:34.840 --> 0:18:41.400
But obviously that's not something that's practical in the long run for for real workloads.

182
0:18:41.400 --> 0:18:48.680
And there's two solutions to this problem I've stumbled upon recently.

183
0:18:48.680 --> 0:19:01.160
It's more or less like say you split the container management APIs into the infrastructure and trusted parts and operate a kind of shadow trusted control plan that users and tools interact with.

184
0:19:01.160 --> 0:19:04.560
And this is also part of a T.E.

185
0:19:04.560 --> 0:19:08.960
And it would kind of mirror the Kubernetes API.

186
0:19:08.960 --> 0:19:25.200
The downside I think it's from what I have seen is it's a large effort and also not sure whether it's maintainable in the long run because you basically have to evolve with the Kubernetes APIs all the time.

187
0:19:25.200 --> 0:19:34.080
And I think the other alternative solution I've seen is more viable from my perspective.

188
0:19:34.080 --> 0:19:46.520
It means we haven't kind of an encrypted transport between the privileged user and the container management tech on the confidential VM, for example.

189
0:19:46.520 --> 0:20:02.920
And the downside to this approach is basically that it's also quite an invasive change because you need to extend many power touch many parts of the stack because you have this kind of confidential transport through all these layers.

190
0:20:02.920 --> 0:20:16.320
It means you have to change components in Kubernetes and container D and even like the clients and tools that sit on top of Kubernetes.

191
0:20:16.320 --> 0:20:23.600
So it's also not an easy thing to do.

192
0:20:23.600 --> 0:20:27.720
Yeah, I think that's the three points I wanted to mention.

193
0:20:27.720 --> 0:20:36.040
In summary, I think confidential computing and cloud native containers are a good match.

194
0:20:36.040 --> 0:20:41.520
From my perspective, it could really boost the adoption of confidential computing.

195
0:20:41.520 --> 0:20:49.960
There's definitely some hairy questions we need to figure out to make this work in practice.

196
0:20:49.960 --> 0:20:55.880
But from what I'm seeing, like there's a very engaged community and yeah, it's very exciting.

197
0:20:55.880 --> 0:21:03.000
So if you want to chime in, it's a confidential containers org on GitHub.

198
0:21:03.000 --> 0:21:05.360
There's meetings.

199
0:21:05.360 --> 0:21:07.400
There's slack.

200
0:21:07.400 --> 0:21:18.040
And I think that's it.

201
0:21:18.040 --> 0:21:20.920
We have some around 10 minutes for questions.

202
0:21:20.920 --> 0:21:24.920
So please.

203
0:21:24.920 --> 0:21:35.160
You mentioned the control plane issue and the fact that the API between the host control plane and the T is not practical in the lower end.

204
0:21:35.160 --> 0:21:38.280
What do you mean by that?

205
0:21:38.280 --> 0:21:40.680
No, no, knocking it out is not practical.

206
0:21:40.680 --> 0:21:47.640
So if you basically say you're not able to use a full API in confidential context.

207
0:21:47.640 --> 0:21:54.880
So if you basically right now, you can lock down the controversial parts of the API because it's very hard.

208
0:21:54.880 --> 0:21:56.120
It's hard to untangle those things.

209
0:21:56.120 --> 0:22:03.520
Some things are like, you know, when the APIs weren't conceived, you didn't think like X would be a problem, but create wouldn't be a problem.

210
0:22:03.520 --> 0:22:14.560
So at the moment, I think we can do this on the container D level or on a data agent level where you can basically say, OK, there's a few things we just don't support.

211
0:22:14.560 --> 0:22:17.960
But I think this is not sustainable in the long run.

212
0:22:17.960 --> 0:22:27.560
So you're saying that you're not saying the architecture is not practical, you're saying that the fact that you're shutting the API off is not a long term solution.

213
0:22:27.560 --> 0:22:42.960
Exactly. But I mean, it's if you want to basically start with something that is definitely like the image pulling on the on the confidential VM, this is I think makes sense if you want to start with this.

214
0:22:42.960 --> 0:22:55.280
Yeah. We think about like we have to get the metadata problem and then having this trusted control plane also as a solution for the metadata problem.

215
0:22:55.280 --> 0:23:02.120
But you're like a controller part of the API server that can give your descriptions and it will enforce them to the control.

216
0:23:02.120 --> 0:23:05.120
I mean, that's obviously. Pardon.

217
0:23:05.120 --> 0:23:18.760
Yeah. So the question is like whether basically the if you move parts of the of the control plane into the T whether you basically get around a lot of those problems.

218
0:23:18.760 --> 0:23:23.720
And it's absolutely true because basically this self-made problems to a huge degree.

219
0:23:23.720 --> 0:23:34.440
So if you just take the whole thing and put it in a T.E., then most of the things that were there or maybe even all of the things there aren't a problem anymore suddenly.

220
0:23:34.440 --> 0:23:49.040
But this is like starting from the from the notion of existing users who basically use existing hosted Kubernetes offerings.

221
0:23:49.040 --> 0:23:52.640
How do you basically migrate those users to confidential?

222
0:23:52.640 --> 0:23:59.440
You would have to do those things. And so I'm not even arguing that this is like maybe the best solution in the long run.

223
0:23:59.440 --> 0:24:04.400
But if you want to do this, then you have to kind of overcome those those issues.

224
0:24:04.400 --> 0:24:15.600
We have like two more questions and I got to give so we can read it. OK.

225
0:24:15.600 --> 0:24:29.080
OK, there's a question. Are there any new challenges related to attestation protocols in containers compared to already existing attestation mechanism in TDX, SAVS and P, etc.

226
0:24:29.080 --> 0:24:37.600
From my perspective, I think this is not something that is necessarily conflicting with this confidential container.

227
0:24:37.600 --> 0:24:49.440
So it's more or less we also have to basically follow the same attestation principles as like non containerized workloads.

228
0:24:49.440 --> 0:25:04.040
Yeah, you're just moving it on a different layer. But I think there's no fundamental difference or problem that are specific to containers.

229
0:25:04.040 --> 0:25:10.720
There's another question about the use of a proxy being considered.

230
0:25:10.720 --> 0:25:19.600
I think it's a bit too broad for me to understand where a proxy would sit.

231
0:25:19.600 --> 0:25:26.600
So you run a proxy in a confidential VM and all your control point APIs go through that.

232
0:25:26.600 --> 0:25:30.800
Yeah, I mean, this is what I meant by like having this kind of transport.

233
0:25:30.800 --> 0:25:38.360
But yeah, I think the proxy pretty much you need to then start also at the client to tweak things.

234
0:25:38.360 --> 0:25:42.520
Maybe you have a distinct cube config for confidential parts.

235
0:25:42.520 --> 0:25:51.680
I don't know. I didn't really look deep into this, but I think there's definitely changes you need to do also when you employ a proxy.

236
0:25:51.680 --> 0:25:57.360
And one question or comment from my side is.

237
0:25:57.360 --> 0:26:01.800
Well, there are a lot of challenges in order to have these done in the infrastructure side.

238
0:26:01.800 --> 0:26:08.960
So you mentioned about like how we pull the parts of whether we can share those or not the image.

239
0:26:08.960 --> 0:26:15.280
That's a problem that affects mostly the people providing the service.

240
0:26:15.280 --> 0:26:25.320
But he might in my mind, like the biggest problem we have is like maintaining the ability and observability for the end users.

241
0:26:25.320 --> 0:26:30.760
And that's something that we will have to think together on how so these.

242
0:26:30.760 --> 0:26:39.720
I mean, the priorities, I think I agree. Sometimes I think are different for this way, because sometimes it's just things are just also KPI driven.

243
0:26:39.720 --> 0:26:45.480
So you say like we have this solution and it starts like in three seconds and we do this, then it doesn't work.

244
0:26:45.480 --> 0:26:52.800
So because then we request in some metric. But this I think a product question from my personal view.

245
0:26:52.800 --> 0:26:57.640
I also don't think that the image pulling time, the startup time is a big issue.

246
0:26:57.640 --> 0:27:05.400
But there's also like if many of those workloads are machine learning workloads, then it's like there's a pie torch image.

247
0:27:05.400 --> 0:27:13.000
It's like I know 21 gigabyte or something. It's really crazy. And I understand that there's like a concern.

248
0:27:13.000 --> 0:27:19.000
And it's not like a problem for for Kubernetes itself. Like we have to wait a lot with my.

249
0:27:19.000 --> 0:27:26.440
But for functional service, it is. Yeah. This is one of the main things that people are looking for.

250
0:27:26.440 --> 0:27:27.880
It's understandable.

