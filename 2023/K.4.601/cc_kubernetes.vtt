WEBVTT

00:00.000 --> 00:14.000
We're going to start here. We have a presentation about constellation, how to manage Kubernetes,

00:14.000 --> 00:24.880
we think Kubernetes. The presenters are Mojit and Malto. So big shout out to them.

00:24.880 --> 00:32.680
Thank you. We'd like to pick up where Nick in his initial presentation of saying we need

00:32.680 --> 00:37.560
to have a let's encrypt movement, we need to make confidential computing a commodity.

00:37.560 --> 00:43.480
Where he started off and then Magnus I think had a great talk showing all those bits and

00:43.480 --> 00:47.640
pieces that we need to have to bring together the use cases, the cloud native world, the

00:47.640 --> 00:53.040
way we develop applications for the cloud and the advantages that the confidential computing

00:53.040 --> 01:00.840
technology gives us, how to bring them together and where are those gaps.

01:00.840 --> 01:06.040
Historically seen or for different kind of use cases, I would roughly split from a use

01:06.040 --> 01:11.600
case perspective of if I want to develop an application, where can CC help me, how can

01:11.600 --> 01:19.320
I play confidential computing. I can roughly split that in three tiers if you will. I think

01:19.320 --> 01:25.640
one is definitely managing keys, having enclaves that hold your cryptographic certificates,

01:25.640 --> 01:37.880
your keys that process the crypto operations for you. Very small TCB, very small kind of

01:37.880 --> 01:44.560
application. Then the second one is where I package my entire application inside an

01:44.560 --> 01:51.280
enclave inside a confidential container and I think that's what we've been doing lately

01:51.280 --> 01:56.960
a lot. Then I think the third thing is what Magnus described, is how can we bring that

01:56.960 --> 02:03.480
together making this orchestratable, making this manageable, deployable. I think there

02:03.480 --> 02:11.960
are different ways of getting from the tier two or the way we are to here. One is I guess

02:11.960 --> 02:17.600
what Magnus described with taking containers, making them confidential containers and then

02:17.600 --> 02:23.460
having the problems with orchestration. An orthogonal approach that we like to present

02:23.460 --> 02:28.160
now is more of the idea of having confidential clusters. Instead of isolating individual

02:28.160 --> 02:37.880
containers, isolating the nodes, the downside probably is a little bit larger TCB and the

02:37.880 --> 02:44.480
advantages being more closely to where we are right now with deploying and developing

02:44.480 --> 02:53.560
cloud native applications. Talking about challenges, for level three definitely I think one of

02:53.560 --> 03:01.120
the biggest ones is UI, UX. There's little hope that people will go ahead and drastically

03:01.120 --> 03:07.840
adjust the way they deploy and develop applications for the cloud just because they want to use

03:07.840 --> 03:14.440
this new type of technology. We need to get very close to where they are and bring those

03:14.440 --> 03:20.440
worlds together. Then of course there are the challenges Magnus described with orchestration,

03:20.440 --> 03:27.120
attesting, how can we attest all those different containers that are running in my cluster

03:27.120 --> 03:33.040
and don't necessarily want to verify each individual instance of it. It could be a thousand

03:33.040 --> 03:41.120
and more of the same. Once I have a cluster, all those day two operations of updating,

03:41.120 --> 03:47.440
upgrading and doing that in a sensitive way where I can always verify what's currently

03:47.440 --> 03:54.680
being running and what are the changes. A big part of what we're going to present today

03:54.680 --> 04:01.200
is the right one here where a big benefit of the cloud is actually that I can give away

04:01.200 --> 04:08.080
some of this orchestration work and I consume managed services that are operated by someone

04:08.080 --> 04:19.400
or autonomously and I just consume them through an API or any other kind of interface. As

04:19.400 --> 04:24.240
Nick has said, infrastructure is rolling out. We see all those confidential technologies

04:24.240 --> 04:33.360
in the cloud, AMD, SCB. We have heard so many today, IBM, RISC-V. Most of them give us a

04:33.360 --> 04:39.920
confidential VM which is as we've seen not necessarily the abstraction we want but still

04:39.920 --> 04:46.040
we can already consume managed Kubernetes that runs on confidential VMs at least for

04:46.040 --> 04:55.360
the worker nodes. I think Azure has it, GCP has it. This exists but it's not really solving

04:55.360 --> 05:00.600
the problem. It gives us runtime encryption for the stuff that lives on that nodes but

05:00.600 --> 05:06.760
all the edges, all the IO is not protected. The API server is not protected. We've seen

05:06.760 --> 05:13.880
that in Michael's talk. The metadata problem, the problem of the trusted control plane,

05:13.880 --> 05:18.400
the way if you want to consume persistent volumes, is that automatically encrypted or

05:18.400 --> 05:24.880
do I need to adjust my application to encrypt before writing to storage? The idea of a confidential

05:24.880 --> 05:32.760
cluster is that I have somebody or something that fills in those gaps so that I have instead

05:32.760 --> 05:39.640
of those individual confidential VMs, I have one big context that I can verify through attestation

05:39.640 --> 05:44.680
that I can establish a secret channel and then if I'm in that context, if I'm in that

05:44.680 --> 05:56.100
cluster, I can just use Kubernetes as I used to and inside there, essentially everything

05:56.100 --> 06:04.920
is trusted. It's a different type of approach. It just creates an envelope around my Kubernetes

06:04.920 --> 06:12.080
and isolates that as a whole. As I said, I think UX and UI and the way we use this is

06:12.080 --> 06:19.320
super important. It's not going to work that we need a lot of adjustments, a lot of additional

06:19.320 --> 06:26.480
steps in the development workflows. This is just an example of consolation here but having

06:26.480 --> 06:32.400
a simple way of creating such a confidential cluster and then using it is important and

06:32.400 --> 06:37.200
all those things that I showed, all the challenges we need to solve below. We need to make this

06:37.200 --> 06:44.480
more or less invisible. In terms of consolation, we try to make the node operating system as

06:44.480 --> 06:50.320
verifiable as possible, strip it down as much as possible, harden it, then strip them together

06:50.320 --> 06:58.000
for a cluster. We need to think about supply chain. We need to think about how we can automatically

06:58.000 --> 07:03.800
encrypt all the stuff that goes over the network, that goes through the storage. Ideally, this

07:03.800 --> 07:10.280
is all open source, so if I didn't mention it's open source and it's cloud agnostic so

07:10.280 --> 07:16.040
it can run everywhere. Then for most of confidential computing stuff, I need some way of recovery

07:16.040 --> 07:26.920
should things go south and everything is down and I need to get back into running mode.

07:26.920 --> 07:32.240
The big problem with the confidential cluster concept is now I can create a cluster and

07:32.240 --> 07:37.640
we will see in a bit of what that means but if I can create a cluster, I have everything

07:37.640 --> 07:43.880
verified, now I have to maintain and run it on my own. This is, I guess, the biggest problem

07:43.880 --> 07:49.600
with that concept. People want to consume managed stuff, want to have managed Kubernetes,

07:49.600 --> 07:57.400
want to run their own, orchestrate their own Kubernetes. This is a big trade-off that people

07:57.400 --> 08:05.560
are facing and we try to work on concepts of making that as easy as possible and Malte

08:05.560 --> 08:15.720
is going to show you how.

08:15.720 --> 08:25.600
Thanks for the introduction. The basic idea that we had was how can we manage Kubernetes

08:25.600 --> 08:33.000
from inside Kubernetes itself and to kind of draft this idea, I will start by explaining

08:33.000 --> 08:43.000
like what typically you can do today. On the left side, you really have the traditional

08:43.000 --> 08:50.160
on-prem model which is you have the whole cluster in your own hand, the control plane,

08:50.160 --> 08:56.760
the worker nodes, it runs on your own hardware which is great for security because you have

08:56.760 --> 09:04.360
full control but it also means you are responsible for every single interaction like scaling

09:04.360 --> 09:11.360
up the cluster, joining the nodes, performing upgrades both on the OS level and also Kubernetes

09:11.360 --> 09:18.400
upgrades and then on the other side, you have something that is super popular. It is just

09:18.400 --> 09:25.040
let the cloud provider deal with it and it means the cloud provider can scale your cluster

09:25.040 --> 09:31.720
up and down just if you have a burst of traffic coming in, you get new nodes. It's all super

09:31.720 --> 09:39.280
easy. You can set it up so that the cloud provider will automatically patch your operating

09:39.280 --> 09:46.160
system. It will automatically upgrade your Kubernetes and this is great from a DevOps

09:46.160 --> 09:54.000
perspective. It's super simple. It scales. It takes away work from the developer and

09:54.000 --> 10:02.040
the operator of the cluster. What we thought is why don't we meet in the middle? That's

10:02.040 --> 10:09.560
kind of like we have to run our own control plane in the confidential context but if we

10:09.560 --> 10:15.800
do this, we lose all of these smart features from the cloud provider so we will just reinvent

10:15.800 --> 10:23.320
them but inside the cluster. That means we can still do auto scaling. We can still join

10:23.320 --> 10:30.200
the nodes by themselves without any human interaction. We can still roll out OS updates

10:30.200 --> 10:38.560
and we can even roll out Kubernetes upgrades inside a running Kubernetes cluster. To explain

10:38.560 --> 10:45.520
how this works, I will first go on to how Kubernetes nodes and consolations can actually

10:45.520 --> 10:53.320
join the cluster without any outside interaction. What you have to understand here is these

10:53.320 --> 11:00.700
are all confidential VMs and they make heavy use of the measured boot chain. I think we

11:00.700 --> 11:05.880
already had some good introductions on this but I will still show you how this works in

11:05.880 --> 11:11.640
an example. First, in the confidential VM, we have the firmware and the firmware is basically

11:11.640 --> 11:21.400
just the first part that starts up. The main task here is to load up the first stage boot

11:21.400 --> 11:28.840
loader and to measure it. We measure it on AMD, SEV in the approach we are currently

11:28.840 --> 11:36.320
doing. It is measured into a virtual TPM. Then we load this boot loader and then we

11:36.320 --> 11:44.520
will start executing it. The boot loader just has the task of, in our case, loading the

11:44.520 --> 11:51.320
next stage and measuring it which is a unified kernel image. This is a very neat trick. It

11:51.320 --> 11:58.080
is basically just one blob that contains the Linux kernel and init-ramfs and also the kernel

11:58.080 --> 12:04.560
command line. The nice property here is we can measure all of these in one blob and don't

12:04.560 --> 12:12.880
have to take care of the individual components which can be quite hard to do correctly. Inside

12:12.880 --> 12:20.800
of this init-ramfs, we will use the kernel command line to extract the hash that we expect

12:20.800 --> 12:26.800
for the root file system. For this, we use the mverity which I will not go into too much

12:26.800 --> 12:33.480
detail about. It just allows us to have a read-only root file system and we know in

12:33.480 --> 12:41.600
advance that it has not been tampered with. We can efficiently check every block while

12:41.600 --> 12:50.280
it is read and before it is actually given to the user land. That is how we get to the

12:50.280 --> 12:55.680
root file system. Inside of this root file system, we have a small application and the

12:55.680 --> 13:05.320
task of this application is to join this node into the Kubernetes cluster. Next to the completely

13:05.320 --> 13:15.160
unmodifiable OS, we have a state disk. The only task of the state disk is to have the

13:15.160 --> 13:21.600
data for Kubernetes itself like container images and state at runtime that has to be

13:21.600 --> 13:30.080
stored on disk. This is initialized to be completely clean. It is encrypted and that

13:30.080 --> 13:44.080
is a component we need to operate. The next question is how do we make these things possible?

13:44.080 --> 13:51.140
For this, we deploy some microservices inside of constellation. These are the node operator.

13:51.140 --> 13:59.360
This is responsible for actually rolling out updates. It is the join service that tests

13:59.360 --> 14:04.120
nodes that are joining the cluster and decides if they are allowed to join or not. We also

14:04.120 --> 14:10.600
have a key service that is handling encryption keys and some more that are not really important

14:10.600 --> 14:20.080
right now. How does a node actually join the cluster? I mentioned there is the boot stripper

14:20.080 --> 14:26.760
that is started inside of the confidential virtual machine. It will autonomously search

14:26.760 --> 14:35.960
for the existing Kubernetes control plan and it will perform remote attestation using a

14:35.960 --> 14:48.280
tested TLS. It will basically use the attestation statement, for example from AMD, SCV, SMP.

14:48.280 --> 14:56.080
The join service already knows what measurements to expect from a correct node that is running

14:56.080 --> 15:03.200
the expected software. It can decide at this point if the booted node is running what you

15:03.200 --> 15:10.320
want it to run and decide if it is allowed to join the cluster. Based on this, the join

15:10.320 --> 15:16.840
service can then offer the node a join token which allows it to join the cluster and it

15:16.840 --> 15:24.440
will also hand out a permanent encryption key for the state disk. Next, we will have

15:24.440 --> 15:31.400
a quick look at how updates work. On a high level, we want the administrator to be in

15:31.400 --> 15:38.040
control. We don't want to give up complete control over the update process, but we want

15:38.040 --> 15:43.440
the actual execution to be completely automatic and seamless. We do this by basically just

15:43.440 --> 15:50.120
telling the cluster what to do and the rest is done by a Kubernetes operator which is

15:50.120 --> 15:59.800
a way to give in a desired state and let the cluster handle moving towards the state.

15:59.800 --> 16:08.200
One important thing to think about here is we are running in the cloud environment and

16:08.200 --> 16:15.800
we don't want you to depend on individual nodes. This is also what GKE and EKS and others

16:15.800 --> 16:23.080
are doing. We're saying if you want to upgrade, we will give you a new node that has the desired

16:23.080 --> 16:32.320
configuration and we will never try to do updates in place. How does the actual update

16:32.320 --> 16:39.600
work? We basically give in custom resources that describe the desired state, so the Kubernetes

16:39.600 --> 16:48.200
version and the OS image that we want to run on and some properties to actually verify

16:48.200 --> 16:53.400
like the expected measurements for the new image and tashes for the individual Kubernetes

16:53.400 --> 17:01.640
components. The operator reads this information and basically checks if the desired state

17:01.640 --> 17:11.040
matches reality and if it detects a mismatch, it will first stop any auto scaling operations

17:11.040 --> 17:17.960
that are happening in the cluster and then it will start replacing the nodes one by one.

17:17.960 --> 17:25.440
For this we use the different APIs by the cloud providers. In this case, we will just

17:25.440 --> 17:31.280
spawn a new node in the correct node group that has the desired configuration. We wait

17:31.280 --> 17:40.000
for the node to autonomously join the cluster and we wait for it to become ready. Next,

17:40.000 --> 17:45.720
we will coordinate and train the node, which just means we will safely move over your running

17:45.720 --> 17:51.440
workloads from this node to other nodes in the cluster and only if we are sure that your

17:51.440 --> 17:58.720
running workloads moved over, we will then remove the old node from the cluster. This

17:58.720 --> 18:08.200
is basically how you can get from having outdated nodes to having updated nodes and this will

18:08.200 --> 18:17.440
just go on until your whole cluster is up to date. You can also parallelize this. When

18:17.440 --> 18:24.040
this is done, you can just restart the auto scaler and move on with your day.

18:24.040 --> 18:38.800
All right, quick conclusion. In summary, the fundamental idea is we take this confidential

18:38.800 --> 18:46.200
cluster concept, enveloping the entire Kubernetes cluster instead of protecting single containers

18:46.200 --> 18:53.000
or parts. What we gain is we basically get all the orchestration for free. We need to

18:53.000 --> 18:59.840
protect the edges, all the ION and so forth. The downside is we can't isolate inside that

18:59.840 --> 19:06.160
cluster so it's one big envelope, of course. This works already. It's an open source tool.

19:06.160 --> 19:12.160
You can check out Constellation on GitHub and then try it locally or on one of the big

19:12.160 --> 19:18.200
clouds. From a Kubernetes perspective, it's just vanilla Kubernetes. Not surprising that

19:18.200 --> 19:23.880
it's certified. To give out some more references, if you're

19:23.880 --> 19:32.040
interested in this whole image part, there was the image-based Linux and TPM Devroom.

19:32.040 --> 19:38.680
There's a lot of talks on these topics. Also very interesting. This is the last talk here,

19:38.680 --> 19:42.280
but if you're interested in more confidential computing, sneaky little advertisement here

19:42.280 --> 19:48.440
for the OC3 Open Confidential Computing Conference that's going to happen in March. It's virtual

19:48.440 --> 19:52.600
free. You can just sign up and listen to the talks if you're interested. A bunch of the

19:52.600 --> 20:01.040
folks that were here also have a talk there. If you have any questions, please feel free

20:01.040 --> 20:30.640
to get in touch. That's it.

20:30.640 --> 20:36.160
The question was about the attested TLS. When we join nodes, we establish a C connection

20:36.160 --> 20:43.160
based on attested TLS. First of all, our implementation is open source. It's part of the Constellation

20:43.160 --> 20:52.680
source on GitHub. I think it's nothing fancy. We use the AMD SCV or Intel TDX and so forth,

20:52.680 --> 21:02.600
attestation statement to exchange a key as part of the data that's sent over. We bind

21:02.600 --> 21:09.200
the TLS session to that attested key. I guess there are a couple of implementations for

21:09.200 --> 21:14.120
attested TLS. They work more or less the same.

21:14.120 --> 21:23.480
I'm really proud to be part of the most familiar with confidential computing. There is this

21:23.480 --> 21:33.480
known flexibility in remote administration. It can be faked by a machine's host. I wonder

21:33.480 --> 21:40.480
if it is possible to fight out from the remote attestation of the whole cluster any single

21:40.480 --> 21:50.480
machine in the cluster may make a station and it goes unnoticed or not. All the others

21:50.480 --> 21:55.280
are, for example, truthful.

21:55.280 --> 22:02.640
Repeat the question. The question was there is a known vulnerability for attestation in

22:02.640 --> 22:09.160
confidential computing. If given this confidential cluster, if from the whole cluster attestation,

22:09.160 --> 22:17.080
I can refer to if one of the nodes is faking attestation. I have to say there were several

22:17.080 --> 22:23.000
vulnerabilities in several of the CC technologies over time. I'm not aware of what vulnerability

22:23.000 --> 22:24.000
you're referring to.

22:24.000 --> 22:39.720
The question is, you don't know if it's a specific gem on a specific provider or someone

22:39.720 --> 22:43.880
located in their garage. Is that what you mean?

22:43.880 --> 22:52.160
The way the cluster attestation works is you give the first node, it has a known configuration,

22:52.160 --> 22:58.160
and it will attest all other nodes based on this known attestation. If one node would

22:58.160 --> 23:04.400
be able to perfectly fake that attestation, you would not know from a cluster attestation

23:04.400 --> 23:08.720
perspective which node this would be.

23:08.720 --> 23:25.280
But yeah, I guess that's what you can say.

23:25.280 --> 23:31.760
It is super simple, but it is big TCB. Do you have any plans to reduce the TCB?

23:31.760 --> 23:41.920
As I said, this is a trade-off. Yes, it's a much larger TCB than SGA, much larger TCB

23:41.920 --> 23:46.280
even than confidential containers. We, of course, will try to make it as minimal as

23:46.280 --> 23:51.600
possible. The biggest leverage is of course the node OS and everything we can do inside

23:51.600 --> 23:55.000
there. We'll definitely try to improve there.

23:55.000 --> 24:05.800
Yes? Very good question.

24:05.800 --> 24:15.440
Sorry. Yeah, the question is part of the confidential VMs is the first component that's booted is

24:15.440 --> 24:21.000
the firmware. Do we have control of the firmware? Ideally, we would have. But what's provided

24:21.000 --> 24:25.320
by the cloud providers right now is Azure has something in preview that allows you to

24:25.320 --> 24:31.120
do that. It's not generally available. And GCP does not allow you that. So the firmware

24:31.120 --> 24:38.840
for at least GCP or Azure is completely controlled by them. On OpenStack with QMOKVM, you can

24:38.840 --> 24:43.520
potentially fully control the firmware. Yes, next question.

24:43.520 --> 24:51.400
Doesn't that create a huge trust problem? Does it have to trust the firmware to be secure?

24:51.400 --> 24:56.040
Does this create a trust problem? That's the question. Yeah. I mean, this is a controversy.

24:56.040 --> 25:00.400
I fully agree with you. This is not how we would like it. This is just the best we can

25:00.400 --> 25:05.800
have.
