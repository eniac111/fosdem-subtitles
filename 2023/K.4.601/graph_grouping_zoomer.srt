1
0:00:00.000 --> 0:00:12.000
The next work is on graph aggregation, graph grouping, especially on streaming graphs.

2
0:00:12.000 --> 0:00:18.000
Graph grouping is a really interesting challenge because we've all seen individualizations,

3
0:00:18.000 --> 0:00:21.000
hairballs, and complex graphs.

4
0:00:21.000 --> 0:00:25.000
When graph grouping allows you to split into these graphs, group them by certain attributes,

5
0:00:25.000 --> 0:00:31.000
and you have these better nodes that then can be selectively expanded, but you can also

6
0:00:31.000 --> 0:00:35.000
then, on a group graph, which is mostly a molar-type graph in many cases, you can also

7
0:00:35.000 --> 0:00:38.000
run graph analytics, which is a really interesting problem.

8
0:00:38.000 --> 0:00:45.000
I'm really excited to have Christopher here because both working on the streaming graphs

9
0:00:45.000 --> 0:00:50.000
as well as on temporal graphs, with graph grouping is a really challenging and interesting

10
0:00:50.000 --> 0:00:55.000
aspect, so I'm really looking forward to the talk.

11
0:00:55.000 --> 0:00:59.000
With our full schedule, welcome to the graph bedroom.

12
0:00:59.000 --> 0:01:00.000
Thanks.

13
0:01:00.000 --> 0:01:07.000
Here's a thanks for that introduction and also thanks for accepting my abstract for

14
0:01:07.000 --> 0:01:08.000
this talk.

15
0:01:08.000 --> 0:01:10.000
My name is Christopher Grost.

16
0:01:10.000 --> 0:01:13.000
I'm a PhD student of the University of Leipzig.

17
0:01:13.000 --> 0:01:19.000
I'm currently writing my thesis, so I'm glad that I bring some free time to doing this

18
0:01:19.000 --> 0:01:20.000
talk.

19
0:01:20.000 --> 0:01:28.000
Yeah, so about us or our team, so that's me and I have also a master's student working

20
0:01:28.000 --> 0:01:37.000
on this project we called Graphstream Zuma, and this project is a result of two master's

21
0:01:37.000 --> 0:01:43.840
thesis of our university from Elias Daman and Radha Nuridin, and I think it's also nice

22
0:01:43.840 --> 0:01:49.640
to show the result of a master's thesis here at the FOSTAIM.

23
0:01:49.640 --> 0:01:54.480
At the top is our professor of the database department, just to say that.

24
0:01:54.480 --> 0:01:55.480
Okay.

25
0:01:55.480 --> 0:02:01.320
I want you to take away from this talk, so you will see what the property Graphstream

26
0:02:01.320 --> 0:02:11.640
is and why it's important to have the streaming idea inside the graph topic, and second why

27
0:02:11.640 --> 0:02:17.400
you should or should not group a Graphstream, and then you will learn what the Graphstream

28
0:02:17.400 --> 0:02:44.800
Zuma's specific project is and the main idea behind it and which specific project you want.

29
0:02:44.800 --> 0:02:49.080
First we provide the Java API to do that.

30
0:02:49.080 --> 0:02:51.760
Okay.

31
0:02:51.760 --> 0:02:54.080
Let's just start with an event stream.

32
0:02:54.080 --> 0:03:01.360
I think I can skip that, so we say that anything that happens at a specific time and that can

33
0:03:01.360 --> 0:03:10.680
be recorded is an event, and if an event stream is now a stream of these events, so a sequence

34
0:03:10.680 --> 0:03:13.040
that is ordered by time.

35
0:03:13.040 --> 0:03:18.880
I think everyone knows why we need event processing, so we cannot store everything into a database

36
0:03:18.880 --> 0:03:25.000
or whatever to analyze it, so I want to identify these meaningful events and respond to them

37
0:03:25.000 --> 0:03:26.800
as quickly as possible.

38
0:03:26.800 --> 0:03:27.800
Okay.

39
0:03:27.800 --> 0:03:29.720
What is now a Graphstream?

40
0:03:29.720 --> 0:03:36.720
A Graphstream is an event stream where each event is a graph element or some graph update.

41
0:03:36.720 --> 0:03:41.760
Yeah, that could be edges, could be vertices, triples, or whatever.

42
0:03:41.760 --> 0:03:44.280
And a graph update could be a modification of this.

43
0:03:44.280 --> 0:03:49.800
For example, the addition of an edge, the removal of an edge, the addition of a property

44
0:03:49.800 --> 0:03:51.400
or to an edge or whatever.

45
0:03:51.400 --> 0:03:53.960
Yeah, so this is just an overview.

46
0:03:53.960 --> 0:03:58.280
Okay, why should I use now a Graphstream?

47
0:03:58.280 --> 0:04:07.040
Because I can execute on this Graphstream all algorithms and also all mathematical stuff

48
0:04:07.040 --> 0:04:10.840
from GraphTeary on this stream of graph data.

49
0:04:10.840 --> 0:04:17.160
For example, calculate patch strength concurrently with the evolving graph of the Graphstream.

50
0:04:17.160 --> 0:04:22.800
Okay, I can update my analysis results with a low latency if I combine that in a stream

51
0:04:22.800 --> 0:04:30.000
processing engine and my goal is to monitor the changes or monitor the changes in the

52
0:04:30.000 --> 0:04:34.360
graph or to create some notification or some reactivity.

53
0:04:34.360 --> 0:04:41.040
For example, if something, some average goes over a threshold, then I create a notification.

54
0:04:41.040 --> 0:04:46.760
Okay, by now the Graphstream could be very heterogeneous.

55
0:04:46.760 --> 0:04:53.760
That means consists of many different types and it can also occur in a high frequency.

56
0:04:53.760 --> 0:04:59.360
So it is advisable to summarize the graph elements in a specific way.

57
0:04:59.360 --> 0:05:05.360
And we can summarize graph elements by three criterias, for example, by time, that means

58
0:05:05.360 --> 0:05:10.680
graph elements that belong together, for example, the time window, we group them together.

59
0:05:10.680 --> 0:05:15.840
By structure, that means, for example, edges that share the same source or target vertex

60
0:05:15.840 --> 0:05:17.480
can be grouped together.

61
0:05:17.480 --> 0:05:22.360
And by content, that means vertices and edges that share the same label or a specific value

62
0:05:22.360 --> 0:05:24.400
of a property.

63
0:05:24.400 --> 0:05:31.160
So and we introduced for our algorithm, so-called grouping key functions, that means it is a

64
0:05:31.160 --> 0:05:35.080
function that maps a vertex or an edge to a grouping key.

65
0:05:35.080 --> 0:05:38.560
And that could be everything that is inside this vertex or edge.

66
0:05:38.560 --> 0:05:42.840
It could be labeled temporal information, some kind of property or whatever.

67
0:05:42.840 --> 0:05:47.960
So you can map everything that is represented by vertex or edge to a key function or to

68
0:05:47.960 --> 0:05:51.760
key and on this key, we group the graph.

69
0:05:51.760 --> 0:05:58.840
So that means at the end, the result is again a graph stream, but the grouped representation

70
0:05:58.840 --> 0:06:01.960
of that.

71
0:06:01.960 --> 0:06:04.040
Now you can question, OK, why I need this?

72
0:06:04.040 --> 0:06:06.360
So what are the applications of that?

73
0:06:06.360 --> 0:06:10.800
You can think about it as a preprocessing step.

74
0:06:10.800 --> 0:06:16.840
For example, before you calculate the patch frames, you just group the vertices to the

75
0:06:16.840 --> 0:06:19.520
city attribute of users together or something like this.

76
0:06:19.520 --> 0:06:23.680
You also use it as a preprocessing step.

77
0:06:23.680 --> 0:06:27.000
Second application could be as a postprocessing step.

78
0:06:27.000 --> 0:06:33.040
For example, after you apply the cross stream analysis, for example, community detection,

79
0:06:33.040 --> 0:06:38.480
you can now group on this cluster ID with our grouping algorithm to summarize the different

80
0:06:38.480 --> 0:06:42.300
communities together.

81
0:06:42.300 --> 0:06:46.000
You can also use it to understand the cross stream in more detail.

82
0:06:46.000 --> 0:06:52.600
For example, OK, just to know which vertex or edge types exist in my graph stream, how

83
0:06:52.600 --> 0:06:59.600
frequent these different types arrive or how vertices and edges with different characteristics

84
0:06:59.600 --> 0:07:01.200
are connected together.

85
0:07:01.200 --> 0:07:08.200
So just to get deeper insights or if you use our aggregation functions or aggregation,

86
0:07:08.200 --> 0:07:13.240
for example, counting or calculating an average on that, you can also get or reveal some hidden

87
0:07:13.240 --> 0:07:17.240
information that you would not see in the graph stream itself.

88
0:07:17.240 --> 0:07:21.280
OK, so this is an introduction.

89
0:07:21.280 --> 0:07:29.160
Now I explain the ideas behind this graph stream zoomer application just by an example.

90
0:07:29.160 --> 0:07:31.320
And then afterwards I summarize this.

91
0:07:31.320 --> 0:07:42.280
OK, for example, we're using bike rental data that can have two different graph schemas.

92
0:07:42.280 --> 0:07:50.240
I named them A and B. So the first graph schema A is that a bike rental is an edge between

93
0:07:50.240 --> 0:07:51.240
two station nodes.

94
0:07:51.240 --> 0:07:53.880
Yeah, you see it on the left side.

95
0:07:53.880 --> 0:07:59.720
So a station has several properties like the name, the number of bikes, latitude, longitude,

96
0:07:59.720 --> 0:08:01.160
and so on.

97
0:08:01.160 --> 0:08:07.000
And the trip edge has properties like the user ID, so who rented the bike, which bike

98
0:08:07.000 --> 0:08:10.640
was used by the bike ID, and from and to.

99
0:08:10.640 --> 0:08:15.520
So when this trip happens, until when, and the duration, for example, in seconds.

100
0:08:15.520 --> 0:08:18.080
So this is schema A on the left side.

101
0:08:18.080 --> 0:08:21.240
On the right side we have a more heterogeneous schema.

102
0:08:21.240 --> 0:08:25.480
So we have also stations and trips as vertices here.

103
0:08:25.480 --> 0:08:31.320
And we have also bike nodes and user nodes with several properties.

104
0:08:31.320 --> 0:08:40.520
And I just divided into this because I can explain the examples that follow a bit better

105
0:08:40.520 --> 0:08:46.160
compared to just using a simple schema like here on the left side.

106
0:08:46.160 --> 0:08:52.400
Okay, so how a graph stream of these schemas could look like this.

107
0:08:52.400 --> 0:08:59.000
Of schema A I have just these trip edges between two stations and all information inside.

108
0:08:59.000 --> 0:09:05.520
And from schema B I have here the trip nodes connected with all the other vertex types.

109
0:09:05.520 --> 0:09:10.800
This is just an excellent review of how a stream of this graph data could look like.

110
0:09:10.800 --> 0:09:21.080
Okay, so now begin with a very simple example of our grouping algorithm.

111
0:09:21.080 --> 0:09:27.260
So the input of the grouping is the graph stream, I think it's clear, and we need a

112
0:09:27.260 --> 0:09:29.300
grouping configuration.

113
0:09:29.300 --> 0:09:34.680
And the grouping configuration consists of five attributes.

114
0:09:34.680 --> 0:09:39.760
The first is the window because we are doing windowing on our graph stream.

115
0:09:39.760 --> 0:09:45.720
So I can define a window size, which is here, for example, 10 minutes.

116
0:09:45.720 --> 0:09:52.320
And then VgKey are the vertex grouping keys, that means the key functions that leads to

117
0:09:52.320 --> 0:09:53.800
the grouping of vertices.

118
0:09:53.800 --> 0:10:01.040
VgKey are the edge grouping keys, that means the key functions that are needed to group

119
0:10:01.040 --> 0:10:02.640
the edges together.

120
0:10:02.640 --> 0:10:10.240
And we also have a collection of vertex aggregate functions, this is VagG and EagG are the group

121
0:10:10.240 --> 0:10:12.840
of edge aggregate functions.

122
0:10:12.840 --> 0:10:20.880
So the four on the bottom are like just an array of several key functions and aggregate

123
0:10:20.880 --> 0:10:21.880
functions.

124
0:10:21.880 --> 0:10:29.640
Okay, and now having the input stream and applying that grouping, we get a result.

125
0:10:29.640 --> 0:10:35.000
And just looking like this, because we defined for the vertex grouping keys a function that

126
0:10:35.000 --> 0:10:38.120
maps every vertex to an integer value.

127
0:10:38.120 --> 0:10:43.960
And that results in, it doesn't matter which vertex exists in our graph stream, we group

128
0:10:43.960 --> 0:10:46.720
everything together to one vertex.

129
0:10:46.720 --> 0:10:50.340
And that's the white one with an empty label.

130
0:10:50.340 --> 0:10:54.320
And the same for edges, that means every edge that exists in our graph stream, we group

131
0:10:54.320 --> 0:10:58.920
them together to a super edge, we call them super vertex and super edge.

132
0:10:58.920 --> 0:11:03.360
With that is displayed here in gray.

133
0:11:03.360 --> 0:11:08.040
And because of the count aggregate functions, we add a new property to the super vertex

134
0:11:08.040 --> 0:11:13.240
with the count of all the vertices that are grouped together, and a new property to the

135
0:11:13.240 --> 0:11:16.720
super edge with also the count value.

136
0:11:16.720 --> 0:11:21.320
And this is now our result for every window that we defined on the graph stream, for example

137
0:11:21.320 --> 0:11:24.040
here the first window, second window and so on.

138
0:11:24.040 --> 0:11:31.080
That means we are creating a stream of grouped graphs here.

139
0:11:31.080 --> 0:11:35.520
Okay that is for schema A, and that's the most zoomed out view, so I group everything

140
0:11:35.520 --> 0:11:39.320
together that exists in the graph stream.

141
0:11:39.320 --> 0:11:45.560
For schema B and same grouping configuration, it looks the same, because it doesn't matter

142
0:11:45.560 --> 0:11:48.240
which type label exists, we group everything together.

143
0:11:48.240 --> 0:11:52.840
So we have just different counts, because we have a bit more vertices and edges, and

144
0:11:52.840 --> 0:11:56.240
also for the second window it looks the same.

145
0:11:56.240 --> 0:12:00.880
Okay so this is my first example, so the most zoomed out way.

146
0:12:00.880 --> 0:12:06.360
The second example are called a graph stream schema.

147
0:12:06.360 --> 0:12:12.320
So that means now we are using as a vertex grouping key a function that maps a vertex

148
0:12:12.320 --> 0:12:17.080
to its label, and a function that maps the edge to its label.

149
0:12:17.080 --> 0:12:24.000
And what's the result here, now our node has a label station and the count, because the

150
0:12:24.000 --> 0:12:29.760
count aggregate function stays the same, and our edge has a label trip.

151
0:12:29.760 --> 0:12:33.600
So it's more or less the same, because our graph streamer has just one node type and

152
0:12:33.600 --> 0:12:37.600
one edge type, and that's it also for the second window.

153
0:12:37.600 --> 0:12:43.600
Now it gets a bit more interesting when we are using schema B.

154
0:12:43.600 --> 0:12:49.400
The result here with the same grouping configuration as before is now this.

155
0:12:49.400 --> 0:12:54.440
That means every vertex is grouped by their label and every edge is grouped by their label,

156
0:12:54.440 --> 0:13:00.200
and now I have a schema representation of my graph stream.

157
0:13:00.200 --> 0:13:05.000
And again with all counts, because we are just using count aggregation.

158
0:13:05.000 --> 0:13:07.600
Okay and that's for the second window and so on and so on.

159
0:13:07.600 --> 0:13:14.280
I just left here the properties.

160
0:13:14.280 --> 0:13:20.000
Okay the next example, we stay with the vertex grouping keys and edge grouping keys, but

161
0:13:20.000 --> 0:13:23.920
now I added several aggregate functions to vertices and edges.

162
0:13:23.920 --> 0:13:29.000
For example I say okay for the vertices I want to calculate the average of all available

163
0:13:29.000 --> 0:13:34.160
bikes for these stations, and for the edge aggregate functions I want to have the minimum,

164
0:13:34.160 --> 0:13:41.160
maximum and average duration that a trip between two vertices has.

165
0:13:41.160 --> 0:13:48.200
And the result would be this, so same grouped graph, but now I have three additional properties

166
0:13:48.200 --> 0:13:54.320
on the edges, minimum duration, it's in seconds here, maximum duration and average duration,

167
0:13:54.320 --> 0:14:00.880
and the average bikes available on this station also as a new property.

168
0:14:00.880 --> 0:14:03.600
Same for the second window.

169
0:14:03.600 --> 0:14:11.640
And now my last example, it is, I called it, well not the last example, there is one more.

170
0:14:11.640 --> 0:14:18.040
So I added now here a second vertex grouping key function, and that's an important thing

171
0:14:18.040 --> 0:14:22.200
of the graph stream grouper, you can also implement your own grouping key function.

172
0:14:22.200 --> 0:14:27.520
For example this one called getdistrict consumes the latitude and longitude property of the

173
0:14:27.520 --> 0:14:34.880
vertices and then calculates like a district identifier, for example here of Prusall whatever,

174
0:14:34.880 --> 0:14:37.600
so in which district that belongs to.

175
0:14:37.600 --> 0:14:44.800
And then the graph is grouped on this representing district identifier.

176
0:14:44.800 --> 0:14:48.960
And we also say that for the edges we want to group the edges on the user type, that

177
0:14:48.960 --> 0:14:54.440
means for every edge, so in this data set we have a user type subscriber and something

178
0:14:54.440 --> 0:14:57.000
else we will see it shortly.

179
0:14:57.000 --> 0:15:02.800
That means for every edge I get new now two edges, one for this one user type, one for

180
0:15:02.800 --> 0:15:07.840
the other one, and also here some aggregate functions added.

181
0:15:07.840 --> 0:15:11.520
And the result is something like this.

182
0:15:11.520 --> 0:15:17.840
So here exemplified for three stations, so you see here the district ID one, two and

183
0:15:17.840 --> 0:15:22.480
three, and the average latitude and longitude, for example for visualization, proposes to

184
0:15:22.480 --> 0:15:31.760
place it on the map, and for the trips between two stations or between two district representers,

185
0:15:31.760 --> 0:15:36.720
we calculated also the minimum, maximum and average duration and counted them.

186
0:15:36.720 --> 0:15:43.720
And you see here the green edges are for the user type customer and the red edges are for

187
0:15:43.720 --> 0:15:48.080
user type subscriber.

188
0:15:48.080 --> 0:15:55.720
So the last example is then this one here, if I say okay, as vertex grouping key functions

189
0:15:55.720 --> 0:16:00.160
I say please extract me the identifier of that, that means every vertex that exists

190
0:16:00.160 --> 0:16:05.560
in the graph stream is placed here, and also for the edge identifier, that means since

191
0:16:05.560 --> 0:16:10.440
we all have unique identifiers, every vertex and edge is placed here, and this is more

192
0:16:10.440 --> 0:16:16.440
or less like a snapshot of the current state of this graph stream for the specific window.

193
0:16:16.440 --> 0:16:21.960
So therefore I call that zoomed in, it's the most zoomed in configuration that you could

194
0:16:21.960 --> 0:16:23.920
use.

195
0:16:23.920 --> 0:16:31.800
Okay, you could imagine implementing this is not that easy, so the master students found

196
0:16:31.800 --> 0:16:38.480
a way using Apache Flink and its table API, so everything works distributed since we are

197
0:16:38.480 --> 0:16:46.000
using just the API functions of Apache Flink, but we also figured out several implementation

198
0:16:46.000 --> 0:16:47.000
challenges.

199
0:16:47.000 --> 0:16:55.520
So first was to find a good graph representation, second one is since we are creating a workflow

200
0:16:55.520 --> 0:17:00.040
of this graph stream we have to ensure the chronological ordering of every step in this

201
0:17:00.040 --> 0:17:03.160
workflow.

202
0:17:03.160 --> 0:17:09.160
As a third point is also do you want to ensure the scalability, since if we scale out this

203
0:17:09.160 --> 0:17:19.240
algorithm the scalability should be also high, and also keep the state as minimum as possible,

204
0:17:19.240 --> 0:17:21.240
and provide a low latency and high throughput.

205
0:17:21.240 --> 0:17:29.160
So these were several challenges the master student solved quite well, and at the end

206
0:17:29.160 --> 0:17:34.960
we created a grouping operator looking like this.

207
0:17:34.960 --> 0:17:40.440
I don't want to get into details, it's just an architectural overview of every Flink

208
0:17:40.440 --> 0:17:43.400
steps we used.

209
0:17:43.400 --> 0:17:48.800
What is quite interesting is that we created an operator encapsulation of this, that means

210
0:17:48.800 --> 0:17:54.800
the operator consumes a graph stream at input and has a graph stream as output, that means

211
0:17:54.800 --> 0:18:00.240
you can combine several of these grouping operators, or if you define another graph

212
0:18:00.240 --> 0:18:06.320
stream algorithm that produces a graph stream as output you can just put them before.

213
0:18:06.320 --> 0:18:11.120
So you can like chaining these grouping operators together.

214
0:18:11.120 --> 0:18:17.760
And like I said so this consists of the mapping of the input data, the deduplication of vertices,

215
0:18:17.760 --> 0:18:26.400
grouping of vertices and edges and then mapping it to an output graph stream.

216
0:18:26.400 --> 0:18:32.760
How an API would look like, so it looks a bit messy but I think it's quite fast here,

217
0:18:32.760 --> 0:18:33.960
what's happening here.

218
0:18:33.960 --> 0:18:39.040
So first you have to define, I don't know if I see my...

219
0:18:39.040 --> 0:18:45.040
First you have to define the execution environment of Flink, then we read the data from some

220
0:18:45.040 --> 0:18:50.160
streaming source, for example a socket source or some Kafka stream or whatever you want,

221
0:18:50.160 --> 0:18:57.360
our Flink supports in our case, then we map it to a graph stream object which is the internal

222
0:18:57.360 --> 0:19:02.360
representation of our stream and here's the interesting part here, you define the grouping

223
0:19:02.360 --> 0:19:03.360
operator.

224
0:19:03.360 --> 0:19:07.680
So in the middle, that's the grouping config I showed you in the examples, you can define

225
0:19:07.680 --> 0:19:13.560
it here by an API, you set the window size, you set the vertex and edge grouping keys,

226
0:19:13.560 --> 0:19:19.120
you set the aggregate functions and so on and at the end you just execute this operator

227
0:19:19.120 --> 0:19:24.600
on this graph stream and then you can define the thing or just bring it to the console

228
0:19:24.600 --> 0:19:28.200
or whatever you want.

229
0:19:28.200 --> 0:19:37.520
So that's the operator call, how you define it in the API and current state, so we are

230
0:19:37.520 --> 0:19:43.200
about, all the students are about at 90% of the complete implementation of that.

231
0:19:43.200 --> 0:19:49.840
There were some, we figured out some bugs at the SQL or at the table API of Flink that

232
0:19:49.840 --> 0:19:57.280
were not fixed yet, so we had to define some workarounds that cost us time but yeah, like

233
0:19:57.280 --> 0:20:01.760
I said, we found a workaround and the next steps are that we plan an evaluation, so how

234
0:20:01.760 --> 0:20:06.800
is the latency and throughput of this complete system and we want to test it on real world

235
0:20:06.800 --> 0:20:12.280
and synthetic graph streams and maybe then publish some results, so let's see.

236
0:20:12.280 --> 0:20:20.160
And also the user defined key and aggregate functions are still under development.

237
0:20:20.160 --> 0:20:24.680
Okay then that's it, that's all folks.

238
0:20:24.680 --> 0:20:28.400
Please check out our GitHub repository or maybe you want to contribute, so we are open

239
0:20:28.400 --> 0:20:30.320
for this.

240
0:20:30.320 --> 0:20:36.480
The two links here at the bottom are also the icons are two other projects, the one

241
0:20:36.480 --> 0:20:43.560
is Gradube, this is a big temporary graph processing engine also based on Apache Flink,

242
0:20:43.560 --> 0:20:50.800
so there were also a main contributor to that project which was initially created by Marty

243
0:20:50.800 --> 0:20:57.920
Nungen, who is now a worker at Neo4j and yeah, also the temporary graph explorer is a user

244
0:20:57.920 --> 0:21:05.680
interface for that system where you can play around with the evolution of a graph but in

245
0:21:05.680 --> 0:21:07.480
a historical dataset.

246
0:21:07.480 --> 0:21:11.280
Okay, so that's it and please ask questions.

247
0:21:11.280 --> 0:21:12.280
Thanks.

248
0:21:12.280 --> 0:21:13.280
Yeah, please.

249
0:21:13.280 --> 0:21:30.280
On one slide you said a problem was to decide on the 520 I think is all, the optimal graph

250
0:21:30.280 --> 0:21:33.280
representation in the streaming model?

251
0:21:33.280 --> 0:21:34.280
Yeah.

252
0:21:34.280 --> 0:21:37.120
What's the answer?

253
0:21:37.120 --> 0:21:42.920
So the question was, so we had this challenge to find the optimal graph representation and

254
0:21:42.920 --> 0:21:44.640
what was the answer?

255
0:21:44.640 --> 0:21:52.960
The answer was a triple stream but a rich triple stream we called it since two property

256
0:21:52.960 --> 0:21:58.480
graph vertices are connected with an edge, that means every vertex consists of the label

257
0:21:58.480 --> 0:22:06.520
and possibly a big set of key value pairs as properties and the same for the edges.

258
0:22:06.520 --> 0:22:13.400
And this was our optimal because you can then model everything with this model but the counterpart

259
0:22:13.400 --> 0:22:18.400
of this was in here that we have to do a vertex dedeplication.

260
0:22:18.400 --> 0:22:22.840
For example, if you have a self-loop, so from one vertex to another one, we have a duplicate

261
0:22:22.840 --> 0:22:26.920
of that vertex, so we have to dedeplicate it afterwards for this model, so this was

262
0:22:26.920 --> 0:22:33.600
one counterpart, but we figured out that using every concept of the property graph model

263
0:22:33.600 --> 0:22:41.480
there as a triple is now the best choice for the students.

264
0:22:41.480 --> 0:22:42.480
So another?

265
0:22:42.480 --> 0:22:43.480
Yeah?

266
0:22:43.480 --> 0:22:49.480
Could you comment a bit more on the scalability, like what's the graph size, the distance,

267
0:22:49.480 --> 0:22:50.480
all in one?

268
0:22:50.480 --> 0:22:55.360
Yeah, so the question was some words about the scalability.

269
0:22:55.360 --> 0:23:01.960
The scalability is an open point of future work, so we don't have concrete results of

270
0:23:01.960 --> 0:23:09.680
that, so we tested it with some city bike data that we interpreted as a stream, so some

271
0:23:09.680 --> 0:23:18.640
historical data and we could process, I think it was about 600,000 edges in a few seconds,

272
0:23:18.640 --> 0:23:26.080
but this is just some first results and we have not tried it on big and high-frequent

273
0:23:26.080 --> 0:23:31.760
graph streams on a cluster, because we have huge fling clusters at our university, so

274
0:23:31.760 --> 0:23:37.200
we can benchmark the scalability of that in a later step.

275
0:23:37.200 --> 0:23:38.200
Yeah, thanks.

276
0:23:38.200 --> 0:23:39.200
Yeah?

277
0:23:39.200 --> 0:23:44.360
These aggregate functions, they part of, you know, do you, like a Java API and how do you

278
0:23:44.360 --> 0:23:45.360
define them exactly?

279
0:23:45.360 --> 0:23:50.760
Yeah, so the question was how we define the aggregate functions, so we have a set of predefined

280
0:23:50.760 --> 0:23:55.160
aggregate functions like the count, average min max, and then you have an interface you

281
0:23:55.160 --> 0:23:59.440
can implement against, so there's an interface called aggregate function and then you have

282
0:23:59.440 --> 0:24:06.040
to implement, I think, two or three functions and then you can define your own and use it

283
0:24:06.040 --> 0:24:12.760
then here on, yeah, there, where you say, where you give the classes of count and average

284
0:24:12.760 --> 0:24:18.000
probably you can give your own class and then it will, is used, yeah.

285
0:24:18.000 --> 0:24:19.000
Yeah?

286
0:24:19.000 --> 0:24:20.000
I have a question.

287
0:24:20.000 --> 0:24:27.720
Could you elaborate more on the real-life use cases or real-life applications?

288
0:24:27.720 --> 0:24:33.480
So the question is if we elaborate more real-life use cases and real-life questions.

289
0:24:33.480 --> 0:24:38.120
So applications.

290
0:24:38.120 --> 0:24:44.280
So since, yeah, so we are in, so I'm at a university that means we are missing real-world

291
0:24:44.280 --> 0:24:50.000
data a lot and we need also some input from companies to provide us with real-world data

292
0:24:50.000 --> 0:24:51.440
that we can use.

293
0:24:51.440 --> 0:24:59.960
So use cases could be, we also have to, we only have this bike sharing stuff or Twitter

294
0:24:59.960 --> 0:25:05.480
data and whatever and I think if you have something like this aggregated function like

295
0:25:05.480 --> 0:25:10.600
here an average property you can use because at the end it's a time series of changing

296
0:25:10.600 --> 0:25:15.960
values for example of the average property and of like defining a threshold and get the

297
0:25:15.960 --> 0:25:17.360
notification afterwards.

298
0:25:17.360 --> 0:25:20.760
I think this is maybe one good application afterwards.

299
0:25:20.760 --> 0:25:26.080
So for example if you have network traffic you see, okay, now the average, I don't know,

300
0:25:26.080 --> 0:25:30.600
packet size is increasing, now I get notified for example like this.

301
0:25:30.600 --> 0:25:32.600
This could be an application, yeah.

302
0:25:32.600 --> 0:25:42.640
I was thinking about something like, I would say, like they can be made in a video, something

303
0:25:42.640 --> 0:25:50.640
happened in a video because it's a frame could be considered as a graph and it could be taken

304
0:25:50.640 --> 0:25:54.600
like an event happening in the video stream of video.

305
0:25:54.600 --> 0:25:55.600
Yeah.

306
0:25:55.600 --> 0:25:56.600
Yeah, interesting application.

307
0:25:56.600 --> 0:26:01.560
Yeah, so the idea was to use like a video stream for that but then the question is how

308
0:26:01.560 --> 0:26:07.480
much graphy that is, could that maybe not be done just in a regular stream processing

309
0:26:07.480 --> 0:26:08.980
way.

310
0:26:08.980 --> 0:26:15.160
So I think this is just advisable to use that if you have some quite complex relationships

311
0:26:15.160 --> 0:26:21.800
between entities then you can use this system besides just an ordinary stream processing

312
0:26:21.800 --> 0:26:25.360
engine or complex event processing engine.

313
0:26:25.360 --> 0:26:33.960
So I think the unique point of this is to have the graph aspects into the streaming world.

314
0:26:33.960 --> 0:26:34.960
So any further questions?

315
0:26:34.960 --> 0:26:35.960
Yeah.

316
0:26:35.960 --> 0:26:36.960
So currently events are already edited, right?

317
0:26:36.960 --> 0:26:42.280
So you can only add to the graph but not delete from the graph by history.

318
0:26:42.280 --> 0:26:49.160
Yeah, so at the moment everything is interpreted as an insult only but since Flink supports

319
0:26:49.160 --> 0:26:56.640
everything like also updates and also deletions it is thinkable about some future work that

320
0:26:56.640 --> 0:27:02.400
we also can support this because at the end the result of us is also since we are using

321
0:27:02.400 --> 0:27:08.200
windowing it's an also an insult only stream at the end but if we maybe think about to

322
0:27:08.200 --> 0:27:13.840
remove the windowing aspect so they have something like a continuous aggregation or whatever

323
0:27:13.840 --> 0:27:21.960
then we need to support like a continuous addition on the end so to update already existing

324
0:27:21.960 --> 0:27:24.280
aggregating results.

325
0:27:24.280 --> 0:27:27.360
Okay, any further questions?

326
0:27:27.360 --> 0:27:29.480
Go out of line then.

327
0:27:29.480 --> 0:27:30.480
Thank you again.

328
0:27:30.480 --> 0:27:48.960
I appreciate it.

