1
0:00:00.000 --> 0:00:08.280
I am Gabor Sarnas and I'm here with David Proha.

2
0:00:08.280 --> 0:00:11.160
We work at CWI Amsterdam and we're here to

3
0:00:11.160 --> 0:00:13.980
present you the IDBC social network benchmark.

4
0:00:13.980 --> 0:00:15.440
What is the IDBC?

5
0:00:15.440 --> 0:00:18.480
The abbreviation stands for Linked Data Benchmark Council.

6
0:00:18.480 --> 0:00:21.320
It is a non-profit company founded in 2012,

7
0:00:21.320 --> 0:00:22.840
and its mission is to accelerate

8
0:00:22.840 --> 0:00:25.480
the progress in the field of graph data management.

9
0:00:25.480 --> 0:00:27.800
To this end, it designs and

10
0:00:27.800 --> 0:00:31.120
governs the use of graph benchmarks and everything we do

11
0:00:31.120 --> 0:00:34.480
is open source under the Apache version 2 license.

12
0:00:34.480 --> 0:00:36.600
From an organizational perspective,

13
0:00:36.600 --> 0:00:39.280
IDBC consists of more than 20 members

14
0:00:39.280 --> 0:00:42.320
who all have some vested interest in graph data management.

15
0:00:42.320 --> 0:00:45.040
We have financial service providers like the end group,

16
0:00:45.040 --> 0:00:48.500
database vendors like Oracle, Neo4j, and TIGR graph,

17
0:00:48.500 --> 0:00:50.520
cloud vendors like AWS,

18
0:00:50.520 --> 0:00:52.600
and hardware vendors like Intel.

19
0:00:52.600 --> 0:00:55.440
Also, we have individual contributors like David and

20
0:00:55.440 --> 0:00:58.000
me who contribute to the benchmarks.

21
0:00:58.000 --> 0:01:00.600
So to put things into context,

22
0:01:00.600 --> 0:01:03.200
the last two decades has seen a rise in

23
0:01:03.200 --> 0:01:06.360
the use of modern graph database management systems.

24
0:01:06.360 --> 0:01:09.120
Typically, the data model used in these systems is

25
0:01:09.120 --> 0:01:11.520
called the property graph which is a labeled graph,

26
0:01:11.520 --> 0:01:13.280
where both the nodes and the edges

27
0:01:13.280 --> 0:01:15.360
can have an arbitrary number of properties.

28
0:01:15.360 --> 0:01:18.400
For example, this is a small social network consisting of

29
0:01:18.400 --> 0:01:21.240
five person nodes and the single city node,

30
0:01:21.240 --> 0:01:22.800
which is the city of Spa.

31
0:01:22.800 --> 0:01:25.040
The properties can be on the nodes.

32
0:01:25.040 --> 0:01:28.160
For example, here the nodes have names and the edges have

33
0:01:28.160 --> 0:01:31.600
attributes like the date when the friendship was established.

34
0:01:31.600 --> 0:01:35.400
We can see that Bob and Carl met in 2015.

35
0:01:35.400 --> 0:01:38.240
If you want to run a query on the system,

36
0:01:38.240 --> 0:01:40.560
we can use a graph query where we

37
0:01:40.560 --> 0:01:42.720
look for matches of a given graph.

38
0:01:42.720 --> 0:01:46.260
So here, the query says we want to start from Bob.

39
0:01:46.260 --> 0:01:48.880
We want to use an arbitrary number of edges to reach

40
0:01:48.880 --> 0:01:51.680
some person who lives in Spa and we want

41
0:01:51.680 --> 0:01:55.460
to do an aggregation to return the number of those people.

42
0:01:55.460 --> 0:01:56.960
If you want to evaluate this,

43
0:01:56.960 --> 0:01:59.440
we then start from the person Bob,

44
0:01:59.440 --> 0:02:02.760
push to all the people transitively,

45
0:02:02.760 --> 0:02:06.480
which are known by Bob directly or via multiple edges.

46
0:02:06.480 --> 0:02:08.420
This means all four people here.

47
0:02:08.420 --> 0:02:11.600
We shrink it down to the people who actually live in Spa,

48
0:02:11.600 --> 0:02:14.600
then add up the results and get the result too.

49
0:02:14.600 --> 0:02:19.540
So graph databases use something called a visual graph syntax,

50
0:02:19.540 --> 0:02:22.040
also known as the SKR graph syntax,

51
0:02:22.040 --> 0:02:27.760
which is similar to the popular cipher language of Neo4j.

52
0:02:27.760 --> 0:02:30.040
Here, this query is actually really

53
0:02:30.040 --> 0:02:32.000
similar to the graph pattern that I have shown.

54
0:02:32.000 --> 0:02:34.760
So there are similarities in how the nodes are formulated,

55
0:02:34.760 --> 0:02:37.160
how the edges are captured in this text,

56
0:02:37.160 --> 0:02:39.760
and also how the transitive closure of

57
0:02:39.760 --> 0:02:42.280
the little asterisk is captured in the query language.

58
0:02:42.280 --> 0:02:46.680
So this is a very intuitive and concise way of formulating the queries.

59
0:02:46.680 --> 0:02:48.320
If we deconstruct this query,

60
0:02:48.320 --> 0:02:51.280
you can see three main components.

61
0:02:51.280 --> 0:02:52.760
The one is relational operators.

62
0:02:52.760 --> 0:02:55.240
Obviously, we still need relational operators.

63
0:02:55.240 --> 0:02:58.600
We want to be able to identify people by filtering.

64
0:02:58.600 --> 0:02:59.720
So we filter for Bob,

65
0:02:59.720 --> 0:03:00.940
we filter for Spa,

66
0:03:00.940 --> 0:03:02.960
and also we want to sometimes aggregate.

67
0:03:02.960 --> 0:03:06.000
So the count aggregation is part of this query.

68
0:03:06.000 --> 0:03:09.720
The pathfinding is really elegant in this formulation because we have

69
0:03:09.720 --> 0:03:13.720
nodes asterisk which captures that we can use an arbitrary number of edges.

70
0:03:13.720 --> 0:03:17.760
The pattern matching which connects the person to Spa is

71
0:03:17.760 --> 0:03:20.080
also very concise and readable.

72
0:03:20.080 --> 0:03:26.240
So what is interesting from a future of our perspective on graph databases?

73
0:03:26.240 --> 0:03:29.640
Obviously, relational operators are quite well known at this point,

74
0:03:29.640 --> 0:03:32.800
and there are endless papers and techniques on how to implement these.

75
0:03:32.800 --> 0:03:36.920
But we believe that pathfinding and pattern matching is really good in

76
0:03:36.920 --> 0:03:40.880
graph databases compared to traditional relational systems because they provide

77
0:03:40.880 --> 0:03:44.380
a more concise syntax and better algorithms and implementations.

78
0:03:44.380 --> 0:03:47.120
Interestingly enough, even in the last 15 years,

79
0:03:47.120 --> 0:03:50.840
there have been lots of papers on better BFS algorithms,

80
0:03:50.840 --> 0:03:54.880
better factorization representations for graph patterns,

81
0:03:54.880 --> 0:03:57.480
multi-waver, squares optimal joins, and so on.

82
0:03:57.480 --> 0:04:00.960
So we believe that this should be adopted by more and more systems.

83
0:04:00.960 --> 0:04:05.240
To this end, we designed benchmarks that try to push the state of the art and

84
0:04:05.240 --> 0:04:07.800
force systems to adopt better and better techniques.

85
0:04:07.800 --> 0:04:10.440
David, we'll talk about these benchmarks.

86
0:04:10.440 --> 0:04:15.040
Yeah. So I will give an overview about the social network benchmark.

87
0:04:15.040 --> 0:04:19.440
So first, we'll go through three steps of this benchmark.

88
0:04:19.440 --> 0:04:22.080
So the datasets, two example queries,

89
0:04:22.080 --> 0:04:25.000
and the update operations done in this benchmark.

90
0:04:25.000 --> 0:04:29.720
So here we see a small example of the datasets where on the left side,

91
0:04:29.720 --> 0:04:33.240
we see persons with friendships, forms, and network,

92
0:04:33.240 --> 0:04:37.840
and these persons post messages on the social network and can

93
0:04:37.840 --> 0:04:41.680
reply to each other forming a tree-shaped data structure.

94
0:04:41.680 --> 0:04:48.400
Now, we will do one query on this very small dataset example.

95
0:04:48.400 --> 0:04:52.280
So with query nine, we want to retrieve messages posted by

96
0:04:52.280 --> 0:04:58.240
a given person friends and friends of friends before a given date.

97
0:04:58.240 --> 0:05:00.280
The dates are here short for simplicity.

98
0:05:00.280 --> 0:05:02.720
So if we would start with Bob,

99
0:05:02.720 --> 0:05:05.880
we will traverse through their friends and friends of friends,

100
0:05:05.880 --> 0:05:11.560
retrieve the messages, and then filter out the ones that are actually before Saturday.

101
0:05:11.560 --> 0:05:14.960
Then we touch upon 10 nodes in this data.

102
0:05:14.960 --> 0:05:17.680
Suppose we would start from another person,

103
0:05:17.680 --> 0:05:19.360
so for example, Finn,

104
0:05:19.360 --> 0:05:23.960
and we traverse again to their friends and friends of friends.

105
0:05:23.960 --> 0:05:30.200
Here we see that we touched upon five different nodes,

106
0:05:30.200 --> 0:05:33.080
so half of the one of Bob.

107
0:05:33.080 --> 0:05:36.660
This difference can actually be troublesome

108
0:05:36.660 --> 0:05:40.800
since run times for the same queries are different,

109
0:05:40.800 --> 0:05:44.960
and therefore, it doesn't help in understanding what's happening.

110
0:05:44.960 --> 0:05:46.680
So for this benchmark,

111
0:05:46.680 --> 0:05:51.760
we actually want to select parameters that have similar run times,

112
0:05:51.760 --> 0:05:56.080
and also to actually stress the technical difficulties in these systems.

113
0:05:56.080 --> 0:05:59.920
So we select the parameters more carefully.

114
0:05:59.920 --> 0:06:04.560
So here we see a example of when we do not select the parameters carefully,

115
0:06:04.560 --> 0:06:06.040
just a uniform random,

116
0:06:06.040 --> 0:06:07.920
and we can see here a trial model,

117
0:06:07.920 --> 0:06:11.720
a distribution by model, and one with many outliers,

118
0:06:11.720 --> 0:06:14.040
and we don't want that.

119
0:06:14.040 --> 0:06:16.440
So in the datasets,

120
0:06:16.440 --> 0:06:18.560
there are also statistics provided.

121
0:06:18.560 --> 0:06:20.800
In this example,

122
0:06:20.800 --> 0:06:24.680
for each person, the number of friends and friends of friends,

123
0:06:24.680 --> 0:06:27.800
and then we want to select persons with

124
0:06:27.800 --> 0:06:31.960
similar number to get more predictable run times.

125
0:06:31.960 --> 0:06:33.960
So if we do that,

126
0:06:33.960 --> 0:06:38.080
then we can see here example that we have unimodal distributions,

127
0:06:38.080 --> 0:06:40.440
a bit very tight run times,

128
0:06:40.440 --> 0:06:47.160
and that improves also in understanding these behavior of the queries.

129
0:06:47.160 --> 0:06:50.320
So now we're going to the updates,

130
0:06:50.320 --> 0:06:55.280
and for example, if we have an GI who wants to be friends,

131
0:06:55.280 --> 0:06:57.040
we insert a nose edge,

132
0:06:57.040 --> 0:07:00.680
and this is then formed into the network.

133
0:07:00.680 --> 0:07:04.080
Suppose that the next operation is inserted in comments.

134
0:07:04.080 --> 0:07:08.560
So GI comments replies on a message posted by Eve,

135
0:07:08.560 --> 0:07:12.120
and both messages are posted on the same date.

136
0:07:12.120 --> 0:07:14.680
Then we have another problem,

137
0:07:14.680 --> 0:07:18.360
because when we are executing these operations concurrently,

138
0:07:18.360 --> 0:07:24.480
it can happen that the reply is earlier than the message in such a network,

139
0:07:24.480 --> 0:07:27.960
closing an error, and to mitigate this,

140
0:07:27.960 --> 0:07:31.200
we introduce dependency tracking.

141
0:07:31.200 --> 0:07:33.480
So for each operation,

142
0:07:33.480 --> 0:07:35.000
and also includes the edges,

143
0:07:35.000 --> 0:07:36.480
but just for simplicity,

144
0:07:36.480 --> 0:07:40.520
the notes are here with the dependent dates.

145
0:07:40.520 --> 0:07:45.280
We include for each operation a creation date and dependent date.

146
0:07:45.280 --> 0:07:49.240
The creation date is when it's scheduled to be executed,

147
0:07:49.240 --> 0:07:51.960
and the dependent date is the one that's like,

148
0:07:51.960 --> 0:07:53.520
in this case for M6,

149
0:07:53.520 --> 0:07:56.840
is the creation date of M3.

150
0:07:56.840 --> 0:08:02.040
Here, we can see actually that each operation is dependent on

151
0:08:02.040 --> 0:08:05.480
each other forming a whole chain in the social network.

152
0:08:05.480 --> 0:08:10.840
Suppose now that Eve wants to leave the social network and removes our account.

153
0:08:10.840 --> 0:08:14.080
So we start with deleting the notes Eve,

154
0:08:14.080 --> 0:08:16.040
and this will trigger cascading effects,

155
0:08:16.040 --> 0:08:19.720
but since we then need to remove the edges connected to Eve,

156
0:08:19.720 --> 0:08:21.640
the messages posted,

157
0:08:21.640 --> 0:08:25.080
and also the replies to those messages.

158
0:08:25.080 --> 0:08:29.800
We can actually see like there's this huge cascading effects,

159
0:08:29.800 --> 0:08:33.880
and that can actually have a large impact on the data distribution,

160
0:08:33.880 --> 0:08:38.040
and also therefore the executability of these operations.

161
0:08:38.040 --> 0:08:43.080
Furthermore, it also influences for selecting the parameters,

162
0:08:43.080 --> 0:08:44.920
which we have shown before,

163
0:08:44.920 --> 0:08:48.360
and we want to include this deletes because it prohibits

164
0:08:48.360 --> 0:08:50.400
appends only data structures in databases,

165
0:08:50.400 --> 0:08:53.560
and also stress the garbage collector of these systems.

166
0:08:53.560 --> 0:08:57.800
Now, we are going to give another example to also stress

167
0:08:57.800 --> 0:09:00.360
the temporal aspect of this benchmark.

168
0:09:00.360 --> 0:09:04.680
So suppose we want to find a path between two persons.

169
0:09:04.680 --> 0:09:08.080
So we have a start person and destination person,

170
0:09:08.080 --> 0:09:10.840
and for example, Finn and Gia.

171
0:09:10.840 --> 0:09:15.480
Then we can see here that we have a four-hole path between these persons.

172
0:09:15.480 --> 0:09:17.160
But at one point in the benchmark,

173
0:09:17.160 --> 0:09:21.720
it can happen that a note edge is removed,

174
0:09:21.720 --> 0:09:24.360
and then there is no path anymore.

175
0:09:24.360 --> 0:09:27.240
It can also happen that there's another edge

176
0:09:27.240 --> 0:09:28.880
inserted between Carl and Gia,

177
0:09:28.880 --> 0:09:31.280
and then we have a path again.

178
0:09:31.280 --> 0:09:33.880
So for the same parameters,

179
0:09:33.880 --> 0:09:36.280
we can actually have three different outcomes.

180
0:09:36.280 --> 0:09:39.400
To mitigate this, we do temporal parameter selection.

181
0:09:39.400 --> 0:09:43.640
So each parameter is assigned in a time bucket to actually ensure that

182
0:09:43.640 --> 0:09:48.520
we have similar results and therefore also similar run times.

183
0:09:48.520 --> 0:09:51.360
Now, going through the benchmark workflow,

184
0:09:51.360 --> 0:09:55.360
so we start by the data gen,

185
0:09:55.360 --> 0:09:57.000
and the data gen provides us with

186
0:09:57.000 --> 0:10:01.600
a temporal graph spanning of social media activity for three years,

187
0:10:01.600 --> 0:10:07.920
and it is simulated similar to Facebook social network.

188
0:10:07.920 --> 0:10:14.680
It's a Spark-based data generator that can generate data up to 30 terabytes,

189
0:10:14.680 --> 0:10:18.480
and it contains skewed datasets,

190
0:10:18.480 --> 0:10:24.400
for example, with the notes and person data in this data.

191
0:10:24.400 --> 0:10:29.840
So the output is a dataset suitable for loading into the system on a test,

192
0:10:29.840 --> 0:10:34.840
updates which are then executed during the benchmark,

193
0:10:34.840 --> 0:10:38.160
and statistics where we can select parameters.

194
0:10:38.160 --> 0:10:41.880
This election of the parameters is done in the parameter generator,

195
0:10:41.880 --> 0:10:45.040
and this ensures the stable query run times,

196
0:10:45.040 --> 0:10:47.960
and assigns parameters into a temporal bucket.

197
0:10:47.960 --> 0:10:52.760
So a parameter, it may include parameters that once are inserted

198
0:10:52.760 --> 0:11:00.120
into the datasets or before they are removed from the network.

199
0:11:00.120 --> 0:11:05.040
So then we have a benchmark driver who schedules these operations,

200
0:11:05.040 --> 0:11:10.520
and it ensures that they can be executed with using the dependency tracking,

201
0:11:10.520 --> 0:11:16.440
and this is especially important when executing the operations concurrently.

202
0:11:16.440 --> 0:11:20.600
Lastly, we have the system on the test where we have, for example,

203
0:11:20.600 --> 0:11:22.160
graph databases, triple stores,

204
0:11:22.160 --> 0:11:24.240
or relational databases, and now,

205
0:11:24.240 --> 0:11:27.760
Garber will go further into the workloads.

206
0:11:30.440 --> 0:11:34.480
Okay. So graph workloads are actually quite

207
0:11:34.480 --> 0:11:37.120
diverse in terms of what they are trying to achieve,

208
0:11:37.120 --> 0:11:40.440
and our benchmark reflects that by having multiple workloads.

209
0:11:40.440 --> 0:11:43.320
We have the social network benchmark interactive workload,

210
0:11:43.320 --> 0:11:45.440
which is transactional in nature.

211
0:11:45.440 --> 0:11:47.640
So it has loads of concurrent operations.

212
0:11:47.640 --> 0:11:50.120
The queries here are relatively simple.

213
0:11:50.120 --> 0:11:52.720
So they always start in one or two person nodes,

214
0:11:52.720 --> 0:11:55.280
the same as David presented before,

215
0:11:55.280 --> 0:11:58.680
and here the systems are striving to achieve a high throughput.

216
0:11:58.680 --> 0:12:02.760
So the competition is getting as many operations per second as possible.

217
0:12:02.760 --> 0:12:07.640
We are happy to report that we have official results from the last three years,

218
0:12:07.640 --> 0:12:11.440
where our system started with slightly above 5,000 operations per second,

219
0:12:11.440 --> 0:12:13.360
and have sped up exponentially,

220
0:12:13.360 --> 0:12:18.760
now being close to 17,000 operations per second on a 100 gigabyte dataset.

221
0:12:18.760 --> 0:12:23.200
The other workload of the social network benchmark is called business intelligence.

222
0:12:23.200 --> 0:12:27.600
This is an analytical workload where the queries touch on large portions of the data.

223
0:12:27.600 --> 0:12:32.760
For example, this query in this slide shows a case where we start from

224
0:12:32.760 --> 0:12:37.440
a given country and then find all triangles of friendships in that country.

225
0:12:37.440 --> 0:12:40.360
It's easy to see that this is a very heavy hitting operation.

226
0:12:40.360 --> 0:12:42.800
It may touch on billions of edges in the graph,

227
0:12:42.800 --> 0:12:46.600
and it also has to do a complex computation to find those people.

228
0:12:46.600 --> 0:12:51.800
So here, system can use either a bulk update or a concurrent update method,

229
0:12:51.800 --> 0:12:57.080
and they should also strive to get both the high throughput and low-query run times.

230
0:12:57.080 --> 0:12:58.880
This benchmark is relatively new.

231
0:12:58.880 --> 0:13:00.520
It was released at the end of last year,

232
0:13:00.520 --> 0:13:05.400
so we only have a single result which was done by a collaboration of Tiger Graph and AMD.

233
0:13:05.400 --> 0:13:08.720
We're happy to report that there are more audits underway,

234
0:13:08.720 --> 0:13:12.760
so we are going to release more results in 2023.

235
0:13:12.760 --> 0:13:16.200
So probably you can see from this presentation that

236
0:13:16.200 --> 0:13:19.960
these benchmarks can get fairly complex and implementing them is not trivial.

237
0:13:19.960 --> 0:13:23.680
So we did our best to provide everything our users need.

238
0:13:23.680 --> 0:13:25.720
For each of the workloads that we have presented,

239
0:13:25.720 --> 0:13:26.920
we have a specification,

240
0:13:26.920 --> 0:13:28.920
we have detailed academic papers who

241
0:13:28.920 --> 0:13:33.440
motivate the design choices and the architecture of these benchmarks.

242
0:13:33.440 --> 0:13:37.280
We release the data generator as well as pre-generated datasets,

243
0:13:37.280 --> 0:13:39.180
and we have benchmark drivers and

244
0:13:39.180 --> 0:13:42.200
at least two reference implementations for each of the workloads.

245
0:13:42.200 --> 0:13:45.880
Moreover, we have guidelines on how to execute these benchmarks correctly,

246
0:13:45.880 --> 0:13:48.160
how to validate the results of a given system,

247
0:13:48.160 --> 0:13:54.160
and how to ensure that the system will not lose your data or mingle up the transactions.

248
0:13:54.160 --> 0:13:57.760
So we have asset compliance tests and recovery tests.

249
0:13:57.760 --> 0:14:00.520
This leads us to our auditing process.

250
0:14:00.520 --> 0:14:01.920
Similarly to the TPC,

251
0:14:01.920 --> 0:14:04.240
the Transaction Processing Performance Council,

252
0:14:04.240 --> 0:14:07.600
we have a rigorous auditing process where vendors can

253
0:14:07.600 --> 0:14:10.560
commission an independent third party who will

254
0:14:10.560 --> 0:14:14.520
rerun the benchmark in an executable and reproducible manner,

255
0:14:14.520 --> 0:14:18.720
and they will write up it as a full disclosure report so that

256
0:14:18.720 --> 0:14:23.360
the benchmark is understandable by whoever wants to see that result.

257
0:14:23.360 --> 0:14:26.600
This is important because ad-dbc is trademarked worldwide,

258
0:14:26.600 --> 0:14:32.080
and we only allow official audited results to use the term ad-dbc benchmark result.

259
0:14:32.080 --> 0:14:35.880
This is not to say that we don't allow people to use this benchmark.

260
0:14:35.880 --> 0:14:40.480
Researchers, practitioners, and developers are welcome to use the benchmark.

261
0:14:40.480 --> 0:14:44.080
They can run it, they can report the results if it is accompanied by

262
0:14:44.080 --> 0:14:48.960
the appropriate disclaimer that this is not an official ad-dbc benchmark result.

263
0:14:48.960 --> 0:14:53.320
I would like to talk a bit about standard GraphQL languages.

264
0:14:53.320 --> 0:14:55.400
This is an important topic because this has been

265
0:14:55.400 --> 0:14:58.260
a pain point for Graph systems for many years.

266
0:14:58.260 --> 0:15:01.600
It's a bit of a tower of Babel out there with many languages,

267
0:15:01.600 --> 0:15:04.800
both of them using some visual Graph syntax,

268
0:15:04.800 --> 0:15:08.280
but always with slightly different semantics and a slightly different syntax,

269
0:15:08.280 --> 0:15:13.160
which makes it difficult for users to adopt these techniques and may put them in

270
0:15:13.160 --> 0:15:15.920
a position of being locked in by their vendors.

271
0:15:15.920 --> 0:15:18.560
So in the next couple of years,

272
0:15:18.560 --> 0:15:21.040
there are going to be new standard query languages.

273
0:15:21.040 --> 0:15:24.600
These focus on pathfinding and pattern matching.

274
0:15:24.600 --> 0:15:26.760
The first one is called SQL-PGQ.

275
0:15:26.760 --> 0:15:28.840
This is an extension to the SQL language,

276
0:15:28.840 --> 0:15:31.240
and PGQ stands for property graph queries.

277
0:15:31.240 --> 0:15:34.200
This is going to be released next summer,

278
0:15:34.200 --> 0:15:39.040
and GQL, the standalone GraphQL language is going to come out in 2024.

279
0:15:39.040 --> 0:15:42.480
We are happy to report that even though we have two new languages,

280
0:15:42.480 --> 0:15:44.520
the pattern matching core of them,

281
0:15:44.520 --> 0:15:48.800
the visual Graph syntax that we all know and love is going to be the same.

282
0:15:48.800 --> 0:15:52.560
So users can port at least those bits of their queries.

283
0:15:52.560 --> 0:15:54.920
To give you a taste of how this will look like,

284
0:15:54.920 --> 0:15:57.520
here is query nine that David presented in

285
0:15:57.520 --> 0:16:00.200
the social network benchmark interactive workload.

286
0:16:00.200 --> 0:16:02.480
This query can be formulated in SQL.

287
0:16:02.480 --> 0:16:04.920
It's not too difficult, but the new variants,

288
0:16:04.920 --> 0:16:09.840
SQL-PGQ and GQL can represent it as terms of a graph pattern,

289
0:16:09.840 --> 0:16:12.760
and this is a much more concise formulation.

290
0:16:12.760 --> 0:16:16.560
The difference is even more pronounced for query 13 with the path queries.

291
0:16:16.560 --> 0:16:19.320
Here, we can see that in SQL-PGQ,

292
0:16:19.320 --> 0:16:22.200
the pattern is really similar to the visual representation.

293
0:16:22.200 --> 0:16:23.840
It just has a source,

294
0:16:23.840 --> 0:16:26.680
a target, and arbitrary amount of

295
0:16:26.680 --> 0:16:29.560
nose edges denoted by nose asterisk in between.

296
0:16:29.560 --> 0:16:32.800
In SQL, this is a lot less readable,

297
0:16:32.800 --> 0:16:34.600
hard to maintain, and it's even less

298
0:16:34.600 --> 0:16:36.240
sufficient because it just implements

299
0:16:36.240 --> 0:16:38.600
a unidirectional search algorithm instead of doing

300
0:16:38.600 --> 0:16:42.320
a bidirectional search which has a better algorithmic complexity.

301
0:16:42.320 --> 0:16:46.320
The way IDBC is involved in these new query languages is manifold.

302
0:16:46.320 --> 0:16:50.240
First, it had the G-core design language released in 2018,

303
0:16:50.240 --> 0:16:52.200
which influenced these benchmarks.

304
0:16:52.200 --> 0:16:55.400
Then IDBC has the formal semantics working group which

305
0:16:55.400 --> 0:16:58.720
formalized the pattern matching core of these new languages.

306
0:16:58.720 --> 0:17:01.800
The IDBC is doing further research to

307
0:17:01.800 --> 0:17:05.680
conduct to advance the state of the art on graph schemas.

308
0:17:05.680 --> 0:17:08.480
We have an industry driven and a theory driven group,

309
0:17:08.480 --> 0:17:12.960
and what they do will end up in the new versions of these languages.

310
0:17:12.960 --> 0:17:16.360
An outlook is the IDBC Graph Analytics benchmark.

311
0:17:16.360 --> 0:17:22.400
This is a more wide benchmark because it can target analytical libraries like

312
0:17:22.400 --> 0:17:27.040
NetworkX, distributed systems like Apache Giraffe or the GraphBlast API.

313
0:17:27.040 --> 0:17:30.680
So this is everything that has to do with analyzing large graphs.

314
0:17:30.680 --> 0:17:33.240
Here, the graph is an untyped,

315
0:17:33.240 --> 0:17:36.440
unattributed graph so there are no properties or no labels.

316
0:17:36.440 --> 0:17:39.400
We do use the IDBC social network benchmark dataset,

317
0:17:39.400 --> 0:17:41.400
but it is stripped down to the person,

318
0:17:41.400 --> 0:17:43.080
knows person, core graph.

319
0:17:43.080 --> 0:17:45.640
Additionally, we have included a number of

320
0:17:45.640 --> 0:17:49.480
well-known datasets like Graph 500, Twitter, and so on.

321
0:17:49.480 --> 0:17:53.240
The algorithms that we run are mostly well-known graph algorithms.

322
0:17:53.240 --> 0:17:55.800
There is the BFS which starts from

323
0:17:55.800 --> 0:17:59.520
a given node and assigns the number of steps that need to be

324
0:17:59.520 --> 0:18:02.080
taken to all of the other nodes to reach them.

325
0:18:02.080 --> 0:18:05.680
We have the famous PageRank centrality algorithm which highlights

326
0:18:05.680 --> 0:18:07.800
the most important nodes in the network,

327
0:18:07.800 --> 0:18:11.760
and we have the local clustering coefficient,

328
0:18:11.760 --> 0:18:14.400
community detection using label propagation,

329
0:18:14.400 --> 0:18:17.160
weekly connected components, and shortest paths.

330
0:18:17.160 --> 0:18:20.760
So this benchmark is a bit simpler than the social network benchmark.

331
0:18:20.760 --> 0:18:23.360
It does not have a rigorous auditing process.

332
0:18:23.360 --> 0:18:26.880
We trust people that they can run this benchmark efficiently and

333
0:18:26.880 --> 0:18:30.560
correctly on their own infrastructure and they can report results.

334
0:18:30.560 --> 0:18:33.560
If they do so, they will be able to participate in

335
0:18:33.560 --> 0:18:35.120
the GraphRitix competition which has

336
0:18:35.120 --> 0:18:37.760
a leaderboard for the best implementations.

337
0:18:37.760 --> 0:18:41.360
So wrapping up, you should consider joining

338
0:18:41.360 --> 0:18:45.040
the IDBC because members can participate in the benchmark design.

339
0:18:45.040 --> 0:18:48.840
They have a say in where we are going in terms of including new features.

340
0:18:48.840 --> 0:18:51.380
They can commission audits if they are vendors,

341
0:18:51.380 --> 0:18:56.080
and members can gain access to these ISO standard drafts that I mentioned,

342
0:18:56.080 --> 0:18:57.800
CEQA, PGQ, and GQL.

343
0:18:57.800 --> 0:19:00.760
Otherwise, these are not available to general public.

344
0:19:00.760 --> 0:19:05.400
Pricing-wise, this is free for individuals and there is a yearly fee for companies.

345
0:19:05.400 --> 0:19:08.760
So to sum up, we have presented three benchmark,

346
0:19:08.760 --> 0:19:11.280
the social network benchmarks interactive workload,

347
0:19:11.280 --> 0:19:12.940
it's business intelligence workload,

348
0:19:12.940 --> 0:19:15.460
and the GraphRitix graph algorithms workload.

349
0:19:15.460 --> 0:19:16.860
We have more benchmarks.

350
0:19:16.860 --> 0:19:21.240
There is semantic publishing benchmark which is targeting RDF systems.

351
0:19:21.240 --> 0:19:24.160
Set in the media and publishing industry.

352
0:19:24.160 --> 0:19:27.240
There is the financial benchmark which is going to be released this year,

353
0:19:27.240 --> 0:19:30.080
which targets distributed systems and it uses

354
0:19:30.080 --> 0:19:33.620
the financial fraud detection domain as its area,

355
0:19:33.620 --> 0:19:36.680
and it imposes strict latency bounds on queries.

356
0:19:36.680 --> 0:19:39.540
So this is quite a different workload from the previous ones.

357
0:19:39.540 --> 0:19:43.520
Of course, graphs are ubiquitous and they have loads of use cases.

358
0:19:43.520 --> 0:19:45.840
So there are many future benchmark ideas

359
0:19:45.840 --> 0:19:48.680
including graph neural network, mining, and streaming.

360
0:19:48.680 --> 0:19:51.720
Thank you very much and we're open to any questions.

361
0:19:57.240 --> 0:19:59.000
Yes.

362
0:19:59.000 --> 0:20:00.560
So in this one overview,

363
0:20:00.560 --> 0:20:04.760
that was the graph dataset and the updates were separated.

364
0:20:04.760 --> 0:20:10.000
Is there a possibility to create a graph dataset where the updates are included in

365
0:20:10.000 --> 0:20:15.400
the dataset so that nodes and vertices get timestamps when they were deleted or when they were added?

366
0:20:15.400 --> 0:20:19.560
Yes. So the question is, is it possible to create something like

367
0:20:19.560 --> 0:20:25.520
a temporal graph with the timestamps of when the specific node is created and deleted?

368
0:20:25.520 --> 0:20:30.840
This is actually very easy because this is the first step that the DataGen creates.

369
0:20:30.840 --> 0:20:35.240
So when David said that it creates a social network of three years,

370
0:20:35.240 --> 0:20:39.240
that has everything that was ever created or deleted during those three years,

371
0:20:39.240 --> 0:20:42.520
and then we have attributes like creation date and deletion date.

372
0:20:42.520 --> 0:20:45.360
Then we turn it into something that's loadable to the database.

373
0:20:45.360 --> 0:20:50.560
We hide deletion dates because the database of course shouldn't be aware of this.

374
0:20:50.560 --> 0:20:55.000
But this is something that the DataGen supports out of the box.

375
0:20:55.000 --> 0:21:01.920
Okay. But then also to get this dataset with the deletion dates because you've already said it's hideable.

376
0:21:01.920 --> 0:21:07.760
It's hideable but we have one which is called the row temporal dataset and that is available.

377
0:21:07.760 --> 0:21:13.200
We even published that. So that's something that has a lot of

378
0:21:13.200 --> 0:21:16.520
chance to be influential in the streaming community I believe.

379
0:21:16.520 --> 0:21:24.640
All right. More questions. Yeah, Michael.

380
0:21:24.640 --> 0:21:29.240
Do you plan to extend to other domains from that perspective?

381
0:21:29.240 --> 0:21:30.800
So you have to social network one.

382
0:21:30.800 --> 0:21:31.200
Yeah.

383
0:21:31.200 --> 0:21:32.640
But it's an interesting one.

384
0:21:32.640 --> 0:21:35.020
It's for the retail or Netflix,

385
0:21:35.020 --> 0:21:40.560
Spotify, these user interactions on a more product and come out of one.

386
0:21:40.560 --> 0:21:44.440
I mean, you could map it to your style,

387
0:21:44.440 --> 0:21:47.920
but what are the other networks that are types of network?

388
0:21:47.920 --> 0:21:51.000
So the question is, can we extend to other domains?

389
0:21:51.000 --> 0:21:54.960
We usually emphasize that social networks is not really

390
0:21:54.960 --> 0:21:58.720
the domain that is the actual primary use case for graphs.

391
0:21:58.720 --> 0:22:01.320
We just use this because this is really easy to understand.

392
0:22:01.320 --> 0:22:03.480
We don't have to explain person to person.

393
0:22:03.480 --> 0:22:09.160
You can put in all sorts of interesting technological challenges to a graph domain like this.

394
0:22:09.160 --> 0:22:14.360
It would make sense and sometimes we are approached by our members saying,

395
0:22:14.360 --> 0:22:17.640
we want to do a new benchmark in the domain X,

396
0:22:17.640 --> 0:22:21.800
and we then send them the process that is

397
0:22:21.800 --> 0:22:24.740
required to get one of these benchmarks completed,

398
0:22:24.740 --> 0:22:26.800
and that's usually the end of the conversation.

399
0:22:26.800 --> 0:22:31.240
But we are definitely open to have more interesting benchmarks.

400
0:22:31.240 --> 0:22:35.120
Of course, a good data generator is worth gold

401
0:22:35.120 --> 0:22:38.480
to all the researchers and the vendors in this community.

402
0:22:38.480 --> 0:22:41.440
So that's usually a hard point and I would be

403
0:22:41.440 --> 0:22:44.880
definitely interested in having retail graph generator.

404
0:22:44.880 --> 0:22:46.360
Carlo?

405
0:22:46.360 --> 0:22:49.560
Hi. Question is basically,

406
0:22:49.560 --> 0:22:56.880
what do you see the impact of this will be on the industry as in it's pushing into,

407
0:22:56.880 --> 0:23:01.120
or it's more of a paradoxical evidence if

408
0:23:01.120 --> 0:23:08.880
the system would have improved or a system will get more robust as in that you detect stuff,

409
0:23:08.880 --> 0:23:13.480
just doing things and stuff get fixed or what was the?

410
0:23:13.480 --> 0:23:17.760
Yeah. So the question is about the potential impact.

411
0:23:17.760 --> 0:23:19.720
What could all this achieve?

412
0:23:19.720 --> 0:23:27.360
We believe that it will help accelerate the field in the sense that systems will get more mature,

413
0:23:27.360 --> 0:23:29.720
because if you want to get an audited result,

414
0:23:29.720 --> 0:23:32.000
you have to pass all the asset tests,

415
0:23:32.000 --> 0:23:35.040
you have to be able to recover after a crash,

416
0:23:35.040 --> 0:23:37.240
and ideally you would have to be fast.

417
0:23:37.240 --> 0:23:41.440
So that is hopefully one of the other things that systems will take away.

418
0:23:41.440 --> 0:23:43.940
They will have better optimizers,

419
0:23:43.940 --> 0:23:48.040
improved storage, better query execution engines.

420
0:23:48.040 --> 0:23:52.280
We have seen this in the aftermath of the TPC benchmarks.

421
0:23:52.280 --> 0:23:56.020
So those resulted in quite a big speed up.

422
0:23:56.020 --> 0:23:58.120
So that's one area.

423
0:23:58.120 --> 0:23:59.920
Of course, there is pricing.

424
0:23:59.920 --> 0:24:04.280
We would like that users can get more transactions per dollar.

425
0:24:04.280 --> 0:24:09.200
The third that we are personally quite interested in is the new accelerators that come out.

426
0:24:09.200 --> 0:24:12.040
So especially in the field of machine learning,

427
0:24:12.040 --> 0:24:16.000
there are cards that do fast sparse matrix multiplications.

428
0:24:16.000 --> 0:24:20.760
Those could be harnessed specifically for the analytical benchmarks that we have,

429
0:24:20.760 --> 0:24:25.100
and that would be interesting to see how big of a hassle it is to implement,

430
0:24:25.100 --> 0:24:29.160
and how big of a speed up they give.

431
0:24:30.000 --> 0:24:33.480
Cool. All right.

432
0:24:33.480 --> 0:25:02.840
Okay. Thank you very much.

