1
0:00:00.000 --> 0:00:09.960
Hi everyone, I'm Jeroen. Well, I'm going to tell you something about parsing some files

2
0:00:09.960 --> 0:00:21.600
really fast and I work for NL Netlabs. Oh, yeah, yeah, so some numbers. There's some

3
0:00:21.600 --> 0:00:27.480
caveats here. So the 50, I did not do measurements because I made finished slides like 2.8. So

4
0:00:27.480 --> 0:00:33.160
I did the measurements on the train. So, but I think the 50 megabytes is actually slower. I'm

5
0:00:33.160 --> 0:00:40.360
pretty sure the 700 megabytes is correct and we will go beyond and I'm going to tell you how. So,

6
0:00:40.360 --> 0:00:46.760
yeah, so basically the motivation is that currently the parser NSD isn't very fast and

7
0:00:46.760 --> 0:00:52.360
we have an operator where someone only takes the better part of an hour and at that point it's,

8
0:00:52.360 --> 0:01:02.680
well, it stops being practical. So, yeah, that's the motivation. And I'd actually like to take you

9
0:01:02.680 --> 0:01:07.240
on a journey so that I went through. I will also show you the new algorithms but I also want to

10
0:01:07.240 --> 0:01:14.160
show, tell you why parsing some files currently in NSD at least is really slow. And to do that,

11
0:01:14.160 --> 0:01:19.960
we have to, I have to tell you a bit on parsing. So I've included an example with a whole world

12
0:01:19.960 --> 0:01:26.680
C program and the NSD parser is based on Lex and Yack and that's really useful if you want to parse

13
0:01:26.680 --> 0:01:33.880
things like a computer language where each token has a meaning of it in and of itself. For some

14
0:01:33.880 --> 0:01:41.920
files, however, that is definitely not the case. So if you look at the, also in this case I provide

15
0:01:41.920 --> 0:01:47.480
as an example but everyone in the room will probably make out that on the last line I try

16
0:01:47.480 --> 0:01:52.280
to define an A record with a corresponding IP address and then what the zone parser actually

17
0:01:52.280 --> 0:01:58.080
does is it takes the A, it makes that the owner and then throws a syntax error because, well,

18
0:01:58.080 --> 0:02:07.840
an IP address is not really a valid record type obviously. So, yeah, Lex and Yack is not a,

19
0:02:07.840 --> 0:02:14.440
is really not a good choice but then there's also the fact that I think zone files are only more or

20
0:02:14.440 --> 0:02:23.920
less standardized, they're not really standardized and putting in MALI and when you combine the two

21
0:02:23.920 --> 0:02:33.520
that, yeah, that just leads to a lot of trouble. And, well, first thing I did was analyze why the

22
0:02:33.520 --> 0:02:38.200
current parser is slow and the current parser is slow. Well, it's actually inherent to the tool

23
0:02:38.200 --> 0:02:44.520
because they're just not a good fit. Because what the lecture does is it gives you each token and

24
0:02:44.520 --> 0:02:52.720
then passes on to the parser but it does so by matching a whole bunch of inputs and then taking

25
0:02:52.720 --> 0:03:00.080
the longest one, the longest match and then executing corresponding action in which in the

26
0:03:00.080 --> 0:03:05.320
case of the NSD zone file parser means that you can, that it copies the token then tries to

27
0:03:05.320 --> 0:03:10.800
unescape it and for names it tries to, it needs the dots right because they have meaning in domain

28
0:03:10.800 --> 0:03:17.360
names and what it does there is that it actually splits the input and passes each label separately.

29
0:03:17.360 --> 0:03:24.320
That is, of course, copied and the parser then concatenates that all back together and that is

30
0:03:24.320 --> 0:03:32.640
not really a fast process. So, my first thought was, well, what if I just changed the process a bit

31
0:03:32.640 --> 0:03:41.000
and scrap Alex and Yack and got all the memory allocations. That gave us better numbers but

32
0:03:41.000 --> 0:03:47.400
they're not the numbers that I wanted to see, right, because under the eight megabytes is, I mean,

33
0:03:47.400 --> 0:03:52.680
you can express it in gigabytes a second but then it becomes even less impressive, right,

34
0:03:52.680 --> 0:04:03.640
than 100 megabytes a second. So, yeah, I started looking into that and I will show you the

35
0:04:03.640 --> 0:04:10.160
algorithms that we, or that I used and I came up with in a minute but to make you understand

36
0:04:10.160 --> 0:04:15.640
why these algorithms work, it's important that each and every one of you knows that your CPU

37
0:04:15.640 --> 0:04:23.160
is a pipeline CPU and all modern CPUs are pipeline CPUs and what that means is that each executing

38
0:04:23.160 --> 0:04:29.760
each instruction is not a single step, it's actually a multi-stage process. So, there's a fetch and

39
0:04:29.760 --> 0:04:36.480
decode step and there's a lot more to it in practice but this is a mechanism that was designed

40
0:04:36.480 --> 0:04:45.800
so that you optimize performance in CPU and the premise on getting fast code is that you keep the

41
0:04:45.800 --> 0:04:54.280
pipeline nice and full. That does not always happen, especially, or one case where that doesn't

42
0:04:54.280 --> 0:05:00.040
happen is when you get a pipeline stall and that happens essentially when there's, when the next

43
0:05:00.040 --> 0:05:04.840
instruction that you want to execute depends on the result of the instruction that you're currently

44
0:05:04.840 --> 0:05:13.800
executing and in that case the CPU has to stall the pipeline, it has to wait until the result is

45
0:05:13.800 --> 0:05:21.680
written back, it can then go on to decode and then only then execute your instruction and you'll

46
0:05:21.680 --> 0:05:29.520
take a hit of a couple cycles. Then there's of course the well-known pipeline flashes and those

47
0:05:29.520 --> 0:05:37.880
happen essentially if you, if there's a conditional jump, for instance, a NIF statement and the CPU

48
0:05:37.880 --> 0:05:42.480
goes on to load the instructions that come after that and if it turns out it actually needs to

49
0:05:42.480 --> 0:05:48.520
execute other instructions then it needs to flush the pipeline and only then can it go on to execute

50
0:05:48.520 --> 0:05:56.680
your code and there's branch prediction that is used to improve the flow and modern CPUs actually

51
0:05:56.680 --> 0:06:01.760
do a pretty good job of that but well it's prediction so it's not a, it's not always right

52
0:06:01.760 --> 0:06:12.120
and it turns out so if we look at that information then we can analyze why parser is, the process of

53
0:06:12.120 --> 0:06:17.480
parsing is just inherently slow because if we go over it byte by byte like the NSD shown parser

54
0:06:17.480 --> 0:06:23.600
for example then before you can analyze the next byte you have to wait until you know you have to

55
0:06:23.600 --> 0:06:32.200
resolve the new state of your current byte and also it turns out that as far as the CPU is

56
0:06:32.200 --> 0:06:37.880
concerned the zone files are just random right that anything can happen at any time so it's hard

57
0:06:37.880 --> 0:06:48.680
to predict branches in that case. Right so the base of the new instructions that I'm using at

58
0:06:48.680 --> 0:06:56.280
base is a thing called a SIMD or single instruction multiple data and my interest in all of this was

59
0:06:56.280 --> 0:07:02.200
really sparked by the SIMD JSON project and it caught my intention because they expressed their

60
0:07:02.200 --> 0:07:07.000
throughput in gigabytes a second. Now in the next slide I'm going to tell you something about the

61
0:07:07.000 --> 0:07:12.120
algorithms but I'm not going to go into them in grade depth because there's not a whole lot of

62
0:07:12.120 --> 0:07:19.000
time so if you want to know more on that then I would advise you to watch the talk or just read

63
0:07:19.000 --> 0:07:25.480
the paper. But yes in D what that is is actually an instruction set and what it does is it adds

64
0:07:25.480 --> 0:07:33.160
factor registers and instructions to operate on those registers and what that allows us to do is

65
0:07:33.160 --> 0:07:42.520
to classify blocks instead of just bytes. There's some trickery involved and there's a super simple

66
0:07:42.520 --> 0:07:52.520
example on the slide but basically we can classify 16, 32 or 64 bytes in one go depending on your CPU

67
0:07:52.520 --> 0:07:59.120
and then we repeat that multiple times for each input that we actually want to know about and the

68
0:07:59.120 --> 0:08:08.920
ideas that we can cut branches and dependencies. So what's good to know about SIMD is that it's

69
0:08:08.920 --> 0:08:18.680
all vertical non-horizontal so it's really an instruction that is executed for each of the

70
0:08:18.680 --> 0:08:27.400
inputs so you can actually do logic in SIMD and the way to work around that is to convert the

71
0:08:27.400 --> 0:08:37.800
inputs to a mask so we would get a 64 bit mask for each of the inputs that we checked and with

72
0:08:37.800 --> 0:08:42.560
those bit masks in hand then the first thing that we are going to do is to classify all the escape

73
0:08:42.560 --> 0:08:49.960
bits because there's so many files allowed for escaping and this is actually an algorithm that

74
0:08:49.960 --> 0:08:59.040
the SIMD JSON guys came up with and basically what we do is that for each uneven number of backslashes

75
0:08:59.040 --> 0:09:07.920
we take the next character and so that bit then represents the character that has actually escaped

76
0:09:07.920 --> 0:09:15.400
and we need that information so that we can identify the quoted sections or in the case of

77
0:09:15.400 --> 0:09:25.360
some zone files also the comment sections and this was actually kind of a hard problem because

78
0:09:25.360 --> 0:09:32.560
they don't have this problem in JSON documents but in zone files comments can cancel out quoted

79
0:09:32.560 --> 0:09:40.400
sections and quoted sections can contain semi-colons and then new lines they delimit comments but we

80
0:09:40.400 --> 0:09:45.960
only want the new lines that actually delimits comment because what we really want to do is

81
0:09:45.960 --> 0:09:54.600
that we want to find out which of the characters that we identify as structural characters are

82
0:09:54.600 --> 0:10:02.680
contained in quoted sequences or in comments and there's a simple example in the bottom there so

83
0:10:02.680 --> 0:10:13.600
oh yeah what I did a number of experiments but in the end it turned out that if there's a semi-colon

84
0:10:13.600 --> 0:10:22.200
in there in the input we just branch so we have a slow path assuming that there's not too many

85
0:10:22.200 --> 0:10:29.440
comments in zone files which for generated zone files I guess it's okay and once we have that

86
0:10:29.440 --> 0:10:35.840
information all the bits that remain automatically belong to the non-quoted are non-quoted strings

87
0:10:35.840 --> 0:10:43.320
and then and this is oversimplifying it but if we shift right and do it on XOR then that would get

88
0:10:43.320 --> 0:10:54.880
us all the transitions and with that information we can then go on to create indexes of those

89
0:10:54.880 --> 0:11:02.040
because your CPU does not only provide SIMD instructions it's also provides bit manipulation

90
0:11:02.040 --> 0:11:07.680
instructions really fast bit manipulation instructions so the first thing that it does

91
0:11:07.680 --> 0:11:12.760
is it takes it takes the population count to find out how many transitions are actually in your

92
0:11:12.760 --> 0:11:19.440
input block and then we use the trailing zero count to find out the relative position of the

93
0:11:19.440 --> 0:11:26.320
bit and if we combine it with the index then that should give us the pointer to the exact input

94
0:11:26.320 --> 0:11:37.920
byte and there's there's some more trickery involved here because of course for zone files

95
0:11:37.920 --> 0:11:43.400
if there's an error we want to report that error and to do that we need the line count and quoted

96
0:11:43.400 --> 0:11:48.600
sections of course may contain just new lines but we don't want to worry about those in the

97
0:11:48.600 --> 0:11:56.720
parser because that would that would mean that each parse function would possibly need to update

98
0:11:56.720 --> 0:12:02.960
the line count and that would just not be very convenient so what we do there is we take an

99
0:12:02.960 --> 0:12:09.280
unlikely branch if there's new line in a quoted section which really doesn't happen it's it's an

100
0:12:09.280 --> 0:12:16.120
edge case in the case of sum files and we take the slope off to count all the new lines in the

101
0:12:16.120 --> 0:12:23.400
input or at least the one in the quoted sections and then once we generate a token for the actual

102
0:12:23.400 --> 0:12:33.160
delimiting new line we add the number of new lines that we found in quoted sections. Yeah and that

103
0:12:33.160 --> 0:12:39.680
gives us basically that gives us a fast scanner in my initial measurements and I think it's a

104
0:12:39.680 --> 0:12:45.320
little bit fast now that would get me a scanning of two gigabytes a second for sum files at least

105
0:12:45.320 --> 0:12:52.680
with an older.com zone etc etc so there's caveats there too but it turns but it turns out that the

106
0:12:52.680 --> 0:12:57.560
rest of the DNS data because we of course we have to parse it we only now tokenized it we also have

107
0:12:57.560 --> 0:13:07.800
to parse it the rest of the DNS data allows for optimizations using SIMD as well and of course we

108
0:13:07.800 --> 0:13:14.640
want to start with the data that occurs the most and there's of course domain names and with the

109
0:13:14.640 --> 0:13:20.560
SIMD instruction we actually just repeat the scanning process we quickly identify all the dots

110
0:13:20.560 --> 0:13:26.400
we turn that into a bitmask and then use the bit manipulation instructions to go over the domain

111
0:13:26.400 --> 0:13:31.360
name because most of the time if we just fill in the length on the dot then that would give us a

112
0:13:31.360 --> 0:13:39.040
proper Y format and of course there's a slow path for edge cases as well there and next of course

113
0:13:39.040 --> 0:13:49.280
this is the record type and normally I guess you would hash I initially just used binary search

114
0:13:49.280 --> 0:13:57.280
which is faster than just linear search of course but that took away quite some performance

115
0:13:58.640 --> 0:14:05.520
so we want a perfect actually we want a hash but then a hash table is pretty big and so I figured

116
0:14:05.520 --> 0:14:12.560
I want a perfect hash and it turns out we can do we can do that so if you take the first character

117
0:14:12.560 --> 0:14:18.000
of the records because there's not that many record types right and there's certainly if you

118
0:14:18.000 --> 0:14:24.720
take the first character there's never more than that many records never more than like eight or

119
0:14:24.720 --> 0:14:29.520
nine record types I start with the first letter so if you then take the last character and at the

120
0:14:29.520 --> 0:14:37.840
length then it turns out that doesn't give me any collisions so we can also the hash if collisions

121
0:14:37.840 --> 0:14:44.080
occur but I mean for all the record types and what for four years it doesn't give me collisions

122
0:14:44.080 --> 0:14:52.400
so I guess we're good on that front um this is a weird width type with a number no for each record

123
0:14:52.400 --> 0:15:04.240
type someone asked if this only works for record types and then the number and answer is no it

124
0:15:04.240 --> 0:15:10.640
works for all record types because they're all closely they're all really close together right

125
0:15:10.640 --> 0:15:15.840
so they're in their alpha numeric most of the time sometimes there's numbers so if you just

126
0:15:15.840 --> 0:15:22.320
uh I think I could case or down case it and then multiply together a good distribution

127
0:15:22.320 --> 0:15:28.400
and then just add length and that gives me a that gives that gives me a unique key without

128
0:15:28.400 --> 0:15:33.840
collisions and from there I can just do a use in the instruction to do compare equal

129
0:15:35.120 --> 0:15:39.680
so I can do the exact right string compare and that gives me a really nice nice speed up

130
0:15:39.680 --> 0:15:50.560
um yeah and it and the people who worked on SIMD JSON actually do a lot of did a lot of projects

131
0:15:50.560 --> 0:15:59.440
like using SIMD for for decoding base 64 so the plan is to incorporate all those things as well

132
0:15:59.440 --> 0:16:09.920
um and then there's one tricky part um your cpu actually supports multiple instruction

133
0:16:09.920 --> 0:16:14.160
sets at least if you have modern cpu if you have like a pending forwarding you only get sse for

134
0:16:14.160 --> 0:16:19.920
the two but we want our software to be able to run on all those devices without recompiling

135
0:16:19.920 --> 0:16:30.560
so we actually compiled it four times in the case of x x86 then use the cpu id instruction to pick

136
0:16:30.560 --> 0:16:42.720
the right one and then well it's still in progress projects I hope to be a little bit further along

137
0:16:42.720 --> 0:16:47.920
but unfortunately not um it will be a standalone library because it might actually be useful to

138
0:16:47.920 --> 0:16:54.080
other people um and that will make it easy to integrate into other projects it was initially

139
0:16:54.080 --> 0:17:01.200
just intended for nsd um yeah the numbers are so far pretty good at least quite a bit better than

140
0:17:01.200 --> 0:17:06.720
what we have now um I think it's possible to go to one gigabytes a second

141
0:17:09.440 --> 0:17:11.600
yeah so if you want to check it out there's a link in there

142
0:17:13.040 --> 0:17:17.600
and finally I want to uh there's a slide with acknowledgement because these people help me a

143
0:17:17.600 --> 0:17:22.880
lot I just send them unsolicited email at first and then I happen to get answers back and they

144
0:17:22.880 --> 0:17:28.960
help me and they even took a look at my presentation help me there as well um so thanks to uh Jeff

145
0:17:28.960 --> 0:17:37.200
Daniel and all the simdiation people and with that I actually finished in time uh it's time for

146
0:17:37.200 --> 0:17:47.440
questions yes maybe my question was clear what I'm wondering about is the hash collations of the

147
0:17:48.640 --> 0:17:55.200
name the type which are encoded as the word type followed by a number oh no but that's that's a

148
0:17:55.200 --> 0:18:00.000
slow path that they're and exactly they're the hashtags more but that's a slow path

149
0:18:00.640 --> 0:18:06.960
so we do a slow path sorry I should repeat the question what um the person in the audience was

150
0:18:06.960 --> 0:18:13.360
actually referring to what happens if you we use a generic type notation where we start the type by

151
0:18:13.360 --> 0:18:19.760
type followed by a number and obviously it doesn't work there but it does the slow path so we have a

152
0:18:19.760 --> 0:18:29.280
slow path there yeah is the parcel so complete that you can parse an output and you get same

153
0:18:29.280 --> 0:18:39.840
output as you parse them uh really good because I would yeah so if the parser is uh good enough

154
0:18:39.840 --> 0:18:46.000
that uh to give you the exact same output as you put in um no it does not do that there's

155
0:18:49.760 --> 0:18:56.320
well you you I mean do you mean by x to wide space exact or just

156
0:18:56.320 --> 0:19:01.200
just yeah no it doesn't do that no but it's also not the

157
0:19:01.200 --> 0:19:06.880
it's so if you see white space but it doesn't strip comments it does also strip comments yes

158
0:19:06.880 --> 0:19:07.520
yes yes yes

159
0:19:11.680 --> 0:19:18.400
yes how do you handle uh escape decimals because in the example you gave you strip and you have

160
0:19:18.400 --> 0:19:22.160
look where the back slashes are yeah and then take the next character but if you have like backslash

161
0:19:22.160 --> 0:19:28.320
003 then you need those four characters as a single unit to encode in the final um I'm not

162
0:19:28.320 --> 0:19:36.480
actually I just do the uh I just uh really good yeah you're gonna have to I hope there's no more

163
0:19:36.480 --> 0:19:46.480
questions because you're gonna have to keep doing that but um what I do so what happens if um I guess

164
0:19:46.480 --> 0:19:55.600
for a certain type of input I record uh record data you can stick like a backslash 003 which

165
0:19:55.600 --> 0:20:00.880
encodes the bytes with a value three single byte yeah how do you do that with your algorithm that

166
0:20:00.880 --> 0:20:06.560
just takes the next character when you have backslashes um but that's just to just just to

167
0:20:06.560 --> 0:20:12.800
so the question is what do I do with escape characters or with escape sequences um and so

168
0:20:12.800 --> 0:20:18.800
what I explained on the um what I do what happens with backslashes is just to tokenize so I don't

169
0:20:18.800 --> 0:20:24.320
strip any data I just mark out the starts and the ends of each string field and then the parsing

170
0:20:24.320 --> 0:20:37.200
comes after that so there's no data actually stripped yeah question over there yeah sorry if

171
0:20:37.200 --> 0:20:42.800
I missed it but what's the output format is this just memory or is this to store in a database like

172
0:20:42.800 --> 0:20:49.520
in a binary format yeah um so the question is um so the question is what's the output format

173
0:20:50.240 --> 0:20:57.040
and the output format in this case is just DNS wire format so the um the id here is that for each

174
0:20:57.040 --> 0:21:02.800
record it will invoke a callback and it will just give you wire format with pointers to where all

175
0:21:02.800 --> 0:21:08.240
the fields are in the like an internal description of the field so that you know the length and you

176
0:21:08.240 --> 0:21:17.280
know the type of the field yeah using the same method you used to convert to cine instructions

177
0:21:25.280 --> 0:21:29.520
there is definitely value to large effectors because it takes less instructions so if you

178
0:21:29.520 --> 0:21:37.760
can do something so um I did not look into using the GPU um but yeah that might benefit so if you

179
0:21:43.200 --> 0:21:44.640
no I have not no

180
0:21:44.640 --> 0:21:48.720
no

181
0:21:57.440 --> 0:22:00.000
so the question is this is really good

182
0:22:01.680 --> 0:22:08.560
why do why does parsing zone files have to be fast well um because they're they get

183
0:22:08.560 --> 0:22:15.280
reloaded quite a lot of times each time there's an update uh to a zone you need to reload you

184
0:22:15.280 --> 0:22:24.560
need to reload the zone um so we wonder yeah so that happens multiple times like an hour it

185
0:22:24.560 --> 0:22:31.360
it's differs per zone right but in our case um if it takes more than an hour and the operator

186
0:22:31.360 --> 0:22:36.640
actually or it takes the better part of an hour and the operator wants to leave more wants to

187
0:22:36.640 --> 0:22:44.800
reload more often then that becomes a problem right so we just need to be faster but then there's

188
0:22:44.800 --> 0:22:51.280
all the and so there's other benefits as well so nsd for instance we support zone verification

189
0:22:51.280 --> 0:22:58.960
where just before the zone goes live um you can have a focal program uh to verify that your dns

190
0:22:58.960 --> 0:23:08.960
seg data is is correct and there you can use an axfr or you can let the nsd feed you the zone data

191
0:23:08.960 --> 0:23:15.280
in which case you just get text representation and if the zone is big enough then you want that to

192
0:23:15.280 --> 0:23:22.640
be fast because it's in the critical path right last question over there yeah it's splitting the

193
0:23:22.640 --> 0:23:29.520
files and multi-threading something you considered uh well actually uh if the question was if splitting

194
0:23:29.520 --> 0:23:34.880
the files and multi-threading is something that we considered well splitting the files no um but

195
0:23:38.240 --> 0:23:46.800
split on them yeah well new lines um yeah that can be tricky because some files can contain

196
0:23:46.800 --> 0:23:53.040
parentheses which would then mean that if the record continues on the next line um but a colleague

197
0:23:53.040 --> 0:24:00.960
actually did uh did do a parallel uh zone loading implementation um and i guess we can even do that

198
0:24:00.960 --> 0:24:07.600
with uh with this implementation right um because there the it was actually quite a bit faster um

199
0:24:09.040 --> 0:24:14.160
but the scanning process still takes a long time because you go over it by by byte but now that we

200
0:24:14.160 --> 0:24:20.240
have a fast scanner there's no reason why we cannot also include like parallel parsing yeah that could

201
0:24:20.240 --> 0:24:28.240
work yeah did you do any experiments of actually saving this into the database like something

202
0:24:28.240 --> 0:24:44.560
relational maybe and if you did what impact did that happen um so the question is if we did

