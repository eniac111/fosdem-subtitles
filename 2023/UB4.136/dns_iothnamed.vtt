WEBVTT

00:00.000 --> 00:07.000
Tell me when.

00:07.000 --> 00:08.000
Tell me when.

00:08.000 --> 00:10.000
Five more seconds.

00:10.000 --> 00:11.000
Okay.

00:11.000 --> 00:18.000
This stuff makes the video cutting process so much easier if we stick to the schedule.

00:18.000 --> 00:20.000
You can go ahead.

00:20.000 --> 00:22.000
Good afternoon everybody.

00:22.000 --> 00:25.000
Thank you for having me here.

00:25.000 --> 00:34.000
Let me start from a short introduction of what Internet of Threats means.

00:34.000 --> 00:37.000
What is an end node of the Internet?

00:37.000 --> 00:42.000
So, which are the communicating nodes of the Internet?

00:42.000 --> 00:49.000
The legacy approach of what is at the beginning, it was the Internet of Hosts.

00:49.000 --> 00:56.000
Internet of Threats were given to each controller.

00:56.000 --> 01:06.000
So, actually the communication took place, takes place between the controllers of hosts.

01:06.000 --> 01:29.000
Nowadays, this concept has been made wider and the network endpoints are virtual controllers of virtual machines or even in spaces.

01:29.000 --> 01:32.000
Internet of Threats is one step further.

01:32.000 --> 01:45.000
It means that we want even just processes, even threads within processes as nodes as in the end zone.

01:45.000 --> 01:56.000
The idea is to give IP addresses, actually IPv6 address, we wouldn't have as many IPv4 address as we need to threads or processes.

01:56.000 --> 02:00.000
The idea can be depicted from this.

02:00.000 --> 02:09.000
Long time ago there were fixed lines and really the telephone number was connected to a place, a room.

02:09.000 --> 02:15.000
It was common to call a number and say, it is Jack Tom.

02:15.000 --> 02:26.000
Nowadays, we use portable phones and the numbers are connected to or mapped to people.

02:26.000 --> 02:30.000
It is very easy.

02:30.000 --> 02:33.000
What do we look for?

02:33.000 --> 02:37.000
We don't look for controllers, we look for services.

02:37.000 --> 02:54.000
The most natural way is to have IP addresses connected to the process with providing that service, not controllers or machines.

02:54.000 --> 03:04.000
The network stack is just a layer between API to the application layer and the API to the data link layer.

03:04.000 --> 03:10.000
Actually, these are two layers, layer three and four of the other stack.

03:10.000 --> 03:16.000
But anyway, it is a slice in the middle.

03:16.000 --> 03:30.000
This implementation is currently most of the time deeply inside the kernel of the machine of the computer you are using.

03:30.000 --> 03:36.000
But it can be seen as a library.

03:36.000 --> 03:56.000
This library can be linked to a user process and this way the user process can directly talk with the network.

03:56.000 --> 04:12.000
We made one further step to this implementation using the libi library for the Internet of Threads.

04:12.000 --> 04:29.000
It is not a library that implements a stack, it is a framework library that allows to load actual implementation for network stacks as plugins.

04:29.000 --> 04:46.000
It provides a unified API to the applications. In such a way, it is possible to run applications that can use either the kernel stack or any implementation of the network stack as a library.

04:46.000 --> 04:58.000
Actually, the actual implementation permits to change the implementation just by changing a string.

04:58.000 --> 05:09.000
The actual stack supported by libi are the kernel stack, woody stack, which is actually a trick.

05:09.000 --> 05:15.000
It is a namespace using a top inside the namespace.

05:15.000 --> 05:33.000
We are borrowing the kernel stack using it at user level. Then a real implementation of user level TCP IP stacks like PicoTCP, the module is named PicoX,

05:33.000 --> 05:48.000
and a working process where I am working to port lightweight IP to this port.

05:48.000 --> 05:52.000
What do we need in the API?

05:52.000 --> 06:05.000
There is a way to communicate. This is quite known, and this is the standard way we use the stack. Open, close communication, point, center. For all of these, there are back-to-sockets.

06:05.000 --> 06:23.000
But it is not common. If you use the kernel stack, you have, as guaranteed, as provided, the definition of the stack as a configuration parameter of the stack,

06:23.000 --> 06:39.000
like which are the API address, which are the routing definitions, and so on. We need to add this support for the API.

06:39.000 --> 06:58.000
The definition of stack needed some syntax, some specific syntax. When using the API, a new stack is created.

06:58.000 --> 07:15.000
It is a pointer to a specific structure that can be used for communication. The only difference for the socket API is that instead of using socket,

07:15.000 --> 07:26.000
there is a new call named MSocket, which has one further parameter, which is the actual stack implementation to use.

07:26.000 --> 07:37.000
And then there are all the other API calls, well-known from Berkeley sockets.

07:37.000 --> 07:57.000
But we needed to create a new stack, so in such a way to do that, we used those calls we have seen two years ago, and the pointer can be used for communication.

07:57.000 --> 08:21.000
And what about configuration? For configuration, there is an RFC used, for example, by the Knynyos kernel, that uses the Addis family netlink to provide messages for configuring the network.

08:21.000 --> 08:44.000
So there is no need of further API entry for configuration. We just need that our stack support AF netlink configuration.

08:44.000 --> 09:06.000
Another point about these sockets, using forum sockets, if it is a library level implementation of the stack, the problem is that this integer could be an internal number of the library.

09:06.000 --> 09:19.000
Instead, we need the thing to be a real file descriptor, because we need the file descriptor to be used, for example, for Paul's socket.

09:19.000 --> 09:42.000
And we have to write a new kernel module in the library, named Vupala, that creates a file descriptor in which the elements can be synthesized.

09:42.000 --> 09:59.000
And so, stack implemented at user level, as a library, can provide real file descriptor, and this file descriptor can be used on select, so on.

09:59.000 --> 10:11.000
So we can use real file descriptor, file descriptor coming from different implementations of the network, and write M and DREAM program altogether.

10:11.000 --> 10:17.000
Just a quick look to give you the feeling of what does it mean.

10:17.000 --> 10:26.000
This is a program just sending a child using a diagram.

10:26.000 --> 10:30.000
This is the legacy way.

10:30.000 --> 10:41.000
This is the same example using internal threads.

10:41.000 --> 10:50.000
Just by changing this string, I can use an implementation I want, provided I have support for that.

10:50.000 --> 11:00.000
Okay, now the core of the presentation, we need an ecosystem around this idea.

11:00.000 --> 11:20.000
A lot of stuff that we currently have in our toolset, but we need them implemented as internal threads.

11:20.000 --> 11:30.000
We need a cause to configure the network.

11:30.000 --> 11:36.000
These are not codes of the internal threads implementation of the library.

11:36.000 --> 11:54.000
These codes generate NetLink messages, so it can be used even for configuring the current stack directly from a program.

11:54.000 --> 12:03.000
Let us pass quickly through this.

12:03.000 --> 12:08.000
We need a library to query for DNS.

12:08.000 --> 12:21.000
Why? Because using the Clib implementation, it uses the kernel stack,

12:21.000 --> 12:33.000
and it uses the definitions in etc.result.conf, and the string etc.result.conf is arc coded in the Clib code.

12:33.000 --> 12:38.000
It is not possible to change even the file to be used.

12:38.000 --> 12:57.000
We designed a proxy for word cache, especially for the internal threads, which are the characteristics of this DNS.

12:57.000 --> 13:10.000
It uses the internal threads, so it can be used for receiving queries or for wording queries, different stacks defined by the user.

13:10.000 --> 13:28.000
But at the same time, they can provide further features specifically useful for the internal threads.

13:28.000 --> 13:52.000
Let us pass through the configuration items, but I prefer to show you some scenarios in which the IOS NMD can be used.

13:52.000 --> 14:07.000
This is a common scenario, a common proxy scenario, like libmasq or similar, but implemented using internal threads.

14:07.000 --> 14:28.000
If we ask the proxy an address provided by a foreign node, we can cache the result and provide it back to the queryer.

14:28.000 --> 14:43.000
Or we can add some specific local addresses. It does not provide a relay from external queryer.

14:43.000 --> 14:52.000
The point is that we have a stack for the query and a stack to forward the query outside.

14:52.000 --> 15:06.000
In this case, in this configuration, I provide the service to all the processes connected to this stack,

15:06.000 --> 15:16.000
and I forward the queries on another different stack with a different implementation.

15:16.000 --> 15:33.000
This is another test just to see that it is able to resolve foreign and local results.

15:33.000 --> 15:47.000
Or it can be used as a delegated subdomain.

15:47.000 --> 16:13.000
It is providing forwarding, defining this pink server as the responsible to the server, dumb, mature, and so on.

16:13.000 --> 16:18.000
It provides back the solution.

16:18.000 --> 16:24.000
Here, the new point is that we can use different stack.

16:24.000 --> 16:47.000
I kept it some time to show you some more ideas. Actually, managing DNS servers for IPv6 is a daunting process,

16:47.000 --> 16:57.000
but it is very error-prone because if you have to write all those huge, long numbers,

16:57.000 --> 17:06.000
it is very hard to not insert errors in the configuration.

17:06.000 --> 17:18.000
That is to create IPv6 addresses using hash code, hash resolution,

17:18.000 --> 17:39.000
using the result of a hash function defined on the full-defined domain name.

17:39.000 --> 17:48.000
This is the host part of IPv6 address for free.

17:48.000 --> 17:54.000
This is the proxy, as in the previous example, the first example.

17:54.000 --> 18:09.000
I can ask the server to solve all the addresses like something hash.local.

18:09.000 --> 18:15.000
I can use any string before hash.local and I get a name resolution.

18:15.000 --> 18:24.000
That means that if I add a new node, which can be a computer or even a process, I just have to baptize it,

18:24.000 --> 18:39.000
to give it a name, and it will be connected on the net without having to write any single line in the DNS server.

18:39.000 --> 18:59.000
The slides are on the website, so if you want to pass through and download the prototype from GitHub, you can test this.

18:59.000 --> 19:12.000
The same things can be done wherever we run, so having a delegated domain that results addresses using hashes.

19:12.000 --> 19:29.000
So we can have a number of local machines that can be seen from the Internet just by giving them a name.

19:29.000 --> 19:46.000
But there is one more result, which is One-Time IP. One-Time IP is a security feature, like a One-Time Password.

19:46.000 --> 20:03.000
One-Time Password means that you have a password that lasts for a short period of time, so if somebody is able to ice-drop the password, it gets useless in a few moments.

20:03.000 --> 20:17.000
The first part of the address is defined by a hash definition that changes during the time.

20:17.000 --> 20:28.000
It is not only the name of the fully qualified domain name, but it includes a password and a time.

20:28.000 --> 20:36.000
So if the legitimate user wants to connect to the server, it knows which is the actual IP address.

20:36.000 --> 20:51.000
Any other user, even if it is able to trace the network traffic, it gets some addresses that will be null in a few seconds.

20:51.000 --> 20:59.000
I think I have no time to show the five minutes, which means time for questions.

20:59.000 --> 21:03.000
We have four minutes left including questions.

21:03.000 --> 21:07.000
Just one point, and then I go to the question.

21:07.000 --> 21:31.000
Name the ACP is another tool which uses DHCP server, but these DHCP servers query the DNS to provide the address to the computer, not the processor whatsoever.

21:31.000 --> 21:49.000
You can just say to your computer, which is the name, and the resolution is provided.

21:49.000 --> 22:03.000
The definition of all the network in configuration is provided, getting replies from the DNS server.

22:03.000 --> 22:11.000
I have to stop. I would have a lot of more examples to give you.

22:11.000 --> 22:15.000
Questions?

22:15.000 --> 22:28.000
What is the overhead of using a different network stack in every thread, especially if you are using the one-time IPs for each connection?

22:28.000 --> 22:36.000
What is the overhead that you get within the application if you have a lot of connections coming in?

22:36.000 --> 22:45.000
For sure, on single connections we have some performance drop.

22:45.000 --> 22:58.000
The point is that the overall bandwidth by all the applications you have on your system can use the entire bandwidth.

22:58.000 --> 23:13.000
You have the experience that you spread democratically, the bandwidth among your processors.

23:13.000 --> 23:23.000
Questions?

23:23.000 --> 23:30.000
What about latency?

23:30.000 --> 23:39.000
Is latency a factor?

23:39.000 --> 23:57.000
The point is that behind the user level stack there is a virtual network, which is virtual distributed internet,

23:57.000 --> 24:09.000
that uses a support like VXVD, which is like VXLAN without VTAP.

24:09.000 --> 24:12.000
Each process is part of the distributed space.

24:12.000 --> 24:28.000
You have a direct UDP connection, a UNICAS connection from end to end, from a processor in one machine to the other.

24:28.000 --> 24:50.000
There is a direct UDP one-to-one.

24:50.000 --> 24:58.000
My idea is to have a new concept of computing elements.

24:58.000 --> 25:08.000
Instead of having your computer read the process, you have a local network of a cluster of computers with all the processors,

25:08.000 --> 25:11.000
which are nodes of different networks.

25:11.000 --> 25:23.000
You have the real network, which is just the basic framework, and you extract this in a number of virtual networks

25:23.000 --> 25:29.000
that connect the processors running in which physical node you like.

25:29.000 --> 25:41.000
That's all the time we have. Thank you, Renzo.
