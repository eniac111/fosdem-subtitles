1
0:00:00.000 --> 0:00:08.100
Last but not least, it's Christophe Massieu.

2
0:00:08.100 --> 0:00:12.520
But first of all, I would like to thank him for his work organizing the dev room all day.

3
0:00:12.520 --> 0:00:13.520
Thank you.

4
0:00:13.520 --> 0:00:14.520
Sat here.

5
0:00:14.520 --> 0:00:15.520
Thank you.

6
0:00:15.520 --> 0:00:16.520
Not too much.

7
0:00:16.520 --> 0:00:20.680
Yes, Fostem is a volunteer event.

8
0:00:20.680 --> 0:00:24.720
And Christophe will be talking about distributing multicast channels to third parties, a case

9
0:00:24.720 --> 0:00:29.360
study with OpenSoft software and virtualization slash SR-IOV.

10
0:00:29.360 --> 0:00:30.360
Thank you, Kirana.

11
0:00:30.360 --> 0:00:35.360
And also thank you, Kirana, for co-organizing the dev room with me.

12
0:00:35.360 --> 0:00:37.600
Exactly.

13
0:00:37.600 --> 0:00:44.400
And if next year there are more volunteers, I think we would be happy to have that.

14
0:00:44.400 --> 0:00:45.400
So I'm Christophe Massieu.

15
0:00:45.400 --> 0:00:50.040
I've been in the broadcast business for quite some time now.

16
0:00:50.040 --> 0:00:54.200
And I run a company called Easy Tools.

17
0:00:54.200 --> 0:00:59.320
One of the purpose of this company is to distribute linear channels.

18
0:00:59.320 --> 0:01:04.680
So either we create them or we get them from satellite or from abroad or we get them in

19
0:01:04.680 --> 0:01:10.440
data centers and we deliver them somewhere in the world or to network operators and so

20
0:01:10.440 --> 0:01:11.440
on.

21
0:01:11.440 --> 0:01:15.600
So one of the critical parts of doing that is knowing how to distribute multicast to

22
0:01:15.600 --> 0:01:18.200
other people.

23
0:01:18.200 --> 0:01:19.680
Usually how does it work?

24
0:01:19.680 --> 0:01:26.160
So first I must clarify, linear channel from my point of view at least, it's basically

25
0:01:26.160 --> 0:01:33.320
transport stream, so it's MPEG-TS over UDP or RTP, depends on your religion, both exist,

26
0:01:33.320 --> 0:01:36.880
and generally multicast.

27
0:01:36.880 --> 0:01:42.040
So usually you put seven TS packets inside one UDP frame and you're done.

28
0:01:42.040 --> 0:01:45.240
And it's a continuous stream all the time.

29
0:01:45.240 --> 0:01:47.440
How do you exchange these kind of streams?

30
0:01:47.440 --> 0:01:50.800
Usually you take a rack in a well-known point of exchange.

31
0:01:50.800 --> 0:01:53.320
In Paris there is one called Terre House.

32
0:01:53.320 --> 0:01:58.040
It's very popular, everybody knows that in Paris.

33
0:01:58.040 --> 0:02:03.520
And so you have your rack, you put your switch and you buy cross-connects from your data

34
0:02:03.520 --> 0:02:05.920
center to your peers.

35
0:02:05.920 --> 0:02:11.080
So your peers can be your sources, people who provide your streams, or it could be where

36
0:02:11.080 --> 0:02:16.320
you distribute them, so the distributors, the operators and so on.

37
0:02:16.320 --> 0:02:23.400
So for safety reasons you will want to have each source or each operator in a different

38
0:02:23.400 --> 0:02:26.360
VLAN so that they don't see each other.

39
0:02:26.360 --> 0:02:32.160
From confidentiality and also some content is a little bit critical.

40
0:02:32.160 --> 0:02:39.760
So the question will be how do you copy basically a multicast from one source VLAN to a destination

41
0:02:39.760 --> 0:02:40.760
VLAN.

42
0:02:40.760 --> 0:02:48.280
It's stupid and perfectly easy to answer, but it's not so easy.

43
0:02:48.280 --> 0:02:50.440
There is a pure network solution.

44
0:02:50.440 --> 0:02:54.720
With some switches you have a feature called multicast service reflection.

45
0:02:54.720 --> 0:02:55.760
That's what Cisco provides.

46
0:02:55.760 --> 0:03:00.960
I'm pretty sure there is the equivalent on Juniper and that kind of thing.

47
0:03:00.960 --> 0:03:03.040
But it's not so widely available.

48
0:03:03.040 --> 0:03:04.920
Lots of switches don't handle it.

49
0:03:04.920 --> 0:03:07.880
Only a small range of that.

50
0:03:07.880 --> 0:03:13.360
And so basically you can type a command there to say that you copy this multicast address

51
0:03:13.360 --> 0:03:20.240
to this destination and so you copy from a VLAN to another VLAN.

52
0:03:20.240 --> 0:03:21.920
So it's not widely available.

53
0:03:21.920 --> 0:03:25.840
There are good chances that your switches do not support it.

54
0:03:25.840 --> 0:03:32.120
And also you cannot handle complex use cases like some operators want RTP, some don't want

55
0:03:32.120 --> 0:03:33.120
RTP.

56
0:03:33.120 --> 0:03:35.200
You cannot remove RTP with that.

57
0:03:35.200 --> 0:03:39.520
And some operators also want you to have specific PIDs, specific service IDs, different service

58
0:03:39.520 --> 0:03:40.520
names and so on.

59
0:03:40.520 --> 0:03:42.840
That you cannot do of course.

60
0:03:42.840 --> 0:03:49.720
So generally you will end up using the devices on top of your switch.

61
0:03:49.720 --> 0:03:54.600
You will have to plug a device that will do that kind of job.

62
0:03:54.600 --> 0:04:00.120
The broadcast solution, well the most popular one is what we call DCM.

63
0:04:00.120 --> 0:04:01.600
It's a very popular brand.

64
0:04:01.600 --> 0:04:05.800
You're initially by Cisco, now it's a company called CineMedia.

65
0:04:05.800 --> 0:04:10.680
It's a hardware service with electronics inside and it does all this work of switching.

66
0:04:10.680 --> 0:04:13.040
It can even transcode actually with some cards.

67
0:04:13.040 --> 0:04:14.400
It does a lot of things.

68
0:04:14.400 --> 0:04:18.920
But it's also very expensive of course.

69
0:04:18.920 --> 0:04:20.840
But we have an open source alternative.

70
0:04:20.840 --> 0:04:23.480
It's actually something I wrote maybe 15 years ago.

71
0:04:23.480 --> 0:04:24.480
It's called DV Blast.

72
0:04:24.480 --> 0:04:28.200
I've been doing a lot of talks about that.

73
0:04:28.200 --> 0:04:31.360
In this use case it also helps.

74
0:04:31.360 --> 0:04:34.000
Usually it was written as DVBDMux.

75
0:04:34.000 --> 0:04:40.440
So you have a satellite card or DTT card and you want to get a transponder and split each

76
0:04:40.440 --> 0:04:42.160
channel into a multicast address.

77
0:04:42.160 --> 0:04:44.400
That was the original goal of DV Blast.

78
0:04:44.400 --> 0:04:51.720
But actually with the dash D option you can also read from a multicast channel.

79
0:04:51.720 --> 0:04:57.000
And in the arguments you can also say which is the IP address of the interface you want

80
0:04:57.000 --> 0:04:58.000
to read it from.

81
0:04:58.000 --> 0:04:59.200
So basically which VLAN.

82
0:04:59.200 --> 0:05:01.600
That means which VLAN you will read it from.

83
0:05:01.600 --> 0:05:07.240
So basically that reads a multicast stream from a specific VLAN.

84
0:05:07.240 --> 0:05:12.760
There is a configuration file associated with DV Blast and you can put as many lines as

85
0:05:12.760 --> 0:05:13.760
you want.

86
0:05:13.760 --> 0:05:15.120
So as many distributors as you want.

87
0:05:15.120 --> 0:05:20.280
And for each line you will just put the multicast address you want to send it to.

88
0:05:20.280 --> 0:05:25.280
And you can also optionally give the address of the interface.

89
0:05:25.280 --> 0:05:27.880
Again you want to send it to.

90
0:05:27.880 --> 0:05:33.880
The VLANs you have on your switch they will appear as network interfaces on your server.

91
0:05:33.880 --> 0:05:38.560
So you can decide to which VLAN you want to send and which multicast address.

92
0:05:38.560 --> 0:05:42.240
You have a number of options to turn on or off RTP.

93
0:05:42.240 --> 0:05:47.000
You can remap PIDs, SID, channel service name and you can even spoof the source address

94
0:05:47.000 --> 0:05:53.640
which is very useful in case your peer wants to do IGMPv3.

95
0:05:53.640 --> 0:05:59.320
In that case you ask you to put a specific source address and that's the easiest way

96
0:05:59.320 --> 0:06:02.320
to spoof it.

97
0:06:02.320 --> 0:06:04.320
So problem solved.

98
0:06:04.320 --> 0:06:05.320
End of talk.

99
0:06:05.320 --> 0:06:06.320
Thank you very much.

100
0:06:06.320 --> 0:06:10.040
Now I wanted to add a little something.

101
0:06:10.040 --> 0:06:14.320
You may want to run hundreds of DV Blast on one server but for some reason I wanted to

102
0:06:14.320 --> 0:06:16.640
do some virtualization.

103
0:06:16.640 --> 0:06:18.040
Why virtualize?

104
0:06:18.040 --> 0:06:23.400
Because I have different customers and each of the customer has different channels.

105
0:06:23.400 --> 0:06:27.280
Some of them are doing adult content, some of them are doing children content.

106
0:06:27.280 --> 0:06:31.360
Maybe I don't want to mix that in the same server and I don't have the money to have

107
0:06:31.360 --> 0:06:35.640
multiple servers at ten hours because it's expensive.

108
0:06:35.640 --> 0:06:40.000
So for client installation it's required.

109
0:06:40.000 --> 0:06:46.400
Also some of my clients have direct access to the VM because that's the service I sell.

110
0:06:46.400 --> 0:06:51.800
So again I don't want them to see the streams of the competitors potentially.

111
0:06:51.800 --> 0:06:59.800
So I used Proxmox that's a very nice distribution based on DBN and it's a very big front end

112
0:06:59.800 --> 0:07:04.000
over KVM, very useful.

113
0:07:04.000 --> 0:07:07.920
And in Proxmox what you will do is each of the VLAN you get from the Twitch you will

114
0:07:07.920 --> 0:07:15.040
bridge that and on your guest virtual machine it will appear as a network interface with

115
0:07:15.040 --> 0:07:16.320
the VERT IOD driver.

116
0:07:16.320 --> 0:07:19.120
The VERT IOD driver is the most optimized one.

117
0:07:19.120 --> 0:07:23.280
You can also emulate a network app but it's much slower.

118
0:07:23.280 --> 0:07:26.960
That's why everybody uses VERT IOD driver.

119
0:07:26.960 --> 0:07:30.720
So everything works fine end of talk and thank you very much for coming.

120
0:07:30.720 --> 0:07:32.280
There is just one little problem.

121
0:07:32.280 --> 0:07:36.880
The morning when you get called by one of your big customers and he says I have discontinuities

122
0:07:36.880 --> 0:07:39.880
on what you send me.

123
0:07:39.880 --> 0:07:46.600
Discontinuities that's what every people in broadcast feels and you listen you start another

124
0:07:46.600 --> 0:07:50.760
DV blast on the IP address that you put in and you see nothing.

125
0:07:50.760 --> 0:07:55.960
Everything looks fine and they insist and another customer complains.

126
0:07:55.960 --> 0:07:58.440
So what you do is you rack another server.

127
0:07:58.440 --> 0:08:02.640
That's a use case by the way that's why I'm saving my life.

128
0:08:02.640 --> 0:08:05.920
So you rack another server and you listen to what the other server is sending and then

129
0:08:05.920 --> 0:08:10.760
you see a lot of discontinuities indeed.

130
0:08:10.760 --> 0:08:17.640
And you dig it and you just see that the VERT IOD driver I don't know if it's guest side

131
0:08:17.640 --> 0:08:20.640
or host side reorder some packets.

132
0:08:20.640 --> 0:08:26.120
I mean probably there are several queues inside the drivers and if this packet does not have

133
0:08:26.120 --> 0:08:31.000
any luck and goes into a queue that for some reason doesn't run for some time it will be

134
0:08:31.000 --> 0:08:33.000
pushed there.

135
0:08:33.000 --> 0:08:39.240
So I hear now the network guy will tell me that while there is no guarantee on the order

136
0:08:39.240 --> 0:08:43.800
of UDP packets that's the specification that's true.

137
0:08:43.800 --> 0:08:49.920
But we are in an industry that relies on it because if you don't have RTP there is absolutely

138
0:08:49.920 --> 0:08:53.920
no way you can reorder your UDP transport string.

139
0:08:53.920 --> 0:08:55.520
So I had to find a solution.

140
0:08:55.520 --> 0:09:00.240
I cannot tell my customer no no you have to use RTP and I don't care.

141
0:09:00.240 --> 0:09:01.680
You cannot.

142
0:09:01.680 --> 0:09:07.600
So I found a first workaround is by using another driver.

143
0:09:07.600 --> 0:09:11.400
It's also a driver designed for virtual system, VMware.

144
0:09:11.400 --> 0:09:14.040
But it's supported by Poxmox KVM and so on.

145
0:09:14.040 --> 0:09:20.320
So it's called VMXnet and probably this one only has one queue and it solves the problem.

146
0:09:20.320 --> 0:09:26.640
The only problem is that it uses 30% more CPU and this server is only doing that.

147
0:09:26.640 --> 0:09:31.240
And I already have 64 core IMD APIC servers.

148
0:09:31.240 --> 0:09:36.400
That's not full but more than 50% already.

149
0:09:36.400 --> 0:09:38.560
So there is some kind of need to optimize.

150
0:09:38.560 --> 0:09:43.600
So that's why I started to look at other alternatives and many big some clever ideas that I found

151
0:09:43.600 --> 0:09:51.240
on the net and one of them that you probably heard of maybe is called SR-ROV.

152
0:09:51.240 --> 0:09:52.240
So what is SR-ROV?

153
0:09:52.240 --> 0:09:56.400
So it's a feature of some network cards not all network cards.

154
0:09:56.400 --> 0:09:58.680
Some network cards are that feature.

155
0:09:58.680 --> 0:10:05.440
In a normal installation for the network card is owned by the host.

156
0:10:05.440 --> 0:10:13.160
You have some kind of software switch that is handled by the virtualizer and here you

157
0:10:13.160 --> 0:10:17.120
have virtual interfaces to each of the virtual machine.

158
0:10:17.120 --> 0:10:23.640
In an SR-ROV setup what happens is that the network card will create new PCI devices.

159
0:10:23.640 --> 0:10:26.240
So it's a different PCI device.

160
0:10:26.240 --> 0:10:30.360
And this PCI device there is a feature called VTD on Intel that you have the equivalent

161
0:10:30.360 --> 0:10:36.120
on IMD that will allow you to dedicate a PCI device to a virtual machine.

162
0:10:36.120 --> 0:10:40.720
And doing that the virtual machine directly talks to the PCI device without anything going

163
0:10:40.720 --> 0:10:43.000
through the host.

164
0:10:43.000 --> 0:10:46.280
So that looks quite interesting.

165
0:10:46.280 --> 0:10:51.760
These new devices are called virtual functions, VF.

166
0:10:51.760 --> 0:10:57.480
And so on the VM you just need to have a VF driver which is included in the Linux anyway.

167
0:10:57.480 --> 0:11:00.520
So in my use case I used Intel cards.

168
0:11:00.520 --> 0:11:04.080
They're not the only ones doing SR-ROV but it may be a little bit different if you use

169
0:11:04.080 --> 0:11:06.240
different cards.

170
0:11:06.240 --> 0:11:10.040
Not all cards have the same features.

171
0:11:10.040 --> 0:11:15.280
So using SR-ROV it's a little bit tricky.

172
0:11:15.280 --> 0:11:21.560
First it requires support from the motherboard, the CPU, the BIOS itself and of course the

173
0:11:21.560 --> 0:11:24.320
network card as I was just saying.

174
0:11:24.320 --> 0:11:31.200
You have to enable a number of features in the BIOS, IOMMU but also HGS, ARR which is

175
0:11:31.200 --> 0:11:35.200
some kind of PCI Express routing protocol.

176
0:11:35.200 --> 0:11:41.000
Lots of things and basically we spent a couple hours I think just setting it up.

177
0:11:41.000 --> 0:11:47.560
My personal advice would be for you to upgrade the card from the drivers, the Intel drivers

178
0:11:47.560 --> 0:11:51.320
to the latest version and the card firmware because there is a firmware running on the

179
0:11:51.320 --> 0:11:57.000
card to the latest version supported by the driver.

180
0:11:57.000 --> 0:12:01.160
Once I did not do that when I started my test and I ended up having half of my VLANs working

181
0:12:01.160 --> 0:12:04.640
and the other half didn't work and there is absolutely no way to know what's going on

182
0:12:04.640 --> 0:12:07.320
I just had to reboot and it was in production.

183
0:12:07.320 --> 0:12:11.960
So I had to imagine what happened.

184
0:12:11.960 --> 0:12:13.680
Creating the virtual function is actually quite easy.

185
0:12:13.680 --> 0:12:21.560
It's just echo into a slash this device and with my Intel card I can create up to 64 virtual

186
0:12:21.560 --> 0:12:27.920
functions so that's up to typically 64 VMs.

187
0:12:27.920 --> 0:12:34.880
So the path through is very easy on the Proxmox, it's just a menu and potentially each VM has

188
0:12:34.880 --> 0:12:36.000
access to other VMs.

189
0:12:36.000 --> 0:12:46.160
So that may be also drawback because it sees all the traffic that it can send to any VLAN

190
0:12:46.160 --> 0:12:51.640
can send packets on any VLAN so you have to trust your clients a little bit so it may

191
0:12:51.640 --> 0:12:54.520
not be adapted to all situations.

192
0:12:54.520 --> 0:12:58.680
But it's quite useful because you don't have to create a bridge every time you want to

193
0:12:58.680 --> 0:13:07.640
create a new VLAN you just create a network interface, a new A-1, Q network interface

194
0:13:07.640 --> 0:13:08.960
into your VM and that's all.

195
0:13:08.960 --> 0:13:13.000
So it's actually easier to manage.

196
0:13:13.000 --> 0:13:19.720
Problem is, so everything looks perfectly fine so that the end of the talk, not yet.

197
0:13:19.720 --> 0:13:25.280
I talked about sending to VLANs but how to receive multicast from the VLAN.

198
0:13:25.280 --> 0:13:27.160
That's another problem.

199
0:13:27.160 --> 0:13:33.320
But initially it looks like it works so you put it in production, maybe a bit early.

200
0:13:33.320 --> 0:13:38.800
So how does it work, how does a network card work with multicast?

201
0:13:38.800 --> 0:13:41.940
First I should maybe remind you how it works.

202
0:13:41.940 --> 0:13:48.760
So you have your multicast IP address, you translate that to the multicast MAC address

203
0:13:48.760 --> 0:13:55.560
so the end is the same and the beginning is just dropped.

204
0:13:55.560 --> 0:14:00.440
And then you tell the card every packet that arrives on this MAC address, I want it.

205
0:14:00.440 --> 0:14:02.720
So that's how it works on any normal PC.

206
0:14:02.720 --> 0:14:05.000
That's quite a standard, it's called a MAC filter.

207
0:14:05.000 --> 0:14:06.880
It's quite a standard feature.

208
0:14:06.880 --> 0:14:12.440
The trick is on these cards the number of MAC filters is limited.

209
0:14:12.440 --> 0:14:15.080
And the way they limit it is a little bit stupid.

210
0:14:15.080 --> 0:14:20.960
They take the whole buffer of the card, the whole number of MAC filters they have and

211
0:14:20.960 --> 0:14:24.700
they divide it by the number of VF you have, so 64.

212
0:14:24.700 --> 0:14:29.880
And in the end, according to my calculation, your limit is around 100 multicast addresses.

213
0:14:29.880 --> 0:14:37.880
So 100 may be a lot but I have hundreds of multicast streams in my network.

214
0:14:37.880 --> 0:14:40.000
So you may reach it.

215
0:14:40.000 --> 0:14:44.480
You may think about segmenting your virtual machines not to go above the threshold interval

216
0:14:44.480 --> 0:14:51.600
but it's still a dangerous game because there is a feature of the interdriver that if you

217
0:14:51.600 --> 0:14:56.120
reach that limit, it's scientific phase of course, while you have a message in the message

218
0:14:56.120 --> 0:14:59.360
but nobody reads that.

219
0:14:59.360 --> 0:15:01.320
So you will try again and try again and try again.

220
0:15:01.320 --> 0:15:08.880
And after just a few trials like five, the kernel decides that your VM is crazy and it

221
0:15:08.880 --> 0:15:10.600
won't talk to it anymore.

222
0:15:10.600 --> 0:15:16.080
So you will still receive your multicast but if you have any other command to send to the

223
0:15:16.080 --> 0:15:21.360
card like creating a new VLAN, which could happen, you have a new distributor so you

224
0:15:21.360 --> 0:15:23.240
can't, you have to reboot.

225
0:15:23.240 --> 0:15:27.560
You have to reboot your VM, fortunately, not the host.

226
0:15:27.560 --> 0:15:33.740
So it's not that practical and to be honest, I have a patch in all my installation that

227
0:15:33.740 --> 0:15:40.640
disables the doing that feature and disables using the Mac filter at all actually because

228
0:15:40.640 --> 0:15:43.040
I have found it not practical in reality.

229
0:15:43.040 --> 0:15:44.040
Yeah.

230
0:15:44.040 --> 0:15:53.040
Is there any need for the Mac filtering at all given that like modern switches, you're

231
0:15:53.040 --> 0:16:13.600
going to beat it or not your

232
0:16:13.600 --> 0:16:16.160
So, promise use mode means what it means.

233
0:16:16.160 --> 0:16:18.780
It means UVM will receive all the traffic

234
0:16:18.780 --> 0:16:20.620
that is received by the network card.

235
0:16:23.020 --> 0:16:24.120
It looks like a good idea,

236
0:16:24.120 --> 0:16:26.460
but it dramatically increases CPU usage

237
0:16:26.460 --> 0:16:29.320
because from maybe two gigabytes per second of data,

238
0:16:29.320 --> 0:16:31.380
you only need 200 megabytes per second of data,

239
0:16:31.380 --> 0:16:33.860
and the rest, the kernel, we have to filter it.

240
0:16:33.860 --> 0:16:36.140
So, your kernel will do a lot of job.

241
0:16:36.140 --> 0:16:38.540
And from what I've calculated,

242
0:16:38.540 --> 0:16:42.340
basically the gain you had from going from VERTIO to SRRUV,

243
0:16:42.340 --> 0:16:44.100
you lose it right there.

244
0:16:45.440 --> 0:16:47.040
So, that's first.

245
0:16:47.040 --> 0:16:48.840
The second problem is that,

246
0:16:50.620 --> 0:16:54.260
imagine you have two gigabytes per second on your network,

247
0:16:54.260 --> 0:16:56.440
and you have 20 virtual machines.

248
0:16:56.440 --> 0:16:59.480
The network card will send 20 times two gigabytes per second

249
0:16:59.480 --> 0:17:02.760
to your virtual machines, and that means 40.

250
0:17:02.760 --> 0:17:05.040
And 40 is the limit of the card.

251
0:17:05.040 --> 0:17:07.900
And at that point, you will start losing packets randomly.

252
0:17:09.200 --> 0:17:12.040
Again, silently, you don't know what happens.

253
0:17:12.040 --> 0:17:15.820
And of course, obviously, you only know that in production

254
0:17:15.820 --> 0:17:18.480
because when you first started with one, two, three VM,

255
0:17:18.480 --> 0:17:19.760
it worked perfectly.

256
0:17:19.760 --> 0:17:22.080
So, it's, yes, I have my solution.

257
0:17:22.080 --> 0:17:23.640
And then you put all your load on it,

258
0:17:23.640 --> 0:17:26.580
and then one day, it just stops working.

259
0:17:28.320 --> 0:17:32.280
So, while activating POMICUS mode is actually quite easy,

260
0:17:32.280 --> 0:17:36.280
again, it's an echo in the FlashSYS file.

261
0:17:36.280 --> 0:17:41.280
So, I have found a second workaround,

262
0:17:42.720 --> 0:17:45.400
which is a little bit better.

263
0:17:45.400 --> 0:17:47.160
I'm using it in production.

264
0:17:48.120 --> 0:17:50.800
It's good, maybe it's only on the Intel card.

265
0:17:50.800 --> 0:17:52.840
I don't know if it exists on other brands,

266
0:17:52.840 --> 0:17:55.240
but it's a feature called VLAN Mirror.

267
0:17:55.240 --> 0:17:57.800
And basically, it tells the card to send all the traffic

268
0:17:57.800 --> 0:18:01.040
belonging to VLAN to a particular virtual function,

269
0:18:01.040 --> 0:18:02.080
to a particular VM.

270
0:18:03.760 --> 0:18:05.600
So, that kind of a POMICUS feature,

271
0:18:05.600 --> 0:18:07.760
but only for one VM.

272
0:18:07.760 --> 0:18:12.760
Which is kind of a good practice because it means that

273
0:18:13.280 --> 0:18:16.480
I think most people who have ever done multicast,

274
0:18:16.480 --> 0:18:17.840
when you have a backbone,

275
0:18:17.840 --> 0:18:21.080
you put all of your multicast addresses in the same VM,

276
0:18:22.000 --> 0:18:24.640
maybe with different address ranges or maybe not.

277
0:18:24.640 --> 0:18:27.200
And you expect at the other end that the receiver

278
0:18:27.200 --> 0:18:31.400
will pick up which multicast address you want.

279
0:18:31.400 --> 0:18:34.440
This approach, it forces you to have different VLANs

280
0:18:34.440 --> 0:18:35.280
per customers.

281
0:18:35.280 --> 0:18:38.520
So, it's actually not a bad idea.

282
0:18:39.960 --> 0:18:42.920
But there is one drawback.

283
0:18:42.920 --> 0:18:46.120
One specific VLAN can only be sent to one VM.

284
0:18:46.120 --> 0:18:49.440
So, if you have, let's say a big broadcaster

285
0:18:49.440 --> 0:18:51.160
that is sending you channels

286
0:18:51.160 --> 0:18:54.760
and several VLANs needs those channels,

287
0:18:54.760 --> 0:18:56.120
you cannot use that solution.

288
0:18:56.120 --> 0:19:00.120
Because only one will be able to read from that VLAN.

289
0:19:01.440 --> 0:19:03.120
So, I have a third workaround.

290
0:19:03.120 --> 0:19:05.760
It's just to go back to the good old vert.io.

291
0:19:05.760 --> 0:19:07.080
After all, why not?

292
0:19:07.080 --> 0:19:09.400
The packets in search or the packet in version

293
0:19:09.400 --> 0:19:14.400
were only on TX, not on RX.

294
0:19:15.040 --> 0:19:16.800
So, it works.

295
0:19:16.800 --> 0:19:18.640
Actually, it has also some additional features

296
0:19:18.640 --> 0:19:21.080
because the bridge in Proxmox at least,

297
0:19:21.080 --> 0:19:25.080
but most of the time, it has IGMP snooping.

298
0:19:25.080 --> 0:19:27.240
So, we only receive the multicast addresses

299
0:19:27.240 --> 0:19:29.480
that you subscribe to.

300
0:19:29.480 --> 0:19:30.760
But it's actually a good solution,

301
0:19:30.760 --> 0:19:33.680
but then it means you have basically interfaces

302
0:19:33.680 --> 0:19:35.160
to read the packet from

303
0:19:35.160 --> 0:19:38.160
and interfaces to send the packets to.

304
0:19:38.160 --> 0:19:41.320
Which is a bit of a mess, but still, it's a good compromise.

305
0:19:41.320 --> 0:19:43.280
That's my compromise currently.

306
0:19:43.280 --> 0:19:44.760
VLAN mirror, all this one,

307
0:19:44.760 --> 0:19:47.800
depending on the nature of the VLAN I have to read from.

308
0:19:48.800 --> 0:19:50.760
So, all is good in the best of words.

309
0:19:51.640 --> 0:19:53.120
At the end of the talk, thank you very much.

310
0:19:53.120 --> 0:19:54.560
Not quite.

311
0:19:54.560 --> 0:19:57.120
There is another topic I haven't mentioned yet.

312
0:19:57.120 --> 0:20:02.120
What if I want to read a multicast stream

313
0:20:03.280 --> 0:20:05.880
coming from another VM on the same server?

314
0:20:07.400 --> 0:20:08.400
That doesn't work.

315
0:20:08.400 --> 0:20:11.600
Because when you write through SROV,

316
0:20:11.600 --> 0:20:13.520
just outputs to the switch.

317
0:20:14.960 --> 0:20:17.600
As far as I know, or maybe some of you are the solution,

318
0:20:17.600 --> 0:20:20.200
there is no way to get the traffic back

319
0:20:20.200 --> 0:20:25.200
to the network card and use it in another virtual machine.

320
0:20:26.320 --> 0:20:31.320
You could do that,

321
0:20:32.160 --> 0:20:35.480
but if you don't want to have different VLANs,

322
0:20:35.480 --> 0:20:37.200
if you want to read from the same VLAN,

323
0:20:37.200 --> 0:20:38.200
then you cannot root.

324
0:20:41.520 --> 0:20:42.680
Okay.

325
0:20:42.680 --> 0:20:47.400
Well, there is another solution with the Intel card again.

326
0:20:47.400 --> 0:20:50.520
It's called egress mirror.

327
0:20:50.520 --> 0:20:53.160
You can make it so that everything that's output

328
0:20:53.160 --> 0:20:54.800
on virtual function number one

329
0:20:54.800 --> 0:20:58.360
will be mirrored to virtual function number seven.

330
0:20:58.360 --> 0:21:01.520
So, virtual function number seven will be on

331
0:21:01.520 --> 0:21:03.880
your receiving side and virtual function number one

332
0:21:03.880 --> 0:21:06.000
will be on the transmitting side.

333
0:21:08.280 --> 0:21:09.600
So, that actually works.

334
0:21:09.600 --> 0:21:11.680
And also, I use that in production.

335
0:21:13.360 --> 0:21:15.400
So, conclusions.

336
0:21:15.400 --> 0:21:18.080
Well, multicast on virtualized environment

337
0:21:18.080 --> 0:21:20.640
is no picnic actually, and I'm surprised.

338
0:21:22.160 --> 0:21:25.680
You don't find many papers about that.

339
0:21:25.680 --> 0:21:29.240
I've struggled literally for years on this problem

340
0:21:29.240 --> 0:21:34.120
with a number of problems in production

341
0:21:34.120 --> 0:21:35.920
because you only see the problem in production

342
0:21:35.920 --> 0:21:37.720
because you only see them under load.

343
0:21:38.600 --> 0:21:41.040
And so, this has been a little bit tiring.

344
0:21:42.520 --> 0:21:45.280
Thank you very much for listening to this.

345
0:21:45.280 --> 0:21:47.240
And if you have any questions.

346
0:21:47.240 --> 0:21:48.080
Yeah.

347
0:21:48.080 --> 0:21:53.080
I'm guessing you tried that, but in virtual,

348
0:21:55.600 --> 0:22:00.600
there are ways to actually ask it to be dumber.

349
0:22:01.040 --> 0:22:03.560
Like, I'm not doing some of the floating balls

350
0:22:03.560 --> 0:22:05.960
and I'm not sure, but probably you tried that.

351
0:22:05.960 --> 0:22:08.280
But you could probably have asked them,

352
0:22:08.280 --> 0:22:12.280
they have to say, hey, I just keep them in order.

353
0:22:12.280 --> 0:22:15.600
So, you say in virtual, there are ways to ask it

354
0:22:15.600 --> 0:22:16.560
to be dumber.

355
0:22:18.200 --> 0:22:19.640
I'm not sure.

356
0:22:19.640 --> 0:22:21.000
We are doing packet slide.

357
0:22:21.000 --> 0:22:23.840
Yeah, I'm not sure I've actually tested it.

358
0:22:23.840 --> 0:22:25.720
I'm pretty sure it would have the same effect

359
0:22:25.720 --> 0:22:27.600
as using VMXNet.

360
0:22:27.600 --> 0:22:31.320
And probably you would see an increase in CPU consumption.

361
0:22:31.320 --> 0:22:33.400
And anyway, I wanted to go to SRIOV

362
0:22:33.400 --> 0:22:36.000
because, for all the reasons,

363
0:22:36.000 --> 0:22:39.160
because I wanted to have my VM talk directly

364
0:22:39.160 --> 0:22:41.760
to the network hub, I think it was better practice

365
0:22:41.760 --> 0:22:42.600
than virtual.

366
0:22:42.600 --> 0:22:46.200
I'm going to have to go that way.

367
0:22:46.200 --> 0:22:47.040
Yeah, James?

368
0:22:47.040 --> 0:22:49.440
I think you can back the previous slide, please.

369
0:22:49.440 --> 0:22:50.280
Yeah.

370
0:22:52.400 --> 0:22:53.760
Okay, there was no question.

371
0:22:54.960 --> 0:22:55.800
Yeah?

372
0:22:55.800 --> 0:22:59.800
Just AWS provide the ECQ instance with the SRIOV.

373
0:23:02.320 --> 0:23:03.320
With SRIOV?

374
0:23:03.320 --> 0:23:04.320
Yeah.

375
0:23:04.320 --> 0:23:05.440
Okay.

376
0:23:05.440 --> 0:23:09.440
So, he says AWS provides instances with SRIOV.

377
0:23:09.440 --> 0:23:10.440
But they have to.

378
0:23:10.440 --> 0:23:11.800
Yeah, yeah, yeah.

379
0:23:11.800 --> 0:23:14.080
I guess so, but I guess in the cloud,

380
0:23:14.080 --> 0:23:15.320
you don't have that kind of problem

381
0:23:15.320 --> 0:23:16.880
because usually you do SRT.

382
0:23:18.000 --> 0:23:18.840
Usually.

383
0:23:20.200 --> 0:23:22.640
And SRT doesn't care if you eat your other packet

384
0:23:22.640 --> 0:23:23.480
because it will.

385
0:23:23.480 --> 0:23:25.160
Similar question to this, gentlemen.

386
0:23:25.160 --> 0:23:26.000
Yeah?

387
0:23:26.000 --> 0:23:27.640
Did you consider forcing, well,

388
0:23:27.640 --> 0:23:30.440
either disable multi-Q on the host

389
0:23:30.440 --> 0:23:33.400
or forcing each VM to a different queue?

390
0:23:33.400 --> 0:23:36.200
So that by definition, you will not be reordering.

391
0:23:36.200 --> 0:23:37.600
Forcing each VM to a different queue,

392
0:23:37.600 --> 0:23:39.320
I'm not sure it works.

393
0:23:39.320 --> 0:23:41.120
A disable queuing probably it will work,

394
0:23:41.120 --> 0:23:44.240
but with additional CPU usage.

395
0:23:44.240 --> 0:23:45.080
Yeah.

396
0:23:45.080 --> 0:23:46.800
So that's what I wanted to,

397
0:23:46.800 --> 0:23:50.200
as I said, I have 60 core APIC servers,

398
0:23:50.200 --> 0:23:53.160
which are pretty big and they're not full, but.

399
0:23:53.160 --> 0:23:56.080
You should be possible to pin,

400
0:23:56.080 --> 0:23:57.840
it's possible to pin a process to the game.

401
0:23:57.840 --> 0:24:00.600
If you do XPS CPUs, you can pin the process.

402
0:24:00.600 --> 0:24:01.600
At the end, I don't know if you can do that.

403
0:24:01.600 --> 0:24:04.200
But I'm not sure if the inversion happens

404
0:24:04.200 --> 0:24:06.480
on the host side or on the guest side.

405
0:24:07.680 --> 0:24:09.040
Disable multi-Q in your host.

406
0:24:09.040 --> 0:24:09.880
Yeah.

407
0:24:09.880 --> 0:24:10.720
Yeah.

408
0:24:10.720 --> 0:24:11.560
Yeah.

409
0:24:11.560 --> 0:24:12.400
So yeah.

410
0:24:14.800 --> 0:24:16.160
Yibei?

411
0:24:16.160 --> 0:24:20.240
Why do you want to use SRUV for,

412
0:24:20.240 --> 0:24:23.120
like, what things differently is like,

413
0:24:23.120 --> 0:24:25.720
what are the other use cases you're thinking about

414
0:24:25.720 --> 0:24:26.560
with SRUV?

415
0:24:28.640 --> 0:24:32.880
So the question is why did I want to use SRUV?

416
0:24:35.600 --> 0:24:36.440
Speed.

417
0:24:36.440 --> 0:24:38.240
Speed is the first argument, actually,

418
0:24:38.240 --> 0:24:41.120
but also some kind of clean design.

419
0:24:41.120 --> 0:24:43.080
Maybe I'm a little bit of a purist

420
0:24:43.080 --> 0:24:46.400
and I wanted my VM to have direct access,

421
0:24:46.400 --> 0:24:48.840
physical access, I mean, to the network.

422
0:24:48.840 --> 0:24:53.520
Also, so if you really want the list of arguments,

423
0:24:53.520 --> 0:24:56.400
the traditional way of doing VLAN bridging

424
0:24:57.560 --> 0:25:00.440
on POC smokes, but probably other environments,

425
0:25:00.440 --> 0:25:03.720
you create a bridge for each VLAN.

426
0:25:03.720 --> 0:25:05.560
And so you have a network interface for each VLAN.

427
0:25:05.560 --> 0:25:07.000
Maybe there is a way to avoid that,

428
0:25:07.000 --> 0:25:10.080
but I didn't look too much into that.

429
0:25:10.080 --> 0:25:10.920
That means you have a limit

430
0:25:10.920 --> 0:25:13.640
because I think you're limited to 32 network interfaces.

431
0:25:13.640 --> 0:25:16.840
The limit is low enough.

432
0:25:16.840 --> 0:25:19.360
You should switch to OVX switch and not to the cloud.

433
0:25:19.360 --> 0:25:21.000
Otherwise you have to switch to OVX switch,

434
0:25:21.000 --> 0:25:25.720
but considering the amount of problems I had without it,

435
0:25:25.720 --> 0:25:28.040
I thought maybe I will not go into that.

436
0:25:29.160 --> 0:25:31.040
I haven't tried, to be honest.

437
0:25:31.040 --> 0:25:31.880
Yeah.

438
0:25:31.880 --> 0:25:32.720
These are probably people,

439
0:25:32.720 --> 0:25:34.360
you don't see papers about the most people

440
0:25:34.360 --> 0:25:37.440
like Apple versus just AT&T,

441
0:25:37.440 --> 0:25:38.440
AT&T and AT&T as well.

442
0:25:38.440 --> 0:25:40.040
Exactly, as you said,

443
0:25:40.040 --> 0:25:42.440
so are you these mainly used by people to do HTTP

444
0:25:42.440 --> 0:25:43.840
or some TCP based.

445
0:25:43.840 --> 0:25:48.560
It depends on using SIOV for non-networking things.

446
0:25:48.560 --> 0:25:51.000
Ah, you may ask if I'm considering using SIOV

447
0:25:51.000 --> 0:25:53.280
for non-networking things.

448
0:25:53.280 --> 0:25:56.320
I suppose you imply GPU maybe.

449
0:25:56.320 --> 0:25:57.960
Maybe, but my question is,

450
0:25:57.960 --> 0:26:01.960
do you envision other places where you meet media

451
0:26:01.960 --> 0:26:06.960
and you could use short GPU is almost obvious one,

452
0:26:07.680 --> 0:26:10.040
but are they, have you ever thought of

453
0:26:10.040 --> 0:26:11.480
this in the policy?

454
0:26:11.480 --> 0:26:14.400
The GPU we've actually tried to,

455
0:26:14.400 --> 0:26:17.560
not sure the one we had supported SIOV,

456
0:26:17.560 --> 0:26:18.400
but that's.

457
0:26:18.400 --> 0:26:19.800
So it's really expensive ones.

458
0:26:19.800 --> 0:26:24.160
Yeah, they have features like VTG, I think.

459
0:26:24.160 --> 0:26:27.400
But in our experience, it did not work well.

460
0:26:27.400 --> 0:26:31.800
But we would like to have the ability to share a GPU

461
0:26:31.800 --> 0:26:34.000
among several instances.

462
0:26:35.040 --> 0:26:38.240
So that you could decode or encode

463
0:26:38.240 --> 0:26:40.040
on different VMs and so on.

464
0:26:40.040 --> 0:26:42.360
This virtual machine that we have at the house,

465
0:26:42.360 --> 0:26:44.960
they don't do that because the house is not the place

466
0:26:44.960 --> 0:26:47.640
to do transcoding because it's too expensive.

467
0:26:47.640 --> 0:26:50.400
But fundamentally, yes, I would be interested in having

468
0:26:50.400 --> 0:26:51.720
that for GPUs as well.

469
0:26:51.720 --> 0:26:54.880
At least GPUs, if you have any other ideas, tell me.

470
0:26:54.880 --> 0:26:57.880
That's not also a different question.

471
0:26:57.880 --> 0:26:59.880
Just mentioning because of practical techniques,

472
0:26:59.880 --> 0:27:01.960
support, sometimes with support as well for

473
0:27:03.400 --> 0:27:06.680
SIOV for record codes and they are expensive.

474
0:27:06.680 --> 0:27:09.320
And it's just landing approximately clearly.

475
0:27:09.320 --> 0:27:11.280
So you can actually play with that.

476
0:27:11.280 --> 0:27:13.920
You can have the way for the transfer plus lambda

477
0:27:13.920 --> 0:27:18.040
and it's like, you have the nearest for like 16

478
0:27:18.040 --> 0:27:19.880
CPUs as well.

479
0:27:19.880 --> 0:27:23.120
Could you use that to do SDI or NDI

480
0:27:24.480 --> 0:27:27.360
path route to get directly to the server?

481
0:27:27.360 --> 0:27:29.160
Yeah, you don't need a SIOV for that.

482
0:27:29.160 --> 0:27:32.240
The question was, can you use that for SDI

483
0:27:32.240 --> 0:27:35.440
or NDI path route?

484
0:27:35.440 --> 0:27:37.680
Well, NDI is network.

485
0:27:37.680 --> 0:27:42.680
But SDI, I think we tried it and you can pass

486
0:27:42.680 --> 0:27:45.560
through the Blackmagic, for instance, device.

487
0:27:45.560 --> 0:27:48.880
You pass it through every TD.

488
0:27:48.880 --> 0:27:50.280
Every TD is one thing.

489
0:27:50.280 --> 0:27:53.000
Every TD, I already pass through DVB tuners.

490
0:27:53.000 --> 0:27:58.000
You can probably pass through DVB ASI, probably SDI.

491
0:27:58.000 --> 0:28:00.520
Even though we don't do that on a regular basis.

492
0:28:00.520 --> 0:28:03.960
But have you tried a different connector on a different VM?

493
0:28:03.960 --> 0:28:04.800
Yes.

494
0:28:04.800 --> 0:28:07.600
You have four inputs.

495
0:28:07.600 --> 0:28:09.080
Different connector on a different VM

496
0:28:09.080 --> 0:28:11.040
that would not be possible with the Blackmagic driver

497
0:28:11.040 --> 0:28:13.720
because it's seen as one device, one PCI device.

498
0:28:13.720 --> 0:28:15.040
So it's not possible.

499
0:28:15.040 --> 0:28:17.480
If you design your own car, maybe,

500
0:28:17.480 --> 0:28:20.960
Siwan, you may want to answer that question.

501
0:28:20.960 --> 0:28:21.800
No?

502
0:28:23.200 --> 0:28:24.920
Yes, we could do it, yes.

503
0:28:24.920 --> 0:28:28.000
There's no technical reason.

504
0:28:28.000 --> 0:28:28.840
Yes, Gav?

505
0:28:28.840 --> 0:28:31.840
Do you have any container technology?

506
0:28:33.320 --> 0:28:36.520
We could use container.

507
0:28:36.520 --> 0:28:40.600
The thing is, as you know, we have our own software.

508
0:28:40.600 --> 0:28:43.640
And our own software is actually delivered as a disk image.

509
0:28:43.640 --> 0:28:46.640
So it can only run as a virtual machine.

510
0:28:46.640 --> 0:28:49.120
The idea to use container is not a bad idea.

511
0:28:50.840 --> 0:28:53.920
There is less isolation, though,

512
0:28:53.920 --> 0:28:56.080
than what you have with virtual machine.

513
0:28:57.800 --> 0:29:00.000
But this is something I would like to try

514
0:29:01.080 --> 0:29:02.520
in the middle term, yeah.

515
0:29:02.520 --> 0:29:04.280
Because there has been a lot of improvement

516
0:29:04.280 --> 0:29:06.320
in the past years, the Galaxy network,

517
0:29:06.320 --> 0:29:08.080
and the next phase.

518
0:29:08.080 --> 0:29:11.600
And you can probably have some direct

519
0:29:11.600 --> 0:29:20.340
Mac-

520
0:29:20.340 --> 0:29:23.340
interface, so I think some kind of physical interface,

521
0:29:23.340 --> 0:29:24.780
we have a direct Mac-

522
0:29:24.780 --> 0:29:28.920
in the container and a direct physical interface.

523
0:29:30.360 --> 0:29:32.400
I think we are running out of time.

524
0:29:33.440 --> 0:29:34.280
Wow, that was a-

525
0:29:34.280 --> 0:29:59.720
$ approve entered an ECLT May

