1
0:00:00.000 --> 0:00:07.000
So we start with the first conversation by Mollibique Haidt, who was the District Manager

2
0:00:07.000 --> 0:00:09.000
of the Union this year.

3
0:00:09.000 --> 0:00:18.000
Hi everyone, I'm Mollibique Haidt. I've been a Digital Developer now for 15 years.

4
0:00:18.000 --> 0:00:24.000
And I'm going to tell you a bit of what has happened in the Digital Community in the last

5
0:00:24.000 --> 0:00:35.000
two years that we last met here. So we've had two major releases, 1.20, 1.22,

6
0:00:35.000 --> 0:00:43.000
quite a lot of merge requests as you can see. And an interesting fact, in 1.22, a third

7
0:00:43.000 --> 0:00:49.000
of the merge of the commits were in the Rust modules. So we've been investing a lot in Rust

8
0:00:49.000 --> 0:00:56.000
and the Community as well as excitement. And that's been a big happening.

9
0:00:56.000 --> 0:01:01.000
One of the big things for us as developers that we did was to merge all of the various

10
0:01:01.000 --> 0:01:09.000
Git repositories into one big giant repository. So now everything is together, except for Rust.

11
0:01:09.000 --> 0:01:15.000
Rust modules are all on the global corner because they're released with the GTK RS infrastructure.

12
0:01:15.000 --> 0:01:23.000
So the rest of these few months. But that's kind of our big change for developers,

13
0:01:23.000 --> 0:01:27.000
but it doesn't change much for the end user because we still release all the packages

14
0:01:27.000 --> 0:01:33.000
in separate card balls, just like these.

15
0:01:33.000 --> 0:01:39.000
Another thing that we did that's with the infrastructure is we improved our build system

16
0:01:39.000 --> 0:01:44.000
and now we can select the elements one by one. Not just the plugins,

17
0:01:44.000 --> 0:01:47.000
but you can select exactly which elements you want in the plugin.

18
0:01:47.000 --> 0:01:52.000
And then we can also link all the plugins, all the elements, all the dependencies

19
0:01:52.000 --> 0:01:57.000
into one giant library. This makes it actually easier to make a smaller build

20
0:01:57.000 --> 0:02:02.000
because we can build only exactly the functionality that you need for a specific application.

21
0:02:02.000 --> 0:02:08.000
So I've fixed several applications, created a big library that only has the attack functions

22
0:02:08.000 --> 0:02:13.000
that are being used, and not anything else. So we did that for embedded systems mostly

23
0:02:13.000 --> 0:02:20.000
so that you can have something a little smaller.

24
0:02:20.000 --> 0:02:23.000
Another area, and there has been quite a lot of excitement in this server

25
0:02:23.000 --> 0:02:28.000
in the last couple of years, is WebRTC. So as for me, all of you are familiar with,

26
0:02:28.000 --> 0:02:33.000
WebRTC is a way to send video and low latency to and from browsers.

27
0:02:33.000 --> 0:02:39.000
And this server has one of the most complete implementations outside of the Google implementation

28
0:02:39.000 --> 0:02:44.000
that's used by the browsers. We were missing one big bit,

29
0:02:44.000 --> 0:02:50.000
and that was a congestion control. And that's been added in the last releases.

30
0:02:50.000 --> 0:02:56.000
So now we have a module that is compatible with what's called Google congestion control,

31
0:02:56.000 --> 0:03:01.000
which is what Chrome and Firefox and Safari use. And this is in Rust.

32
0:03:01.000 --> 0:03:04.000
And to make that work, so we have a WebRTC implementation,

33
0:03:04.000 --> 0:03:10.000
but that did not do any of the actual encoding. It was left separate on purpose.

34
0:03:10.000 --> 0:03:16.000
Now we have a module in Rust plugin that will plug the encoder and WebRTC

35
0:03:16.000 --> 0:03:22.000
and do the congestion control so that you can adapt the bitrate of the encoder to the encoding.

36
0:03:22.000 --> 0:03:28.000
And this is all automatic if you use this plugin.

37
0:03:28.000 --> 0:03:34.000
We also have WIP and WEP. So these are also within Rust there.

38
0:03:34.000 --> 0:03:38.000
WIP and WEP are a way to replace RTMP, but based on WebRTC,

39
0:03:38.000 --> 0:03:45.000
so it's a single request, a single HTTP request way to set up a WebRTC stream.

40
0:03:45.000 --> 0:03:48.000
It's mostly meant to stream to and from a server.

41
0:03:48.000 --> 0:03:57.000
So it's really a replacement for RTMP for low latency video transmission.

42
0:03:57.000 --> 0:04:02.000
Speaking of WebRTC, WebRTC is based on RITP, and this is an area where they're also

43
0:04:02.000 --> 0:04:10.000
been quite a bit of development. So starting with 2022.1, 4D recorrection,

44
0:04:10.000 --> 0:04:16.000
that's a system used mostly for latency broadcast transmission,

45
0:04:16.000 --> 0:04:22.000
and we have the 2D 4D recorrection. So what does it mean 2D?

46
0:04:22.000 --> 0:04:27.000
It means that we do 4D recorrection, which is basically you absorb multiple packets,

47
0:04:27.000 --> 0:04:32.000
and you generate a new one. And if you have any of these packets except one,

48
0:04:32.000 --> 0:04:36.000
then you can regenerate the missing one. That's what the parallel recorrection is.

49
0:04:36.000 --> 0:04:39.000
Traditionally, you would do packets one, two, and three and four,

50
0:04:39.000 --> 0:04:41.000
and then you would generate a fifth packet.

51
0:04:41.000 --> 0:04:45.000
But losses tend to come in bursts in networks.

52
0:04:45.000 --> 0:04:49.000
So with 2D recorrection, we have this kind of traditional version,

53
0:04:49.000 --> 0:04:52.000
and also a version where you do packet one and five and ten, right?

54
0:04:52.000 --> 0:04:59.000
So if you have a burst, you can recover more of the packets.

55
0:04:59.000 --> 0:05:02.000
The other thing that we've added, so just for a long time,

56
0:05:02.000 --> 0:05:05.000
we've had APIs to add RTP header extensions.

57
0:05:05.000 --> 0:05:10.000
That's a way to reach packets to add a little header with something else.

58
0:05:10.000 --> 0:05:12.000
So for a long time, we had libraries to actually write these,

59
0:05:12.000 --> 0:05:18.000
but we didn't have something in the system to easily plug in

60
0:05:18.000 --> 0:05:21.000
something to insert this header in every packet

61
0:05:21.000 --> 0:05:23.000
without having to write application code.

62
0:05:23.000 --> 0:05:25.000
So this is something that we've added,

63
0:05:25.000 --> 0:05:28.000
and we've added a bunch of different modules.

64
0:05:28.000 --> 0:05:31.000
Notable is this client to mixer audio level.

65
0:05:31.000 --> 0:05:36.000
This makes it possible for a sender of audio to say,

66
0:05:36.000 --> 0:05:38.000
the volume that I'm sending is this,

67
0:05:38.000 --> 0:05:42.000
so that the add mixer or some kind of service

68
0:05:42.000 --> 0:05:45.000
can select the person who speaks loudest

69
0:05:45.000 --> 0:05:47.000
without having to decode all of the audio.

70
0:05:47.000 --> 0:05:50.000
So you can know from this level who's speaking louder

71
0:05:50.000 --> 0:05:54.000
and just forward that one.

72
0:05:54.000 --> 0:05:56.000
Then, Color Space.

73
0:05:56.000 --> 0:05:58.000
So this is for VP9.

74
0:05:58.000 --> 0:06:02.000
If you want to send HDR over VP9,

75
0:06:02.000 --> 0:06:08.000
we now have this archive access to make it work.

76
0:06:08.000 --> 0:06:10.000
It's compatible with Chrome again.

77
0:06:10.000 --> 0:06:13.000
So this is a place where people are experimenting with it.

78
0:06:13.000 --> 0:06:15.000
And we have an AP1 header in the builder.

79
0:06:15.000 --> 0:06:19.000
So this is, I think, for the first thing where we've decided

80
0:06:19.000 --> 0:06:23.000
that the official implementation of a major feature

81
0:06:23.000 --> 0:06:25.000
is only available in Rust.

82
0:06:25.000 --> 0:06:29.000
So this is something that we're pretty happy about.

83
0:06:29.000 --> 0:06:33.000
Another thing, so H.264, H.265, they have two kinds of timestamp.

84
0:06:33.000 --> 0:06:37.000
Presentation timestamp, decoding timestamp.

85
0:06:37.000 --> 0:06:41.000
When you send RTP, normally you only send a presentation timestamp.

86
0:06:41.000 --> 0:06:44.000
You need to apply an algorithm to recover the decoding timestamp.

87
0:06:44.000 --> 0:06:49.000
We now have modules that generate that.

88
0:06:49.000 --> 0:06:59.000
We also support RFC 6051, so that's a way to synchronize streams immediately.

89
0:06:59.000 --> 0:07:04.000
So traditionally with RTP, we send two streams, audio, video, separate timestamps.

90
0:07:04.000 --> 0:07:08.000
And then sometimes later you get a packet telling you what the correspondence is.

91
0:07:08.000 --> 0:07:13.000
With 6051, we add the RTP header extension in every packet

92
0:07:13.000 --> 0:07:17.000
so that we can be synchronized from the first packet.

93
0:07:17.000 --> 0:07:22.000
And we also improve our base class for video decoders a bit.

94
0:07:22.000 --> 0:07:28.000
So now it can recognize that there's a corruption

95
0:07:28.000 --> 0:07:32.000
and use that to request a retransmission.

96
0:07:32.000 --> 0:07:36.000
Previously, we did, we kind of applied the error,

97
0:07:36.000 --> 0:07:38.000
but we let the application do the decision.

98
0:07:38.000 --> 0:07:46.000
Now we've added something to the base class.

99
0:07:46.000 --> 0:07:50.000
Another big feature that was worked on,

100
0:07:50.000 --> 0:07:54.000
that we basically rewrote the HLS and dash base class.

101
0:07:54.000 --> 0:07:58.000
So the previous one was over 10 years old

102
0:07:58.000 --> 0:08:02.000
and had been written largely without the specs.

103
0:08:02.000 --> 0:08:07.000
And even when we had the specs, HLS has changed quite a lot the last 10 years.

104
0:08:07.000 --> 0:08:13.000
So now we have what we call a state-of-the-art implementation

105
0:08:13.000 --> 0:08:17.000
based on 10 years more knowledge.

106
0:08:17.000 --> 0:08:20.000
It's much more simple, has fewer trends,

107
0:08:20.000 --> 0:08:25.000
much better control on how we download things on the buffering.

108
0:08:25.000 --> 0:08:27.000
We do a little bit of the parsing in there

109
0:08:27.000 --> 0:08:30.000
because sadly for many of these for us, you have to have two parts

110
0:08:30.000 --> 0:08:33.000
that they stream to handle it properly.

111
0:08:33.000 --> 0:08:41.000
So this is all implemented as one in this decade.

112
0:08:41.000 --> 0:08:45.000
We've put a few things around decoding, mostly video decoding.

113
0:08:45.000 --> 0:08:48.000
One thing I'm quite excited about is the subframe decoding

114
0:08:48.000 --> 0:08:50.000
that has been quite a few years coming.

115
0:08:50.000 --> 0:08:55.000
And this means that we now have infrastructure in our base classes

116
0:08:55.000 --> 0:08:59.000
to start decoding a frame before you receive the entire frame.

117
0:08:59.000 --> 0:09:06.000
So some formats, issue six, version six, five,

118
0:09:06.000 --> 0:09:08.000
we can split the frame in slices and now, from this personnel,

119
0:09:08.000 --> 0:09:11.000
you can actually start doing the decoding.

120
0:09:11.000 --> 0:09:13.000
We have two implementations of this.

121
0:09:13.000 --> 0:09:17.000
One is based on a thing, which you can do with this only for issue six four.

122
0:09:17.000 --> 0:09:19.000
And the other one is for the Xilinx hardware

123
0:09:19.000 --> 0:09:22.000
because they have the hardware features to do that.

124
0:09:22.000 --> 0:09:27.000
So they can super low latency decoding.

125
0:09:27.000 --> 0:09:32.000
We have WebM alpha, so the WebM format.

126
0:09:32.000 --> 0:09:36.000
So each VPE, VPE9 don't have support for transparency built in.

127
0:09:36.000 --> 0:09:40.000
But there's a WebM extension where we basically store

128
0:09:40.000 --> 0:09:43.000
two separate video streams, one with the colors

129
0:09:43.000 --> 0:09:45.000
and one with the transparency.

130
0:09:45.000 --> 0:09:48.000
And now we have an element that will spin up two decoders

131
0:09:48.000 --> 0:09:54.000
and then recombine them into a 1v stream.

132
0:09:54.000 --> 0:09:58.000
We have a DirectX element library, so make it easier

133
0:09:58.000 --> 0:10:02.000
to integrate DirectX 3D element applications in the streamer

134
0:10:02.000 --> 0:10:09.000
to do zero copy encoding, for example, from a Windows application.

135
0:10:09.000 --> 0:10:13.000
And also speaking of Windows, our DirectX 3D element decoders

136
0:10:13.000 --> 0:10:15.000
are now the default.

137
0:10:15.000 --> 0:10:18.000
So they're becoming the choice that will get auto-plugged

138
0:10:18.000 --> 0:10:22.000
if you have them, so you get hardware acceleration decoding

139
0:10:22.000 --> 0:10:24.000
on the Windows that works.

140
0:10:26.000 --> 0:10:27.000
Yep?

141
0:10:27.000 --> 0:10:30.000
What about Mac codes?

142
0:10:30.000 --> 0:10:32.000
Yes, there's also...

143
0:10:32.000 --> 0:10:33.000
We've got a question already.

144
0:10:33.000 --> 0:10:35.000
Yeah, about Mac codes.

145
0:10:35.000 --> 0:10:39.000
So we have a hardware decoder for Mac and iOS,

146
0:10:39.000 --> 0:10:41.000
which is the same API.

147
0:10:41.000 --> 0:10:46.000
CUDA, so some people use proprietary software

148
0:10:46.000 --> 0:10:51.000
and proprietary drivers, so we have now also a CUDA library

149
0:10:51.000 --> 0:10:56.000
so that you can insert more easily CUDA data into the streamer

150
0:10:56.000 --> 0:11:00.000
for encoding, streaming, decoding, all of these things.

151
0:11:00.000 --> 0:11:03.000
We have some more CUDA-native elements,

152
0:11:03.000 --> 0:11:08.000
one that is a converter and a scaler, so using CUDA itself.

153
0:11:08.000 --> 0:11:12.000
We have CUDA and direct 3D integration for Windows again.

154
0:11:12.000 --> 0:11:16.000
And this whole thing basically gives you zero copy encoding

155
0:11:16.000 --> 0:11:19.000
on NVIDIA hardware, especially when you match

156
0:11:19.000 --> 0:11:23.000
with some other CUDA-based software.

157
0:11:25.000 --> 0:11:27.000
But some people prefer free software,

158
0:11:27.000 --> 0:11:32.000
so we also have the API, we have a new plugin for the API.

159
0:11:32.000 --> 0:11:36.000
So we've had this user of the API for a long, long time.

160
0:11:36.000 --> 0:11:38.000
It was getting quite freaky.

161
0:11:38.000 --> 0:11:40.000
It was not based on any of the base classes

162
0:11:40.000 --> 0:11:43.000
that we have improved since then.

163
0:11:43.000 --> 0:11:47.000
So it's been completely rewritten from scratch,

164
0:11:47.000 --> 0:11:50.000
with a new plugin that we call VA.

165
0:11:50.000 --> 0:11:53.000
It supports all the major codecs now,

166
0:11:53.000 --> 0:11:55.000
we've implemented all the ones with VA.

167
0:11:55.000 --> 0:12:00.000
Supports APY, it supports just 5, APA, DT90, MB2.

168
0:12:00.000 --> 0:12:04.000
Encoding, we only have the H2645 codecs for now,

169
0:12:04.000 --> 0:12:08.000
but the rest are being worked on, as we speak.

170
0:12:08.000 --> 0:12:14.000
And using libv, we have a bit more than the API.

171
0:12:14.000 --> 0:12:17.000
We have a bit more features, we have a compositor,

172
0:12:17.000 --> 0:12:20.000
so that's an element that will take two or more streams

173
0:12:20.000 --> 0:12:24.000
and composite them into the same video.

174
0:12:24.000 --> 0:12:29.000
We have the Dinterlacer, and we have a post-prosterer element

175
0:12:29.000 --> 0:12:32.000
with scaling and color space conversion

176
0:12:32.000 --> 0:12:39.000
using the video functionality instead of the GPU.

177
0:12:39.000 --> 0:12:43.000
And all of our work has happened on AV1 also,

178
0:12:43.000 --> 0:12:46.000
in the last two years.

179
0:12:46.000 --> 0:12:51.000
So we have quite a lot now, we have support for AV1,

180
0:12:51.000 --> 0:12:55.000
both in the legacy VA plugin and in the new VA plugin.

181
0:12:55.000 --> 0:12:58.000
We have it for AMD, the coders,

182
0:12:58.000 --> 0:13:04.000
we have it for Direct3D on Windows, using the NPD APIs.

183
0:13:04.000 --> 0:13:09.000
For Intel, using their multiple libraries,

184
0:13:09.000 --> 0:13:12.000
either QuickSync, QSV, or the new SDK.

185
0:13:12.000 --> 0:13:15.000
So we have pretty comprehensive AV1 support,

186
0:13:15.000 --> 0:13:20.000
in addition to the RTD plugin that I mentioned earlier.

187
0:13:20.000 --> 0:13:23.000
Another new thing that we've done is,

188
0:13:23.000 --> 0:13:27.000
this is our first official machine learning integration

189
0:13:27.000 --> 0:13:29.000
that is in G-streamer itself.

190
0:13:29.000 --> 0:13:31.000
So it's a first step.

191
0:13:31.000 --> 0:13:38.000
And we've written a plugin to use the Onyx runtime from Microsoft

192
0:13:38.000 --> 0:13:43.000
to basically take some video frames,

193
0:13:43.000 --> 0:13:50.000
some 3D model and recognize objects,

194
0:13:50.000 --> 0:13:53.000
put little boxes in the metadata,

195
0:13:53.000 --> 0:13:56.000
and then another element that can take these boxes

196
0:13:56.000 --> 0:13:58.000
and draw them on the image.

197
0:13:58.000 --> 0:14:01.000
So this is the first step, a lot of work is happening right now

198
0:14:01.000 --> 0:14:05.000
to have a better video analytics framework as part of G-streamer.

199
0:14:10.000 --> 0:14:14.000
All these things are nice, but sometimes you want to have a UI.

200
0:14:14.000 --> 0:14:18.000
And a few features were recently added there.

201
0:14:18.000 --> 0:14:23.000
We have a GTK4 paintable, so that's an object.

202
0:14:23.000 --> 0:14:27.000
I would say that you can use the GTK4 to actually draw something on the screen.

203
0:14:27.000 --> 0:14:34.000
So now you can easily integrate G-streamer with GTK4, 0.P, playback.

204
0:14:34.000 --> 0:14:37.000
This one is in Rust, which is kind of cool.

205
0:14:37.000 --> 0:14:42.000
Qt 6 also has a theme, so that we have something that is very similar to what we have for Qt 5,

206
0:14:42.000 --> 0:14:47.000
which is a QML item, so that you can integrate a G-streamer sync,

207
0:14:47.000 --> 0:14:52.000
the output width, Qt, and draw in your Qt application.

208
0:14:52.000 --> 0:14:57.000
And the last one is a bit of a niche case.

209
0:14:57.000 --> 0:15:00.000
So we had a Wayland sync for a long time,

210
0:15:00.000 --> 0:15:04.000
and what this Wayland sync allows you to do is basically take the video buffer

211
0:15:04.000 --> 0:15:09.000
and give it to the Wayland compositor directly without going through the toolkit.

212
0:15:09.000 --> 0:15:14.000
So you can use the 2D hardware planes of the platform.

213
0:15:14.000 --> 0:15:17.000
This is multi-use, useful, and embedded.

214
0:15:17.000 --> 0:15:22.000
It allows you to do things like greater performance, not use the GPU.

215
0:15:22.000 --> 0:15:26.000
Unembedded systems or the GPU might be too slow to do.

216
0:15:26.000 --> 0:15:32.000
Up to now, this all works fine, but you have to write low-level Wayland code,

217
0:15:32.000 --> 0:15:35.000
and that's non-trivial.

218
0:15:35.000 --> 0:15:39.000
So we've written a GTK widget that wraps that.

219
0:15:39.000 --> 0:15:42.000
So now you can write your application in GTK, just add the widget,

220
0:15:42.000 --> 0:15:47.000
and you get all of these performance benefits for free.

221
0:15:47.000 --> 0:15:54.000
Last but not least, in this category, we added touch event navigation.

222
0:15:54.000 --> 0:15:57.000
So previously we had navigation.

223
0:15:57.000 --> 0:15:59.000
You could send letters, you could send mouse clicks,

224
0:15:59.000 --> 0:16:04.000
but now we can also send touch events so that you can have elements in your pipeline

225
0:16:04.000 --> 0:16:09.000
that are controlled by the user, such as we have a WebR app,

226
0:16:09.000 --> 0:16:15.000
a WebView, a WebKit-based source.

227
0:16:15.000 --> 0:16:17.000
And we have some new tracers.

228
0:16:17.000 --> 0:16:22.000
So these are tools for developers to know actually what's going on in a pipeline live.

229
0:16:22.000 --> 0:16:27.000
We have a bunch of tracers already, four more were added.

230
0:16:27.000 --> 0:16:29.000
Some of them are quite useful.

231
0:16:29.000 --> 0:16:35.000
We have one to generate, read the buffer lateness, and one to trace the Q-level,

232
0:16:35.000 --> 0:16:39.000
and these will output the information in a CSV file that you can then load

233
0:16:39.000 --> 0:16:42.000
and make nice graphs and understand, like, what's the live performance

234
0:16:42.000 --> 0:16:45.000
of your pipeline, what's going on.

235
0:16:45.000 --> 0:16:49.000
We have one to draw pretty pipeline snapshots.

236
0:16:49.000 --> 0:16:52.000
So for a long time, we had a feature where we could draw a DOS file

237
0:16:52.000 --> 0:16:56.000
to draw a picture of the pipeline, but this required it,

238
0:16:56.000 --> 0:16:59.000
added some code to your application to actually trigger it.

239
0:16:59.000 --> 0:17:03.000
With this tracer, now you can just listen to a Unix signal

240
0:17:03.000 --> 0:17:05.000
and trigger it on the spot.

241
0:17:05.000 --> 0:17:07.000
The last one is the factory tracer.

242
0:17:07.000 --> 0:17:11.000
This is the very first feature I mentioned.

243
0:17:11.000 --> 0:17:15.000
So it's nice to say, oh, I'm going to build a G-streamer,

244
0:17:15.000 --> 0:17:18.000
build specifically for my application with only the elements I use.

245
0:17:18.000 --> 0:17:04.500
But if you use Play

246
0:17:19.000 --> 0:17:21.000
then there's a lot of automated things,

247
0:17:21.000 --> 0:17:25.000
and you might not know exactly which plugins you've been using.

248
0:17:25.000 --> 0:17:28.000
So with this tracer, we can actually trace all the plugins that get loaded,

249
0:17:28.000 --> 0:17:32.000
all the elements that are used, and print, when you use the application,

250
0:17:32.000 --> 0:17:36.000
print the list of what was actually used.

251
0:17:36.000 --> 0:17:39.000
Question about that.

252
0:17:39.000 --> 0:17:44.000
Playbin sometimes tries to use plugin, but it's not related because it is working.

253
0:17:44.000 --> 0:17:47.000
It's going to be listed in a bit.

254
0:17:47.000 --> 0:17:48.000
So yeah, sorry.

255
0:17:48.000 --> 0:17:50.000
Playbin sometimes tries to use something,

256
0:17:50.000 --> 0:17:54.000
but it doesn't work because the hardware is not there or something.

257
0:17:54.000 --> 0:17:58.000
So in this case, the tracer will still list it.

258
0:17:58.000 --> 0:18:00.000
It's everything that is loaded, right?

259
0:18:00.000 --> 0:18:04.000
When they're tried or loaded, you call a function in it and it says no.

260
0:18:04.000 --> 0:18:08.000
So let's just really get the loading stage.

261
0:18:10.000 --> 0:18:13.000
Thank you. Any questions? Yes.

262
0:18:13.000 --> 0:18:16.000
You mentioned the RTP support in there.

263
0:18:16.000 --> 0:18:20.000
So did you also add the SPC extension to that?

264
0:18:20.000 --> 0:18:23.000
I have no idea.

265
0:18:23.000 --> 0:18:26.000
Anyone knows? No.

266
0:18:26.000 --> 0:18:31.000
RTC is only one extension, SVC extension, in...

267
0:18:31.000 --> 0:18:33.000
I don't know if it's there.

268
0:18:33.000 --> 0:18:40.000
We do layer selection of the highest quality right now, but it's not.

269
0:18:40.000 --> 0:18:42.000
There is an external...

270
0:18:42.000 --> 0:18:45.000
There is a dependency in the script turner to the extension

271
0:18:45.000 --> 0:18:48.000
where you can read information about the SPC layers in there,

272
0:18:48.000 --> 0:18:50.000
which is under that.

273
0:18:50.000 --> 0:18:53.000
So you encode the SPC layers into one screen,

274
0:18:53.000 --> 0:18:58.000
then you use these external extensions to carry information about what's in there

275
0:18:58.000 --> 0:19:01.000
and put these in so that you can do a good collection for that.

276
0:19:01.000 --> 0:19:03.000
So that's what I have to do.

277
0:19:03.000 --> 0:19:07.000
None of the RTP SPC stuff, including the VP1,

278
0:19:07.000 --> 0:19:11.000
which is quickly required because there's no SVC inside the screen.

279
0:19:11.000 --> 0:19:13.000
It's not implemented yet. Thank you.

280
0:19:13.000 --> 0:19:14.000
And supervise.

281
0:19:14.000 --> 0:19:19.000
Yes. So the question was, RTP, AV1, SVC, there's an extension

282
0:19:19.000 --> 0:19:21.000
to actually make it really useful.

283
0:19:21.000 --> 0:19:24.000
It's being standardized and it's not implemented yet.

284
0:19:24.000 --> 0:19:27.000
But it will be at some point.

285
0:19:27.000 --> 0:19:31.000
I forgot to mention we have an online question,

286
0:19:31.000 --> 0:19:35.000
an online question at number 43401.

287
0:19:35.000 --> 0:19:39.000
So I'm at home on that question as possible.

288
0:19:39.000 --> 0:19:43.000
But since we don't have any questions, are there any more questions on the floor?

289
0:19:43.000 --> 0:19:44.000
There's one there.

290
0:19:44.000 --> 0:19:47.000
Q6, does it support different rendering backends,

291
0:19:47.000 --> 0:19:51.000
direct picks on Windows and both and on Linux or something?

292
0:19:51.000 --> 0:19:55.000
Will there be QML and then Q6 support those different stuff

293
0:19:55.000 --> 0:19:59.000
because it directly talks directly to QML?

294
0:19:59.000 --> 0:20:04.000
So does Q6 support other backends than OpenGL?

295
0:20:04.000 --> 0:20:11.000
And I think the answer right now is will only support OpenGL.

296
0:20:11.000 --> 0:20:13.000
Are you all pushed in? Yes.

297
0:20:13.000 --> 0:20:15.000
Can you do a statically linked winery?

298
0:20:15.000 --> 0:20:17.000
Can you do a statically linked winery? Yes, you can.

299
0:20:17.000 --> 0:20:19.000
That's kind of one of the use cases,

300
0:20:19.000 --> 0:20:21.000
that we create this statically linked library

301
0:20:21.000 --> 0:20:23.000
and then your application can link to it

302
0:20:23.000 --> 0:20:25.000
and only link the required bits.

303
0:20:25.000 --> 0:20:29.000
That's kind of part of the trick to make it a little smaller.

304
0:20:35.000 --> 0:20:39.000
Q7, there is a congestion control that works as you know.

305
0:20:39.000 --> 0:20:40.000
Yes, you are.

306
0:20:40.000 --> 0:20:48.000
Is it the same feature set as in Google's implementation?

307
0:20:48.000 --> 0:20:52.000
The question is about congestion control in WebRTC.

308
0:20:52.000 --> 0:20:55.000
Is it the same feature set as the Google implementation?

309
0:20:55.000 --> 0:21:01.000
As far as I know, yes, because it's basically a copy of the implementation in Rust.

310
0:21:01.000 --> 0:21:06.000
So they basically re-implement the same algorithm.

311
0:21:06.000 --> 0:21:09.000
So that is compatible.

312
0:21:09.000 --> 0:21:11.000
But it's pluggable, so you could write your own.

313
0:21:11.000 --> 0:21:14.000
There is actually a plug-in mechanism.

314
0:21:14.000 --> 0:21:17.000
The core representation is in C,

315
0:21:17.000 --> 0:21:19.000
but this one is in Rust with a separate plug-in.

316
0:21:19.000 --> 0:21:23.000
One could write a different implementation.

317
0:21:23.000 --> 0:21:26.000
Because there's a bunch of heuristics in there.

318
0:21:26.000 --> 0:21:30.000
There's no perfect answer.

319
0:21:30.000 --> 0:21:32.000
Thank you so much.

320
0:21:32.000 --> 0:21:34.000
Do you have a question? Yes.

321
0:21:34.000 --> 0:21:39.000
If I have an application that does WebRTC signaling over matrix, for example,

322
0:21:39.000 --> 0:21:43.000
would I benefit from switching to WebRTC sync,

323
0:21:43.000 --> 0:21:46.000
or would I stick with WebRTC sync?

324
0:21:46.000 --> 0:21:50.000
So the question is, if you have an implementation of WebRTC

325
0:21:50.000 --> 0:21:52.000
that does signaling something custom,

326
0:21:52.000 --> 0:21:56.000
for example, other matrix, would you benefit using WebRTC sync?

327
0:21:56.000 --> 0:22:00.000
The answer to that is yes,

328
0:22:00.000 --> 0:22:07.000
because it will do all the encoding and congestion control hooked up for you.

329
0:22:07.000 --> 0:22:12.000
And there's an interface that's actually implemented for your own signal.

330
0:22:12.000 --> 0:22:15.000
So the signaling is separate from this WebRTC sync.

331
0:22:15.000 --> 0:22:18.000
There's still a module that we can implement.

332
0:22:18.000 --> 0:22:22.000
We have one implemented for, like, custom signaling mechanism,

333
0:22:22.000 --> 0:22:25.000
where there's a server that's implemented, but people write your own.

334
0:22:25.000 --> 0:22:27.000
In Rust.

335
0:22:27.000 --> 0:22:35.000
Last question? No, just a comment to the question before the QT6 started 3D integration.

336
0:22:35.000 --> 0:22:38.000
There is a merge request for it.

337
0:22:38.000 --> 0:22:49.000
Okay. So Tim says that for the QT, there's a direct 3D merge request open

338
0:22:49.000 --> 0:22:53.000
to integrate that. That's not merged yet, but do test it at home

339
0:22:53.000 --> 0:22:55.000
and complain when it doesn't work.

340
0:22:55.000 --> 0:22:59.000
Last question? No? Okay, thank you, everybody.

