1
0:00:00.000 --> 0:00:15.720
Okay, we can start. Thank you for attempting to this meeting, which is about Linfon and

2
0:00:15.720 --> 0:00:25.960
video conferencing. My name is Jean Monier. I'm involved in the Linfon project since 2010.

3
0:00:25.960 --> 0:00:33.960
And I'm also part of the company which is backing the Linfon project since 10 years

4
0:00:33.960 --> 0:00:42.360
almost. So first, I'm going to provide you with a quick introduction on Linfon. And then

5
0:00:42.360 --> 0:00:52.840
I have a couple of words around video conferencing with SIP, followed by an introduction on the

6
0:00:52.840 --> 0:01:02.000
selecting forwarding unit, which is the art of the modern video conferencing systems. And later,

7
0:01:02.000 --> 0:01:09.080
to talk about what is required on the SIP client part to be able to interrupt with this kind of

8
0:01:09.080 --> 0:01:18.480
video conferencing system. And finally, the conclusion. Okay, so just a couple of words

9
0:01:18.480 --> 0:01:28.520
about Linfon. Linfon is a voice over IP client implementing the SIP protocol, which started in

10
0:01:28.520 --> 0:01:40.680
early 2000. It's available on Linux, Android, iOS, Windows, and Mac. It uses SIP as the base

11
0:01:40.680 --> 0:01:48.480
standard for almost everything, including audio, video call, and instant messaging presence.

12
0:01:48.480 --> 0:01:57.560
Everything which is required for real-time communication. And it also provides some end-to-end

13
0:01:57.560 --> 0:02:07.080
encryption for messaging based on the signal protocol more or less. The Linfon team developed

14
0:02:07.080 --> 0:02:13.200
the Linfon software, but also SIP rover, which is basically a SIP proxy. And if you want to use

15
0:02:13.200 --> 0:02:23.280
SIP account, it's possible to create them on our website for testing purpose mainly. Okay,

16
0:02:23.280 --> 0:02:31.560
video conferencing with SIP in a couple of words. It's around a couple of standards. The first one

17
0:02:31.560 --> 0:02:40.000
is SIP, basic SIP with invite, refer, and buy to create conference, join the conference to be

18
0:02:40.000 --> 0:02:50.840
able to invite other participants to the conference. And it's almost based on the RFC 4579, which

19
0:02:50.840 --> 0:02:59.880
defines how to create the conference and how to join the conference. And there is also some

20
0:02:59.880 --> 0:03:09.120
interesting standard, which is the RFC 5366, which is about defining the list of the participants of

21
0:03:09.120 --> 0:03:18.120
the conference. So it's for the establishment of the conference. And the next important standard

22
0:03:18.120 --> 0:03:27.000
is what we call the conferencing events package, which is based on the subscribe notify RFC. And

23
0:03:27.000 --> 0:03:34.640
the idea is that when a participant joined the conference, it initiates SIP subscribe to the

24
0:03:34.640 --> 0:03:41.800
server and the server then notified to every participant of the conference, which are the

25
0:03:41.800 --> 0:03:48.880
states of the conference within who is going to join is their audio, video, everything which

26
0:03:48.880 --> 0:04:01.840
is related to the status of the conference. On the media port is regular RTP. And for this video

27
0:04:01.840 --> 0:04:09.680
conferencing project, we added the support of two important RFC, which are about bundling all the

28
0:04:09.680 --> 0:04:19.760
media stream into the same socket in order to avoid to have too many RTP sockets, RTP stream per

29
0:04:19.760 --> 0:04:29.560
supply. And regarding the security, it's a regular CPTLS for the for the signaling and for the media

30
0:04:29.560 --> 0:04:40.440
path. It's either SDS where the symmetric key is set in the SDP or the RTP or even SRTPDTLS. And

31
0:04:40.440 --> 0:04:53.480
for the RTP itself, it's a standard AES. Okay, now let's introduce what the selected forwarding

32
0:04:53.480 --> 0:05:03.240
units. And I'm going to start with the description on what we used to have for conferencing server.

33
0:05:03.240 --> 0:05:15.680
So in the past, the media mixer received the video from every user, decode the video stream,

34
0:05:15.680 --> 0:05:25.520
perform the mixing, it could be mosaic or any layout, and then re-encode all the stream to be

35
0:05:25.520 --> 0:05:40.520
sent to every participant. This kind of software exists since 30 years, maybe 20. Here I just want

36
0:05:40.520 --> 0:05:53.320
to show you a page that I saw in the RFC 7667, which is about the RTP topology of

37
0:05:53.320 --> 0:06:06.640
the

38
0:06:06.640 --> 0:06:15.520
RTP stream, which come from the media server to each client and its server side that everything

39
0:06:15.520 --> 0:06:25.280
is decoded, mixed and sent back to client. The advantage of this approach was that it

40
0:06:25.280 --> 0:06:31.760
was very simple from the client side as the calling a conferencing server was almost the

41
0:06:31.760 --> 0:06:43.280
same as calling a regular user agent. The drawbacks of this approach was that video layout was

42
0:06:43.280 --> 0:06:51.040
defined server side, so you could have one or two different layouts. It requires a lot

43
0:06:51.040 --> 0:07:00.320
of CPU resources server side as every video stream has to be decoded and then re-encoded.

44
0:07:00.320 --> 0:07:11.600
The end-to-end encryption was not possible due to the fact that video was decoded. Now

45
0:07:11.600 --> 0:07:19.840
if we go to the selecting forwarding units, the idea is that the media server is no more

46
0:07:19.840 --> 0:07:28.200
decoding and then encoding every video stream, but is more switching video coming from every

47
0:07:28.200 --> 0:07:39.040
device to every other device. It could be done depending on several policies like active

48
0:07:39.040 --> 0:07:45.720
speaker or mosaic. For that, we also need some information coming from each client like

49
0:07:45.720 --> 0:07:52.400
the volume of the audio stream in order to be able to know who is talking without having

50
0:07:52.400 --> 0:08:02.360
to decode the audio stream as well. If I go to the same schema, still from the

51
0:08:02.360 --> 0:08:13.680
RFC 7667, now you can see that from the RTP standpoint, you still have one RTP stream

52
0:08:13.680 --> 0:08:24.920
for each client going to the media server, but now you have also one incoming video stream

53
0:08:24.920 --> 0:08:32.800
per participant of the conference. If we follow the audio, the video stream from the client

54
0:08:32.800 --> 0:08:45.400
A, you can see that it is copied to client B, but as well to client F. It's no more a

55
0:08:45.400 --> 0:08:57.200
media mixer, but a switching matrix more or less. What are the advantages of this architecture

56
0:08:57.200 --> 0:09:06.240
is that video layout is no more defined server side, but the client can decide where to display

57
0:09:06.240 --> 0:09:14.640
every participant of the conference. It's an application choice, no more a server choice.

58
0:09:14.640 --> 0:09:22.480
It scales very well as there are no resources which are used server side to decode or encode

59
0:09:22.480 --> 0:09:28.720
the video stream. Finally, it's opened the door for end-to-end encryption as the media

60
0:09:28.720 --> 0:09:35.360
server no more has to know the content of video stream.

61
0:09:35.360 --> 0:09:43.600
The drawback of this approach is that it requires the client to be able to manage the stream,

62
0:09:43.600 --> 0:09:56.960
which was not the case for standard one-to-one calls. For the CPU's agent, what we had to

63
0:09:56.960 --> 0:10:05.320
change are the following mainly. It's mainly about multi-stream requirements. In the past,

64
0:10:05.320 --> 0:10:15.880
the C client was able to send one audio stream plus one video stream. Now, it requires the

65
0:10:15.880 --> 0:10:26.200
client to be able to send one, but most of the time two video streams, one for high resolution

66
0:10:26.200 --> 0:10:33.880
video and another second one for thumbnail, as well as being able to receive one video

67
0:10:33.880 --> 0:10:41.880
stream per participant of the video conference.

68
0:10:41.880 --> 0:10:53.920
Just an example of the SDP to show what is involved. So, bundle mode, as I said, which

69
0:10:53.920 --> 0:11:03.320
is required, RTP mux as well, is to limit the number of sockets used for the media.

70
0:11:03.320 --> 0:11:10.080
This extension is related to audio level in order to be able to display who is talking

71
0:11:10.080 --> 0:11:17.960
and also for the server to be able to select which video stream is talking. It still uses

72
0:11:17.960 --> 0:11:29.840
IC to be able to limit the usage of media release. From the video part, what you can

73
0:11:29.840 --> 0:11:38.920
see is that there are two video streams in the receiver only, one for the high resolution

74
0:11:38.920 --> 0:11:45.080
of the camera and another for the thumbnail. So, it means that we encode two times the

75
0:11:45.080 --> 0:11:55.240
video. It could be replaced by some video encoder like H.264 AVC, which supports the

76
0:11:55.240 --> 0:12:05.840
multi-layer functionality. But if you want to be able to do that with a simple VP8, it's

77
0:12:05.840 --> 0:12:09.480
better to encode two times the video.

78
0:12:09.480 --> 0:12:17.560
And for the receiving side, there is one video stream because in this example, there is only

79
0:12:17.560 --> 0:12:33.680
one participant in the video conference. But this part would be multiplied by the number

80
0:12:33.680 --> 0:12:37.760
of participants of the conference.

81
0:12:37.760 --> 0:12:50.280
Okay, so this is for what we have done on the LinFone project for this feature. It could

82
0:12:50.280 --> 0:12:58.240
be tested with the Flexi SIPsurf, SIPsurf, which is currently running on our infrastructure.

83
0:12:58.240 --> 0:13:07.320
So you can create a video conference thanks to this Conference Factory CPU-RI. And using

84
0:13:07.320 --> 0:13:15.800
LinFone client with version above 5.0, it's possible to join a video conference. Okay,

85
0:13:15.800 --> 0:13:18.800
thank you.

86
0:13:18.800 --> 0:13:20.800
Conclusion.

87
0:13:20.800 --> 0:13:30.960
Okay, so now LinFone is capable of joining video conference into modes, modes like an

88
0:13:30.960 --> 0:13:40.160
active speaker using a selective forwarding unit, which allows us to scale. Possible evolution

89
0:13:40.160 --> 0:13:45.800
that we have in mind is to implement the XCON Conferencing Server in order to be able to

90
0:13:45.800 --> 0:13:51.280
manage conference from a website or to have something more advanced. We are also thinking

91
0:13:51.280 --> 0:13:56.920
about adding end-to-end encryption to this video conferencing server and why not to provide

92
0:13:56.920 --> 0:14:03.880
a compatibility with WebRTC, knowing that the media protocol that we use are very close

93
0:14:03.880 --> 0:14:07.800
than WebRTC.

94
0:14:07.800 --> 0:14:12.720
Useful link if you want to have more information about this work, you can go to the LinFone

95
0:14:12.720 --> 0:14:22.560
website and to have a look on our GitHub. That's it if you have a question. Thank you.

96
0:14:22.560 --> 0:14:41.160
Are you aware of any other SIP client that implements multi-party video with SoundCloud?

97
0:14:41.160 --> 0:14:50.080
Not yet, because the work to move from a regular SIP phone with only supporting one audio stream

98
0:14:50.080 --> 0:15:00.440
and one video stream to support this multi-stream is very significant and I'm not aware of any

99
0:15:00.440 --> 0:15:07.400
work in progress so far. So if you want to use it, you have to go with LinFone, even

100
0:15:07.400 --> 0:15:14.160
if it's fully standardized, if we are following SoundCloud.

101
0:15:14.160 --> 0:15:23.680
Thank you.

102
0:15:23.680 --> 0:15:29.600
Not yet, but we are quite confident that it's going to scale as we have removed all the

103
0:15:29.600 --> 0:15:38.000
needs for audio or video encoding server-sign. So it's really about switching of packets.

104
0:15:38.000 --> 0:15:43.040
Maybe the question might be around the network on the client side.

105
0:15:43.040 --> 0:15:53.880
One network on the client side, we are sending two resolutions from the client, high resolution

106
0:15:53.880 --> 0:16:01.120
and low resolution. In the case of active speaker, we only send back to every client

107
0:16:01.120 --> 0:16:08.520
the high resolution of the people who is currently talking and low resolution of every other

108
0:16:08.520 --> 0:16:14.120
participant. So it highly limited the needs of bandwidth.

109
0:16:14.120 --> 0:16:21.600
On the client side, you now decode more than one stream?

110
0:16:21.600 --> 0:16:22.600
Correct.

111
0:16:22.600 --> 0:16:26.840
Do the codes support this?

112
0:16:26.840 --> 0:16:34.440
It's almost the same answer. We decode one high resolution and many low resolution and

113
0:16:34.440 --> 0:16:42.560
the CPU resources depend on the resolution of the video that you have to decode.

114
0:16:42.560 --> 0:16:50.240
One quick question about the SDP that you showed before. So there were two receive-only

115
0:16:50.240 --> 0:16:54.560
streams for the client. Was that from the client perspective?

116
0:16:54.560 --> 0:16:55.560
It was from the server.

117
0:16:55.560 --> 0:16:56.560
Okay, yeah, because it looks like the client.

118
0:16:56.560 --> 0:17:04.720
My bad. So the server received two videos from the

119
0:17:04.720 --> 0:17:11.560
client, one in high resolution and one in low resolution and sent one video to this

120
0:17:11.560 --> 0:17:15.160
client as there is only one participant in this conference.

121
0:17:15.160 --> 0:17:19.960
From the client perspective, when you switch from big resolution to low resolution, you

122
0:17:19.960 --> 0:17:25.560
still use the same M line that you have on the board for two send to the client.

123
0:17:25.560 --> 0:17:27.560
Yes, exactly.

124
0:17:27.560 --> 0:17:56.560
Okay, thank you very much.

