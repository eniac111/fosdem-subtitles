WEBVTT

00:00.000 --> 00:11.040
So our next talk is about 4K HDR video with AV1.

00:11.040 --> 00:13.040
Please welcome Vibhuti.

00:13.040 --> 00:15.040
Yeah, thanks.

00:15.040 --> 00:18.880
Hi everybody.

00:18.880 --> 00:21.040
So this is my first post-dub talk.

00:21.040 --> 00:26.640
So the main idea for this talk is that I just want to have a brief introduction about 4K

00:26.640 --> 00:32.320
HDR with AV1 and to know how it actually works.

00:32.320 --> 00:34.000
That's the main idea for the talk.

00:34.000 --> 00:37.960
And to give a brief introduction about myself, I am currently a PhD student.

00:37.960 --> 00:42.600
My second year, currently working on video coding, trying to learn how video works.

00:42.600 --> 00:46.160
And I'm also involved in open source multimedia since 2018.

00:46.160 --> 00:50.000
Start with video and zip and AOM media.

00:50.000 --> 00:51.880
So what are we going to do today?

00:51.880 --> 00:56.280
Okay, this picture was captured when we were trying to play multiple streams at the same

00:56.280 --> 00:58.920
time for K AV1 HDR stream.

00:58.920 --> 01:02.680
So this was some snapshot of that.

01:02.680 --> 01:06.960
So main idea is that we want to talk about the main technical challenges when we want

01:06.960 --> 01:13.120
to play back AV1 streams, which is in HDR and to retain the same colors as such.

01:13.120 --> 01:15.880
That's the main idea for the talk.

01:15.880 --> 01:22.560
So before I get into that, I just want to have an introduction about HDR.

01:22.560 --> 01:28.480
We all heard about that and there are multiple aspects of HDR right now, which is happening.

01:28.480 --> 01:31.920
So first aspect of HDR is let's break down into multiple parts.

01:31.920 --> 01:34.480
So first aspect is that it has brighter pixels.

01:34.480 --> 01:40.560
So here is a snapshot of an image, which is tonemap down to BT7094 display.

01:40.560 --> 01:42.920
So this is in SDR, even though the picture was in SDR.

01:42.920 --> 01:47.480
So what happens here is that you could see, I don't know if I can see the cursor.

01:47.480 --> 01:53.000
So somewhere here it's like super dark at like one nits and somewhere here it is super

01:53.000 --> 01:54.520
bright at 1000 nits.

01:54.520 --> 01:58.880
So in theory, the picture is at like 4000 nits, but our display is only capable of showing

01:58.880 --> 02:00.320
at 1000 nits.

02:00.320 --> 02:02.480
So that's captured at 1000.

02:02.480 --> 02:07.880
And on comparison for a normal standard dynamic range images, it's usually in 100 to 200

02:07.880 --> 02:12.880
nits and flat panels, more than flat panels go like 500 nits or something like that.

02:12.880 --> 02:18.600
But in theory, HDR can go up to 10,000 nits, but most of the displays can't do that.

02:18.600 --> 02:22.360
So second aspect of HDR is that it has more bits.

02:22.360 --> 02:30.200
So in the, so this is a representation of same brightness in 8 bits, 8 to n SDR and

02:30.200 --> 02:31.280
in HDR.

02:31.280 --> 02:36.840
So you could see that there is a lot of quantization happening when it is in SDR.

02:36.840 --> 02:41.680
So if you want to show a code something like 200 or 200 nits, you could get away with something

02:41.680 --> 02:42.960
like 8 bits.

02:42.960 --> 02:50.840
And when you want to go to something like 1000 nits, 8 bits is not, it's not, not to

02:50.840 --> 02:53.960
say, it's not, we need more than 8 bits.

02:53.960 --> 02:56.240
So that's one aspect of HDR.

02:56.240 --> 02:58.260
So that means we need more bits.

02:58.260 --> 03:04.020
So what happened is that we need a mechanism to combine both nits and bits.

03:04.020 --> 03:06.760
So that's how this transfer function was born.

03:06.760 --> 03:12.160
So what happens is that our human eyes is quite sensitive to dark and mid gray areas

03:12.160 --> 03:14.680
and we are okay with bright sports.

03:14.680 --> 03:21.800
So with keeping this in mind, different standard bodies and organization tried to create something

03:21.800 --> 03:26.440
called transfer function where they tried to spend more amount of bits in the bright

03:26.440 --> 03:33.400
and low mid gray areas and spend a lot of spend, have a coarser quantization for bright,

03:33.400 --> 03:34.480
bright areas.

03:34.480 --> 03:38.760
So that's how this transfer function is born.

03:38.760 --> 03:44.680
And in HDR, one of the common thing is perceptual quantization PQ, which is based upon human

03:44.680 --> 03:46.060
perception of banding.

03:46.060 --> 03:47.440
So I'm not getting into details.

03:47.440 --> 03:52.040
So main idea of HDR transfer function is this and there is multiple transfer function, but

03:52.040 --> 03:58.400
the core idea is that spend more bits on darker and mid gray areas, right?

03:58.400 --> 03:59.400
Then comes the color.

03:59.400 --> 04:00.400
Color is complex.

04:00.400 --> 04:01.760
I don't know what the color is.

04:01.760 --> 04:07.040
So how to say is that to keep it simple and short.

04:07.040 --> 04:11.480
So display technologies since standardization of HDR, which was like back in nineties or

04:11.480 --> 04:14.400
early 2000, has evolved quite a lot.

04:14.400 --> 04:20.000
And now the display technologies can, is capable of displaying more brightness and more colors

04:20.000 --> 04:21.120
and things like that.

04:21.120 --> 04:24.520
So if you see this diagram here, right?

04:24.520 --> 04:30.480
So to keep it simple, what happens is that this shaded region is the standard dynamic

04:30.480 --> 04:32.280
range and SDR range.

04:32.280 --> 04:38.520
So that is this SDR and what happens is that in HDR domain or not in HDR, the white color

04:38.520 --> 04:42.200
gamut domain, it is expanded to be something like this.

04:42.200 --> 04:47.400
So it can have more wider colors to have a quick background.

04:47.400 --> 04:51.760
So you remember the picture, which we were talking earlier about a picture, which we

04:51.760 --> 04:52.760
are talking.

04:52.760 --> 04:56.320
So in this picture, when we actually check the color distribution of how it is actually

04:56.320 --> 04:57.520
spread out.

04:57.520 --> 05:02.880
So you could see this was manually measured with the help of a color library.

05:02.880 --> 05:08.200
So you could see that the trees and the green areas in the picture is actually beyond 709.

05:08.200 --> 05:11.880
So in this picture, you could see that this red triangle is the standard dynamic range

05:11.880 --> 05:17.000
in SDR and the green one is the BD2020.

05:17.000 --> 05:21.040
You could see that the greens is in the wider region.

05:21.040 --> 05:25.720
So the main idea to keep it simple is that there is reds and green are wider range and

05:25.720 --> 05:27.640
blues do not change much.

05:27.640 --> 05:32.560
So that is why most of the HDR pictures and videos which you see might have vibrant colors

05:32.560 --> 05:33.800
with reds and green.

05:33.800 --> 05:37.520
That is one of the main reason for that.

05:37.520 --> 05:40.960
So now comes the question, where do you find this HDR sequences?

05:40.960 --> 05:43.840
Again these are tone map sequences just for representation.

05:43.840 --> 05:48.520
So in practice or in reality, this may not be how it looks.

05:48.520 --> 05:49.920
So there are a bunch of sequences.

05:49.920 --> 05:54.040
Netflix has raised some of the sequences as open content, which is available in public

05:54.040 --> 05:58.520
and some other broadcasters have also published some of them, which is pretty good.

05:58.520 --> 06:04.480
And lately, maybe like early last year, Academy and Academy Software Foundation and partnered

06:04.480 --> 06:09.560
with American Cinematographers also produced proper cinema grading material with different

06:09.560 --> 06:11.600
color spaces, which is also available.

06:11.600 --> 06:14.480
I haven't tried tested that I found that like last week.

06:14.480 --> 06:15.480
Right.

06:15.480 --> 06:18.760
Kalayonana, let's come back to AV1.

06:18.760 --> 06:21.200
So I had given a brief idea about HDR.

06:21.200 --> 06:26.800
Now talking about AV1, JB has given a brief idea about the current storyline of AV1 decoding

06:26.800 --> 06:28.200
to have a quick refresher.

06:28.200 --> 06:31.800
So for people who didn't know, so AV1 is a current video coding standard developed

06:31.800 --> 06:34.560
from Alliance for Open Media around 2018.

06:34.560 --> 06:39.760
And it claims to be around 30 to 50% more efficient over predecessors.

06:39.760 --> 06:41.680
And that's around 200 K line.

06:41.680 --> 06:44.520
So it was an old number which I wrote.

06:44.520 --> 06:48.840
So for the video decoding and playback, David was there.

06:48.840 --> 06:52.280
So David is quite popular from video and released that.

06:52.280 --> 06:56.200
And even major browsers and also operating systems supports that, including Apple.

06:56.200 --> 06:58.520
Yeah, I don't know how to use AV1 in Apple.

06:58.520 --> 07:03.920
No, I don't know if anyone was able to actually use AV1 in Apple, even though Apple ships

07:03.920 --> 07:05.640
that.

07:05.640 --> 07:07.400
So that's the storyline.

07:07.400 --> 07:08.400
So that sounds good.

07:08.400 --> 07:11.200
So what's the actual problem when it comes to HDR and AV1?

07:11.200 --> 07:14.320
We see that it's able to play back and things like that.

07:14.320 --> 07:20.960
So the problem is playing back signals, playing back HDR videos with correct metadata and

07:20.960 --> 07:25.320
signaling them correctly on a display is not actually easy.

07:25.320 --> 07:30.440
So if you break down into three first parties like the macOS, we know that the display

07:30.440 --> 07:33.080
support is there, latest macOS has the support.

07:33.080 --> 07:34.960
The OS is having support for signaling that.

07:34.960 --> 07:35.960
That's good.

07:35.960 --> 07:42.080
But the problem is video output drivers and playback in natively macOS is a bit problematic.

07:42.080 --> 07:47.400
And most of the players try to do tone mapping and others try to do, others have support,

07:47.400 --> 07:48.400
but they are very limited.

07:48.400 --> 07:51.960
So you are not be able to actually visualize them as you want.

07:51.960 --> 07:58.160
And in Linux, I believe we can't do because protocols are still work in progress.

07:58.160 --> 08:03.160
And in Windows, if we see that there is display and voice level support and also players do

08:03.160 --> 08:05.460
support that with the help of direct drivers.

08:05.460 --> 08:09.080
So we could drive HDR playback in Windows and it works fine.

08:09.080 --> 08:13.460
But the problem I noticed mainly was like the display transition is there when you play

08:13.460 --> 08:18.320
an HDR window, windows try to flip from SDR to HDR that takes a few seconds and sometimes

08:18.320 --> 08:19.960
some black screen.

08:19.960 --> 08:23.740
So that's a problem which I noticed when you try to do in Windows.

08:23.740 --> 08:28.440
So that's a problematic thing which I noticed.

08:28.440 --> 08:31.080
So what we did is that we took a different approach.

08:31.080 --> 08:36.960
So this is kind of not too many people, probably most of the people in this room.

08:36.960 --> 08:42.200
But we took a, this is the most common approach used in broadcast and standardization body

08:42.200 --> 08:43.200
industry.

08:43.200 --> 08:47.580
So that we are just using playback capture, playback cards to play the video streams.

08:47.580 --> 08:49.680
And we are using a black cards from the Blackmagic.

08:49.680 --> 08:51.680
We use something called Decklink 8K Pro.

08:51.680 --> 08:58.080
So it can play the streams with and it can send these signals with the help of SDI as

08:58.080 --> 09:02.880
output and to play them, we are using FFmpeg and G Streamer for driving the playback.

09:02.880 --> 09:09.560
So we can work in any operating system if there is SDK support for that.

09:09.560 --> 09:10.680
So that's good.

09:10.680 --> 09:12.560
So we found a solution for playback.

09:12.560 --> 09:16.860
Now comes to the question like how do you display that?

09:16.860 --> 09:23.380
So first thing is that we need to display this HDR signal with little to no changes.

09:23.380 --> 09:27.360
So with the playback card we can send the signal, that's good.

09:27.360 --> 09:31.960
Then we need to make sure that the TV is not doing any sort of funky things when doing

09:31.960 --> 09:36.600
the playback because most of the TV sometimes tend to do some sort of tone mapping or try

09:36.600 --> 09:40.600
to play with the brightness and things based upon the ambient display.

09:40.600 --> 09:44.280
So we need to make sure that there is no sort of tone mapping and TV is not playing with

09:44.280 --> 09:47.540
that and it should be calibrated as per standard.

09:47.540 --> 09:51.380
So that's the other thing and it should have minimum of 1000 its brightness.

09:51.380 --> 09:55.600
So that's the main requirement which we have for HDR testing, right?

09:55.600 --> 09:58.380
So what we went is that we went to the reference monitor.

09:58.380 --> 10:02.240
So we use something called Sony's reference monitor.

10:02.240 --> 10:07.840
So that's a 32 inch OLED panel as strictly calibrated as per the standard and it ticks

10:07.840 --> 10:12.400
all the boxes which we want and even we can force the signal metadata on the playback.

10:12.400 --> 10:18.160
So that's good even though that's an expensive guy.

10:18.160 --> 10:21.920
So the main idea with the reference monitor is that once we establish playback with HDR

10:21.920 --> 10:25.440
we know that this is the source of truth or ground truth which we have.

10:25.440 --> 10:29.840
Once we do that we can now show that this is how it actually look then we can extend

10:29.840 --> 10:36.480
to normal consumer displays which says to have HDR support, right?

10:36.480 --> 10:38.560
So that's good.

10:38.560 --> 10:45.480
Now how do we say that the signal and video which we play is actually HDR or how do we

10:45.480 --> 10:47.280
say that that's the actual thing, right?

10:47.280 --> 10:49.480
So we need to confirm that thing.

10:49.480 --> 10:54.440
So first thing is that we need to confirm the brightness part and so confirming the

10:54.440 --> 11:00.000
brightness part we are using the European broadcasters has released a chart called EOTF

11:00.000 --> 11:01.000
chart.

11:01.000 --> 11:03.040
So it is going from 0 to 1000 nits.

11:03.040 --> 11:04.880
You won't be able to see that here anyway.

11:04.880 --> 11:08.960
So the idea here is that so with this chart if you play back in your stream you could

11:08.960 --> 11:12.760
see the peak brightness which is available in your display.

11:12.760 --> 11:18.460
So that's the first part which for confirmation for the streams.

11:18.460 --> 11:22.800
The second part which we need to test is that what's the viewing area which you were able

11:22.800 --> 11:23.800
to see that.

11:23.800 --> 11:27.440
So when you're playing in a consumer TVs some TV says that they may have something like

11:27.440 --> 11:29.080
4500 nits.

11:29.080 --> 11:35.760
So in theory that 4500 nits, I mean in practice that 4500 nits might be only for like 2 percent

11:35.760 --> 11:39.880
area for few minutes or few seconds of your whole screen and it goes back to something

11:39.880 --> 11:43.240
like 2000 nits or 1500 nits on other times.

11:43.240 --> 11:47.360
So we need to be sure that what's the actual area which you are able to show that.

11:47.360 --> 11:50.200
So that's one thing we can use this.

11:50.200 --> 11:54.360
So European protocols has again released bunch of test patterns which you can confirm the

11:54.360 --> 11:57.780
maximum viewing area.

11:57.780 --> 12:02.320
So next comes that to confirm the signal which you are sending from the screen is okay or

12:02.320 --> 12:03.320
not.

12:03.320 --> 12:08.800
So for that we are just using a cross converter monitor which can it's just check the existence

12:08.800 --> 12:09.800
of the signal.

12:09.800 --> 12:14.400
You pass through the signal through this and it just says what's the signal which you are

12:14.400 --> 12:15.720
sending is okay or not.

12:15.720 --> 12:19.860
This is just to make sure that the cables which you send and the is correct or not because

12:19.860 --> 12:21.840
sometimes cable can mess with you.

12:21.840 --> 12:24.720
So that's one thing which you can try.

12:24.720 --> 12:25.720
Then comes the color.

12:25.720 --> 12:29.560
This is the most important because it can play lot of tricks to you and you don't know

12:29.560 --> 12:32.000
how what's this ground truth.

12:32.000 --> 12:36.640
So for that in the reference if you had the privilege of reference monitor then the reference

12:36.640 --> 12:39.780
monitor has such settings where called gamma marker.

12:39.780 --> 12:44.960
So it's actually essentially turns this some as like something like a zebra line on top

12:44.960 --> 12:48.940
of the screen if it is above 709 that's the main idea.

12:48.940 --> 12:52.480
But we may not be having reference monitor all the time right.

12:52.480 --> 12:55.640
So we need some other mechanism to validate this thing.

12:55.640 --> 13:00.020
So we had to use some we tried to use something called spectroradiometer.

13:00.020 --> 13:04.760
So this is a device which is used to measure something called radiance from the screen.

13:04.760 --> 13:10.080
So with this device we can measure the color volume that is the color space and also the

13:10.080 --> 13:11.220
brightness.

13:11.220 --> 13:15.400
So with this device you can just point to the screen which we are playing the screen

13:15.400 --> 13:17.480
and we know that this area might be in HDR.

13:17.480 --> 13:21.080
So once we point to the screen and measure the color you can know that whether this is

13:21.080 --> 13:27.020
in HDR space and what's the is the pixel above 709 or not and what's the actual brightness

13:27.020 --> 13:29.340
which we are seeing and things like that.

13:29.340 --> 13:32.520
Based upon the brightness it varies.

13:32.520 --> 13:38.520
Brightness in the varies in the sense like the time it takes to capture that it varies.

13:38.520 --> 13:43.760
So next important part is that the making sure the whole pipeline which you are using

13:43.760 --> 13:45.120
is in 10 bit.

13:45.120 --> 13:51.520
So this is a very important thing which might be bit tricky to see in real life because

13:51.520 --> 13:53.840
most of the to give a background right.

13:53.840 --> 14:01.560
So if you have a so to make this easier so the main idea here is that we make custom

14:01.560 --> 14:07.680
code a 10 bit image having 1024 levels of the brightness.

14:07.680 --> 14:12.960
So once you send that and if the if the whole pipeline is in 10 bit you won't be seeing

14:12.960 --> 14:19.800
band and you will be having a smooth gradient and if it is not in 10 bit you may see some

14:19.800 --> 14:20.800
ramps here like this.

14:20.800 --> 14:23.980
It's not visible here so essentially you will see some ramps here.

14:23.980 --> 14:29.200
So when in practice when you send a signal and if you try to do in consumer displays

14:29.200 --> 14:33.000
even if you may even get the color volume and brightness and things like that but it

14:33.000 --> 14:37.200
can be still in 8 bits that what I am talking is like it can be an 8 bit PQ.

14:37.200 --> 14:38.840
It's a real thing.

14:38.840 --> 14:41.680
I didn't know that when I before starting this.

14:41.680 --> 14:46.760
So with this you can be sure the reason for saying this is that if you have some noise

14:46.760 --> 14:52.560
in your signal and due to TVs debanding and dithering algorithm and all the other things

14:52.560 --> 14:58.720
which I don't know what TV is doing it can make the bands not visible and it can be smooth

14:58.720 --> 15:04.280
as this 10 bit even though the video might be in 8 bits.

15:04.280 --> 15:08.080
So that's the main things which we focus to say that for testing the HDR things we

15:08.080 --> 15:13.240
need to do at least one of them from for each of these things brightness, signal, color

15:13.240 --> 15:17.080
and bit depth.

15:17.080 --> 15:20.920
So next question comes up that can we extend this to consumer displays.

15:20.920 --> 15:25.280
Yes we can do that because we now establish the ground tooth and it works then we can

15:25.280 --> 15:30.600
use an SDI to HDMI converter then we can send the HD we can play it in a normal consumer

15:30.600 --> 15:36.080
displays but that comes to the question that whether the consumer TVs can actually signal

15:36.080 --> 15:41.320
the metadata or whether this playback card or the FMV drivers which how you play can

15:41.320 --> 15:43.340
actually transport the metadata.

15:43.340 --> 15:49.760
So in the TV modern consumer TVs can force the HDM metadata that's okay and if you don't

15:49.760 --> 15:53.960
have that there is some device which we found recently but it's there in industry for many

15:53.960 --> 15:56.160
years that's called doctor HDMI.

15:56.160 --> 16:02.040
So this guy is like you plug in HDMI and it will insert the HDM metadata how we want with

16:02.040 --> 16:09.880
a web server and it's like it's a magic device I would say and fun fact it can even do Dolby

16:09.880 --> 16:15.520
vision with 8K 60 FPS it can even fake Dolby vision metadata to the TV so that TV will

16:15.520 --> 16:20.000
be happy and this device is like less than 200 euro.

16:20.000 --> 16:25.680
So it actually works for consumer TVs and old TVs we just have HDR this guy is magic

16:25.680 --> 16:30.960
and most of the time many people who do HDR demos have this in the background in NAB or

16:30.960 --> 16:33.680
IBC.

16:33.680 --> 16:39.960
So that's one part now comes to the question that is this okay for HDR so with the HDR

16:39.960 --> 16:44.280
right the viewing environment is crazy so that's one thing and based upon the viewing

16:44.280 --> 16:49.000
environment you had different perception of colors so main things here is that you just

16:49.000 --> 16:52.680
need to be sure that what's the display panel technology which you are using like I mentioned

16:52.680 --> 16:58.040
previous test would be happy for that would be you can have some sort of confidence with

16:58.040 --> 17:02.640
that then comes the ambient lighting condition on how you view that if your room is getting

17:02.640 --> 17:08.080
dark and based upon your ambient lighting this varies so you need to be sure what how

17:08.080 --> 17:13.080
what's the lighting in your room when you are watching the HDR videos and again video

17:13.080 --> 17:17.400
materials plays an important role and lastly perception of color there different people

17:17.400 --> 17:22.880
are different tolerance of color so that's one thing and last important fact which I

17:22.880 --> 17:28.000
need to talk is that based upon viewing environment and certain individuals can experience different

17:28.000 --> 17:32.680
fatigue and dizziness when you watch HDR that's a true thing and because of the super bright

17:32.680 --> 17:37.680
and super vivid colors you need to be careful when you are watching HDR videos for a long

17:37.680 --> 17:43.740
time so ITU has laid out some methodology when you do this for scientific testing and

17:43.740 --> 17:49.240
subjective testing environment it's not strictly written for HDR but if you follow that it

17:49.240 --> 17:55.400
works on big picture just keep it under 30 minutes when you watch HDR videos continuously

17:55.400 --> 17:59.880
so this is some snapshot of how these set up scientific testing environment which we

17:59.880 --> 18:08.020
went and when we had a person to view that this is how it happened so yeah so the main

18:08.020 --> 18:14.480
things what I was talking here is that HDR signaling HDR metadata and things are different

18:14.480 --> 18:18.640
and one of the main two things of HDR is that there is wide range of brightness due to different

18:18.640 --> 18:23.120
quantization scheme and white color gamut this was an entirely different concept now

18:23.120 --> 18:27.360
the current HDR videos and things has clubbed together and that's these current standard

18:27.360 --> 18:33.240
says so that I enhance the colors and last thing is that setting up the whole set is

18:33.240 --> 18:38.440
subjective testing environment and things or scientific testing environment is non-trivial

18:38.440 --> 18:42.960
and it's accompanied by high cost even though this is standard back in like 10 years back

18:42.960 --> 18:49.680
the whole HDR and the viewing environment plays an important role currently we are doing

18:49.680 --> 18:56.280
some sort set of subjective test for HDR viewing and things like that so I have put some references

18:56.280 --> 19:03.360
if you like to read more and that's it thanks would be like to hear more questions and I'm

19:03.360 --> 19:06.800
still learning these things so I could be wrong please correct me if there is anything

19:06.800 --> 19:07.800
wrong.

19:07.800 --> 19:22.360
Any question in the room?

19:22.360 --> 19:26.760
And also I have put some additional resources how you can do these things some more information

19:26.760 --> 19:28.760
which I skipped in this thing.

19:28.760 --> 19:41.920
Yes, not sure which pipeline I would.

19:41.920 --> 19:50.320
Okay this one right so this one you're talking so what happens here so what here I'm trying

19:50.320 --> 19:56.320
to convey is that the HDR videos like I mentioned earlier is coded in 10 bits or more than 10

19:56.320 --> 20:02.520
bits so the whole pipeline which you are trying to so what have to boil down so the TVs or

20:02.520 --> 20:08.720
some devices can decimate 2 bits and make it 8 bit when you play that it can be so you

20:08.720 --> 20:14.400
may see in HDR in 8 bits so when you have it in 8 bits the whole nature of HDR in 8

20:14.400 --> 20:19.720
bits you can represent 1000 nits brightness so HDR having like quite high wide range of

20:19.720 --> 20:25.600
brightness and when you have less bits you can't actually view that so you just that's

20:25.600 --> 20:28.960
the one thing which I mentioned here.

20:28.960 --> 20:29.960
Yes.

20:29.960 --> 20:37.240
Because HDR is 10 bits right now but there are plans to make 12 bits in the future.

20:37.240 --> 20:38.240
Yes.

20:38.240 --> 20:44.240
So how are you going to extend your calibration and scientific testing equipment to code with

20:44.240 --> 20:45.240
2 bits?

20:45.240 --> 20:46.240
I don't know.

20:46.240 --> 20:47.240
Can you please?

20:47.240 --> 20:51.200
Yeah so sorry yeah so he was asking like in future HDR might become 12 bits so how am

20:51.200 --> 20:55.440
I going to extend this whole setup for 12 bits I think that's after like six years

20:55.440 --> 20:59.880
right yeah so probably the capture.

20:59.880 --> 21:02.880
It is still 10 bits.

21:02.880 --> 21:06.440
So already compliant with 12 bits.

21:06.440 --> 21:07.440
Yeah but in.

21:07.440 --> 21:08.440
It's also both the same capture.

21:08.440 --> 21:13.480
Yeah okay so probably I think the capture card can do 12 bits and probably I probably

21:13.480 --> 21:19.360
would do something similar I need to dig up how would can we actually differentiate between

21:19.360 --> 21:21.360
10 bit and 12 bits I actually don't know.

21:21.360 --> 21:23.600
Yeah it's difficult to differentiate with 10 bits already.

21:23.600 --> 21:24.600
Yeah 10 bits yeah.

21:24.600 --> 21:25.600
So probably it's going to be.

21:25.600 --> 21:28.880
Yeah it would be bit tricky but we'll find some way for sure.

21:28.880 --> 21:32.720
After this like if I do a waveform monitor like there are tools you'll look at the wire

21:32.720 --> 21:33.720
on the signal.

21:33.720 --> 21:34.720
Yeah.

21:34.720 --> 21:37.600
You see if you look at the brain I can see the actual YE values.

21:37.600 --> 21:38.600
Yes.

21:38.600 --> 21:40.600
Can you repeat what he says to the mic?

21:40.600 --> 21:43.880
Alright so he was suggesting we could use something like a waveform monitor to monitor

21:43.880 --> 21:49.400
the signal yeah so that is part of something like this I know more advanced device can have

21:49.400 --> 21:54.080
that so this thing is like a cheap device but even though it's one grant.

21:54.080 --> 21:59.280
So this can show some waveforms so yeah he was suggesting we can use more advanced tools

21:59.280 --> 22:00.280
to show waveforms.

22:00.280 --> 22:01.280
Yeah.

22:01.280 --> 22:09.280
Can you give us a method to repeat this setup in every other place so that you have a common

22:09.280 --> 22:10.280
reference system.

22:10.280 --> 22:11.280
Yeah so.

22:11.280 --> 22:12.400
You have exactly the same calibration.

22:12.400 --> 22:16.240
So that's a tricky thing so we know so we just so if you are repeating this for other

22:16.240 --> 22:20.360
setups we just need to make sure that these this for some of these things are like test

22:20.360 --> 22:24.200
patterns should be used to make sure the peak brightness is there in the maximum viewing

22:24.200 --> 22:30.160
area then we can use the spectroradiometer to make sure that the signal is in HDR and

22:30.160 --> 22:34.880
to so make sure that's okay and I think this is repeatable in other places even if you

22:34.880 --> 22:38.960
don't have reference monitor because we tried this on a normal consumer device with the

22:38.960 --> 22:40.960
same setup and it worked.

22:40.960 --> 22:41.960
Yes.

22:41.960 --> 22:44.960
Why is the view on less suitable for HDR?

22:44.960 --> 22:48.680
Is it only because of the compression?

22:48.680 --> 22:50.480
Yes I would say so and.

22:50.480 --> 22:55.920
So, sorry sorry again so he was suggesting why is AV1 better suited for HDR?

22:55.920 --> 23:01.640
So I don't know if AV1 is best suited for HDR also it's one of the good codec which

23:01.640 --> 23:08.520
can do better compression so natively it can do lot of contents which is in a better quantization

23:08.520 --> 23:13.560
and the things are which bit better compared to other codecs compared to predecessor codecs

23:13.560 --> 23:20.080
and so due to that nature it's okay to do HDR content but still there is lot of work

23:20.080 --> 23:26.360
to do in AV1 to be better compile and with HDR because other codecs have way better handling

23:26.360 --> 23:31.400
for HDR because most of the codecs right now even though they have HDR they don't treat

23:31.400 --> 23:33.600
the HDR signal or HDR videos differently.

23:33.600 --> 23:37.840
So, it's still a research and development process currently which is happening in codec

23:37.840 --> 23:41.880
development to treat HDR differently and AV1 is slowly trying to do that.

23:41.880 --> 23:43.280
What are these?

23:43.280 --> 23:48.840
So codecs in the sense like HEVC has some other extensions which can handle HDR in a

23:48.840 --> 23:53.120
different way with help of having different quantization for colors and things like that

23:53.120 --> 23:59.000
AV1 is slowly add so current AV1 master has that feature I believe so it's slowly in development

23:59.000 --> 24:01.840
research and development process all right.

24:01.840 --> 24:05.800
We still have time for one or two questions.

24:05.800 --> 24:11.200
Is there a big difference in power consumption when a TV goes into HDR modes?

24:11.200 --> 24:16.160
Yes, so the question he was asking is that whether is the power consumption is higher

24:16.160 --> 24:21.200
in HDR or not I actually didn't measure but I know that when I play HDR and if I keep

24:21.200 --> 24:24.280
my hand on top of the screen I can feel the heat.

24:24.280 --> 24:30.960
So, the OLED is really burning so if I keep like this distance I can feel the heat in

24:30.960 --> 24:33.960
my hand so it's really bad I would say.

24:33.960 --> 24:48.000
Yes, so he was asking why is it.

24:48.000 --> 24:49.960
Why is dark here?

24:49.960 --> 24:51.360
Why is it dark more darker?

24:51.360 --> 24:57.000
So to answer that question I was expecting this I have some.

24:57.000 --> 25:01.640
So I have us doing a subjective test for the same thing for a dark video with bunch of

25:01.640 --> 25:05.720
people I didn't include that this was this request more background and explanation to

25:05.720 --> 25:12.760
explain so took so this was a dark image so this is in HDR so in actual setup and proper

25:12.760 --> 25:17.080
environment you could see that clearly but now this is purely washed out in blacks.

25:17.080 --> 25:25.120
So because on HDR it can do why is it like that so I don't know because in HDR you can

25:25.120 --> 25:26.840
spend more bits right.

25:26.840 --> 25:34.000
So people tend to focus the importance of the dark videos and things like that so I

25:34.000 --> 25:39.040
think it's just that you can exploit the wide range of luminance and brightness and

25:39.040 --> 25:42.800
quantization scheme of HDR when it is in dark.

25:42.800 --> 25:50.760
So what I'm here trying to show is that this is bit complex to explain so like like the

25:50.760 --> 25:56.560
perception of this is very hard and if you have a different lighting condition the blacks

25:56.560 --> 26:02.080
and darks would be entirely different so I think it's just the nature of the content

26:02.080 --> 26:06.520
and they just want to utilize the HDR to be that I'm not exactly sure how to answer that

26:06.520 --> 26:08.800
I don't know.

26:08.800 --> 26:13.400
So I just like this what I'm here trying to show is that even though you had different

26:13.400 --> 26:18.480
videos with different brightness and all this is like a dark video and when people try to

26:18.480 --> 26:23.160
view this in this environment and when you have the worst compression and the best compression

26:23.160 --> 26:28.760
people like these are different kind of noises people just thought that they are same even

26:28.760 --> 26:33.040
in the proper environment so that's a whole different storyline so it's a bit hard to

26:33.040 --> 26:38.080
answer why is it is like that and how we can do that probably in another time I could explain

26:38.080 --> 26:54.520
this in a clear way but I don't know but yeah thanks.
