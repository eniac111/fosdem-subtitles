1
0:00:00.000 --> 0:00:07.000
All right.

2
0:00:07.000 --> 0:00:28.640
Yes, go ahead.

3
0:00:28.640 --> 0:00:29.960
We continue with our next speaker,

4
0:00:29.960 --> 0:00:32.280
which is kind of the follow up of the previous one,

5
0:00:32.280 --> 0:00:34.040
because it's approximately the same topic,

6
0:00:34.040 --> 0:00:36.360
but this time about risk five and arm.

7
0:00:36.360 --> 0:00:37.960
So please welcome Renny.

8
0:00:37.960 --> 0:00:42.840
Renny.

9
0:00:42.840 --> 0:00:45.320
I heard good afternoon, everyone.

10
0:00:45.320 --> 0:00:47.480
I hope you are done with the digestion.

11
0:00:47.480 --> 0:00:51.280
So yes, this pretty much follows up

12
0:00:51.280 --> 0:00:54.320
on Keren's previous speech.

13
0:00:54.320 --> 0:00:57.640
But before I go into the details,

14
0:00:57.640 --> 0:00:59.920
obviously, I work for big companies,

15
0:00:59.920 --> 0:01:01.400
so I have to put this disclaimer.

16
0:01:01.400 --> 0:01:05.000
And then if I speak too fast or if I don't articulate properly,

17
0:01:05.000 --> 0:01:08.400
please stop me.

18
0:01:08.400 --> 0:01:10.120
With that said, who am I?

19
0:01:10.120 --> 0:01:11.360
I don't think it matters much.

20
0:01:11.360 --> 0:01:05.880
Well, this is my 16th time in F

21
0:01:12.880 --> 0:01:16.640
and it's only my first presentation, so bear with me.

22
0:01:16.640 --> 0:01:18.520
Having said that, I don't work in this field at all,

23
0:01:18.520 --> 0:01:20.840
so just a fancy thing for me to do.

24
0:01:20.840 --> 0:01:23.000
So some history.

25
0:01:23.000 --> 0:01:25.400
So has anybody ever seen this?

26
0:01:25.400 --> 0:01:28.520
Outside the computer museum, right?

27
0:01:28.520 --> 0:01:29.560
Yeah, so that's the K1.

28
0:01:29.560 --> 0:01:33.160
It's the first vector processor.

29
0:01:33.160 --> 0:01:36.400
It's from the late or second half of the 70s.

30
0:01:36.400 --> 0:01:38.880
I wasn't even born back then.

31
0:01:38.880 --> 0:01:41.200
But I'm pointing it's the first vector processor

32
0:01:41.200 --> 0:01:43.760
and we all now finally, after almost 50 years,

33
0:01:43.760 --> 0:01:45.600
are coming back to this kind of,

34
0:01:45.600 --> 0:01:47.200
maybe coming back to this approach

35
0:01:47.200 --> 0:01:50.080
to calculations in computers.

36
0:01:50.080 --> 0:01:51.600
But for people in my generation,

37
0:01:51.600 --> 0:01:55.360
this is more what we associate with the SIMD

38
0:01:55.360 --> 0:01:56.480
for multimedia.

39
0:01:56.480 --> 0:02:00.400
So this is a pod, the first video game

40
0:02:00.400 --> 0:02:01.880
that I could use the MMX,

41
0:02:01.880 --> 0:02:06.400
which MMX being the first SIMD extensions

42
0:02:06.400 --> 0:02:09.440
in the consumer business, in the consumer space, let's say.

43
0:02:10.640 --> 0:02:14.360
So as I said, the MMX came in 1997

44
0:02:14.360 --> 0:02:16.000
and that was 64-bit vector,

45
0:02:16.000 --> 0:02:18.880
so you could compute over 64-bit at a time.

46
0:02:18.880 --> 0:02:21.680
Mind that back then, computers was pretty much only 32-bits.

47
0:02:22.720 --> 0:02:24.440
And two years later, I came SSE

48
0:02:24.440 --> 0:02:26.880
and many, many, many versions of SSE, SSE2,

49
0:02:26.880 --> 0:02:30.640
which is more popular in multimedia use cases, 2000.

50
0:02:30.640 --> 0:02:32.600
I'm not gonna go through all the details of SSE

51
0:02:32.600 --> 0:02:35.600
because there's like a billion, million versions.

52
0:02:35.600 --> 0:02:39.720
And AVX1 came in 2008, AVX2,

53
0:02:39.720 --> 0:02:41.720
which Kian mentioned came in 2011.

54
0:02:41.720 --> 0:02:46.720
That was the first to have 256-bits vectors.

55
0:02:48.360 --> 0:02:52.800
Then AVX512, which was the topic of the previous presentation,

56
0:02:52.800 --> 0:02:56.240
officially came in 2013, but as Kian mentioned,

57
0:02:56.240 --> 0:03:00.240
the only real proper CPUs were only out in 2017.

58
0:03:02.800 --> 0:03:06.320
On ARM side, the first SIMD was actually 32-bit

59
0:03:06.320 --> 0:03:09.040
only on ARM B6, 2002.

60
0:03:10.000 --> 0:03:11.640
That doesn't really seem to make sense,

61
0:03:11.640 --> 0:03:13.960
but that's because it's basically calculating

62
0:03:13.960 --> 0:03:16.880
as a four times eight-bit, so two times 16-bit at a time.

63
0:03:18.520 --> 0:03:22.760
Then 128-bits came, there was no 64-bit SIMD on ARM.

64
0:03:22.760 --> 0:03:26.400
128-bit came with ARM E7, so got XA8,

65
0:03:26.400 --> 0:03:31.400
in usually called NEON, officially called Advanced SIMD

66
0:03:31.480 --> 0:03:35.880
in 2005, and on ARM V8, it's pretty much the same.

67
0:03:35.880 --> 0:03:38.080
Now it's not actually compatible on like on X86,

68
0:03:38.080 --> 0:03:41.040
64, 64, the same, but came in,

69
0:03:41.040 --> 0:03:43.080
well, came with basically ARM V8 in 2012.

70
0:03:43.080 --> 0:03:46.040
It's also officially called Advanced SIMD,

71
0:03:46.040 --> 0:03:48.000
and it's also colloquially known as NEON.

72
0:03:49.480 --> 0:03:51.800
As for RACE-5, well, RACE-5 is much more recent.

73
0:03:51.800 --> 0:03:53.120
There is no SIMD.

74
0:03:54.320 --> 0:03:57.200
Problem, and I've only summarized,

75
0:03:57.200 --> 0:03:58.360
this is only a short summary,

76
0:03:58.360 --> 0:04:00.760
there's way more extension, especially on the 86 side,

77
0:04:00.760 --> 0:04:03.280
is that every damn time you have to rewrite your assembly,

78
0:04:03.280 --> 0:04:06.560
and as the question then answers in the previous talks

79
0:04:06.560 --> 0:04:08.920
and even the previous previous talk covered,

80
0:04:08.920 --> 0:04:10.920
this is kind of time-consuming.

81
0:04:10.920 --> 0:04:15.720
So with that said, this was all fixed size SIMD,

82
0:04:15.720 --> 0:04:19.240
so what about viable length SIMD,

83
0:04:19.240 --> 0:04:21.600
which is what we will be talking about today.

84
0:04:21.600 --> 0:04:24.120
So how would you go about doing it?

85
0:04:24.120 --> 0:04:27.480
Well, simple way to do it is to just ask the CPU

86
0:04:27.480 --> 0:04:29.680
what is your vector size,

87
0:04:29.680 --> 0:04:31.640
and if you do S5, this is how you do it.

88
0:04:31.640 --> 0:04:35.040
So control register rate operation,

89
0:04:35.040 --> 0:04:37.960
the vector is called VLANB for vector length in bytes,

90
0:04:37.960 --> 0:04:40.480
and it will store in this case, T0, whatever,

91
0:04:40.480 --> 0:04:44.360
it's one register, the number of bytes in a vector,

92
0:04:44.360 --> 0:04:45.960
and with that you could then iterate.

93
0:04:45.960 --> 0:04:47.880
So if you want to know the number of elements,

94
0:04:47.880 --> 0:04:50.320
well, you have to do a left shift to,

95
0:04:50.320 --> 0:04:53.320
you have to compute the number of elements,

96
0:04:53.320 --> 0:04:54.880
so if you want to have 32-bit elements,

97
0:04:54.880 --> 0:04:57.520
you divide by four, shift by two bits.

98
0:04:57.520 --> 0:04:59.880
You could do it like that,

99
0:04:59.880 --> 0:05:01.720
and then you would write your main,

100
0:05:01.720 --> 0:05:02.560
you would take your C loop,

101
0:05:02.560 --> 0:05:03.840
you would convert it into assembly,

102
0:05:03.840 --> 0:05:06.120
to operate on over many elements at a time,

103
0:05:06.120 --> 0:05:11.360
then you would probably, if you have space in your vector bank,

104
0:05:11.360 --> 0:05:13.440
you'd probably, in order to eliminate,

105
0:05:13.440 --> 0:05:15.440
try to heat up the latency a little bit,

106
0:05:15.440 --> 0:05:17.320
because usually between instructions,

107
0:05:17.320 --> 0:05:19.120
if you operate only on one data set,

108
0:05:19.120 --> 0:05:21.320
you will have inter-instruction latencies,

109
0:05:21.320 --> 0:05:22.920
which are going to hurt your performance,

110
0:05:22.920 --> 0:05:26.760
so you're typically in multimedia and all twice,

111
0:05:26.760 --> 0:05:29.600
so you will do work over two set of vectors

112
0:05:29.600 --> 0:05:31.320
at the same time in parallel,

113
0:05:31.320 --> 0:05:33.200
and when you have done all of that,

114
0:05:33.200 --> 0:05:35.720
you will be working on over many like say 32-bit,

115
0:05:35.720 --> 0:05:38.320
32, sorry, 32-item, 32 elements at a time,

116
0:05:38.320 --> 0:05:39.400
so you have to deal with edges,

117
0:05:39.400 --> 0:05:40.800
because you might not have a multiple

118
0:05:40.800 --> 0:05:43.000
of 32 elements that you are dealing with,

119
0:05:43.000 --> 0:05:45.600
and that's fine, and that's one way to do it.

120
0:05:45.600 --> 0:05:47.520
In fact, as I checked,

121
0:05:47.520 --> 0:05:50.600
that's how Clang LLVM does the tree vectorization on RISC-V,

122
0:05:50.600 --> 0:05:52.120
if you enable it, and so you have,

123
0:05:52.120 --> 0:05:54.360
it literally starts by reading the vector length,

124
0:05:54.360 --> 0:05:57.480
and then deal with edges and all twice,

125
0:05:57.480 --> 0:06:01.440
and if it manages to implement tree,

126
0:06:01.440 --> 0:06:03.000
I mean, if you have enabled tree vectorization,

127
0:06:03.000 --> 0:06:05.680
and you have enabled RISC-V vectors,

128
0:06:05.680 --> 0:06:09.440
but that's not really how you want to do it,

129
0:06:09.440 --> 0:06:12.840
but before we go on how to actually do it,

130
0:06:12.840 --> 0:06:15.120
what vector lengths are we dealing with here?

131
0:06:15.120 --> 0:06:20.120
So obviously, whereas mentioned earlier,

132
0:06:21.000 --> 0:06:25.840
common values are 128 and 512 bits,

133
0:06:25.840 --> 0:06:29.080
so both ARM and RISC-V are ground T's

134
0:06:29.080 --> 0:06:31.880
that even if you have the vibratory vector lengths,

135
0:06:31.880 --> 0:06:34.320
it's going to be at least 128 bits,

136
0:06:34.320 --> 0:06:37.320
and it's also going to be a power of two bits,

137
0:06:37.320 --> 0:06:39.760
which is kind of convenient for our calculations.

138
0:06:39.760 --> 0:06:44.760
So as far as I've seen,

139
0:06:44.760 --> 0:06:47.560
there are announcements for real hardware,

140
0:06:47.560 --> 0:06:50.560
which would have 256 and 512 bits,

141
0:06:50.560 --> 0:06:55.920
that you should be able to buy at some point in the near future.

142
0:06:56.440 --> 0:07:03.760
More crazy stuff, I've seen actually designs also being announced with 1000 bits.

143
0:07:03.760 --> 0:07:05.960
I don't know if they're going to store all those bits in

144
0:07:05.960 --> 0:07:11.120
the physical register bank, but it would be interesting if it happens.

145
0:07:11.120 --> 0:07:15.080
I haven't seen theoretical designs at 4000 bits,

146
0:07:15.080 --> 0:07:18.760
and theoretical to the point that there is a schematic.

147
0:07:18.760 --> 0:07:22.840
Theoretically, in this case, I mean that there are actual schematics of how you could

148
0:07:22.840 --> 0:07:25.040
write a chip and they have

149
0:07:25.040 --> 0:07:28.880
a simulation of the performance that the chip would get into the algorithms.

150
0:07:28.880 --> 0:07:35.520
As to whether it's actually practically implementable in an existing industrial process.

151
0:07:35.520 --> 0:07:39.240
I don't know, I'm not a specialist in electronics,

152
0:07:39.240 --> 0:07:41.680
but that sounds a little bit questionable.

153
0:07:41.680 --> 0:07:44.120
So in theory, on the syntactic level,

154
0:07:44.120 --> 0:07:45.640
so in the instruction and coding level,

155
0:07:45.640 --> 0:07:49.240
you can have up to two power of 16 bits,

156
0:07:49.240 --> 0:07:52.960
at least on H5, I'm not sure about how much you need.

157
0:07:52.960 --> 0:07:56.560
So how you actually do vector length?

158
0:07:56.560 --> 0:08:02.040
Are you supposed to do a viable vector length SIMD or vector processing as it's often called,

159
0:08:02.040 --> 0:08:05.880
particularly vector and SIMD synonyms.

160
0:08:06.200 --> 0:08:09.160
Well, first you have to use predication,

161
0:08:09.160 --> 0:08:15.840
which is very highly prevalent in variable vector length scenarios.

162
0:08:15.840 --> 0:08:17.840
Now, it's not a completely new concept.

163
0:08:17.840 --> 0:08:22.240
Here I mentioned the K mask in AVX,

164
0:08:22.240 --> 0:08:24.960
so AVX also has predication.

165
0:08:24.960 --> 0:08:28.080
But in variable vector length,

166
0:08:28.080 --> 0:08:31.320
it's really essential because this is

167
0:08:31.320 --> 0:08:35.600
basically the programming model on variable vector lengths.

168
0:08:35.600 --> 0:08:39.040
No, loops is essentially built on predication,

169
0:08:39.040 --> 0:08:41.960
and that's true both for ARM and H5.

170
0:08:41.960 --> 0:08:44.920
So a predicate is a vector on Boolean,

171
0:08:44.920 --> 0:08:47.240
so like the K mask in x86,

172
0:08:47.240 --> 0:08:51.880
it's called the P vector in ARM,

173
0:08:51.880 --> 0:08:55.560
and it's the mask vector in H5.

174
0:08:55.560 --> 0:08:58.360
As Ciaran said, kind of repeating,

175
0:08:58.360 --> 0:09:02.920
but it just specifies which of the element in your vector,

176
0:09:02.920 --> 0:09:08.240
it specify which ones you will be loading or modifying or storing out of a given instruction.

177
0:09:08.240 --> 0:09:13.600
So if it's a load instruction which values you load from memory and overwrite into the register,

178
0:09:13.600 --> 0:09:15.200
if it's a store instructions going to be the other way,

179
0:09:15.200 --> 0:09:19.520
which values in memory are going to overwrite versus which ones are going to live as they are.

180
0:09:19.520 --> 0:09:22.560
If it's a calculation instruction vector to vector,

181
0:09:22.560 --> 0:09:27.880
then it's going to affect which results are actually stored into the register,

182
0:09:27.880 --> 0:09:30.560
versus which ones are just discarded.

183
0:09:30.560 --> 0:09:33.720
So on ARMv9 or SVE,

184
0:09:33.720 --> 0:09:38.880
one way you would typically do now your SVE loop instead of say your neuron loop,

185
0:09:38.880 --> 0:09:42.000
is you would start by counting down,

186
0:09:42.000 --> 0:09:44.560
you would initialize say extend to zero,

187
0:09:44.560 --> 0:09:46.800
and then you would,

188
0:09:46.800 --> 0:09:52.760
so you have to imagine here that you have your actual neuron or SVE loop.

189
0:09:52.760 --> 0:09:54.360
So you will check,

190
0:09:54.360 --> 0:09:58.280
you have this funny instruction which is actually called while LTA or while LO,

191
0:09:58.280 --> 0:10:00.520
and you initialize the P0 vector in this case,

192
0:10:00.520 --> 0:10:03.000
which is one of the predicate register,

193
0:10:03.000 --> 0:10:06.560
to say that essentially,

194
0:10:06.560 --> 0:10:09.960
you want to count how many elements you still have in your input data.

195
0:10:09.960 --> 0:10:11.200
So here we have X0,

196
0:10:11.200 --> 0:10:15.200
we imagine that X0 is the number of elements we've been given to this function,

197
0:10:15.200 --> 0:10:17.120
extend is count of of how we've been,

198
0:10:17.120 --> 0:10:19.240
so it's our iterator.

199
0:10:19.640 --> 0:10:23.280
Essentially, what we'll say is as long as extend is

200
0:10:23.280 --> 0:10:26.480
larger as long as the number of elements we still have.

201
0:10:26.480 --> 0:10:31.800
So as long as X0 is larger than the size of the vector that the CPU can handle,

202
0:10:31.800 --> 0:10:34.560
we'll just set the predicate to handle to be clear.

203
0:10:34.560 --> 0:10:39.240
So we'll use the full size of the vector for our programming.

204
0:10:39.240 --> 0:10:43.240
Once the number of elements is more than zero,

205
0:10:43.240 --> 0:10:46.320
but less, strictly less than the vector size in the CPU can handle,

206
0:10:46.320 --> 0:10:50.160
then we'll start basically just ignoring the values at the end of the vector.

207
0:10:50.160 --> 0:10:53.200
So we'll have a bunch of ones and then at the end a bunch of zeros.

208
0:10:53.200 --> 0:10:56.360
This is how you abstract away and hide away the complexity of dealing

209
0:10:56.360 --> 0:10:58.880
with the edge in your loop.

210
0:10:58.880 --> 0:11:01.080
So essentially, by doing this,

211
0:11:01.080 --> 0:11:03.440
you don't care what is the actual capacity of,

212
0:11:03.440 --> 0:11:06.960
you don't actually need at any point to know how many elements are

213
0:11:06.960 --> 0:11:10.720
dealing with any interaction of your loop because it's all hidden away by the,

214
0:11:10.720 --> 0:11:13.520
essentially, the size of the vector and the size of the predicate are matched,

215
0:11:13.520 --> 0:11:14.760
so you don't actually care.

216
0:11:14.760 --> 0:11:18.000
You also don't need to deal with edges because when there's

217
0:11:18.000 --> 0:11:20.880
one or two or three or four elements left over at the end,

218
0:11:20.880 --> 0:11:23.440
you can just deal with them in the lattiteration,

219
0:11:23.440 --> 0:11:27.360
which of course will be a little bit less efficient than using

220
0:11:27.360 --> 0:11:29.560
the full size of the vector but it's still much faster than having

221
0:11:29.560 --> 0:11:32.040
a separate edge if only because you will not be

222
0:11:32.040 --> 0:11:35.240
stressing the instruction cache of the CPU.

223
0:11:35.840 --> 0:11:38.760
So that's predication.

224
0:11:38.760 --> 0:11:41.480
Now, unrolling. If you thought about it,

225
0:11:41.480 --> 0:11:43.880
what I just said with predication,

226
0:11:43.880 --> 0:11:47.280
it doesn't really work with unrolling because now you've counted down,

227
0:11:47.280 --> 0:11:50.320
you've set your predicate vector to count down how many elements you

228
0:11:50.320 --> 0:11:53.800
have still in your total count of elements.

229
0:11:53.800 --> 0:11:57.200
You can't and all because now you said,

230
0:11:57.200 --> 0:11:58.200
oh, I have 10 elements left,

231
0:11:58.200 --> 0:12:01.080
I'm going to use 10 elements in my vector but if you have,

232
0:12:01.080 --> 0:12:05.280
it just doesn't work because if you had one and a half vector left,

233
0:12:05.280 --> 0:12:07.960
you would want to have one predicate with all the bits set

234
0:12:07.960 --> 0:12:09.440
and another predicate with half of the bits set,

235
0:12:09.440 --> 0:12:11.360
this doesn't really work very well.

236
0:12:11.360 --> 0:12:14.720
Yes, it's a bit of a hot take.

237
0:12:14.720 --> 0:12:17.840
Maybe I will never be ever again allowed to write a system pec code after this,

238
0:12:17.840 --> 0:12:23.160
but just don't unroll if you use a viable vector lengths.

239
0:12:23.160 --> 0:12:27.480
Now, there may be cases where you can't unroll because naturally have

240
0:12:27.480 --> 0:12:33.480
parallel some kind of parallel in your design aspect in your algorithm.

241
0:12:33.480 --> 0:12:37.760
But the idea of vector processing is

242
0:12:37.760 --> 0:12:41.400
that we have higher latency and larger vectors,

243
0:12:41.400 --> 0:12:47.480
which in the end result in higher throughput and we shouldn't need to unroll.

244
0:12:47.480 --> 0:12:50.320
I'm sure you will find actual designs,

245
0:12:50.320 --> 0:12:53.800
real hardware, real processor where it will be faster if you do an all,

246
0:12:53.800 --> 0:12:58.080
and how much you need to unroll will depend on the design.

247
0:12:58.480 --> 0:13:03.080
Of course, if you are trying to squeeze the very last MIPS

248
0:13:03.080 --> 0:13:06.160
out of a given specific piece of hardware,

249
0:13:06.160 --> 0:13:09.760
then maybe you should unroll but I think generally speaking,

250
0:13:09.760 --> 0:13:12.400
at least you shouldn't start by unrolling.

251
0:13:12.400 --> 0:13:15.200
Another interesting thing to keep in mind,

252
0:13:15.200 --> 0:13:21.960
which I already mentioned in the previous slide is that you don't have alignment issues.

253
0:13:21.960 --> 0:13:25.160
So one common problem with SIMD instruction set is that

254
0:13:25.160 --> 0:13:27.840
the load and store instructions require over-aligned data,

255
0:13:27.840 --> 0:13:29.840
typically aligned on the side of the vector,

256
0:13:29.840 --> 0:13:33.680
which is very inconvenient when you're operating from C or C plus plus code,

257
0:13:33.680 --> 0:13:37.080
because it's usually C or C plus plus allocator will only allocate

258
0:13:37.080 --> 0:13:40.240
aligned on whatever the ABI specifies which are now on

259
0:13:40.240 --> 0:13:44.560
B8 would be 16 bytes for the stack and eight bytes for the heap.

260
0:13:44.560 --> 0:13:47.160
So usually, well,

261
0:13:47.160 --> 0:13:49.960
at least both SV and RISC-V vectors,

262
0:13:49.960 --> 0:13:51.440
they don't need the alignment,

263
0:13:51.440 --> 0:13:54.120
it is only the alignment of the element and it's not the alignment,

264
0:13:54.120 --> 0:13:55.520
it's not the size of the vector.

265
0:13:55.520 --> 0:14:00.440
So if you are operating on say four bytes pieces of data,

266
0:14:00.440 --> 0:14:03.760
data elements, then you only need your vectors to be aligned on four bytes,

267
0:14:03.760 --> 0:14:07.400
which is a very nice property for dealing especially on the edge,

268
0:14:07.400 --> 0:14:10.720
on edge cases and also you don't have to deal with like,

269
0:14:10.720 --> 0:14:13.280
if you have one input that is

270
0:14:13.280 --> 0:14:16.760
aligned that is perfectly aligned and the output is not perfectly aligned,

271
0:14:16.760 --> 0:14:19.680
then you end up having this weird mismatch and you end up

272
0:14:19.680 --> 0:14:21.400
having to deal with different edge cases,

273
0:14:21.400 --> 0:14:22.600
it's really a mess.

274
0:14:22.600 --> 0:14:24.600
With vector processing, you don't do that.

275
0:14:24.600 --> 0:14:27.400
So you don't actually have to worry about it.

276
0:14:28.120 --> 0:14:31.480
So with that, we've covered generality.

277
0:14:31.480 --> 0:14:34.000
So how is it looking on ARM side?

278
0:14:34.000 --> 0:14:37.800
Then we'll see RISC-V side because it's a bit weird if I would,

279
0:14:37.800 --> 0:14:39.760
I sort of like to put everything together,

280
0:14:39.760 --> 0:14:40.760
but then it becomes a huge mess.

281
0:14:40.760 --> 0:14:43.200
So it's going to be a bit repetitive because of course,

282
0:14:43.200 --> 0:14:44.720
there's a lot of similarities.

283
0:14:44.720 --> 0:14:50.240
But so SVE came about like five years ago,

284
0:14:50.240 --> 0:14:52.160
a little bit more than five years ago,

285
0:14:52.160 --> 0:14:56.680
I think it was announced late 2016 if I recall correctly.

286
0:14:56.680 --> 0:14:58.800
It was pretty much with some intermediate.

287
0:14:58.800 --> 0:15:01.640
It was explicitly meant for all the things like

288
0:15:01.640 --> 0:15:04.920
scientific applications or

289
0:15:04.920 --> 0:15:08.960
engineering modeling and then it's kind of stuff,

290
0:15:08.960 --> 0:15:13.840
my HPC and so nobody used it.

291
0:15:13.840 --> 0:15:16.160
At least nobody in this room used it.

292
0:15:16.160 --> 0:15:19.040
This was fixed with SVE2,

293
0:15:19.040 --> 0:15:23.320
which is sometimes called ARMv9 because it kind of

294
0:15:23.320 --> 0:15:28.680
comes with ARMv9 but it's really called SVE2.

295
0:15:28.680 --> 0:15:32.040
Fixed that issue, they realize that actually this is a good idea.

296
0:15:32.040 --> 0:15:34.280
This pattern programming model is also interesting from

297
0:15:34.280 --> 0:15:39.320
Intimedia and crypto which was also missing from SVE1.

298
0:15:39.440 --> 0:15:41.760
So what they did is they just took,

299
0:15:41.760 --> 0:15:47.280
yes, so which Neomnemonics are missing and added those,

300
0:15:47.280 --> 0:15:48.760
and it's pretty much the same Neomnemonics,

301
0:15:48.760 --> 0:15:49.920
you just add your prime,

302
0:15:49.920 --> 0:15:51.560
the predicate register.

303
0:15:51.560 --> 0:15:53.920
That's what it's of course a little bit more complicated.

304
0:15:53.920 --> 0:15:55.680
But as I mentioned,

305
0:15:55.680 --> 0:15:57.440
you just use a while instruction which

306
0:15:57.440 --> 0:15:59.560
will then provision your predicate,

307
0:15:59.560 --> 0:16:01.720
then you have to pick the element size so that of course,

308
0:16:01.720 --> 0:16:03.980
this adds up correctly and at the end,

309
0:16:03.980 --> 0:16:05.560
you have a new set of branch conditions,

310
0:16:05.560 --> 0:16:08.760
so first element, last element, and so on and so forth.

311
0:16:10.160 --> 0:16:14.320
So the remaining elements will be determined by

312
0:16:14.320 --> 0:16:16.440
the predicate register and the predicate register which

313
0:16:16.440 --> 0:16:20.360
is the condition flag and the while instruction will also subtract.

314
0:16:20.360 --> 0:16:22.320
There is a count, the number of

315
0:16:22.320 --> 0:16:25.520
a processed element from your output register.

316
0:16:25.520 --> 0:16:29.560
Yes, this one stop pretending that I'm missing risk.

317
0:16:29.560 --> 0:16:31.120
How do you detect this stuff?

318
0:16:31.120 --> 0:16:33.480
So there's a processor macro,

319
0:16:33.480 --> 0:16:34.760
otherwise as usual on ARMv8,

320
0:16:34.760 --> 0:16:38.880
you have a bunch of privileged register for the OS to look at,

321
0:16:38.880 --> 0:16:40.320
and then you have also on Linux,

322
0:16:40.320 --> 0:16:43.640
you have a bunch of flags in the OCI vector bits,

323
0:16:43.640 --> 0:16:45.280
so it's all classic,

324
0:16:45.280 --> 0:16:48.280
otherwise you're out of luck.

325
0:16:48.280 --> 0:16:51.640
Availability, so as we said 2016,

326
0:16:51.640 --> 0:16:53.200
but they didn't really work for us,

327
0:16:53.200 --> 0:16:55.480
as V2 was specified in 2019.

328
0:16:55.480 --> 0:17:00.040
But so the real hardware came earlier last year,

329
0:17:00.040 --> 0:17:03.120
so Cortex-X2 and all the other things

330
0:17:03.120 --> 0:17:05.680
from dynamic IQ 110.

331
0:17:06.320 --> 0:17:10.880
So Samsung actually has 2200 and so forth.

332
0:17:10.880 --> 0:17:12.080
If you find your cases are Francis,

333
0:17:12.080 --> 0:17:14.560
they do have SVE,

334
0:17:14.560 --> 0:17:17.320
unfortunately it's only 128 bit vectors,

335
0:17:17.320 --> 0:17:19.240
and it's pretty damn expensive.

336
0:17:19.240 --> 0:17:20.680
But if you want to do it,

337
0:17:20.680 --> 0:17:22.240
you can find the hardware.

338
0:17:22.240 --> 0:17:26.080
So race five, so different model.

339
0:17:26.080 --> 0:17:26.800
Yeah.

340
0:17:26.800 --> 0:17:29.720
There's also the Alibaba one, the Yitian.

341
0:17:29.720 --> 0:17:32.000
Yeah, maybe. It's possible.

342
0:17:32.000 --> 0:17:34.320
It's only available in China.

343
0:17:35.040 --> 0:17:37.000
So on race five,

344
0:17:37.000 --> 0:17:38.320
the prediction is a little bit different.

345
0:17:38.320 --> 0:17:40.400
So they have separation between

346
0:17:40.400 --> 0:17:43.880
element count and the actual predicate.

347
0:17:43.880 --> 0:17:45.800
So in practice in multi-major,

348
0:17:45.800 --> 0:17:47.280
maybe not in David, but usually you

349
0:17:47.280 --> 0:17:48.320
don't use a predicate at all.

350
0:17:48.320 --> 0:17:51.240
So we will instead just count the elements.

351
0:17:51.240 --> 0:17:53.160
This is the instruction you always found at

352
0:17:53.160 --> 0:17:55.840
the beginning of the loop which consider the vectors.

353
0:17:55.840 --> 0:17:58.640
So in this case, what we say is that we

354
0:17:58.640 --> 0:18:01.240
have a certain number of input elements.

355
0:18:01.240 --> 0:18:03.560
We want to get the number of output.

356
0:18:03.560 --> 0:18:05.920
The output of the parameter is the number of element

357
0:18:05.920 --> 0:18:09.520
the CPU will deal with in the iteration.

358
0:18:09.520 --> 0:18:12.440
We then have to say the size of the element in bits.

359
0:18:12.440 --> 0:18:14.880
In this case, for instance, 16 bits.

360
0:18:14.880 --> 0:18:17.480
The group size which is free and rolling,

361
0:18:17.480 --> 0:18:19.240
it will automatically, if you set it to two,

362
0:18:19.240 --> 0:18:21.160
it will use all the,

363
0:18:21.160 --> 0:18:22.760
and you say you want to use vector eight,

364
0:18:22.760 --> 0:18:23.960
it will use vector eight and vector nine

365
0:18:23.960 --> 0:18:26.120
at the same time for instance.

366
0:18:26.120 --> 0:18:28.840
Tail mode, we always set it to

367
0:18:28.840 --> 0:18:30.720
agnostic because we don't really care about tail mode,

368
0:18:30.720 --> 0:18:33.120
and mask mode, we always set it to agnostic.

369
0:18:33.120 --> 0:18:35.040
There may be use cases where you need to do something else,

370
0:18:35.040 --> 0:18:36.240
which might be a little bit slower,

371
0:18:36.240 --> 0:18:38.560
but usually you don't.

372
0:18:38.560 --> 0:18:41.440
This is about how to deal with the stuff that is masked,

373
0:18:41.440 --> 0:18:43.520
or with the element that are at the end of the vector,

374
0:18:43.520 --> 0:18:45.280
which we don't care about. Usually you don't care about them,

375
0:18:45.280 --> 0:18:47.920
so you just tell the CPU you don't care about them.

376
0:18:48.440 --> 0:18:50.720
One cool thing about H5,

377
0:18:50.720 --> 0:18:52.680
the floating point condition are separate from the vectors,

378
0:18:52.680 --> 0:18:55.440
unlike on ARM, so you have more registers available.

379
0:18:55.440 --> 0:18:57.040
If you have hybrid calculations

380
0:18:57.040 --> 0:18:58.640
between scalar and vector side,

381
0:18:58.640 --> 0:19:00.840
but do mind the floating point convention,

382
0:19:00.840 --> 0:19:03.600
and the coding convention when this happens,

383
0:19:03.600 --> 0:19:06.480
otherwise you will screw up your register state and

384
0:19:06.480 --> 0:19:08.400
configure your CPU.

385
0:19:09.440 --> 0:19:12.080
Interesting stuff also about H5,

386
0:19:12.080 --> 0:19:13.240
they have segmented load and store,

387
0:19:13.240 --> 0:19:16.520
which similar to structured load and store in ARM,

388
0:19:16.520 --> 0:19:18.720
but they can do it up to eight structures,

389
0:19:18.720 --> 0:19:21.840
whereas ARM is only up to four.

390
0:19:21.840 --> 0:19:25.760
What is much more interesting perhaps is

391
0:19:25.760 --> 0:19:28.960
stride in loads and store where you can say,

392
0:19:28.960 --> 0:19:31.240
I have this register X which contains

393
0:19:31.240 --> 0:19:33.160
a value and that's going to be my stride.

394
0:19:33.160 --> 0:19:35.200
So for instance, with that you can put

395
0:19:35.200 --> 0:19:37.680
width of your video inside one register,

396
0:19:37.680 --> 0:19:41.160
and you can load all the pixels in a column in an instruction,

397
0:19:41.160 --> 0:19:44.120
without having to then do weird shuffling and whatever.

398
0:19:44.120 --> 0:19:46.320
Does that actually perform a practice?

399
0:19:46.320 --> 0:19:48.320
I think that's going to depend on the design,

400
0:19:48.320 --> 0:19:52.640
but normally it should be in the data cache which would be okay.

401
0:19:52.640 --> 0:19:56.160
We've talked about hypothetical architectures where it could be fostered.

402
0:19:56.160 --> 0:19:58.800
So I'll come to that.

403
0:20:00.600 --> 0:20:03.720
Yes. On the downside,

404
0:20:03.720 --> 0:20:05.920
you don't have transpose or zipping instructions,

405
0:20:05.920 --> 0:20:08.400
which should be the annoying, which is the same.

406
0:20:08.400 --> 0:20:10.440
So you have to replace it with slides.

407
0:20:10.440 --> 0:20:12.920
So it's fine if you want to take

408
0:20:12.920 --> 0:20:15.960
every second element from one vector and so on.

409
0:20:15.960 --> 0:20:19.560
Future detection, they have very,

410
0:20:19.560 --> 0:20:24.680
very detailed pre-processor feature flags.

411
0:20:24.680 --> 0:20:27.200
I mean, you can download the slides if you're interested.

412
0:20:27.200 --> 0:20:29.520
On the other hand, on the runtime detection is pretty crappy,

413
0:20:29.520 --> 0:20:35.000
you have to trust the bootloader to

414
0:20:35.000 --> 0:20:40.040
actually tell it to your OS correctly in the device tree data structure,

415
0:20:40.040 --> 0:20:42.200
and otherwise there is a flag in there.

416
0:20:42.200 --> 0:20:45.800
So the v-speed, so the bit 21,

417
0:20:45.800 --> 0:20:48.000
because v is a 20 second later in the alphabet,

418
0:20:48.000 --> 0:20:52.560
is a vector flag in the auxiliary vector for other capabilities on Linux.

419
0:20:52.560 --> 0:20:56.720
Availability, unfortunately at this time, there is no hardware.

420
0:20:57.320 --> 0:21:02.360
Alibaba has made hardware available,

421
0:21:02.360 --> 0:21:05.840
but it's implementing version 0.71

422
0:21:05.840 --> 0:21:11.680
from about 18 months before the standardized specification,

423
0:21:11.680 --> 0:21:14.720
which is implemented by the Clang and GCC.

424
0:21:14.720 --> 0:21:17.040
So you can work with that and it gives you

425
0:21:17.040 --> 0:21:20.000
some ideas of performance but you're going to have to rewrite

426
0:21:20.000 --> 0:21:22.760
stuff because it's not completely bit compatible,

427
0:21:22.760 --> 0:21:24.360
so it's annoying.

428
0:21:24.360 --> 0:21:26.480
I don't know when the stuff is going to happen,

429
0:21:26.480 --> 0:21:27.680
I'm pretty sure it's going to happen,

430
0:21:27.680 --> 0:21:30.240
but I would guess by the end of this year,

431
0:21:30.240 --> 0:21:32.720
we are going to see hardware available.

432
0:21:34.360 --> 0:21:39.840
Also, I think one note answering or dodging the previous question,

433
0:21:39.840 --> 0:21:42.560
but because we have so many different vendors on RISC-V,

434
0:21:42.560 --> 0:21:44.400
and I think there's more than I did,

435
0:21:44.400 --> 0:21:45.760
I only listed three here,

436
0:21:45.760 --> 0:21:49.280
but I think there might be

437
0:21:49.280 --> 0:21:51.240
big difference between the performance characteristics of

438
0:21:51.240 --> 0:21:55.440
the different vendors with our references.

439
0:22:00.560 --> 0:22:05.240
Yes, I have just a few questions.

440
0:22:05.240 --> 0:22:10.640
I just heard of the SVP64 project from Liver SOC yet,

441
0:22:10.640 --> 0:22:16.360
which is a similar vector approach for PowerPC.

442
0:22:16.360 --> 0:22:18.720
No, I haven't looked at PowerPC at all.

443
0:22:18.720 --> 0:22:20.520
So another question that I have with

444
0:22:20.520 --> 0:22:22.600
my own simply programming work is,

445
0:22:22.600 --> 0:22:25.760
we often have applications that are inherently horizontal.

446
0:22:25.760 --> 0:22:28.040
For example, let's say you are writing

447
0:22:28.040 --> 0:22:29.960
a vectorized string search operation,

448
0:22:29.960 --> 0:22:32.200
or you're doing something like decoding JPEGs,

449
0:22:32.200 --> 0:22:34.040
where you have these A5 blocks,

450
0:22:34.040 --> 0:22:36.520
where you want to do some close-end transform on them,

451
0:22:36.520 --> 0:22:39.480
and they have this fixed size and depending on the vector size,

452
0:22:39.480 --> 0:22:40.880
you either have to break them up or you

453
0:22:40.880 --> 0:22:43.080
maybe can process multiple of them at the same time.

454
0:22:43.080 --> 0:22:44.680
Is there an intelligent way to solve this?

455
0:22:44.680 --> 0:22:46.840
So I've had this case.

456
0:22:46.840 --> 0:22:48.400
Yes. So the question is,

457
0:22:48.400 --> 0:22:50.800
when you have a naturally fixed size data,

458
0:22:50.800 --> 0:22:52.920
the input block, block kind of block

459
0:22:52.920 --> 0:22:54.560
that you want to process at the time,

460
0:22:54.560 --> 0:22:56.520
how do you do this because then you

461
0:22:56.520 --> 0:22:59.200
actually want to have a fixed size vector in effect,

462
0:22:59.200 --> 0:23:00.480
path as in the question.

463
0:23:00.480 --> 0:23:02.080
So yes, I've had this case with

464
0:23:02.080 --> 0:23:03.600
a sample a couple of times.

465
0:23:03.600 --> 0:23:05.840
One way is to just check that

466
0:23:05.840 --> 0:23:06.840
the vector size of the CPU is

467
0:23:06.840 --> 0:23:08.960
big enough and just do one at a time.

468
0:23:08.960 --> 0:23:11.560
If you can try to do n at a time,

469
0:23:11.560 --> 0:23:12.840
because it's always going to be a power off too,

470
0:23:12.840 --> 0:23:15.720
so you should be able relatively easily to parallelize.

471
0:23:15.720 --> 0:23:17.840
I mean, obviously, the ideal situation is to parallelize,

472
0:23:17.840 --> 0:23:19.040
where you will have a problem is if

473
0:23:19.040 --> 0:23:20.840
your data set is larger than the vector,

474
0:23:20.840 --> 0:23:23.080
then it's going to become complicated for you.

475
0:23:23.080 --> 0:23:25.520
On the RISC-V, you can deal with

476
0:23:25.520 --> 0:23:28.440
this with a group multiplier,

477
0:23:28.440 --> 0:23:32.240
which will allow you to use multiple vectors as a single vector.

478
0:23:32.240 --> 0:23:34.440
The last question I have is,

479
0:23:34.440 --> 0:23:39.520
how do you realistically test vectorized kernels,

480
0:23:39.520 --> 0:23:42.760
when the hardware you have only supports one vector length at most?

481
0:23:42.760 --> 0:23:44.400
So you have to probably use some sort of

482
0:23:44.400 --> 0:23:46.120
validation to be set up for this?

483
0:23:46.120 --> 0:23:48.600
Most of the loops will not depend.

484
0:23:48.600 --> 0:23:50.920
So the question is, do you test a different vector size,

485
0:23:50.920 --> 0:23:52.800
for validation, I guess?

486
0:23:52.800 --> 0:23:55.480
Most of the loops don't really care about the vector size,

487
0:23:55.480 --> 0:23:57.600
because if you have the simple case where you follow

488
0:23:57.600 --> 0:24:00.760
the simple pattern, it doesn't really care what the vector size is.

489
0:24:00.760 --> 0:24:02.560
Except for benchmarking, of course,

490
0:24:02.560 --> 0:24:05.640
and you have a problem, but otherwise,

491
0:24:05.640 --> 0:24:08.160
QMU and Spark at this far is to have support

492
0:24:08.160 --> 0:24:09.560
any vector size to give that,

493
0:24:09.560 --> 0:24:12.280
as long as it's a valid one from specification 10 point.

494
0:24:12.280 --> 0:24:13.720
You realistically really test for that,

495
0:24:13.720 --> 0:24:16.600
or do you just say it's simply not going to be a problem?

496
0:24:16.600 --> 0:24:19.360
I mean, personally,

497
0:24:19.360 --> 0:24:20.960
when I've had the situation where I had

498
0:24:20.960 --> 0:24:23.600
a fixed size input and I had to test with different vector size,

499
0:24:23.600 --> 0:24:25.240
and I've tested with different vector size,

500
0:24:25.240 --> 0:24:27.400
in most cases, you don't actually care.

501
0:24:27.400 --> 0:24:30.720
I mean, then it's a matter of choice of duty or testing,

502
0:24:30.720 --> 0:24:33.680
and I know it's tricky one to be with a validation, I think.

503
0:24:33.680 --> 0:24:35.400
Thank you.

504
0:24:35.400 --> 0:24:37.080
We have no one on the question.

505
0:24:37.080 --> 0:24:39.320
Yeah. First a disclaimer,

506
0:24:39.320 --> 0:24:42.520
I'm related to the Liversock project with SB64.

507
0:24:42.520 --> 0:24:44.920
It's related similar to RISC-V vectors,

508
0:24:44.920 --> 0:24:46.040
but not exactly the same,

509
0:24:46.040 --> 0:24:48.640
but they share a lot of the common ideas.

510
0:24:48.640 --> 0:24:50.840
You mentioned a very good point,

511
0:24:50.840 --> 0:24:53.040
that CMD is not vector processing.

512
0:24:53.040 --> 0:24:57.960
In order, we tried to import some code from a NEON to SB2.

513
0:24:57.960 --> 0:25:02.080
It was less than suboptimal, let's say.

514
0:25:02.080 --> 0:25:31.400
We had to revert back to the original C algorithm.

