WEBVTT

00:00.000 --> 00:15.720
Okay, we can start. Thank you for attempting to this meeting, which is about Linfon and

00:15.720 --> 00:25.960
video conferencing. My name is Jean Monier. I'm involved in the Linfon project since 2010.

00:25.960 --> 00:33.960
And I'm also part of the company which is backing the Linfon project since 10 years

00:33.960 --> 00:42.360
almost. So first, I'm going to provide you with a quick introduction on Linfon. And then

00:42.360 --> 00:52.840
I have a couple of words around video conferencing with SIP, followed by an introduction on the

00:52.840 --> 01:02.000
selecting forwarding unit, which is the art of the modern video conferencing systems. And later,

01:02.000 --> 01:09.080
to talk about what is required on the SIP client part to be able to interrupt with this kind of

01:09.080 --> 01:18.480
video conferencing system. And finally, the conclusion. Okay, so just a couple of words

01:18.480 --> 01:28.520
about Linfon. Linfon is a voice over IP client implementing the SIP protocol, which started in

01:28.520 --> 01:40.680
early 2000. It's available on Linux, Android, iOS, Windows, and Mac. It uses SIP as the base

01:40.680 --> 01:48.480
standard for almost everything, including audio, video call, and instant messaging presence.

01:48.480 --> 01:57.560
Everything which is required for real-time communication. And it also provides some end-to-end

01:57.560 --> 02:07.080
encryption for messaging based on the signal protocol more or less. The Linfon team developed

02:07.080 --> 02:13.200
the Linfon software, but also SIP rover, which is basically a SIP proxy. And if you want to use

02:13.200 --> 02:23.280
SIP account, it's possible to create them on our website for testing purpose mainly. Okay,

02:23.280 --> 02:31.560
video conferencing with SIP in a couple of words. It's around a couple of standards. The first one

02:31.560 --> 02:40.000
is SIP, basic SIP with invite, refer, and buy to create conference, join the conference to be

02:40.000 --> 02:50.840
able to invite other participants to the conference. And it's almost based on the RFC 4579, which

02:50.840 --> 02:59.880
defines how to create the conference and how to join the conference. And there is also some

02:59.880 --> 03:09.120
interesting standard, which is the RFC 5366, which is about defining the list of the participants of

03:09.120 --> 03:18.120
the conference. So it's for the establishment of the conference. And the next important standard

03:18.120 --> 03:27.000
is what we call the conferencing events package, which is based on the subscribe notify RFC. And

03:27.000 --> 03:34.640
the idea is that when a participant joined the conference, it initiates SIP subscribe to the

03:34.640 --> 03:41.800
server and the server then notified to every participant of the conference, which are the

03:41.800 --> 03:48.880
states of the conference within who is going to join is their audio, video, everything which

03:48.880 --> 04:01.840
is related to the status of the conference. On the media port is regular RTP. And for this video

04:01.840 --> 04:09.680
conferencing project, we added the support of two important RFC, which are about bundling all the

04:09.680 --> 04:19.760
media stream into the same socket in order to avoid to have too many RTP sockets, RTP stream per

04:19.760 --> 04:29.560
supply. And regarding the security, it's a regular CPTLS for the for the signaling and for the media

04:29.560 --> 04:40.440
path. It's either SDS where the symmetric key is set in the SDP or the RTP or even SRTPDTLS. And

04:40.440 --> 04:53.480
for the RTP itself, it's a standard AES. Okay, now let's introduce what the selected forwarding

04:53.480 --> 05:03.240
units. And I'm going to start with the description on what we used to have for conferencing server.

05:03.240 --> 05:15.680
So in the past, the media mixer received the video from every user, decode the video stream,

05:15.680 --> 05:25.520
perform the mixing, it could be mosaic or any layout, and then re-encode all the stream to be

05:25.520 --> 05:40.520
sent to every participant. This kind of software exists since 30 years, maybe 20. Here I just want

05:40.520 --> 05:53.320
to show you a page that I saw in the RFC 7667, which is about the RTP topology of

05:53.320 --> 06:06.640
the

06:06.640 --> 06:15.520
RTP stream, which come from the media server to each client and its server side that everything

06:15.520 --> 06:25.280
is decoded, mixed and sent back to client. The advantage of this approach was that it

06:25.280 --> 06:31.760
was very simple from the client side as the calling a conferencing server was almost the

06:31.760 --> 06:43.280
same as calling a regular user agent. The drawbacks of this approach was that video layout was

06:43.280 --> 06:51.040
defined server side, so you could have one or two different layouts. It requires a lot

06:51.040 --> 07:00.320
of CPU resources server side as every video stream has to be decoded and then re-encoded.

07:00.320 --> 07:11.600
The end-to-end encryption was not possible due to the fact that video was decoded. Now

07:11.600 --> 07:19.840
if we go to the selecting forwarding units, the idea is that the media server is no more

07:19.840 --> 07:28.200
decoding and then encoding every video stream, but is more switching video coming from every

07:28.200 --> 07:39.040
device to every other device. It could be done depending on several policies like active

07:39.040 --> 07:45.720
speaker or mosaic. For that, we also need some information coming from each client like

07:45.720 --> 07:52.400
the volume of the audio stream in order to be able to know who is talking without having

07:52.400 --> 08:02.360
to decode the audio stream as well. If I go to the same schema, still from the

08:02.360 --> 08:13.680
RFC 7667, now you can see that from the RTP standpoint, you still have one RTP stream

08:13.680 --> 08:24.920
for each client going to the media server, but now you have also one incoming video stream

08:24.920 --> 08:32.800
per participant of the conference. If we follow the audio, the video stream from the client

08:32.800 --> 08:45.400
A, you can see that it is copied to client B, but as well to client F. It's no more a

08:45.400 --> 08:57.200
media mixer, but a switching matrix more or less. What are the advantages of this architecture

08:57.200 --> 09:06.240
is that video layout is no more defined server side, but the client can decide where to display

09:06.240 --> 09:14.640
every participant of the conference. It's an application choice, no more a server choice.

09:14.640 --> 09:22.480
It scales very well as there are no resources which are used server side to decode or encode

09:22.480 --> 09:28.720
the video stream. Finally, it's opened the door for end-to-end encryption as the media

09:28.720 --> 09:35.360
server no more has to know the content of video stream.

09:35.360 --> 09:43.600
The drawback of this approach is that it requires the client to be able to manage the stream,

09:43.600 --> 09:56.960
which was not the case for standard one-to-one calls. For the CPU's agent, what we had to

09:56.960 --> 10:05.320
change are the following mainly. It's mainly about multi-stream requirements. In the past,

10:05.320 --> 10:15.880
the C client was able to send one audio stream plus one video stream. Now, it requires the

10:15.880 --> 10:26.200
client to be able to send one, but most of the time two video streams, one for high resolution

10:26.200 --> 10:33.880
video and another second one for thumbnail, as well as being able to receive one video

10:33.880 --> 10:41.880
stream per participant of the video conference.

10:41.880 --> 10:53.920
Just an example of the SDP to show what is involved. So, bundle mode, as I said, which

10:53.920 --> 11:03.320
is required, RTP mux as well, is to limit the number of sockets used for the media.

11:03.320 --> 11:10.080
This extension is related to audio level in order to be able to display who is talking

11:10.080 --> 11:17.960
and also for the server to be able to select which video stream is talking. It still uses

11:17.960 --> 11:29.840
IC to be able to limit the usage of media release. From the video part, what you can

11:29.840 --> 11:38.920
see is that there are two video streams in the receiver only, one for the high resolution

11:38.920 --> 11:45.080
of the camera and another for the thumbnail. So, it means that we encode two times the

11:45.080 --> 11:55.240
video. It could be replaced by some video encoder like H.264 AVC, which supports the

11:55.240 --> 12:05.840
multi-layer functionality. But if you want to be able to do that with a simple VP8, it's

12:05.840 --> 12:09.480
better to encode two times the video.

12:09.480 --> 12:17.560
And for the receiving side, there is one video stream because in this example, there is only

12:17.560 --> 12:33.680
one participant in the video conference. But this part would be multiplied by the number

12:33.680 --> 12:37.760
of participants of the conference.

12:37.760 --> 12:50.280
Okay, so this is for what we have done on the LinFone project for this feature. It could

12:50.280 --> 12:58.240
be tested with the Flexi SIPsurf, SIPsurf, which is currently running on our infrastructure.

12:58.240 --> 13:07.320
So you can create a video conference thanks to this Conference Factory CPU-RI. And using

13:07.320 --> 13:15.800
LinFone client with version above 5.0, it's possible to join a video conference. Okay,

13:15.800 --> 13:18.800
thank you.

13:18.800 --> 13:20.800
Conclusion.

13:20.800 --> 13:30.960
Okay, so now LinFone is capable of joining video conference into modes, modes like an

13:30.960 --> 13:40.160
active speaker using a selective forwarding unit, which allows us to scale. Possible evolution

13:40.160 --> 13:45.800
that we have in mind is to implement the XCON Conferencing Server in order to be able to

13:45.800 --> 13:51.280
manage conference from a website or to have something more advanced. We are also thinking

13:51.280 --> 13:56.920
about adding end-to-end encryption to this video conferencing server and why not to provide

13:56.920 --> 14:03.880
a compatibility with WebRTC, knowing that the media protocol that we use are very close

14:03.880 --> 14:07.800
than WebRTC.

14:07.800 --> 14:12.720
Useful link if you want to have more information about this work, you can go to the LinFone

14:12.720 --> 14:22.560
website and to have a look on our GitHub. That's it if you have a question. Thank you.

14:22.560 --> 14:41.160
Are you aware of any other SIP client that implements multi-party video with SoundCloud?

14:41.160 --> 14:50.080
Not yet, because the work to move from a regular SIP phone with only supporting one audio stream

14:50.080 --> 15:00.440
and one video stream to support this multi-stream is very significant and I'm not aware of any

15:00.440 --> 15:07.400
work in progress so far. So if you want to use it, you have to go with LinFone, even

15:07.400 --> 15:14.160
if it's fully standardized, if we are following SoundCloud.

15:14.160 --> 15:23.680
Thank you.

15:23.680 --> 15:29.600
Not yet, but we are quite confident that it's going to scale as we have removed all the

15:29.600 --> 15:38.000
needs for audio or video encoding server-sign. So it's really about switching of packets.

15:38.000 --> 15:43.040
Maybe the question might be around the network on the client side.

15:43.040 --> 15:53.880
One network on the client side, we are sending two resolutions from the client, high resolution

15:53.880 --> 16:01.120
and low resolution. In the case of active speaker, we only send back to every client

16:01.120 --> 16:08.520
the high resolution of the people who is currently talking and low resolution of every other

16:08.520 --> 16:14.120
participant. So it highly limited the needs of bandwidth.

16:14.120 --> 16:21.600
On the client side, you now decode more than one stream?

16:21.600 --> 16:22.600
Correct.

16:22.600 --> 16:26.840
Do the codes support this?

16:26.840 --> 16:34.440
It's almost the same answer. We decode one high resolution and many low resolution and

16:34.440 --> 16:42.560
the CPU resources depend on the resolution of the video that you have to decode.

16:42.560 --> 16:50.240
One quick question about the SDP that you showed before. So there were two receive-only

16:50.240 --> 16:54.560
streams for the client. Was that from the client perspective?

16:54.560 --> 16:55.560
It was from the server.

16:55.560 --> 16:56.560
Okay, yeah, because it looks like the client.

16:56.560 --> 17:04.720
My bad. So the server received two videos from the

17:04.720 --> 17:11.560
client, one in high resolution and one in low resolution and sent one video to this

17:11.560 --> 17:15.160
client as there is only one participant in this conference.

17:15.160 --> 17:19.960
From the client perspective, when you switch from big resolution to low resolution, you

17:19.960 --> 17:25.560
still use the same M line that you have on the board for two send to the client.

17:25.560 --> 17:27.560
Yes, exactly.

17:27.560 --> 17:56.560
Okay, thank you very much.
