WEBVTT

00:00.000 --> 00:08.100
Last but not least, it's Christophe Massieu.

00:08.100 --> 00:12.520
But first of all, I would like to thank him for his work organizing the dev room all day.

00:12.520 --> 00:13.520
Thank you.

00:13.520 --> 00:14.520
Sat here.

00:14.520 --> 00:15.520
Thank you.

00:15.520 --> 00:16.520
Not too much.

00:16.520 --> 00:20.680
Yes, Fostem is a volunteer event.

00:20.680 --> 00:24.720
And Christophe will be talking about distributing multicast channels to third parties, a case

00:24.720 --> 00:29.360
study with OpenSoft software and virtualization slash SR-IOV.

00:29.360 --> 00:30.360
Thank you, Kirana.

00:30.360 --> 00:35.360
And also thank you, Kirana, for co-organizing the dev room with me.

00:35.360 --> 00:37.600
Exactly.

00:37.600 --> 00:44.400
And if next year there are more volunteers, I think we would be happy to have that.

00:44.400 --> 00:45.400
So I'm Christophe Massieu.

00:45.400 --> 00:50.040
I've been in the broadcast business for quite some time now.

00:50.040 --> 00:54.200
And I run a company called Easy Tools.

00:54.200 --> 00:59.320
One of the purpose of this company is to distribute linear channels.

00:59.320 --> 01:04.680
So either we create them or we get them from satellite or from abroad or we get them in

01:04.680 --> 01:10.440
data centers and we deliver them somewhere in the world or to network operators and so

01:10.440 --> 01:11.440
on.

01:11.440 --> 01:15.600
So one of the critical parts of doing that is knowing how to distribute multicast to

01:15.600 --> 01:18.200
other people.

01:18.200 --> 01:19.680
Usually how does it work?

01:19.680 --> 01:26.160
So first I must clarify, linear channel from my point of view at least, it's basically

01:26.160 --> 01:33.320
transport stream, so it's MPEG-TS over UDP or RTP, depends on your religion, both exist,

01:33.320 --> 01:36.880
and generally multicast.

01:36.880 --> 01:42.040
So usually you put seven TS packets inside one UDP frame and you're done.

01:42.040 --> 01:45.240
And it's a continuous stream all the time.

01:45.240 --> 01:47.440
How do you exchange these kind of streams?

01:47.440 --> 01:50.800
Usually you take a rack in a well-known point of exchange.

01:50.800 --> 01:53.320
In Paris there is one called Terre House.

01:53.320 --> 01:58.040
It's very popular, everybody knows that in Paris.

01:58.040 --> 02:03.520
And so you have your rack, you put your switch and you buy cross-connects from your data

02:03.520 --> 02:05.920
center to your peers.

02:05.920 --> 02:11.080
So your peers can be your sources, people who provide your streams, or it could be where

02:11.080 --> 02:16.320
you distribute them, so the distributors, the operators and so on.

02:16.320 --> 02:23.400
So for safety reasons you will want to have each source or each operator in a different

02:23.400 --> 02:26.360
VLAN so that they don't see each other.

02:26.360 --> 02:32.160
From confidentiality and also some content is a little bit critical.

02:32.160 --> 02:39.760
So the question will be how do you copy basically a multicast from one source VLAN to a destination

02:39.760 --> 02:40.760
VLAN.

02:40.760 --> 02:48.280
It's stupid and perfectly easy to answer, but it's not so easy.

02:48.280 --> 02:50.440
There is a pure network solution.

02:50.440 --> 02:54.720
With some switches you have a feature called multicast service reflection.

02:54.720 --> 02:55.760
That's what Cisco provides.

02:55.760 --> 03:00.960
I'm pretty sure there is the equivalent on Juniper and that kind of thing.

03:00.960 --> 03:03.040
But it's not so widely available.

03:03.040 --> 03:04.920
Lots of switches don't handle it.

03:04.920 --> 03:07.880
Only a small range of that.

03:07.880 --> 03:13.360
And so basically you can type a command there to say that you copy this multicast address

03:13.360 --> 03:20.240
to this destination and so you copy from a VLAN to another VLAN.

03:20.240 --> 03:21.920
So it's not widely available.

03:21.920 --> 03:25.840
There are good chances that your switches do not support it.

03:25.840 --> 03:32.120
And also you cannot handle complex use cases like some operators want RTP, some don't want

03:32.120 --> 03:33.120
RTP.

03:33.120 --> 03:35.200
You cannot remove RTP with that.

03:35.200 --> 03:39.520
And some operators also want you to have specific PIDs, specific service IDs, different service

03:39.520 --> 03:40.520
names and so on.

03:40.520 --> 03:42.840
That you cannot do of course.

03:42.840 --> 03:49.720
So generally you will end up using the devices on top of your switch.

03:49.720 --> 03:54.600
You will have to plug a device that will do that kind of job.

03:54.600 --> 04:00.120
The broadcast solution, well the most popular one is what we call DCM.

04:00.120 --> 04:01.600
It's a very popular brand.

04:01.600 --> 04:05.800
You're initially by Cisco, now it's a company called CineMedia.

04:05.800 --> 04:10.680
It's a hardware service with electronics inside and it does all this work of switching.

04:10.680 --> 04:13.040
It can even transcode actually with some cards.

04:13.040 --> 04:14.400
It does a lot of things.

04:14.400 --> 04:18.920
But it's also very expensive of course.

04:18.920 --> 04:20.840
But we have an open source alternative.

04:20.840 --> 04:23.480
It's actually something I wrote maybe 15 years ago.

04:23.480 --> 04:24.480
It's called DV Blast.

04:24.480 --> 04:28.200
I've been doing a lot of talks about that.

04:28.200 --> 04:31.360
In this use case it also helps.

04:31.360 --> 04:34.000
Usually it was written as DVBDMux.

04:34.000 --> 04:40.440
So you have a satellite card or DTT card and you want to get a transponder and split each

04:40.440 --> 04:42.160
channel into a multicast address.

04:42.160 --> 04:44.400
That was the original goal of DV Blast.

04:44.400 --> 04:51.720
But actually with the dash D option you can also read from a multicast channel.

04:51.720 --> 04:57.000
And in the arguments you can also say which is the IP address of the interface you want

04:57.000 --> 04:58.000
to read it from.

04:58.000 --> 04:59.200
So basically which VLAN.

04:59.200 --> 05:01.600
That means which VLAN you will read it from.

05:01.600 --> 05:07.240
So basically that reads a multicast stream from a specific VLAN.

05:07.240 --> 05:12.760
There is a configuration file associated with DV Blast and you can put as many lines as

05:12.760 --> 05:13.760
you want.

05:13.760 --> 05:15.120
So as many distributors as you want.

05:15.120 --> 05:20.280
And for each line you will just put the multicast address you want to send it to.

05:20.280 --> 05:25.280
And you can also optionally give the address of the interface.

05:25.280 --> 05:27.880
Again you want to send it to.

05:27.880 --> 05:33.880
The VLANs you have on your switch they will appear as network interfaces on your server.

05:33.880 --> 05:38.560
So you can decide to which VLAN you want to send and which multicast address.

05:38.560 --> 05:42.240
You have a number of options to turn on or off RTP.

05:42.240 --> 05:47.000
You can remap PIDs, SID, channel service name and you can even spoof the source address

05:47.000 --> 05:53.640
which is very useful in case your peer wants to do IGMPv3.

05:53.640 --> 05:59.320
In that case you ask you to put a specific source address and that's the easiest way

05:59.320 --> 06:02.320
to spoof it.

06:02.320 --> 06:04.320
So problem solved.

06:04.320 --> 06:05.320
End of talk.

06:05.320 --> 06:06.320
Thank you very much.

06:06.320 --> 06:10.040
Now I wanted to add a little something.

06:10.040 --> 06:14.320
You may want to run hundreds of DV Blast on one server but for some reason I wanted to

06:14.320 --> 06:16.640
do some virtualization.

06:16.640 --> 06:18.040
Why virtualize?

06:18.040 --> 06:23.400
Because I have different customers and each of the customer has different channels.

06:23.400 --> 06:27.280
Some of them are doing adult content, some of them are doing children content.

06:27.280 --> 06:31.360
Maybe I don't want to mix that in the same server and I don't have the money to have

06:31.360 --> 06:35.640
multiple servers at ten hours because it's expensive.

06:35.640 --> 06:40.000
So for client installation it's required.

06:40.000 --> 06:46.400
Also some of my clients have direct access to the VM because that's the service I sell.

06:46.400 --> 06:51.800
So again I don't want them to see the streams of the competitors potentially.

06:51.800 --> 06:59.800
So I used Proxmox that's a very nice distribution based on DBN and it's a very big front end

06:59.800 --> 07:04.000
over KVM, very useful.

07:04.000 --> 07:07.920
And in Proxmox what you will do is each of the VLAN you get from the Twitch you will

07:07.920 --> 07:15.040
bridge that and on your guest virtual machine it will appear as a network interface with

07:15.040 --> 07:16.320
the VERT IOD driver.

07:16.320 --> 07:19.120
The VERT IOD driver is the most optimized one.

07:19.120 --> 07:23.280
You can also emulate a network app but it's much slower.

07:23.280 --> 07:26.960
That's why everybody uses VERT IOD driver.

07:26.960 --> 07:30.720
So everything works fine end of talk and thank you very much for coming.

07:30.720 --> 07:32.280
There is just one little problem.

07:32.280 --> 07:36.880
The morning when you get called by one of your big customers and he says I have discontinuities

07:36.880 --> 07:39.880
on what you send me.

07:39.880 --> 07:46.600
Discontinuities that's what every people in broadcast feels and you listen you start another

07:46.600 --> 07:50.760
DV blast on the IP address that you put in and you see nothing.

07:50.760 --> 07:55.960
Everything looks fine and they insist and another customer complains.

07:55.960 --> 07:58.440
So what you do is you rack another server.

07:58.440 --> 08:02.640
That's a use case by the way that's why I'm saving my life.

08:02.640 --> 08:05.920
So you rack another server and you listen to what the other server is sending and then

08:05.920 --> 08:10.760
you see a lot of discontinuities indeed.

08:10.760 --> 08:17.640
And you dig it and you just see that the VERT IOD driver I don't know if it's guest side

08:17.640 --> 08:20.640
or host side reorder some packets.

08:20.640 --> 08:26.120
I mean probably there are several queues inside the drivers and if this packet does not have

08:26.120 --> 08:31.000
any luck and goes into a queue that for some reason doesn't run for some time it will be

08:31.000 --> 08:33.000
pushed there.

08:33.000 --> 08:39.240
So I hear now the network guy will tell me that while there is no guarantee on the order

08:39.240 --> 08:43.800
of UDP packets that's the specification that's true.

08:43.800 --> 08:49.920
But we are in an industry that relies on it because if you don't have RTP there is absolutely

08:49.920 --> 08:53.920
no way you can reorder your UDP transport string.

08:53.920 --> 08:55.520
So I had to find a solution.

08:55.520 --> 09:00.240
I cannot tell my customer no no you have to use RTP and I don't care.

09:00.240 --> 09:01.680
You cannot.

09:01.680 --> 09:07.600
So I found a first workaround is by using another driver.

09:07.600 --> 09:11.400
It's also a driver designed for virtual system, VMware.

09:11.400 --> 09:14.040
But it's supported by Poxmox KVM and so on.

09:14.040 --> 09:20.320
So it's called VMXnet and probably this one only has one queue and it solves the problem.

09:20.320 --> 09:26.640
The only problem is that it uses 30% more CPU and this server is only doing that.

09:26.640 --> 09:31.240
And I already have 64 core IMD APIC servers.

09:31.240 --> 09:36.400
That's not full but more than 50% already.

09:36.400 --> 09:38.560
So there is some kind of need to optimize.

09:38.560 --> 09:43.600
So that's why I started to look at other alternatives and many big some clever ideas that I found

09:43.600 --> 09:51.240
on the net and one of them that you probably heard of maybe is called SR-ROV.

09:51.240 --> 09:52.240
So what is SR-ROV?

09:52.240 --> 09:56.400
So it's a feature of some network cards not all network cards.

09:56.400 --> 09:58.680
Some network cards are that feature.

09:58.680 --> 10:05.440
In a normal installation for the network card is owned by the host.

10:05.440 --> 10:13.160
You have some kind of software switch that is handled by the virtualizer and here you

10:13.160 --> 10:17.120
have virtual interfaces to each of the virtual machine.

10:17.120 --> 10:23.640
In an SR-ROV setup what happens is that the network card will create new PCI devices.

10:23.640 --> 10:26.240
So it's a different PCI device.

10:26.240 --> 10:30.360
And this PCI device there is a feature called VTD on Intel that you have the equivalent

10:30.360 --> 10:36.120
on IMD that will allow you to dedicate a PCI device to a virtual machine.

10:36.120 --> 10:40.720
And doing that the virtual machine directly talks to the PCI device without anything going

10:40.720 --> 10:43.000
through the host.

10:43.000 --> 10:46.280
So that looks quite interesting.

10:46.280 --> 10:51.760
These new devices are called virtual functions, VF.

10:51.760 --> 10:57.480
And so on the VM you just need to have a VF driver which is included in the Linux anyway.

10:57.480 --> 11:00.520
So in my use case I used Intel cards.

11:00.520 --> 11:04.080
They're not the only ones doing SR-ROV but it may be a little bit different if you use

11:04.080 --> 11:06.240
different cards.

11:06.240 --> 11:10.040
Not all cards have the same features.

11:10.040 --> 11:15.280
So using SR-ROV it's a little bit tricky.

11:15.280 --> 11:21.560
First it requires support from the motherboard, the CPU, the BIOS itself and of course the

11:21.560 --> 11:24.320
network card as I was just saying.

11:24.320 --> 11:31.200
You have to enable a number of features in the BIOS, IOMMU but also HGS, ARR which is

11:31.200 --> 11:35.200
some kind of PCI Express routing protocol.

11:35.200 --> 11:41.000
Lots of things and basically we spent a couple hours I think just setting it up.

11:41.000 --> 11:47.560
My personal advice would be for you to upgrade the card from the drivers, the Intel drivers

11:47.560 --> 11:51.320
to the latest version and the card firmware because there is a firmware running on the

11:51.320 --> 11:57.000
card to the latest version supported by the driver.

11:57.000 --> 12:01.160
Once I did not do that when I started my test and I ended up having half of my VLANs working

12:01.160 --> 12:04.640
and the other half didn't work and there is absolutely no way to know what's going on

12:04.640 --> 12:07.320
I just had to reboot and it was in production.

12:07.320 --> 12:11.960
So I had to imagine what happened.

12:11.960 --> 12:13.680
Creating the virtual function is actually quite easy.

12:13.680 --> 12:21.560
It's just echo into a slash this device and with my Intel card I can create up to 64 virtual

12:21.560 --> 12:27.920
functions so that's up to typically 64 VMs.

12:27.920 --> 12:34.880
So the path through is very easy on the Proxmox, it's just a menu and potentially each VM has

12:34.880 --> 12:36.000
access to other VMs.

12:36.000 --> 12:46.160
So that may be also drawback because it sees all the traffic that it can send to any VLAN

12:46.160 --> 12:51.640
can send packets on any VLAN so you have to trust your clients a little bit so it may

12:51.640 --> 12:54.520
not be adapted to all situations.

12:54.520 --> 12:58.680
But it's quite useful because you don't have to create a bridge every time you want to

12:58.680 --> 13:07.640
create a new VLAN you just create a network interface, a new A-1, Q network interface

13:07.640 --> 13:08.960
into your VM and that's all.

13:08.960 --> 13:13.000
So it's actually easier to manage.

13:13.000 --> 13:19.720
Problem is, so everything looks perfectly fine so that the end of the talk, not yet.

13:19.720 --> 13:25.280
I talked about sending to VLANs but how to receive multicast from the VLAN.

13:25.280 --> 13:27.160
That's another problem.

13:27.160 --> 13:33.320
But initially it looks like it works so you put it in production, maybe a bit early.

13:33.320 --> 13:38.800
So how does it work, how does a network card work with multicast?

13:38.800 --> 13:41.940
First I should maybe remind you how it works.

13:41.940 --> 13:48.760
So you have your multicast IP address, you translate that to the multicast MAC address

13:48.760 --> 13:55.560
so the end is the same and the beginning is just dropped.

13:55.560 --> 14:00.440
And then you tell the card every packet that arrives on this MAC address, I want it.

14:00.440 --> 14:02.720
So that's how it works on any normal PC.

14:02.720 --> 14:05.000
That's quite a standard, it's called a MAC filter.

14:05.000 --> 14:06.880
It's quite a standard feature.

14:06.880 --> 14:12.440
The trick is on these cards the number of MAC filters is limited.

14:12.440 --> 14:15.080
And the way they limit it is a little bit stupid.

14:15.080 --> 14:20.960
They take the whole buffer of the card, the whole number of MAC filters they have and

14:20.960 --> 14:24.700
they divide it by the number of VF you have, so 64.

14:24.700 --> 14:29.880
And in the end, according to my calculation, your limit is around 100 multicast addresses.

14:29.880 --> 14:37.880
So 100 may be a lot but I have hundreds of multicast streams in my network.

14:37.880 --> 14:40.000
So you may reach it.

14:40.000 --> 14:44.480
You may think about segmenting your virtual machines not to go above the threshold interval

14:44.480 --> 14:51.600
but it's still a dangerous game because there is a feature of the interdriver that if you

14:51.600 --> 14:56.120
reach that limit, it's scientific phase of course, while you have a message in the message

14:56.120 --> 14:59.360
but nobody reads that.

14:59.360 --> 15:01.320
So you will try again and try again and try again.

15:01.320 --> 15:08.880
And after just a few trials like five, the kernel decides that your VM is crazy and it

15:08.880 --> 15:10.600
won't talk to it anymore.

15:10.600 --> 15:16.080
So you will still receive your multicast but if you have any other command to send to the

15:16.080 --> 15:21.360
card like creating a new VLAN, which could happen, you have a new distributor so you

15:21.360 --> 15:23.240
can't, you have to reboot.

15:23.240 --> 15:27.560
You have to reboot your VM, fortunately, not the host.

15:27.560 --> 15:33.740
So it's not that practical and to be honest, I have a patch in all my installation that

15:33.740 --> 15:40.640
disables the doing that feature and disables using the Mac filter at all actually because

15:40.640 --> 15:43.040
I have found it not practical in reality.

15:43.040 --> 15:44.040
Yeah.

15:44.040 --> 15:53.040
Is there any need for the Mac filtering at all given that like modern switches, you're

15:53.040 --> 16:13.600
going to beat it or not your

16:13.600 --> 16:16.160
So, promise use mode means what it means.

16:16.160 --> 16:18.780
It means UVM will receive all the traffic

16:18.780 --> 16:20.620
that is received by the network card.

16:23.020 --> 16:24.120
It looks like a good idea,

16:24.120 --> 16:26.460
but it dramatically increases CPU usage

16:26.460 --> 16:29.320
because from maybe two gigabytes per second of data,

16:29.320 --> 16:31.380
you only need 200 megabytes per second of data,

16:31.380 --> 16:33.860
and the rest, the kernel, we have to filter it.

16:33.860 --> 16:36.140
So, your kernel will do a lot of job.

16:36.140 --> 16:38.540
And from what I've calculated,

16:38.540 --> 16:42.340
basically the gain you had from going from VERTIO to SRRUV,

16:42.340 --> 16:44.100
you lose it right there.

16:45.440 --> 16:47.040
So, that's first.

16:47.040 --> 16:48.840
The second problem is that,

16:50.620 --> 16:54.260
imagine you have two gigabytes per second on your network,

16:54.260 --> 16:56.440
and you have 20 virtual machines.

16:56.440 --> 16:59.480
The network card will send 20 times two gigabytes per second

16:59.480 --> 17:02.760
to your virtual machines, and that means 40.

17:02.760 --> 17:05.040
And 40 is the limit of the card.

17:05.040 --> 17:07.900
And at that point, you will start losing packets randomly.

17:09.200 --> 17:12.040
Again, silently, you don't know what happens.

17:12.040 --> 17:15.820
And of course, obviously, you only know that in production

17:15.820 --> 17:18.480
because when you first started with one, two, three VM,

17:18.480 --> 17:19.760
it worked perfectly.

17:19.760 --> 17:22.080
So, it's, yes, I have my solution.

17:22.080 --> 17:23.640
And then you put all your load on it,

17:23.640 --> 17:26.580
and then one day, it just stops working.

17:28.320 --> 17:32.280
So, while activating POMICUS mode is actually quite easy,

17:32.280 --> 17:36.280
again, it's an echo in the FlashSYS file.

17:36.280 --> 17:41.280
So, I have found a second workaround,

17:42.720 --> 17:45.400
which is a little bit better.

17:45.400 --> 17:47.160
I'm using it in production.

17:48.120 --> 17:50.800
It's good, maybe it's only on the Intel card.

17:50.800 --> 17:52.840
I don't know if it exists on other brands,

17:52.840 --> 17:55.240
but it's a feature called VLAN Mirror.

17:55.240 --> 17:57.800
And basically, it tells the card to send all the traffic

17:57.800 --> 18:01.040
belonging to VLAN to a particular virtual function,

18:01.040 --> 18:02.080
to a particular VM.

18:03.760 --> 18:05.600
So, that kind of a POMICUS feature,

18:05.600 --> 18:07.760
but only for one VM.

18:07.760 --> 18:12.760
Which is kind of a good practice because it means that

18:13.280 --> 18:16.480
I think most people who have ever done multicast,

18:16.480 --> 18:17.840
when you have a backbone,

18:17.840 --> 18:21.080
you put all of your multicast addresses in the same VM,

18:22.000 --> 18:24.640
maybe with different address ranges or maybe not.

18:24.640 --> 18:27.200
And you expect at the other end that the receiver

18:27.200 --> 18:31.400
will pick up which multicast address you want.

18:31.400 --> 18:34.440
This approach, it forces you to have different VLANs

18:34.440 --> 18:35.280
per customers.

18:35.280 --> 18:38.520
So, it's actually not a bad idea.

18:39.960 --> 18:42.920
But there is one drawback.

18:42.920 --> 18:46.120
One specific VLAN can only be sent to one VM.

18:46.120 --> 18:49.440
So, if you have, let's say a big broadcaster

18:49.440 --> 18:51.160
that is sending you channels

18:51.160 --> 18:54.760
and several VLANs needs those channels,

18:54.760 --> 18:56.120
you cannot use that solution.

18:56.120 --> 19:00.120
Because only one will be able to read from that VLAN.

19:01.440 --> 19:03.120
So, I have a third workaround.

19:03.120 --> 19:05.760
It's just to go back to the good old vert.io.

19:05.760 --> 19:07.080
After all, why not?

19:07.080 --> 19:09.400
The packets in search or the packet in version

19:09.400 --> 19:14.400
were only on TX, not on RX.

19:15.040 --> 19:16.800
So, it works.

19:16.800 --> 19:18.640
Actually, it has also some additional features

19:18.640 --> 19:21.080
because the bridge in Proxmox at least,

19:21.080 --> 19:25.080
but most of the time, it has IGMP snooping.

19:25.080 --> 19:27.240
So, we only receive the multicast addresses

19:27.240 --> 19:29.480
that you subscribe to.

19:29.480 --> 19:30.760
But it's actually a good solution,

19:30.760 --> 19:33.680
but then it means you have basically interfaces

19:33.680 --> 19:35.160
to read the packet from

19:35.160 --> 19:38.160
and interfaces to send the packets to.

19:38.160 --> 19:41.320
Which is a bit of a mess, but still, it's a good compromise.

19:41.320 --> 19:43.280
That's my compromise currently.

19:43.280 --> 19:44.760
VLAN mirror, all this one,

19:44.760 --> 19:47.800
depending on the nature of the VLAN I have to read from.

19:48.800 --> 19:50.760
So, all is good in the best of words.

19:51.640 --> 19:53.120
At the end of the talk, thank you very much.

19:53.120 --> 19:54.560
Not quite.

19:54.560 --> 19:57.120
There is another topic I haven't mentioned yet.

19:57.120 --> 20:02.120
What if I want to read a multicast stream

20:03.280 --> 20:05.880
coming from another VM on the same server?

20:07.400 --> 20:08.400
That doesn't work.

20:08.400 --> 20:11.600
Because when you write through SROV,

20:11.600 --> 20:13.520
just outputs to the switch.

20:14.960 --> 20:17.600
As far as I know, or maybe some of you are the solution,

20:17.600 --> 20:20.200
there is no way to get the traffic back

20:20.200 --> 20:25.200
to the network card and use it in another virtual machine.

20:26.320 --> 20:31.320
You could do that,

20:32.160 --> 20:35.480
but if you don't want to have different VLANs,

20:35.480 --> 20:37.200
if you want to read from the same VLAN,

20:37.200 --> 20:38.200
then you cannot root.

20:41.520 --> 20:42.680
Okay.

20:42.680 --> 20:47.400
Well, there is another solution with the Intel card again.

20:47.400 --> 20:50.520
It's called egress mirror.

20:50.520 --> 20:53.160
You can make it so that everything that's output

20:53.160 --> 20:54.800
on virtual function number one

20:54.800 --> 20:58.360
will be mirrored to virtual function number seven.

20:58.360 --> 21:01.520
So, virtual function number seven will be on

21:01.520 --> 21:03.880
your receiving side and virtual function number one

21:03.880 --> 21:06.000
will be on the transmitting side.

21:08.280 --> 21:09.600
So, that actually works.

21:09.600 --> 21:11.680
And also, I use that in production.

21:13.360 --> 21:15.400
So, conclusions.

21:15.400 --> 21:18.080
Well, multicast on virtualized environment

21:18.080 --> 21:20.640
is no picnic actually, and I'm surprised.

21:22.160 --> 21:25.680
You don't find many papers about that.

21:25.680 --> 21:29.240
I've struggled literally for years on this problem

21:29.240 --> 21:34.120
with a number of problems in production

21:34.120 --> 21:35.920
because you only see the problem in production

21:35.920 --> 21:37.720
because you only see them under load.

21:38.600 --> 21:41.040
And so, this has been a little bit tiring.

21:42.520 --> 21:45.280
Thank you very much for listening to this.

21:45.280 --> 21:47.240
And if you have any questions.

21:47.240 --> 21:48.080
Yeah.

21:48.080 --> 21:53.080
I'm guessing you tried that, but in virtual,

21:55.600 --> 22:00.600
there are ways to actually ask it to be dumber.

22:01.040 --> 22:03.560
Like, I'm not doing some of the floating balls

22:03.560 --> 22:05.960
and I'm not sure, but probably you tried that.

22:05.960 --> 22:08.280
But you could probably have asked them,

22:08.280 --> 22:12.280
they have to say, hey, I just keep them in order.

22:12.280 --> 22:15.600
So, you say in virtual, there are ways to ask it

22:15.600 --> 22:16.560
to be dumber.

22:18.200 --> 22:19.640
I'm not sure.

22:19.640 --> 22:21.000
We are doing packet slide.

22:21.000 --> 22:23.840
Yeah, I'm not sure I've actually tested it.

22:23.840 --> 22:25.720
I'm pretty sure it would have the same effect

22:25.720 --> 22:27.600
as using VMXNet.

22:27.600 --> 22:31.320
And probably you would see an increase in CPU consumption.

22:31.320 --> 22:33.400
And anyway, I wanted to go to SRIOV

22:33.400 --> 22:36.000
because, for all the reasons,

22:36.000 --> 22:39.160
because I wanted to have my VM talk directly

22:39.160 --> 22:41.760
to the network hub, I think it was better practice

22:41.760 --> 22:42.600
than virtual.

22:42.600 --> 22:46.200
I'm going to have to go that way.

22:46.200 --> 22:47.040
Yeah, James?

22:47.040 --> 22:49.440
I think you can back the previous slide, please.

22:49.440 --> 22:50.280
Yeah.

22:52.400 --> 22:53.760
Okay, there was no question.

22:54.960 --> 22:55.800
Yeah?

22:55.800 --> 22:59.800
Just AWS provide the ECQ instance with the SRIOV.

23:02.320 --> 23:03.320
With SRIOV?

23:03.320 --> 23:04.320
Yeah.

23:04.320 --> 23:05.440
Okay.

23:05.440 --> 23:09.440
So, he says AWS provides instances with SRIOV.

23:09.440 --> 23:10.440
But they have to.

23:10.440 --> 23:11.800
Yeah, yeah, yeah.

23:11.800 --> 23:14.080
I guess so, but I guess in the cloud,

23:14.080 --> 23:15.320
you don't have that kind of problem

23:15.320 --> 23:16.880
because usually you do SRT.

23:18.000 --> 23:18.840
Usually.

23:20.200 --> 23:22.640
And SRT doesn't care if you eat your other packet

23:22.640 --> 23:23.480
because it will.

23:23.480 --> 23:25.160
Similar question to this, gentlemen.

23:25.160 --> 23:26.000
Yeah?

23:26.000 --> 23:27.640
Did you consider forcing, well,

23:27.640 --> 23:30.440
either disable multi-Q on the host

23:30.440 --> 23:33.400
or forcing each VM to a different queue?

23:33.400 --> 23:36.200
So that by definition, you will not be reordering.

23:36.200 --> 23:37.600
Forcing each VM to a different queue,

23:37.600 --> 23:39.320
I'm not sure it works.

23:39.320 --> 23:41.120
A disable queuing probably it will work,

23:41.120 --> 23:44.240
but with additional CPU usage.

23:44.240 --> 23:45.080
Yeah.

23:45.080 --> 23:46.800
So that's what I wanted to,

23:46.800 --> 23:50.200
as I said, I have 60 core APIC servers,

23:50.200 --> 23:53.160
which are pretty big and they're not full, but.

23:53.160 --> 23:56.080
You should be possible to pin,

23:56.080 --> 23:57.840
it's possible to pin a process to the game.

23:57.840 --> 24:00.600
If you do XPS CPUs, you can pin the process.

24:00.600 --> 24:01.600
At the end, I don't know if you can do that.

24:01.600 --> 24:04.200
But I'm not sure if the inversion happens

24:04.200 --> 24:06.480
on the host side or on the guest side.

24:07.680 --> 24:09.040
Disable multi-Q in your host.

24:09.040 --> 24:09.880
Yeah.

24:09.880 --> 24:10.720
Yeah.

24:10.720 --> 24:11.560
Yeah.

24:11.560 --> 24:12.400
So yeah.

24:14.800 --> 24:16.160
Yibei?

24:16.160 --> 24:20.240
Why do you want to use SRUV for,

24:20.240 --> 24:23.120
like, what things differently is like,

24:23.120 --> 24:25.720
what are the other use cases you're thinking about

24:25.720 --> 24:26.560
with SRUV?

24:28.640 --> 24:32.880
So the question is why did I want to use SRUV?

24:35.600 --> 24:36.440
Speed.

24:36.440 --> 24:38.240
Speed is the first argument, actually,

24:38.240 --> 24:41.120
but also some kind of clean design.

24:41.120 --> 24:43.080
Maybe I'm a little bit of a purist

24:43.080 --> 24:46.400
and I wanted my VM to have direct access,

24:46.400 --> 24:48.840
physical access, I mean, to the network.

24:48.840 --> 24:53.520
Also, so if you really want the list of arguments,

24:53.520 --> 24:56.400
the traditional way of doing VLAN bridging

24:57.560 --> 25:00.440
on POC smokes, but probably other environments,

25:00.440 --> 25:03.720
you create a bridge for each VLAN.

25:03.720 --> 25:05.560
And so you have a network interface for each VLAN.

25:05.560 --> 25:07.000
Maybe there is a way to avoid that,

25:07.000 --> 25:10.080
but I didn't look too much into that.

25:10.080 --> 25:10.920
That means you have a limit

25:10.920 --> 25:13.640
because I think you're limited to 32 network interfaces.

25:13.640 --> 25:16.840
The limit is low enough.

25:16.840 --> 25:19.360
You should switch to OVX switch and not to the cloud.

25:19.360 --> 25:21.000
Otherwise you have to switch to OVX switch,

25:21.000 --> 25:25.720
but considering the amount of problems I had without it,

25:25.720 --> 25:28.040
I thought maybe I will not go into that.

25:29.160 --> 25:31.040
I haven't tried, to be honest.

25:31.040 --> 25:31.880
Yeah.

25:31.880 --> 25:32.720
These are probably people,

25:32.720 --> 25:34.360
you don't see papers about the most people

25:34.360 --> 25:37.440
like Apple versus just AT&T,

25:37.440 --> 25:38.440
AT&T and AT&T as well.

25:38.440 --> 25:40.040
Exactly, as you said,

25:40.040 --> 25:42.440
so are you these mainly used by people to do HTTP

25:42.440 --> 25:43.840
or some TCP based.

25:43.840 --> 25:48.560
It depends on using SIOV for non-networking things.

25:48.560 --> 25:51.000
Ah, you may ask if I'm considering using SIOV

25:51.000 --> 25:53.280
for non-networking things.

25:53.280 --> 25:56.320
I suppose you imply GPU maybe.

25:56.320 --> 25:57.960
Maybe, but my question is,

25:57.960 --> 26:01.960
do you envision other places where you meet media

26:01.960 --> 26:06.960
and you could use short GPU is almost obvious one,

26:07.680 --> 26:10.040
but are they, have you ever thought of

26:10.040 --> 26:11.480
this in the policy?

26:11.480 --> 26:14.400
The GPU we've actually tried to,

26:14.400 --> 26:17.560
not sure the one we had supported SIOV,

26:17.560 --> 26:18.400
but that's.

26:18.400 --> 26:19.800
So it's really expensive ones.

26:19.800 --> 26:24.160
Yeah, they have features like VTG, I think.

26:24.160 --> 26:27.400
But in our experience, it did not work well.

26:27.400 --> 26:31.800
But we would like to have the ability to share a GPU

26:31.800 --> 26:34.000
among several instances.

26:35.040 --> 26:38.240
So that you could decode or encode

26:38.240 --> 26:40.040
on different VMs and so on.

26:40.040 --> 26:42.360
This virtual machine that we have at the house,

26:42.360 --> 26:44.960
they don't do that because the house is not the place

26:44.960 --> 26:47.640
to do transcoding because it's too expensive.

26:47.640 --> 26:50.400
But fundamentally, yes, I would be interested in having

26:50.400 --> 26:51.720
that for GPUs as well.

26:51.720 --> 26:54.880
At least GPUs, if you have any other ideas, tell me.

26:54.880 --> 26:57.880
That's not also a different question.

26:57.880 --> 26:59.880
Just mentioning because of practical techniques,

26:59.880 --> 27:01.960
support, sometimes with support as well for

27:03.400 --> 27:06.680
SIOV for record codes and they are expensive.

27:06.680 --> 27:09.320
And it's just landing approximately clearly.

27:09.320 --> 27:11.280
So you can actually play with that.

27:11.280 --> 27:13.920
You can have the way for the transfer plus lambda

27:13.920 --> 27:18.040
and it's like, you have the nearest for like 16

27:18.040 --> 27:19.880
CPUs as well.

27:19.880 --> 27:23.120
Could you use that to do SDI or NDI

27:24.480 --> 27:27.360
path route to get directly to the server?

27:27.360 --> 27:29.160
Yeah, you don't need a SIOV for that.

27:29.160 --> 27:32.240
The question was, can you use that for SDI

27:32.240 --> 27:35.440
or NDI path route?

27:35.440 --> 27:37.680
Well, NDI is network.

27:37.680 --> 27:42.680
But SDI, I think we tried it and you can pass

27:42.680 --> 27:45.560
through the Blackmagic, for instance, device.

27:45.560 --> 27:48.880
You pass it through every TD.

27:48.880 --> 27:50.280
Every TD is one thing.

27:50.280 --> 27:53.000
Every TD, I already pass through DVB tuners.

27:53.000 --> 27:58.000
You can probably pass through DVB ASI, probably SDI.

27:58.000 --> 28:00.520
Even though we don't do that on a regular basis.

28:00.520 --> 28:03.960
But have you tried a different connector on a different VM?

28:03.960 --> 28:04.800
Yes.

28:04.800 --> 28:07.600
You have four inputs.

28:07.600 --> 28:09.080
Different connector on a different VM

28:09.080 --> 28:11.040
that would not be possible with the Blackmagic driver

28:11.040 --> 28:13.720
because it's seen as one device, one PCI device.

28:13.720 --> 28:15.040
So it's not possible.

28:15.040 --> 28:17.480
If you design your own car, maybe,

28:17.480 --> 28:20.960
Siwan, you may want to answer that question.

28:20.960 --> 28:21.800
No?

28:23.200 --> 28:24.920
Yes, we could do it, yes.

28:24.920 --> 28:28.000
There's no technical reason.

28:28.000 --> 28:28.840
Yes, Gav?

28:28.840 --> 28:31.840
Do you have any container technology?

28:33.320 --> 28:36.520
We could use container.

28:36.520 --> 28:40.600
The thing is, as you know, we have our own software.

28:40.600 --> 28:43.640
And our own software is actually delivered as a disk image.

28:43.640 --> 28:46.640
So it can only run as a virtual machine.

28:46.640 --> 28:49.120
The idea to use container is not a bad idea.

28:50.840 --> 28:53.920
There is less isolation, though,

28:53.920 --> 28:56.080
than what you have with virtual machine.

28:57.800 --> 29:00.000
But this is something I would like to try

29:01.080 --> 29:02.520
in the middle term, yeah.

29:02.520 --> 29:04.280
Because there has been a lot of improvement

29:04.280 --> 29:06.320
in the past years, the Galaxy network,

29:06.320 --> 29:08.080
and the next phase.

29:08.080 --> 29:11.600
And you can probably have some direct

29:11.600 --> 29:20.340
Mac-

29:20.340 --> 29:23.340
interface, so I think some kind of physical interface,

29:23.340 --> 29:24.780
we have a direct Mac-

29:24.780 --> 29:28.920
in the container and a direct physical interface.

29:30.360 --> 29:32.400
I think we are running out of time.

29:33.440 --> 29:34.280
Wow, that was a-

29:34.280 --> 29:59.720
$ approve entered an ECLT May
