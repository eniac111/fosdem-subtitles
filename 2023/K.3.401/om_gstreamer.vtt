WEBVTT

00:00.000 --> 00:07.000
So we start with the first conversation by Mollibique Haidt, who was the District Manager

00:07.000 --> 00:09.000
of the Union this year.

00:09.000 --> 00:18.000
Hi everyone, I'm Mollibique Haidt. I've been a Digital Developer now for 15 years.

00:18.000 --> 00:24.000
And I'm going to tell you a bit of what has happened in the Digital Community in the last

00:24.000 --> 00:35.000
two years that we last met here. So we've had two major releases, 1.20, 1.22,

00:35.000 --> 00:43.000
quite a lot of merge requests as you can see. And an interesting fact, in 1.22, a third

00:43.000 --> 00:49.000
of the merge of the commits were in the Rust modules. So we've been investing a lot in Rust

00:49.000 --> 00:56.000
and the Community as well as excitement. And that's been a big happening.

00:56.000 --> 01:01.000
One of the big things for us as developers that we did was to merge all of the various

01:01.000 --> 01:09.000
Git repositories into one big giant repository. So now everything is together, except for Rust.

01:09.000 --> 01:15.000
Rust modules are all on the global corner because they're released with the GTK RS infrastructure.

01:15.000 --> 01:23.000
So the rest of these few months. But that's kind of our big change for developers,

01:23.000 --> 01:27.000
but it doesn't change much for the end user because we still release all the packages

01:27.000 --> 01:33.000
in separate card balls, just like these.

01:33.000 --> 01:39.000
Another thing that we did that's with the infrastructure is we improved our build system

01:39.000 --> 01:44.000
and now we can select the elements one by one. Not just the plugins,

01:44.000 --> 01:47.000
but you can select exactly which elements you want in the plugin.

01:47.000 --> 01:52.000
And then we can also link all the plugins, all the elements, all the dependencies

01:52.000 --> 01:57.000
into one giant library. This makes it actually easier to make a smaller build

01:57.000 --> 02:02.000
because we can build only exactly the functionality that you need for a specific application.

02:02.000 --> 02:08.000
So I've fixed several applications, created a big library that only has the attack functions

02:08.000 --> 02:13.000
that are being used, and not anything else. So we did that for embedded systems mostly

02:13.000 --> 02:20.000
so that you can have something a little smaller.

02:20.000 --> 02:23.000
Another area, and there has been quite a lot of excitement in this server

02:23.000 --> 02:28.000
in the last couple of years, is WebRTC. So as for me, all of you are familiar with,

02:28.000 --> 02:33.000
WebRTC is a way to send video and low latency to and from browsers.

02:33.000 --> 02:39.000
And this server has one of the most complete implementations outside of the Google implementation

02:39.000 --> 02:44.000
that's used by the browsers. We were missing one big bit,

02:44.000 --> 02:50.000
and that was a congestion control. And that's been added in the last releases.

02:50.000 --> 02:56.000
So now we have a module that is compatible with what's called Google congestion control,

02:56.000 --> 03:01.000
which is what Chrome and Firefox and Safari use. And this is in Rust.

03:01.000 --> 03:04.000
And to make that work, so we have a WebRTC implementation,

03:04.000 --> 03:10.000
but that did not do any of the actual encoding. It was left separate on purpose.

03:10.000 --> 03:16.000
Now we have a module in Rust plugin that will plug the encoder and WebRTC

03:16.000 --> 03:22.000
and do the congestion control so that you can adapt the bitrate of the encoder to the encoding.

03:22.000 --> 03:28.000
And this is all automatic if you use this plugin.

03:28.000 --> 03:34.000
We also have WIP and WEP. So these are also within Rust there.

03:34.000 --> 03:38.000
WIP and WEP are a way to replace RTMP, but based on WebRTC,

03:38.000 --> 03:45.000
so it's a single request, a single HTTP request way to set up a WebRTC stream.

03:45.000 --> 03:48.000
It's mostly meant to stream to and from a server.

03:48.000 --> 03:57.000
So it's really a replacement for RTMP for low latency video transmission.

03:57.000 --> 04:02.000
Speaking of WebRTC, WebRTC is based on RITP, and this is an area where they're also

04:02.000 --> 04:10.000
been quite a bit of development. So starting with 2022.1, 4D recorrection,

04:10.000 --> 04:16.000
that's a system used mostly for latency broadcast transmission,

04:16.000 --> 04:22.000
and we have the 2D 4D recorrection. So what does it mean 2D?

04:22.000 --> 04:27.000
It means that we do 4D recorrection, which is basically you absorb multiple packets,

04:27.000 --> 04:32.000
and you generate a new one. And if you have any of these packets except one,

04:32.000 --> 04:36.000
then you can regenerate the missing one. That's what the parallel recorrection is.

04:36.000 --> 04:39.000
Traditionally, you would do packets one, two, and three and four,

04:39.000 --> 04:41.000
and then you would generate a fifth packet.

04:41.000 --> 04:45.000
But losses tend to come in bursts in networks.

04:45.000 --> 04:49.000
So with 2D recorrection, we have this kind of traditional version,

04:49.000 --> 04:52.000
and also a version where you do packet one and five and ten, right?

04:52.000 --> 04:59.000
So if you have a burst, you can recover more of the packets.

04:59.000 --> 05:02.000
The other thing that we've added, so just for a long time,

05:02.000 --> 05:05.000
we've had APIs to add RTP header extensions.

05:05.000 --> 05:10.000
That's a way to reach packets to add a little header with something else.

05:10.000 --> 05:12.000
So for a long time, we had libraries to actually write these,

05:12.000 --> 05:18.000
but we didn't have something in the system to easily plug in

05:18.000 --> 05:21.000
something to insert this header in every packet

05:21.000 --> 05:23.000
without having to write application code.

05:23.000 --> 05:25.000
So this is something that we've added,

05:25.000 --> 05:28.000
and we've added a bunch of different modules.

05:28.000 --> 05:31.000
Notable is this client to mixer audio level.

05:31.000 --> 05:36.000
This makes it possible for a sender of audio to say,

05:36.000 --> 05:38.000
the volume that I'm sending is this,

05:38.000 --> 05:42.000
so that the add mixer or some kind of service

05:42.000 --> 05:45.000
can select the person who speaks loudest

05:45.000 --> 05:47.000
without having to decode all of the audio.

05:47.000 --> 05:50.000
So you can know from this level who's speaking louder

05:50.000 --> 05:54.000
and just forward that one.

05:54.000 --> 05:56.000
Then, Color Space.

05:56.000 --> 05:58.000
So this is for VP9.

05:58.000 --> 06:02.000
If you want to send HDR over VP9,

06:02.000 --> 06:08.000
we now have this archive access to make it work.

06:08.000 --> 06:10.000
It's compatible with Chrome again.

06:10.000 --> 06:13.000
So this is a place where people are experimenting with it.

06:13.000 --> 06:15.000
And we have an AP1 header in the builder.

06:15.000 --> 06:19.000
So this is, I think, for the first thing where we've decided

06:19.000 --> 06:23.000
that the official implementation of a major feature

06:23.000 --> 06:25.000
is only available in Rust.

06:25.000 --> 06:29.000
So this is something that we're pretty happy about.

06:29.000 --> 06:33.000
Another thing, so H.264, H.265, they have two kinds of timestamp.

06:33.000 --> 06:37.000
Presentation timestamp, decoding timestamp.

06:37.000 --> 06:41.000
When you send RTP, normally you only send a presentation timestamp.

06:41.000 --> 06:44.000
You need to apply an algorithm to recover the decoding timestamp.

06:44.000 --> 06:49.000
We now have modules that generate that.

06:49.000 --> 06:59.000
We also support RFC 6051, so that's a way to synchronize streams immediately.

06:59.000 --> 07:04.000
So traditionally with RTP, we send two streams, audio, video, separate timestamps.

07:04.000 --> 07:08.000
And then sometimes later you get a packet telling you what the correspondence is.

07:08.000 --> 07:13.000
With 6051, we add the RTP header extension in every packet

07:13.000 --> 07:17.000
so that we can be synchronized from the first packet.

07:17.000 --> 07:22.000
And we also improve our base class for video decoders a bit.

07:22.000 --> 07:28.000
So now it can recognize that there's a corruption

07:28.000 --> 07:32.000
and use that to request a retransmission.

07:32.000 --> 07:36.000
Previously, we did, we kind of applied the error,

07:36.000 --> 07:38.000
but we let the application do the decision.

07:38.000 --> 07:46.000
Now we've added something to the base class.

07:46.000 --> 07:50.000
Another big feature that was worked on,

07:50.000 --> 07:54.000
that we basically rewrote the HLS and dash base class.

07:54.000 --> 07:58.000
So the previous one was over 10 years old

07:58.000 --> 08:02.000
and had been written largely without the specs.

08:02.000 --> 08:07.000
And even when we had the specs, HLS has changed quite a lot the last 10 years.

08:07.000 --> 08:13.000
So now we have what we call a state-of-the-art implementation

08:13.000 --> 08:17.000
based on 10 years more knowledge.

08:17.000 --> 08:20.000
It's much more simple, has fewer trends,

08:20.000 --> 08:25.000
much better control on how we download things on the buffering.

08:25.000 --> 08:27.000
We do a little bit of the parsing in there

08:27.000 --> 08:30.000
because sadly for many of these for us, you have to have two parts

08:30.000 --> 08:33.000
that they stream to handle it properly.

08:33.000 --> 08:41.000
So this is all implemented as one in this decade.

08:41.000 --> 08:45.000
We've put a few things around decoding, mostly video decoding.

08:45.000 --> 08:48.000
One thing I'm quite excited about is the subframe decoding

08:48.000 --> 08:50.000
that has been quite a few years coming.

08:50.000 --> 08:55.000
And this means that we now have infrastructure in our base classes

08:55.000 --> 08:59.000
to start decoding a frame before you receive the entire frame.

08:59.000 --> 09:06.000
So some formats, issue six, version six, five,

09:06.000 --> 09:08.000
we can split the frame in slices and now, from this personnel,

09:08.000 --> 09:11.000
you can actually start doing the decoding.

09:11.000 --> 09:13.000
We have two implementations of this.

09:13.000 --> 09:17.000
One is based on a thing, which you can do with this only for issue six four.

09:17.000 --> 09:19.000
And the other one is for the Xilinx hardware

09:19.000 --> 09:22.000
because they have the hardware features to do that.

09:22.000 --> 09:27.000
So they can super low latency decoding.

09:27.000 --> 09:32.000
We have WebM alpha, so the WebM format.

09:32.000 --> 09:36.000
So each VPE, VPE9 don't have support for transparency built in.

09:36.000 --> 09:40.000
But there's a WebM extension where we basically store

09:40.000 --> 09:43.000
two separate video streams, one with the colors

09:43.000 --> 09:45.000
and one with the transparency.

09:45.000 --> 09:48.000
And now we have an element that will spin up two decoders

09:48.000 --> 09:54.000
and then recombine them into a 1v stream.

09:54.000 --> 09:58.000
We have a DirectX element library, so make it easier

09:58.000 --> 10:02.000
to integrate DirectX 3D element applications in the streamer

10:02.000 --> 10:09.000
to do zero copy encoding, for example, from a Windows application.

10:09.000 --> 10:13.000
And also speaking of Windows, our DirectX 3D element decoders

10:13.000 --> 10:15.000
are now the default.

10:15.000 --> 10:18.000
So they're becoming the choice that will get auto-plugged

10:18.000 --> 10:22.000
if you have them, so you get hardware acceleration decoding

10:22.000 --> 10:24.000
on the Windows that works.

10:26.000 --> 10:27.000
Yep?

10:27.000 --> 10:30.000
What about Mac codes?

10:30.000 --> 10:32.000
Yes, there's also...

10:32.000 --> 10:33.000
We've got a question already.

10:33.000 --> 10:35.000
Yeah, about Mac codes.

10:35.000 --> 10:39.000
So we have a hardware decoder for Mac and iOS,

10:39.000 --> 10:41.000
which is the same API.

10:41.000 --> 10:46.000
CUDA, so some people use proprietary software

10:46.000 --> 10:51.000
and proprietary drivers, so we have now also a CUDA library

10:51.000 --> 10:56.000
so that you can insert more easily CUDA data into the streamer

10:56.000 --> 11:00.000
for encoding, streaming, decoding, all of these things.

11:00.000 --> 11:03.000
We have some more CUDA-native elements,

11:03.000 --> 11:08.000
one that is a converter and a scaler, so using CUDA itself.

11:08.000 --> 11:12.000
We have CUDA and direct 3D integration for Windows again.

11:12.000 --> 11:16.000
And this whole thing basically gives you zero copy encoding

11:16.000 --> 11:19.000
on NVIDIA hardware, especially when you match

11:19.000 --> 11:23.000
with some other CUDA-based software.

11:25.000 --> 11:27.000
But some people prefer free software,

11:27.000 --> 11:32.000
so we also have the API, we have a new plugin for the API.

11:32.000 --> 11:36.000
So we've had this user of the API for a long, long time.

11:36.000 --> 11:38.000
It was getting quite freaky.

11:38.000 --> 11:40.000
It was not based on any of the base classes

11:40.000 --> 11:43.000
that we have improved since then.

11:43.000 --> 11:47.000
So it's been completely rewritten from scratch,

11:47.000 --> 11:50.000
with a new plugin that we call VA.

11:50.000 --> 11:53.000
It supports all the major codecs now,

11:53.000 --> 11:55.000
we've implemented all the ones with VA.

11:55.000 --> 12:00.000
Supports APY, it supports just 5, APA, DT90, MB2.

12:00.000 --> 12:04.000
Encoding, we only have the H2645 codecs for now,

12:04.000 --> 12:08.000
but the rest are being worked on, as we speak.

12:08.000 --> 12:14.000
And using libv, we have a bit more than the API.

12:14.000 --> 12:17.000
We have a bit more features, we have a compositor,

12:17.000 --> 12:20.000
so that's an element that will take two or more streams

12:20.000 --> 12:24.000
and composite them into the same video.

12:24.000 --> 12:29.000
We have the Dinterlacer, and we have a post-prosterer element

12:29.000 --> 12:32.000
with scaling and color space conversion

12:32.000 --> 12:39.000
using the video functionality instead of the GPU.

12:39.000 --> 12:43.000
And all of our work has happened on AV1 also,

12:43.000 --> 12:46.000
in the last two years.

12:46.000 --> 12:51.000
So we have quite a lot now, we have support for AV1,

12:51.000 --> 12:55.000
both in the legacy VA plugin and in the new VA plugin.

12:55.000 --> 12:58.000
We have it for AMD, the coders,

12:58.000 --> 13:04.000
we have it for Direct3D on Windows, using the NPD APIs.

13:04.000 --> 13:09.000
For Intel, using their multiple libraries,

13:09.000 --> 13:12.000
either QuickSync, QSV, or the new SDK.

13:12.000 --> 13:15.000
So we have pretty comprehensive AV1 support,

13:15.000 --> 13:20.000
in addition to the RTD plugin that I mentioned earlier.

13:20.000 --> 13:23.000
Another new thing that we've done is,

13:23.000 --> 13:27.000
this is our first official machine learning integration

13:27.000 --> 13:29.000
that is in G-streamer itself.

13:29.000 --> 13:31.000
So it's a first step.

13:31.000 --> 13:38.000
And we've written a plugin to use the Onyx runtime from Microsoft

13:38.000 --> 13:43.000
to basically take some video frames,

13:43.000 --> 13:50.000
some 3D model and recognize objects,

13:50.000 --> 13:53.000
put little boxes in the metadata,

13:53.000 --> 13:56.000
and then another element that can take these boxes

13:56.000 --> 13:58.000
and draw them on the image.

13:58.000 --> 14:01.000
So this is the first step, a lot of work is happening right now

14:01.000 --> 14:05.000
to have a better video analytics framework as part of G-streamer.

14:10.000 --> 14:14.000
All these things are nice, but sometimes you want to have a UI.

14:14.000 --> 14:18.000
And a few features were recently added there.

14:18.000 --> 14:23.000
We have a GTK4 paintable, so that's an object.

14:23.000 --> 14:27.000
I would say that you can use the GTK4 to actually draw something on the screen.

14:27.000 --> 14:34.000
So now you can easily integrate G-streamer with GTK4, 0.P, playback.

14:34.000 --> 14:37.000
This one is in Rust, which is kind of cool.

14:37.000 --> 14:42.000
Qt 6 also has a theme, so that we have something that is very similar to what we have for Qt 5,

14:42.000 --> 14:47.000
which is a QML item, so that you can integrate a G-streamer sync,

14:47.000 --> 14:52.000
the output width, Qt, and draw in your Qt application.

14:52.000 --> 14:57.000
And the last one is a bit of a niche case.

14:57.000 --> 15:00.000
So we had a Wayland sync for a long time,

15:00.000 --> 15:04.000
and what this Wayland sync allows you to do is basically take the video buffer

15:04.000 --> 15:09.000
and give it to the Wayland compositor directly without going through the toolkit.

15:09.000 --> 15:14.000
So you can use the 2D hardware planes of the platform.

15:14.000 --> 15:17.000
This is multi-use, useful, and embedded.

15:17.000 --> 15:22.000
It allows you to do things like greater performance, not use the GPU.

15:22.000 --> 15:26.000
Unembedded systems or the GPU might be too slow to do.

15:26.000 --> 15:32.000
Up to now, this all works fine, but you have to write low-level Wayland code,

15:32.000 --> 15:35.000
and that's non-trivial.

15:35.000 --> 15:39.000
So we've written a GTK widget that wraps that.

15:39.000 --> 15:42.000
So now you can write your application in GTK, just add the widget,

15:42.000 --> 15:47.000
and you get all of these performance benefits for free.

15:47.000 --> 15:54.000
Last but not least, in this category, we added touch event navigation.

15:54.000 --> 15:57.000
So previously we had navigation.

15:57.000 --> 15:59.000
You could send letters, you could send mouse clicks,

15:59.000 --> 16:04.000
but now we can also send touch events so that you can have elements in your pipeline

16:04.000 --> 16:09.000
that are controlled by the user, such as we have a WebR app,

16:09.000 --> 16:15.000
a WebView, a WebKit-based source.

16:15.000 --> 16:17.000
And we have some new tracers.

16:17.000 --> 16:22.000
So these are tools for developers to know actually what's going on in a pipeline live.

16:22.000 --> 16:27.000
We have a bunch of tracers already, four more were added.

16:27.000 --> 16:29.000
Some of them are quite useful.

16:29.000 --> 16:35.000
We have one to generate, read the buffer lateness, and one to trace the Q-level,

16:35.000 --> 16:39.000
and these will output the information in a CSV file that you can then load

16:39.000 --> 16:42.000
and make nice graphs and understand, like, what's the live performance

16:42.000 --> 16:45.000
of your pipeline, what's going on.

16:45.000 --> 16:49.000
We have one to draw pretty pipeline snapshots.

16:49.000 --> 16:52.000
So for a long time, we had a feature where we could draw a DOS file

16:52.000 --> 16:56.000
to draw a picture of the pipeline, but this required it,

16:56.000 --> 16:59.000
added some code to your application to actually trigger it.

16:59.000 --> 17:03.000
With this tracer, now you can just listen to a Unix signal

17:03.000 --> 17:05.000
and trigger it on the spot.

17:05.000 --> 17:07.000
The last one is the factory tracer.

17:07.000 --> 17:11.000
This is the very first feature I mentioned.

17:11.000 --> 17:15.000
So it's nice to say, oh, I'm going to build a G-streamer,

17:15.000 --> 17:18.000
build specifically for my application with only the elements I use.

17:18.000 --> 17:19.000
But if you use Play

17:19.000 --> 17:21.000
then there's a lot of automated things,

17:21.000 --> 17:25.000
and you might not know exactly which plugins you've been using.

17:25.000 --> 17:28.000
So with this tracer, we can actually trace all the plugins that get loaded,

17:28.000 --> 17:32.000
all the elements that are used, and print, when you use the application,

17:32.000 --> 17:36.000
print the list of what was actually used.

17:36.000 --> 17:39.000
Question about that.

17:39.000 --> 17:44.000
Playbin sometimes tries to use plugin, but it's not related because it is working.

17:44.000 --> 17:47.000
It's going to be listed in a bit.

17:47.000 --> 17:48.000
So yeah, sorry.

17:48.000 --> 17:50.000
Playbin sometimes tries to use something,

17:50.000 --> 17:54.000
but it doesn't work because the hardware is not there or something.

17:54.000 --> 17:58.000
So in this case, the tracer will still list it.

17:58.000 --> 18:00.000
It's everything that is loaded, right?

18:00.000 --> 18:04.000
When they're tried or loaded, you call a function in it and it says no.

18:04.000 --> 18:08.000
So let's just really get the loading stage.

18:10.000 --> 18:13.000
Thank you. Any questions? Yes.

18:13.000 --> 18:16.000
You mentioned the RTP support in there.

18:16.000 --> 18:20.000
So did you also add the SPC extension to that?

18:20.000 --> 18:23.000
I have no idea.

18:23.000 --> 18:26.000
Anyone knows? No.

18:26.000 --> 18:31.000
RTC is only one extension, SVC extension, in...

18:31.000 --> 18:33.000
I don't know if it's there.

18:33.000 --> 18:40.000
We do layer selection of the highest quality right now, but it's not.

18:40.000 --> 18:42.000
There is an external...

18:42.000 --> 18:45.000
There is a dependency in the script turner to the extension

18:45.000 --> 18:48.000
where you can read information about the SPC layers in there,

18:48.000 --> 18:50.000
which is under that.

18:50.000 --> 18:53.000
So you encode the SPC layers into one screen,

18:53.000 --> 18:58.000
then you use these external extensions to carry information about what's in there

18:58.000 --> 19:01.000
and put these in so that you can do a good collection for that.

19:01.000 --> 19:03.000
So that's what I have to do.

19:03.000 --> 19:07.000
None of the RTP SPC stuff, including the VP1,

19:07.000 --> 19:11.000
which is quickly required because there's no SVC inside the screen.

19:11.000 --> 19:13.000
It's not implemented yet. Thank you.

19:13.000 --> 19:14.000
And supervise.

19:14.000 --> 19:19.000
Yes. So the question was, RTP, AV1, SVC, there's an extension

19:19.000 --> 19:21.000
to actually make it really useful.

19:21.000 --> 19:24.000
It's being standardized and it's not implemented yet.

19:24.000 --> 19:27.000
But it will be at some point.

19:27.000 --> 19:31.000
I forgot to mention we have an online question,

19:31.000 --> 19:35.000
an online question at number 43401.

19:35.000 --> 19:39.000
So I'm at home on that question as possible.

19:39.000 --> 19:43.000
But since we don't have any questions, are there any more questions on the floor?

19:43.000 --> 19:44.000
There's one there.

19:44.000 --> 19:47.000
Q6, does it support different rendering backends,

19:47.000 --> 19:51.000
direct picks on Windows and both and on Linux or something?

19:51.000 --> 19:55.000
Will there be QML and then Q6 support those different stuff

19:55.000 --> 19:59.000
because it directly talks directly to QML?

19:59.000 --> 20:04.000
So does Q6 support other backends than OpenGL?

20:04.000 --> 20:11.000
And I think the answer right now is will only support OpenGL.

20:11.000 --> 20:13.000
Are you all pushed in? Yes.

20:13.000 --> 20:15.000
Can you do a statically linked winery?

20:15.000 --> 20:17.000
Can you do a statically linked winery? Yes, you can.

20:17.000 --> 20:19.000
That's kind of one of the use cases,

20:19.000 --> 20:21.000
that we create this statically linked library

20:21.000 --> 20:23.000
and then your application can link to it

20:23.000 --> 20:25.000
and only link the required bits.

20:25.000 --> 20:29.000
That's kind of part of the trick to make it a little smaller.

20:35.000 --> 20:39.000
Q7, there is a congestion control that works as you know.

20:39.000 --> 20:40.000
Yes, you are.

20:40.000 --> 20:48.000
Is it the same feature set as in Google's implementation?

20:48.000 --> 20:52.000
The question is about congestion control in WebRTC.

20:52.000 --> 20:55.000
Is it the same feature set as the Google implementation?

20:55.000 --> 21:01.000
As far as I know, yes, because it's basically a copy of the implementation in Rust.

21:01.000 --> 21:06.000
So they basically re-implement the same algorithm.

21:06.000 --> 21:09.000
So that is compatible.

21:09.000 --> 21:11.000
But it's pluggable, so you could write your own.

21:11.000 --> 21:14.000
There is actually a plug-in mechanism.

21:14.000 --> 21:17.000
The core representation is in C,

21:17.000 --> 21:19.000
but this one is in Rust with a separate plug-in.

21:19.000 --> 21:23.000
One could write a different implementation.

21:23.000 --> 21:26.000
Because there's a bunch of heuristics in there.

21:26.000 --> 21:30.000
There's no perfect answer.

21:30.000 --> 21:32.000
Thank you so much.

21:32.000 --> 21:34.000
Do you have a question? Yes.

21:34.000 --> 21:39.000
If I have an application that does WebRTC signaling over matrix, for example,

21:39.000 --> 21:43.000
would I benefit from switching to WebRTC sync,

21:43.000 --> 21:46.000
or would I stick with WebRTC sync?

21:46.000 --> 21:50.000
So the question is, if you have an implementation of WebRTC

21:50.000 --> 21:52.000
that does signaling something custom,

21:52.000 --> 21:56.000
for example, other matrix, would you benefit using WebRTC sync?

21:56.000 --> 22:00.000
The answer to that is yes,

22:00.000 --> 22:07.000
because it will do all the encoding and congestion control hooked up for you.

22:07.000 --> 22:12.000
And there's an interface that's actually implemented for your own signal.

22:12.000 --> 22:15.000
So the signaling is separate from this WebRTC sync.

22:15.000 --> 22:18.000
There's still a module that we can implement.

22:18.000 --> 22:22.000
We have one implemented for, like, custom signaling mechanism,

22:22.000 --> 22:25.000
where there's a server that's implemented, but people write your own.

22:25.000 --> 22:27.000
In Rust.

22:27.000 --> 22:35.000
Last question? No, just a comment to the question before the QT6 started 3D integration.

22:35.000 --> 22:38.000
There is a merge request for it.

22:38.000 --> 22:49.000
Okay. So Tim says that for the QT, there's a direct 3D merge request open

22:49.000 --> 22:53.000
to integrate that. That's not merged yet, but do test it at home

22:53.000 --> 22:55.000
and complain when it doesn't work.

22:55.000 --> 22:59.000
Last question? No? Okay, thank you, everybody.
