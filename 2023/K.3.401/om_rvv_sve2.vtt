WEBVTT

00:00.000 --> 00:07.000
All right.

00:07.000 --> 00:28.640
Yes, go ahead.

00:28.640 --> 00:29.960
We continue with our next speaker,

00:29.960 --> 00:32.280
which is kind of the follow up of the previous one,

00:32.280 --> 00:34.040
because it's approximately the same topic,

00:34.040 --> 00:36.360
but this time about risk five and arm.

00:36.360 --> 00:37.960
So please welcome Renny.

00:37.960 --> 00:42.840
Renny.

00:42.840 --> 00:45.320
I heard good afternoon, everyone.

00:45.320 --> 00:47.480
I hope you are done with the digestion.

00:47.480 --> 00:51.280
So yes, this pretty much follows up

00:51.280 --> 00:54.320
on Keren's previous speech.

00:54.320 --> 00:57.640
But before I go into the details,

00:57.640 --> 00:59.920
obviously, I work for big companies,

00:59.920 --> 01:01.400
so I have to put this disclaimer.

01:01.400 --> 01:05.000
And then if I speak too fast or if I don't articulate properly,

01:05.000 --> 01:08.400
please stop me.

01:08.400 --> 01:10.120
With that said, who am I?

01:10.120 --> 01:11.360
I don't think it matters much.

01:11.360 --> 01:12.880
Well, this is my 16th time in F

01:12.880 --> 01:16.640
and it's only my first presentation, so bear with me.

01:16.640 --> 01:18.520
Having said that, I don't work in this field at all,

01:18.520 --> 01:20.840
so just a fancy thing for me to do.

01:20.840 --> 01:23.000
So some history.

01:23.000 --> 01:25.400
So has anybody ever seen this?

01:25.400 --> 01:28.520
Outside the computer museum, right?

01:28.520 --> 01:29.560
Yeah, so that's the K1.

01:29.560 --> 01:33.160
It's the first vector processor.

01:33.160 --> 01:36.400
It's from the late or second half of the 70s.

01:36.400 --> 01:38.880
I wasn't even born back then.

01:38.880 --> 01:41.200
But I'm pointing it's the first vector processor

01:41.200 --> 01:43.760
and we all now finally, after almost 50 years,

01:43.760 --> 01:45.600
are coming back to this kind of,

01:45.600 --> 01:47.200
maybe coming back to this approach

01:47.200 --> 01:50.080
to calculations in computers.

01:50.080 --> 01:51.600
But for people in my generation,

01:51.600 --> 01:55.360
this is more what we associate with the SIMD

01:55.360 --> 01:56.480
for multimedia.

01:56.480 --> 02:00.400
So this is a pod, the first video game

02:00.400 --> 02:01.880
that I could use the MMX,

02:01.880 --> 02:06.400
which MMX being the first SIMD extensions

02:06.400 --> 02:09.440
in the consumer business, in the consumer space, let's say.

02:10.640 --> 02:14.360
So as I said, the MMX came in 1997

02:14.360 --> 02:16.000
and that was 64-bit vector,

02:16.000 --> 02:18.880
so you could compute over 64-bit at a time.

02:18.880 --> 02:21.680
Mind that back then, computers was pretty much only 32-bits.

02:22.720 --> 02:24.440
And two years later, I came SSE

02:24.440 --> 02:26.880
and many, many, many versions of SSE, SSE2,

02:26.880 --> 02:30.640
which is more popular in multimedia use cases, 2000.

02:30.640 --> 02:32.600
I'm not gonna go through all the details of SSE

02:32.600 --> 02:35.600
because there's like a billion, million versions.

02:35.600 --> 02:39.720
And AVX1 came in 2008, AVX2,

02:39.720 --> 02:41.720
which Kian mentioned came in 2011.

02:41.720 --> 02:46.720
That was the first to have 256-bits vectors.

02:48.360 --> 02:52.800
Then AVX512, which was the topic of the previous presentation,

02:52.800 --> 02:56.240
officially came in 2013, but as Kian mentioned,

02:56.240 --> 03:00.240
the only real proper CPUs were only out in 2017.

03:02.800 --> 03:06.320
On ARM side, the first SIMD was actually 32-bit

03:06.320 --> 03:09.040
only on ARM B6, 2002.

03:10.000 --> 03:11.640
That doesn't really seem to make sense,

03:11.640 --> 03:13.960
but that's because it's basically calculating

03:13.960 --> 03:16.880
as a four times eight-bit, so two times 16-bit at a time.

03:18.520 --> 03:22.760
Then 128-bits came, there was no 64-bit SIMD on ARM.

03:22.760 --> 03:26.400
128-bit came with ARM E7, so got XA8,

03:26.400 --> 03:31.400
in usually called NEON, officially called Advanced SIMD

03:31.480 --> 03:35.880
in 2005, and on ARM V8, it's pretty much the same.

03:35.880 --> 03:38.080
Now it's not actually compatible on like on X86,

03:38.080 --> 03:41.040
64, 64, the same, but came in,

03:41.040 --> 03:43.080
well, came with basically ARM V8 in 2012.

03:43.080 --> 03:46.040
It's also officially called Advanced SIMD,

03:46.040 --> 03:48.000
and it's also colloquially known as NEON.

03:49.480 --> 03:51.800
As for RACE-5, well, RACE-5 is much more recent.

03:51.800 --> 03:53.120
There is no SIMD.

03:54.320 --> 03:57.200
Problem, and I've only summarized,

03:57.200 --> 03:58.360
this is only a short summary,

03:58.360 --> 04:00.760
there's way more extension, especially on the 86 side,

04:00.760 --> 04:03.280
is that every damn time you have to rewrite your assembly,

04:03.280 --> 04:06.560
and as the question then answers in the previous talks

04:06.560 --> 04:08.920
and even the previous previous talk covered,

04:08.920 --> 04:10.920
this is kind of time-consuming.

04:10.920 --> 04:15.720
So with that said, this was all fixed size SIMD,

04:15.720 --> 04:19.240
so what about viable length SIMD,

04:19.240 --> 04:21.600
which is what we will be talking about today.

04:21.600 --> 04:24.120
So how would you go about doing it?

04:24.120 --> 04:27.480
Well, simple way to do it is to just ask the CPU

04:27.480 --> 04:29.680
what is your vector size,

04:29.680 --> 04:31.640
and if you do S5, this is how you do it.

04:31.640 --> 04:35.040
So control register rate operation,

04:35.040 --> 04:37.960
the vector is called VLANB for vector length in bytes,

04:37.960 --> 04:40.480
and it will store in this case, T0, whatever,

04:40.480 --> 04:44.360
it's one register, the number of bytes in a vector,

04:44.360 --> 04:45.960
and with that you could then iterate.

04:45.960 --> 04:47.880
So if you want to know the number of elements,

04:47.880 --> 04:50.320
well, you have to do a left shift to,

04:50.320 --> 04:53.320
you have to compute the number of elements,

04:53.320 --> 04:54.880
so if you want to have 32-bit elements,

04:54.880 --> 04:57.520
you divide by four, shift by two bits.

04:57.520 --> 04:59.880
You could do it like that,

04:59.880 --> 05:01.720
and then you would write your main,

05:01.720 --> 05:02.560
you would take your C loop,

05:02.560 --> 05:03.840
you would convert it into assembly,

05:03.840 --> 05:06.120
to operate on over many elements at a time,

05:06.120 --> 05:11.360
then you would probably, if you have space in your vector bank,

05:11.360 --> 05:13.440
you'd probably, in order to eliminate,

05:13.440 --> 05:15.440
try to heat up the latency a little bit,

05:15.440 --> 05:17.320
because usually between instructions,

05:17.320 --> 05:19.120
if you operate only on one data set,

05:19.120 --> 05:21.320
you will have inter-instruction latencies,

05:21.320 --> 05:22.920
which are going to hurt your performance,

05:22.920 --> 05:26.760
so you're typically in multimedia and all twice,

05:26.760 --> 05:29.600
so you will do work over two set of vectors

05:29.600 --> 05:31.320
at the same time in parallel,

05:31.320 --> 05:33.200
and when you have done all of that,

05:33.200 --> 05:35.720
you will be working on over many like say 32-bit,

05:35.720 --> 05:38.320
32, sorry, 32-item, 32 elements at a time,

05:38.320 --> 05:39.400
so you have to deal with edges,

05:39.400 --> 05:40.800
because you might not have a multiple

05:40.800 --> 05:43.000
of 32 elements that you are dealing with,

05:43.000 --> 05:45.600
and that's fine, and that's one way to do it.

05:45.600 --> 05:47.520
In fact, as I checked,

05:47.520 --> 05:50.600
that's how Clang LLVM does the tree vectorization on RISC-V,

05:50.600 --> 05:52.120
if you enable it, and so you have,

05:52.120 --> 05:54.360
it literally starts by reading the vector length,

05:54.360 --> 05:57.480
and then deal with edges and all twice,

05:57.480 --> 06:01.440
and if it manages to implement tree,

06:01.440 --> 06:03.000
I mean, if you have enabled tree vectorization,

06:03.000 --> 06:05.680
and you have enabled RISC-V vectors,

06:05.680 --> 06:09.440
but that's not really how you want to do it,

06:09.440 --> 06:12.840
but before we go on how to actually do it,

06:12.840 --> 06:15.120
what vector lengths are we dealing with here?

06:15.120 --> 06:20.120
So obviously, whereas mentioned earlier,

06:21.000 --> 06:25.840
common values are 128 and 512 bits,

06:25.840 --> 06:29.080
so both ARM and RISC-V are ground T's

06:29.080 --> 06:31.880
that even if you have the vibratory vector lengths,

06:31.880 --> 06:34.320
it's going to be at least 128 bits,

06:34.320 --> 06:37.320
and it's also going to be a power of two bits,

06:37.320 --> 06:39.760
which is kind of convenient for our calculations.

06:39.760 --> 06:44.760
So as far as I've seen,

06:44.760 --> 06:47.560
there are announcements for real hardware,

06:47.560 --> 06:50.560
which would have 256 and 512 bits,

06:50.560 --> 06:55.920
that you should be able to buy at some point in the near future.

06:56.440 --> 07:03.760
More crazy stuff, I've seen actually designs also being announced with 1000 bits.

07:03.760 --> 07:05.960
I don't know if they're going to store all those bits in

07:05.960 --> 07:11.120
the physical register bank, but it would be interesting if it happens.

07:11.120 --> 07:15.080
I haven't seen theoretical designs at 4000 bits,

07:15.080 --> 07:18.760
and theoretical to the point that there is a schematic.

07:18.760 --> 07:22.840
Theoretically, in this case, I mean that there are actual schematics of how you could

07:22.840 --> 07:25.040
write a chip and they have

07:25.040 --> 07:28.880
a simulation of the performance that the chip would get into the algorithms.

07:28.880 --> 07:35.520
As to whether it's actually practically implementable in an existing industrial process.

07:35.520 --> 07:39.240
I don't know, I'm not a specialist in electronics,

07:39.240 --> 07:41.680
but that sounds a little bit questionable.

07:41.680 --> 07:44.120
So in theory, on the syntactic level,

07:44.120 --> 07:45.640
so in the instruction and coding level,

07:45.640 --> 07:49.240
you can have up to two power of 16 bits,

07:49.240 --> 07:52.960
at least on H5, I'm not sure about how much you need.

07:52.960 --> 07:56.560
So how you actually do vector length?

07:56.560 --> 08:02.040
Are you supposed to do a viable vector length SIMD or vector processing as it's often called,

08:02.040 --> 08:05.880
particularly vector and SIMD synonyms.

08:06.200 --> 08:09.160
Well, first you have to use predication,

08:09.160 --> 08:15.840
which is very highly prevalent in variable vector length scenarios.

08:15.840 --> 08:17.840
Now, it's not a completely new concept.

08:17.840 --> 08:22.240
Here I mentioned the K mask in AVX,

08:22.240 --> 08:24.960
so AVX also has predication.

08:24.960 --> 08:28.080
But in variable vector length,

08:28.080 --> 08:31.320
it's really essential because this is

08:31.320 --> 08:35.600
basically the programming model on variable vector lengths.

08:35.600 --> 08:39.040
No, loops is essentially built on predication,

08:39.040 --> 08:41.960
and that's true both for ARM and H5.

08:41.960 --> 08:44.920
So a predicate is a vector on Boolean,

08:44.920 --> 08:47.240
so like the K mask in x86,

08:47.240 --> 08:51.880
it's called the P vector in ARM,

08:51.880 --> 08:55.560
and it's the mask vector in H5.

08:55.560 --> 08:58.360
As Ciaran said, kind of repeating,

08:58.360 --> 09:02.920
but it just specifies which of the element in your vector,

09:02.920 --> 09:08.240
it specify which ones you will be loading or modifying or storing out of a given instruction.

09:08.240 --> 09:13.600
So if it's a load instruction which values you load from memory and overwrite into the register,

09:13.600 --> 09:15.200
if it's a store instructions going to be the other way,

09:15.200 --> 09:19.520
which values in memory are going to overwrite versus which ones are going to live as they are.

09:19.520 --> 09:22.560
If it's a calculation instruction vector to vector,

09:22.560 --> 09:27.880
then it's going to affect which results are actually stored into the register,

09:27.880 --> 09:30.560
versus which ones are just discarded.

09:30.560 --> 09:33.720
So on ARMv9 or SVE,

09:33.720 --> 09:38.880
one way you would typically do now your SVE loop instead of say your neuron loop,

09:38.880 --> 09:42.000
is you would start by counting down,

09:42.000 --> 09:44.560
you would initialize say extend to zero,

09:44.560 --> 09:46.800
and then you would,

09:46.800 --> 09:52.760
so you have to imagine here that you have your actual neuron or SVE loop.

09:52.760 --> 09:54.360
So you will check,

09:54.360 --> 09:58.280
you have this funny instruction which is actually called while LTA or while LO,

09:58.280 --> 10:00.520
and you initialize the P0 vector in this case,

10:00.520 --> 10:03.000
which is one of the predicate register,

10:03.000 --> 10:06.560
to say that essentially,

10:06.560 --> 10:09.960
you want to count how many elements you still have in your input data.

10:09.960 --> 10:11.200
So here we have X0,

10:11.200 --> 10:15.200
we imagine that X0 is the number of elements we've been given to this function,

10:15.200 --> 10:17.120
extend is count of of how we've been,

10:17.120 --> 10:19.240
so it's our iterator.

10:19.640 --> 10:23.280
Essentially, what we'll say is as long as extend is

10:23.280 --> 10:26.480
larger as long as the number of elements we still have.

10:26.480 --> 10:31.800
So as long as X0 is larger than the size of the vector that the CPU can handle,

10:31.800 --> 10:34.560
we'll just set the predicate to handle to be clear.

10:34.560 --> 10:39.240
So we'll use the full size of the vector for our programming.

10:39.240 --> 10:43.240
Once the number of elements is more than zero,

10:43.240 --> 10:46.320
but less, strictly less than the vector size in the CPU can handle,

10:46.320 --> 10:50.160
then we'll start basically just ignoring the values at the end of the vector.

10:50.160 --> 10:53.200
So we'll have a bunch of ones and then at the end a bunch of zeros.

10:53.200 --> 10:56.360
This is how you abstract away and hide away the complexity of dealing

10:56.360 --> 10:58.880
with the edge in your loop.

10:58.880 --> 11:01.080
So essentially, by doing this,

11:01.080 --> 11:03.440
you don't care what is the actual capacity of,

11:03.440 --> 11:06.960
you don't actually need at any point to know how many elements are

11:06.960 --> 11:10.720
dealing with any interaction of your loop because it's all hidden away by the,

11:10.720 --> 11:13.520
essentially, the size of the vector and the size of the predicate are matched,

11:13.520 --> 11:14.760
so you don't actually care.

11:14.760 --> 11:18.000
You also don't need to deal with edges because when there's

11:18.000 --> 11:20.880
one or two or three or four elements left over at the end,

11:20.880 --> 11:23.440
you can just deal with them in the lattiteration,

11:23.440 --> 11:27.360
which of course will be a little bit less efficient than using

11:27.360 --> 11:29.560
the full size of the vector but it's still much faster than having

11:29.560 --> 11:32.040
a separate edge if only because you will not be

11:32.040 --> 11:35.240
stressing the instruction cache of the CPU.

11:35.840 --> 11:38.760
So that's predication.

11:38.760 --> 11:41.480
Now, unrolling. If you thought about it,

11:41.480 --> 11:43.880
what I just said with predication,

11:43.880 --> 11:47.280
it doesn't really work with unrolling because now you've counted down,

11:47.280 --> 11:50.320
you've set your predicate vector to count down how many elements you

11:50.320 --> 11:53.800
have still in your total count of elements.

11:53.800 --> 11:57.200
You can't and all because now you said,

11:57.200 --> 11:58.200
oh, I have 10 elements left,

11:58.200 --> 12:01.080
I'm going to use 10 elements in my vector but if you have,

12:01.080 --> 12:05.280
it just doesn't work because if you had one and a half vector left,

12:05.280 --> 12:07.960
you would want to have one predicate with all the bits set

12:07.960 --> 12:09.440
and another predicate with half of the bits set,

12:09.440 --> 12:11.360
this doesn't really work very well.

12:11.360 --> 12:14.720
Yes, it's a bit of a hot take.

12:14.720 --> 12:17.840
Maybe I will never be ever again allowed to write a system pec code after this,

12:17.840 --> 12:23.160
but just don't unroll if you use a viable vector lengths.

12:23.160 --> 12:27.480
Now, there may be cases where you can't unroll because naturally have

12:27.480 --> 12:33.480
parallel some kind of parallel in your design aspect in your algorithm.

12:33.480 --> 12:37.760
But the idea of vector processing is

12:37.760 --> 12:41.400
that we have higher latency and larger vectors,

12:41.400 --> 12:47.480
which in the end result in higher throughput and we shouldn't need to unroll.

12:47.480 --> 12:50.320
I'm sure you will find actual designs,

12:50.320 --> 12:53.800
real hardware, real processor where it will be faster if you do an all,

12:53.800 --> 12:58.080
and how much you need to unroll will depend on the design.

12:58.480 --> 13:03.080
Of course, if you are trying to squeeze the very last MIPS

13:03.080 --> 13:06.160
out of a given specific piece of hardware,

13:06.160 --> 13:09.760
then maybe you should unroll but I think generally speaking,

13:09.760 --> 13:12.400
at least you shouldn't start by unrolling.

13:12.400 --> 13:15.200
Another interesting thing to keep in mind,

13:15.200 --> 13:21.960
which I already mentioned in the previous slide is that you don't have alignment issues.

13:21.960 --> 13:25.160
So one common problem with SIMD instruction set is that

13:25.160 --> 13:27.840
the load and store instructions require over-aligned data,

13:27.840 --> 13:29.840
typically aligned on the side of the vector,

13:29.840 --> 13:33.680
which is very inconvenient when you're operating from C or C plus plus code,

13:33.680 --> 13:37.080
because it's usually C or C plus plus allocator will only allocate

13:37.080 --> 13:40.240
aligned on whatever the ABI specifies which are now on

13:40.240 --> 13:44.560
B8 would be 16 bytes for the stack and eight bytes for the heap.

13:44.560 --> 13:47.160
So usually, well,

13:47.160 --> 13:49.960
at least both SV and RISC-V vectors,

13:49.960 --> 13:51.440
they don't need the alignment,

13:51.440 --> 13:54.120
it is only the alignment of the element and it's not the alignment,

13:54.120 --> 13:55.520
it's not the size of the vector.

13:55.520 --> 14:00.440
So if you are operating on say four bytes pieces of data,

14:00.440 --> 14:03.760
data elements, then you only need your vectors to be aligned on four bytes,

14:03.760 --> 14:07.400
which is a very nice property for dealing especially on the edge,

14:07.400 --> 14:10.720
on edge cases and also you don't have to deal with like,

14:10.720 --> 14:13.280
if you have one input that is

14:13.280 --> 14:16.760
aligned that is perfectly aligned and the output is not perfectly aligned,

14:16.760 --> 14:19.680
then you end up having this weird mismatch and you end up

14:19.680 --> 14:21.400
having to deal with different edge cases,

14:21.400 --> 14:22.600
it's really a mess.

14:22.600 --> 14:24.600
With vector processing, you don't do that.

14:24.600 --> 14:27.400
So you don't actually have to worry about it.

14:28.120 --> 14:31.480
So with that, we've covered generality.

14:31.480 --> 14:34.000
So how is it looking on ARM side?

14:34.000 --> 14:37.800
Then we'll see RISC-V side because it's a bit weird if I would,

14:37.800 --> 14:39.760
I sort of like to put everything together,

14:39.760 --> 14:40.760
but then it becomes a huge mess.

14:40.760 --> 14:43.200
So it's going to be a bit repetitive because of course,

14:43.200 --> 14:44.720
there's a lot of similarities.

14:44.720 --> 14:50.240
But so SVE came about like five years ago,

14:50.240 --> 14:52.160
a little bit more than five years ago,

14:52.160 --> 14:56.680
I think it was announced late 2016 if I recall correctly.

14:56.680 --> 14:58.800
It was pretty much with some intermediate.

14:58.800 --> 15:01.640
It was explicitly meant for all the things like

15:01.640 --> 15:04.920
scientific applications or

15:04.920 --> 15:08.960
engineering modeling and then it's kind of stuff,

15:08.960 --> 15:13.840
my HPC and so nobody used it.

15:13.840 --> 15:16.160
At least nobody in this room used it.

15:16.160 --> 15:19.040
This was fixed with SVE2,

15:19.040 --> 15:23.320
which is sometimes called ARMv9 because it kind of

15:23.320 --> 15:28.680
comes with ARMv9 but it's really called SVE2.

15:28.680 --> 15:32.040
Fixed that issue, they realize that actually this is a good idea.

15:32.040 --> 15:34.280
This pattern programming model is also interesting from

15:34.280 --> 15:39.320
Intimedia and crypto which was also missing from SVE1.

15:39.440 --> 15:41.760
So what they did is they just took,

15:41.760 --> 15:47.280
yes, so which Neomnemonics are missing and added those,

15:47.280 --> 15:48.760
and it's pretty much the same Neomnemonics,

15:48.760 --> 15:49.920
you just add your prime,

15:49.920 --> 15:51.560
the predicate register.

15:51.560 --> 15:53.920
That's what it's of course a little bit more complicated.

15:53.920 --> 15:55.680
But as I mentioned,

15:55.680 --> 15:57.440
you just use a while instruction which

15:57.440 --> 15:59.560
will then provision your predicate,

15:59.560 --> 16:01.720
then you have to pick the element size so that of course,

16:01.720 --> 16:03.980
this adds up correctly and at the end,

16:03.980 --> 16:05.560
you have a new set of branch conditions,

16:05.560 --> 16:08.760
so first element, last element, and so on and so forth.

16:10.160 --> 16:14.320
So the remaining elements will be determined by

16:14.320 --> 16:16.440
the predicate register and the predicate register which

16:16.440 --> 16:20.360
is the condition flag and the while instruction will also subtract.

16:20.360 --> 16:22.320
There is a count, the number of

16:22.320 --> 16:25.520
a processed element from your output register.

16:25.520 --> 16:29.560
Yes, this one stop pretending that I'm missing risk.

16:29.560 --> 16:31.120
How do you detect this stuff?

16:31.120 --> 16:33.480
So there's a processor macro,

16:33.480 --> 16:34.760
otherwise as usual on ARMv8,

16:34.760 --> 16:38.880
you have a bunch of privileged register for the OS to look at,

16:38.880 --> 16:40.320
and then you have also on Linux,

16:40.320 --> 16:43.640
you have a bunch of flags in the OCI vector bits,

16:43.640 --> 16:45.280
so it's all classic,

16:45.280 --> 16:48.280
otherwise you're out of luck.

16:48.280 --> 16:51.640
Availability, so as we said 2016,

16:51.640 --> 16:53.200
but they didn't really work for us,

16:53.200 --> 16:55.480
as V2 was specified in 2019.

16:55.480 --> 17:00.040
But so the real hardware came earlier last year,

17:00.040 --> 17:03.120
so Cortex-X2 and all the other things

17:03.120 --> 17:05.680
from dynamic IQ 110.

17:06.320 --> 17:10.880
So Samsung actually has 2200 and so forth.

17:10.880 --> 17:12.080
If you find your cases are Francis,

17:12.080 --> 17:14.560
they do have SVE,

17:14.560 --> 17:17.320
unfortunately it's only 128 bit vectors,

17:17.320 --> 17:19.240
and it's pretty damn expensive.

17:19.240 --> 17:20.680
But if you want to do it,

17:20.680 --> 17:22.240
you can find the hardware.

17:22.240 --> 17:26.080
So race five, so different model.

17:26.080 --> 17:26.800
Yeah.

17:26.800 --> 17:29.720
There's also the Alibaba one, the Yitian.

17:29.720 --> 17:32.000
Yeah, maybe. It's possible.

17:32.000 --> 17:34.320
It's only available in China.

17:35.040 --> 17:37.000
So on race five,

17:37.000 --> 17:38.320
the prediction is a little bit different.

17:38.320 --> 17:40.400
So they have separation between

17:40.400 --> 17:43.880
element count and the actual predicate.

17:43.880 --> 17:45.800
So in practice in multi-major,

17:45.800 --> 17:47.280
maybe not in David, but usually you

17:47.280 --> 17:48.320
don't use a predicate at all.

17:48.320 --> 17:51.240
So we will instead just count the elements.

17:51.240 --> 17:53.160
This is the instruction you always found at

17:53.160 --> 17:55.840
the beginning of the loop which consider the vectors.

17:55.840 --> 17:58.640
So in this case, what we say is that we

17:58.640 --> 18:01.240
have a certain number of input elements.

18:01.240 --> 18:03.560
We want to get the number of output.

18:03.560 --> 18:05.920
The output of the parameter is the number of element

18:05.920 --> 18:09.520
the CPU will deal with in the iteration.

18:09.520 --> 18:12.440
We then have to say the size of the element in bits.

18:12.440 --> 18:14.880
In this case, for instance, 16 bits.

18:14.880 --> 18:17.480
The group size which is free and rolling,

18:17.480 --> 18:19.240
it will automatically, if you set it to two,

18:19.240 --> 18:21.160
it will use all the,

18:21.160 --> 18:22.760
and you say you want to use vector eight,

18:22.760 --> 18:23.960
it will use vector eight and vector nine

18:23.960 --> 18:26.120
at the same time for instance.

18:26.120 --> 18:28.840
Tail mode, we always set it to

18:28.840 --> 18:30.720
agnostic because we don't really care about tail mode,

18:30.720 --> 18:33.120
and mask mode, we always set it to agnostic.

18:33.120 --> 18:35.040
There may be use cases where you need to do something else,

18:35.040 --> 18:36.240
which might be a little bit slower,

18:36.240 --> 18:38.560
but usually you don't.

18:38.560 --> 18:41.440
This is about how to deal with the stuff that is masked,

18:41.440 --> 18:43.520
or with the element that are at the end of the vector,

18:43.520 --> 18:45.280
which we don't care about. Usually you don't care about them,

18:45.280 --> 18:47.920
so you just tell the CPU you don't care about them.

18:48.440 --> 18:50.720
One cool thing about H5,

18:50.720 --> 18:52.680
the floating point condition are separate from the vectors,

18:52.680 --> 18:55.440
unlike on ARM, so you have more registers available.

18:55.440 --> 18:57.040
If you have hybrid calculations

18:57.040 --> 18:58.640
between scalar and vector side,

18:58.640 --> 19:00.840
but do mind the floating point convention,

19:00.840 --> 19:03.600
and the coding convention when this happens,

19:03.600 --> 19:06.480
otherwise you will screw up your register state and

19:06.480 --> 19:08.400
configure your CPU.

19:09.440 --> 19:12.080
Interesting stuff also about H5,

19:12.080 --> 19:13.240
they have segmented load and store,

19:13.240 --> 19:16.520
which similar to structured load and store in ARM,

19:16.520 --> 19:18.720
but they can do it up to eight structures,

19:18.720 --> 19:21.840
whereas ARM is only up to four.

19:21.840 --> 19:25.760
What is much more interesting perhaps is

19:25.760 --> 19:28.960
stride in loads and store where you can say,

19:28.960 --> 19:31.240
I have this register X which contains

19:31.240 --> 19:33.160
a value and that's going to be my stride.

19:33.160 --> 19:35.200
So for instance, with that you can put

19:35.200 --> 19:37.680
width of your video inside one register,

19:37.680 --> 19:41.160
and you can load all the pixels in a column in an instruction,

19:41.160 --> 19:44.120
without having to then do weird shuffling and whatever.

19:44.120 --> 19:46.320
Does that actually perform a practice?

19:46.320 --> 19:48.320
I think that's going to depend on the design,

19:48.320 --> 19:52.640
but normally it should be in the data cache which would be okay.

19:52.640 --> 19:56.160
We've talked about hypothetical architectures where it could be fostered.

19:56.160 --> 19:58.800
So I'll come to that.

20:00.600 --> 20:03.720
Yes. On the downside,

20:03.720 --> 20:05.920
you don't have transpose or zipping instructions,

20:05.920 --> 20:08.400
which should be the annoying, which is the same.

20:08.400 --> 20:10.440
So you have to replace it with slides.

20:10.440 --> 20:12.920
So it's fine if you want to take

20:12.920 --> 20:15.960
every second element from one vector and so on.

20:15.960 --> 20:19.560
Future detection, they have very,

20:19.560 --> 20:24.680
very detailed pre-processor feature flags.

20:24.680 --> 20:27.200
I mean, you can download the slides if you're interested.

20:27.200 --> 20:29.520
On the other hand, on the runtime detection is pretty crappy,

20:29.520 --> 20:35.000
you have to trust the bootloader to

20:35.000 --> 20:40.040
actually tell it to your OS correctly in the device tree data structure,

20:40.040 --> 20:42.200
and otherwise there is a flag in there.

20:42.200 --> 20:45.800
So the v-speed, so the bit 21,

20:45.800 --> 20:48.000
because v is a 20 second later in the alphabet,

20:48.000 --> 20:52.560
is a vector flag in the auxiliary vector for other capabilities on Linux.

20:52.560 --> 20:56.720
Availability, unfortunately at this time, there is no hardware.

20:57.320 --> 21:02.360
Alibaba has made hardware available,

21:02.360 --> 21:05.840
but it's implementing version 0.71

21:05.840 --> 21:11.680
from about 18 months before the standardized specification,

21:11.680 --> 21:14.720
which is implemented by the Clang and GCC.

21:14.720 --> 21:17.040
So you can work with that and it gives you

21:17.040 --> 21:20.000
some ideas of performance but you're going to have to rewrite

21:20.000 --> 21:22.760
stuff because it's not completely bit compatible,

21:22.760 --> 21:24.360
so it's annoying.

21:24.360 --> 21:26.480
I don't know when the stuff is going to happen,

21:26.480 --> 21:27.680
I'm pretty sure it's going to happen,

21:27.680 --> 21:30.240
but I would guess by the end of this year,

21:30.240 --> 21:32.720
we are going to see hardware available.

21:34.360 --> 21:39.840
Also, I think one note answering or dodging the previous question,

21:39.840 --> 21:42.560
but because we have so many different vendors on RISC-V,

21:42.560 --> 21:44.400
and I think there's more than I did,

21:44.400 --> 21:45.760
I only listed three here,

21:45.760 --> 21:49.280
but I think there might be

21:49.280 --> 21:51.240
big difference between the performance characteristics of

21:51.240 --> 21:55.440
the different vendors with our references.

22:00.560 --> 22:05.240
Yes, I have just a few questions.

22:05.240 --> 22:10.640
I just heard of the SVP64 project from Liver SOC yet,

22:10.640 --> 22:16.360
which is a similar vector approach for PowerPC.

22:16.360 --> 22:18.720
No, I haven't looked at PowerPC at all.

22:18.720 --> 22:20.520
So another question that I have with

22:20.520 --> 22:22.600
my own simply programming work is,

22:22.600 --> 22:25.760
we often have applications that are inherently horizontal.

22:25.760 --> 22:28.040
For example, let's say you are writing

22:28.040 --> 22:29.960
a vectorized string search operation,

22:29.960 --> 22:32.200
or you're doing something like decoding JPEGs,

22:32.200 --> 22:34.040
where you have these A5 blocks,

22:34.040 --> 22:36.520
where you want to do some close-end transform on them,

22:36.520 --> 22:39.480
and they have this fixed size and depending on the vector size,

22:39.480 --> 22:40.880
you either have to break them up or you

22:40.880 --> 22:43.080
maybe can process multiple of them at the same time.

22:43.080 --> 22:44.680
Is there an intelligent way to solve this?

22:44.680 --> 22:46.840
So I've had this case.

22:46.840 --> 22:48.400
Yes. So the question is,

22:48.400 --> 22:50.800
when you have a naturally fixed size data,

22:50.800 --> 22:52.920
the input block, block kind of block

22:52.920 --> 22:54.560
that you want to process at the time,

22:54.560 --> 22:56.520
how do you do this because then you

22:56.520 --> 22:59.200
actually want to have a fixed size vector in effect,

22:59.200 --> 23:00.480
path as in the question.

23:00.480 --> 23:02.080
So yes, I've had this case with

23:02.080 --> 23:03.600
a sample a couple of times.

23:03.600 --> 23:05.840
One way is to just check that

23:05.840 --> 23:06.840
the vector size of the CPU is

23:06.840 --> 23:08.960
big enough and just do one at a time.

23:08.960 --> 23:11.560
If you can try to do n at a time,

23:11.560 --> 23:12.840
because it's always going to be a power off too,

23:12.840 --> 23:15.720
so you should be able relatively easily to parallelize.

23:15.720 --> 23:17.840
I mean, obviously, the ideal situation is to parallelize,

23:17.840 --> 23:19.040
where you will have a problem is if

23:19.040 --> 23:20.840
your data set is larger than the vector,

23:20.840 --> 23:23.080
then it's going to become complicated for you.

23:23.080 --> 23:25.520
On the RISC-V, you can deal with

23:25.520 --> 23:28.440
this with a group multiplier,

23:28.440 --> 23:32.240
which will allow you to use multiple vectors as a single vector.

23:32.240 --> 23:34.440
The last question I have is,

23:34.440 --> 23:39.520
how do you realistically test vectorized kernels,

23:39.520 --> 23:42.760
when the hardware you have only supports one vector length at most?

23:42.760 --> 23:44.400
So you have to probably use some sort of

23:44.400 --> 23:46.120
validation to be set up for this?

23:46.120 --> 23:48.600
Most of the loops will not depend.

23:48.600 --> 23:50.920
So the question is, do you test a different vector size,

23:50.920 --> 23:52.800
for validation, I guess?

23:52.800 --> 23:55.480
Most of the loops don't really care about the vector size,

23:55.480 --> 23:57.600
because if you have the simple case where you follow

23:57.600 --> 24:00.760
the simple pattern, it doesn't really care what the vector size is.

24:00.760 --> 24:02.560
Except for benchmarking, of course,

24:02.560 --> 24:05.640
and you have a problem, but otherwise,

24:05.640 --> 24:08.160
QMU and Spark at this far is to have support

24:08.160 --> 24:09.560
any vector size to give that,

24:09.560 --> 24:12.280
as long as it's a valid one from specification 10 point.

24:12.280 --> 24:13.720
You realistically really test for that,

24:13.720 --> 24:16.600
or do you just say it's simply not going to be a problem?

24:16.600 --> 24:19.360
I mean, personally,

24:19.360 --> 24:20.960
when I've had the situation where I had

24:20.960 --> 24:23.600
a fixed size input and I had to test with different vector size,

24:23.600 --> 24:25.240
and I've tested with different vector size,

24:25.240 --> 24:27.400
in most cases, you don't actually care.

24:27.400 --> 24:30.720
I mean, then it's a matter of choice of duty or testing,

24:30.720 --> 24:33.680
and I know it's tricky one to be with a validation, I think.

24:33.680 --> 24:35.400
Thank you.

24:35.400 --> 24:37.080
We have no one on the question.

24:37.080 --> 24:39.320
Yeah. First a disclaimer,

24:39.320 --> 24:42.520
I'm related to the Liversock project with SB64.

24:42.520 --> 24:44.920
It's related similar to RISC-V vectors,

24:44.920 --> 24:46.040
but not exactly the same,

24:46.040 --> 24:48.640
but they share a lot of the common ideas.

24:48.640 --> 24:50.840
You mentioned a very good point,

24:50.840 --> 24:53.040
that CMD is not vector processing.

24:53.040 --> 24:57.960
In order, we tried to import some code from a NEON to SB2.

24:57.960 --> 25:02.080
It was less than suboptimal, let's say.

25:02.080 --> 25:31.400
We had to revert back to the original C algorithm.
