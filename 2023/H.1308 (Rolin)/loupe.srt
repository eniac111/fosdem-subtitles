1
0:00:00.000 --> 0:00:08.520
At our final talk for this session, we have Pierre here.

2
0:00:08.520 --> 0:00:14.560
He's going to discuss about Loop, a tool that, well, he and we have been using to measure

3
0:00:14.560 --> 0:00:16.160
compatibility for different OSes.

4
0:00:16.160 --> 0:00:17.680
Pierre, you have the floor.

5
0:00:17.680 --> 0:00:22.040
Thank you, Rizvan, and thanks everyone for attending my talk.

6
0:00:22.040 --> 0:00:28.080
This is joint work with a bunch of colleagues and students, including Hugo, my PhD student.

7
0:00:28.080 --> 0:00:30.320
He's a key player behind his work.

8
0:00:30.320 --> 0:00:35.880
I'm just getting all the media suggestion, but he has built all this stuff, so all the

9
0:00:35.880 --> 0:00:39.120
credits go to him.

10
0:00:39.120 --> 0:00:46.240
In this brief talk, I want to speak a bit about application compatibility for custom

11
0:00:46.240 --> 0:00:49.080
operating systems.

12
0:00:49.080 --> 0:00:55.840
I guess most of you don't need to be convinced that we still need custom operating systems

13
0:00:55.840 --> 0:00:56.840
today.

14
0:00:56.840 --> 0:01:05.720
We still need both research operating systems and prototype operating systems from the industry.

15
0:01:05.720 --> 0:01:12.720
The thinking that Linux has solved everything is not true, in my opinion.

16
0:01:12.720 --> 0:01:17.640
We still need things like UniCraft if we want to go fast or if we want to specialize like

17
0:01:17.640 --> 0:01:18.640
crazy.

18
0:01:18.640 --> 0:01:23.320
We still need things like Resty-R-Met if we want security or SCL4.

19
0:01:23.320 --> 0:01:28.280
We still need custom operating systems.

20
0:01:28.280 --> 0:01:32.840
The thing is, with this operating system, they're only as good as the applications that

21
0:01:32.840 --> 0:01:36.840
they can run.

22
0:01:36.840 --> 0:01:39.200
Compatibility is key.

23
0:01:39.200 --> 0:01:42.920
Compatibility with existing applications is extremely important.

24
0:01:42.920 --> 0:01:47.880
If you want to build a community, you want your user to go to your website, compile your

25
0:01:47.880 --> 0:01:54.440
custom OS and then try some of their favorite applications or try some of the highly popular

26
0:01:54.440 --> 0:01:59.680
applications in a given application domain like engineering or Redis for cloud.

27
0:01:59.680 --> 0:02:05.800
If you want to attract sponsors or investors, or even if you, like me, are a scientist,

28
0:02:05.800 --> 0:02:10.920
you want to gather some early numbers to make a publication, well, you need to do that on

29
0:02:10.920 --> 0:02:14.480
standard applications.

30
0:02:14.480 --> 0:02:19.360
Compatibility is important.

31
0:02:19.360 --> 0:02:24.960
Another argument would be like how many times did you hear the one POSIX spoken today?

32
0:02:24.960 --> 0:02:29.800
There were some slides, there was POSIX like three or four times written in the single

33
0:02:29.800 --> 0:02:32.080
slide.

34
0:02:32.080 --> 0:02:40.240
Compatibility is important and it can be achieved in a few different ways as we have seen with

35
0:02:40.240 --> 0:02:41.240
Simon.

36
0:02:41.240 --> 0:02:47.360
But one important thing to note is, in my opinion, porting is not sustainable.

37
0:02:47.360 --> 0:02:49.720
Porting is what many of us do.

38
0:02:49.720 --> 0:02:54.440
We build a custom operating system and then we take Redis and obviously it doesn't work

39
0:02:54.440 --> 0:02:57.360
as this with our operating system.

40
0:02:57.360 --> 0:03:03.400
We modify Redis a bit, we disable some features because we know that they make our OS crash.

41
0:03:03.400 --> 0:03:07.240
And then we have Redis like a version customized for our operating system.

42
0:03:07.240 --> 0:03:12.560
This is not sustainable because you can't maintain a branch of Redis for every operating

43
0:03:12.560 --> 0:03:13.560
system out there.

44
0:03:13.560 --> 0:03:17.520
In the long term, it doesn't work so well.

45
0:03:17.520 --> 0:03:26.440
So porting also basically means that you as the OS developer, you ask the users of your

46
0:03:26.440 --> 0:03:31.520
application to make some effort to the application developers, they need to make some effort

47
0:03:31.520 --> 0:03:33.600
to be compatible with your operating system.

48
0:03:33.600 --> 0:03:38.440
This doesn't work, nobody is ready to make that kind of effort.

49
0:03:38.440 --> 0:03:45.320
Maybe if you give them 10x performance speed up, but this is unrealistic.

50
0:03:45.320 --> 0:03:53.560
What you want to do is, once again in my opinion, as an OS developer, you want to provide compatibility

51
0:03:53.560 --> 0:03:56.480
as transparently as possible.

52
0:03:56.480 --> 0:04:05.080
And this means you emulate a popular operating system, for example Linux or a popular abstraction

53
0:04:05.080 --> 0:04:08.960
like POSIX or the standard C library.

54
0:04:08.960 --> 0:04:13.400
And then you can be compatible at three different levels.

55
0:04:13.400 --> 0:04:17.600
The first level is source level or API level compatibility.

56
0:04:17.600 --> 0:04:29.400
So you ask the user to compile the application code against the sources of your kernel in

57
0:04:29.400 --> 0:04:31.280
the case of a unique kernel.

58
0:04:31.280 --> 0:04:35.800
So you're still asking some effort from the users.

59
0:04:35.800 --> 0:04:38.760
In many scenarios, you don't have access to sources.

60
0:04:38.760 --> 0:04:46.080
If you have proprietary binary or pre-compiled binary, you can't have source level compatibility.

61
0:04:46.080 --> 0:04:54.360
So it's not perfect and binary compatibility is a pure version of compatibility.

62
0:04:54.360 --> 0:04:56.520
And there are many ways to achieve it.

63
0:04:56.520 --> 0:05:01.760
You can do that at the level of the standard C library like OSV.

64
0:05:01.760 --> 0:05:11.600
So you will dynamically link your kernel plus a standard C library against your application

65
0:05:11.600 --> 0:05:17.160
compiled as a position independent executable or as a shared library itself.

66
0:05:17.160 --> 0:05:24.000
This is great, but if the application is making directly system calls to the kernel without

67
0:05:24.000 --> 0:05:28.960
going through the C standard library, well, once again, it doesn't work.

68
0:05:28.960 --> 0:05:34.560
And as a matter of fact, I have counted more than 500 executables in the Debian repository

69
0:05:34.560 --> 0:05:36.360
that contains the C score instruction.

70
0:05:36.360 --> 0:05:39.400
So they make C scores directly to the kernel.

71
0:05:39.400 --> 0:05:41.880
They don't go through the C library.

72
0:05:41.880 --> 0:05:47.880
Go, for example, is making most of its C score put directly to the kernel and not through

73
0:05:47.880 --> 0:05:48.880
the C library.

74
0:05:48.880 --> 0:05:53.000
So what you want to do is to be compatible at the level of the system call.

75
0:05:53.000 --> 0:05:58.560
So your kernel needs to emulate the C score API that Linux is providing.

76
0:05:58.560 --> 0:06:03.040
This is the most transparent way of achieving compatibility.

77
0:06:03.040 --> 0:06:06.720
Now this is scary, right?

78
0:06:06.720 --> 0:06:11.720
Linux has more than 350 system calls we need to implement them all.

79
0:06:11.720 --> 0:06:17.640
Will we be and aren't we going to re-implement Linux by doing so?

80
0:06:17.640 --> 0:06:21.120
And some of them are extremely scary by themselves, right?

81
0:06:21.120 --> 0:06:28.680
You have like hundreds of IO controls and each of them require its own implementation.

82
0:06:28.680 --> 0:06:32.520
The Linux API even goes beyond system calls.

83
0:06:32.520 --> 0:06:40.720
You have things like slash proc slash dev that are used by many applications.

84
0:06:40.720 --> 0:06:48.440
The first thing a muscle binary does when it runs is to look in, I believe it's slash

85
0:06:48.440 --> 0:06:51.400
proc or slash sys to get to the size of the terminal, right?

86
0:06:51.400 --> 0:06:55.480
So you need to have emulation for this part of the API too.

87
0:06:55.480 --> 0:07:02.480
And this, because it seems like a big engineering effort, this creates, it hinders the development

88
0:07:02.480 --> 0:07:04.120
of custom operating systems.

89
0:07:04.120 --> 0:07:12.880
So this is inspired by the keynote by Eti Mati Roscoe at ATC and OSDI 2021.

90
0:07:12.880 --> 0:07:14.800
We looked at all the papers.

91
0:07:14.800 --> 0:07:19.120
So these are top tier operating systems conferences.

92
0:07:19.120 --> 0:07:28.080
And we look over the past 10 years over a total of more than 1,000 papers, how many were

93
0:07:28.080 --> 0:07:32.600
about proposing a new operating system as opposed to things like security or machine

94
0:07:32.600 --> 0:07:34.160
learning?

95
0:07:34.160 --> 0:07:39.920
And among them, how many were just hacking Linux versus proposing an actual operating

96
0:07:39.920 --> 0:07:42.160
system implemented from scratch?

97
0:07:42.160 --> 0:07:46.680
And the numbers are similar to what we saw earlier, right?

98
0:07:46.680 --> 0:07:52.600
You have just a very, very, very few papers proposing a new operating system because it's

99
0:07:52.600 --> 0:07:55.800
a significant engineering effort.

100
0:07:55.800 --> 0:08:01.480
And part of the effort is providing compatibility to run applications like Apache or Redis to

101
0:08:01.480 --> 0:08:04.400
get a few numbers at the end of the paper, right?

102
0:08:04.400 --> 0:08:07.440
So this is a problem.

103
0:08:07.440 --> 0:08:15.640
Now the particular problem that I want to talk about is how I'm sure several people

104
0:08:15.640 --> 0:08:22.080
in this room have attempted to build some form of compatibility layer for an operating

105
0:08:22.080 --> 0:08:23.320
systems.

106
0:08:23.320 --> 0:08:28.640
And we are all kind of working on the same thing in parallel with some form of ad hoc

107
0:08:28.640 --> 0:08:32.240
processes that may benefit from some optimization.

108
0:08:32.240 --> 0:08:40.200
So I've just listed here a few projects that have a Cisco level binary compatibility layers,

109
0:08:40.200 --> 0:08:43.800
but actually there are many more.

110
0:08:43.800 --> 0:08:48.920
And from what I understand, it is a very organic process.

111
0:08:48.920 --> 0:08:50.880
So first of all, it is application driven, right?

112
0:08:50.880 --> 0:08:56.280
People have a few sets of application in mind that they want to support.

113
0:08:56.280 --> 0:09:01.720
If you are doing cloud, you want to support the user and suspect, Redis, Apache, whatever.

114
0:09:01.720 --> 0:09:05.040
And the process basically looks like that.

115
0:09:05.040 --> 0:09:10.480
You take an app, you try to run it on top of your operating system.

116
0:09:10.480 --> 0:09:12.240
Obviously it fails.

117
0:09:12.240 --> 0:09:17.320
You investigate, you're like, oh, I'm missing the implementation from this system code.

118
0:09:17.320 --> 0:09:22.520
So you implement whatever operating system features are required to fix that particular

119
0:09:22.520 --> 0:09:27.080
issue, rinse and repeat until the app is working.

120
0:09:27.080 --> 0:09:29.440
And then you go to the next app.

121
0:09:29.440 --> 0:09:33.280
So it's a very intuitive and organic process.

122
0:09:33.280 --> 0:09:38.040
So when I built the Almetix, this is exactly what I was doing.

123
0:09:38.040 --> 0:09:46.280
So something that comes to mind is, can't we have some form of generic compatibility

124
0:09:46.280 --> 0:09:50.000
layer that we could plug?

125
0:09:50.000 --> 0:09:55.600
Something a bit like Newlib that would provide generic interface.

126
0:09:55.600 --> 0:10:00.440
And I believe it's not really possible because most of this implementation to support the

127
0:10:00.440 --> 0:10:05.600
system code is very specific to whatever operating system you are using.

128
0:10:05.600 --> 0:10:11.560
And it's not clear if a generic compatibility layer can be achieved.

129
0:10:11.560 --> 0:10:17.120
But can we still somehow optimize that process?

130
0:10:17.120 --> 0:10:19.480
Some have tried static analysis.

131
0:10:19.480 --> 0:10:26.000
So they take the binaries of the applications they want to support and they look, okay,

132
0:10:26.000 --> 0:10:29.680
so what are the system codes that are made by these applications?

133
0:10:29.680 --> 0:10:38.120
So this has been done in the best paper in Erosys 2016, analyzed all the binaries from

134
0:10:38.120 --> 0:10:39.120
Ubuntu.

135
0:10:39.120 --> 0:10:43.720
I believe it was 1404 repositories.

136
0:10:43.720 --> 0:10:49.960
And they concluded that every Ubuntu installation, including the smallest one, require more than

137
0:10:49.960 --> 0:10:56.440
200 system codes, and 200 IO controls, five controls, and PRCTL codes, and hundreds of

138
0:10:56.440 --> 0:10:57.920
pseudo-fives.

139
0:10:57.920 --> 0:10:59.680
So this doesn't help.

140
0:10:59.680 --> 0:11:01.440
It is still quite scary.

141
0:11:01.440 --> 0:11:08.240
It's still represent like a gigantic engineering effort.

142
0:11:08.240 --> 0:11:18.720
But do we want full compatibility with Ubuntu installation?

143
0:11:18.720 --> 0:11:23.280
In the end, especially in the early stage of the development of an operating system,

144
0:11:23.280 --> 0:11:26.560
you just want to get a few applications up and running.

145
0:11:26.560 --> 0:11:30.040
And do you even want 100% compatibility?

146
0:11:30.040 --> 0:11:33.840
When I write a paper, I don't really care if everything is stable.

147
0:11:33.840 --> 0:11:38.000
I just want to get some numbers.

148
0:11:38.000 --> 0:11:41.840
So isn't there a better way?

149
0:11:41.840 --> 0:11:45.800
And obviously, maybe you think about, yeah, let's do dynamic analysis, right?

150
0:11:45.800 --> 0:11:50.400
Let's run the applications that we want to support.

151
0:11:50.400 --> 0:11:55.920
We send them some input that we want to support, like I'm running an engineering and I'm submitting

152
0:11:55.920 --> 0:11:58.280
some HTTP or something like that.

153
0:11:58.280 --> 0:12:00.560
And then we trace the system codes that are done, right?

154
0:12:00.560 --> 0:12:05.880
So this is going to give us a subset of the system codes that can be identified through

155
0:12:05.880 --> 0:12:09.360
static analysis that has a tendency to overestimate.

156
0:12:09.360 --> 0:12:15.080
So with this trace, the engineering effort to support an application and a set of input

157
0:12:15.080 --> 0:12:16.960
is a bit lower.

158
0:12:16.960 --> 0:12:21.160
But it's still not a panacea because it's not taking into account two things that we

159
0:12:21.160 --> 0:12:23.880
do when we implement compatibility layers.

160
0:12:23.880 --> 0:12:25.280
So this is my code.

161
0:12:25.280 --> 0:12:27.080
Don't judge me.

162
0:12:27.080 --> 0:12:34.040
One thing that I did from Hermitax was at some point, it was a map that was calling

163
0:12:34.040 --> 0:12:38.320
MNCORE to check if some page of memory wasn't swapped or not.

164
0:12:38.320 --> 0:12:43.840
It's as actually, there is no swap in most unique channels, so it really didn't matter

165
0:12:43.840 --> 0:12:45.640
to implement this.

166
0:12:45.640 --> 0:12:50.000
So inosis means operation not supported, right?

167
0:12:50.000 --> 0:12:53.480
So stubbing a system code is just saying, yeah, we don't support it.

168
0:12:53.480 --> 0:12:58.000
And you cross your finger that the application has some kind of fallback pass to do something

169
0:12:58.000 --> 0:13:00.600
else if the syscall fails.

170
0:13:00.600 --> 0:13:02.480
And it works in some cases.

171
0:13:02.480 --> 0:13:05.120
Then we can do something even more nasty.

172
0:13:05.120 --> 0:13:06.600
Don't judge me again.

173
0:13:06.600 --> 0:13:09.840
You can fake the success of a system code, right?

174
0:13:09.840 --> 0:13:19.200
Surprisingly, in some situation, returning a success code, even if the system code doesn't

175
0:13:19.200 --> 0:13:23.440
have any implementation in your operating system, it's going to work in some cases.

176
0:13:23.440 --> 0:13:27.320
I'll tell a bit more about why this works sometimes.

177
0:13:27.320 --> 0:13:32.240
So stubbing and faking lets you implement even less system codes than what you would

178
0:13:32.240 --> 0:13:35.200
trace with a trace.

179
0:13:35.200 --> 0:13:40.360
So in the end, if you want to support an app or a set of applications in your custom operating

180
0:13:40.360 --> 0:13:46.760
system, the amount of system codes that you actually need to implement, so obviously it's

181
0:13:46.760 --> 0:13:50.240
more than the entire Linux syscall API.

182
0:13:50.240 --> 0:13:55.520
Static binary analysis will, on the binaries of the applications you want to support, will

183
0:13:55.520 --> 0:13:58.240
identify a subset of that, still pretty big.

184
0:13:58.240 --> 0:14:00.240
It's another estimate.

185
0:14:00.240 --> 0:14:07.000
Glass analysis gets you more precise results, but it is pretty hard to achieve, and it is

186
0:14:07.000 --> 0:14:10.280
still overestimating.

187
0:14:10.280 --> 0:14:13.240
S trace will give you, once again, a subset.

188
0:14:13.240 --> 0:14:14.640
Things start to look better.

189
0:14:14.640 --> 0:14:19.440
And among these trace by S trace, you actually don't need to implement everything.

190
0:14:19.440 --> 0:14:22.720
You can stub and fake some of the syscalls.

191
0:14:22.720 --> 0:14:26.360
So can we measure that?

192
0:14:26.360 --> 0:14:28.120
Yes, with loop.

193
0:14:28.120 --> 0:14:31.920
So loop means magnifying glass in French.

194
0:14:31.920 --> 0:14:36.600
It's a tool that was built by Hugo, my student, and it's some kind of super S trace that is

195
0:14:36.600 --> 0:14:41.440
measuring the syscalls that are required to support an application, and that can also

196
0:14:41.440 --> 0:14:47.320
tell you which one you can stub and which one you can fake.

197
0:14:47.320 --> 0:14:56.120
So we used it to build a database of measurement for a relatively large set of applications.

198
0:14:56.120 --> 0:15:01.560
And with loop, if you give me a description of your operating system, basically the list

199
0:15:01.560 --> 0:15:05.880
of syscalls that you already support, and you give me the list of applications that

200
0:15:05.880 --> 0:15:11.440
you would like to support, we run them through loop, and loop can derive a support plan,

201
0:15:11.440 --> 0:15:17.600
which basically will tell you, okay, for this set of target applications, and given the

202
0:15:17.600 --> 0:15:23.120
set of syscalls that you already support, what is the optimized order of syscalls to

203
0:15:23.120 --> 0:15:26.560
implement to support as many applications as soon as possible?

204
0:15:26.560 --> 0:15:33.040
Okay, so I will give you an example of support plan by the end of the presentation.

205
0:15:33.040 --> 0:15:37.720
So from the user point of view, loop needs two things to perform its measurement on a

206
0:15:37.720 --> 0:15:38.720
given application.

207
0:15:38.720 --> 0:15:45.880
You give it a Docker file that is describing how you want to build and run the application

208
0:15:45.880 --> 0:15:50.200
for which you want to measure the syscalls needed.

209
0:15:50.200 --> 0:15:54.800
And optionally, you may need an input workload, think about a web server, it's not going to

210
0:15:54.800 --> 0:16:00.680
call many system calls until you actually start to send requests to it.

211
0:16:00.680 --> 0:16:06.400
Loop will instantiate the application, launch it on a standard Linux kernel, and analyze

212
0:16:06.400 --> 0:16:12.520
the syscalls that have done, and with a few tricks, we'll be able to know which ones can

213
0:16:12.520 --> 0:16:15.200
be faked or stubbed.

214
0:16:15.200 --> 0:16:19.280
The results are, it's basically just a CSV file.

215
0:16:19.280 --> 0:16:25.800
For a syscall that is made by the application, can it be faked?

216
0:16:25.800 --> 0:16:26.800
Can it be stubbed?

217
0:16:26.800 --> 0:16:29.720
Or does it require a full implementation?

218
0:16:29.720 --> 0:16:36.840
We start that in a database, and later, so we populate the database with as many measurements

219
0:16:36.840 --> 0:16:43.720
as possible, and this database can, given the list of syscalls that is already supported

220
0:16:43.720 --> 0:16:49.080
by your operating systems, give you some form of optimized super plan given which of the

221
0:16:49.080 --> 0:16:53.080
applications you want to support.

222
0:16:53.080 --> 0:16:56.280
So how does it work?

223
0:16:56.280 --> 0:17:02.440
When Loop runs the application, first it does a quick pass of a trace to measure all the

224
0:17:02.440 --> 0:17:09.080
system calls that are done by the application, and then for each system call that we identify,

225
0:17:09.080 --> 0:17:17.240
we use seccomp to hook into the execution of each of the system calls, and rather than

226
0:17:17.240 --> 0:17:24.880
actually executing them through the Linux kernel, we emulate the fact that the syscall

227
0:17:24.880 --> 0:17:25.880
is stubbed.

228
0:17:25.880 --> 0:17:29.040
So we just return in assist without executing the syscall.

229
0:17:29.040 --> 0:17:34.960
We can also emulate the fact that the syscall is faked, return zero, and then we check

230
0:17:34.960 --> 0:17:42.480
if the application works or not, following the stubbing or the faking of this particular

231
0:17:42.480 --> 0:17:43.480
syscall.

232
0:17:43.480 --> 0:17:48.480
We use that for each system call that we have identified with this trace.

233
0:17:48.480 --> 0:17:54.040
How do we actually check for the success of the execution of the application?

234
0:17:54.040 --> 0:17:56.600
So we identify two types of apps.

235
0:17:56.600 --> 0:17:58.480
Some we call them run to completion.

236
0:17:58.480 --> 0:18:02.880
There will be something like fio, when you start fio, it runs for one minute and then

237
0:18:02.880 --> 0:18:08.400
it exits outputting some stuff on the external output.

238
0:18:08.400 --> 0:18:14.880
So with run to completion apps, we run the app instrumented with loop, we check its exit

239
0:18:14.880 --> 0:18:15.880
code.

240
0:18:15.880 --> 0:18:19.320
If it's different from zero, we consider that the run was a failure, could have been killed

241
0:18:19.320 --> 0:18:22.640
by a signal or things like that.

242
0:18:22.640 --> 0:18:28.600
And we can also run a script optionally in addition to that, after each run of the application

243
0:18:28.600 --> 0:18:35.880
to check its standard output, we can grab error values, we can grab for success printing,

244
0:18:35.880 --> 0:18:40.880
something like 50 requests per second have been achieved.

245
0:18:40.880 --> 0:18:46.040
The files that may have been created by the application and so on.

246
0:18:46.040 --> 0:18:49.440
And then another type of application is client servers.

247
0:18:49.440 --> 0:18:55.760
So with client servers, we run the app instrumented by loop and in parallel, we run a workload,

248
0:18:55.760 --> 0:19:01.080
could be WRC, HTTP, the Redis benchmark for Redis and so on.

249
0:19:01.080 --> 0:19:03.680
And we check the success of both.

250
0:19:03.680 --> 0:19:08.120
We check that the app doesn't crash, we generally say others are not supposed to exit.

251
0:19:08.120 --> 0:19:11.880
So we check that it doesn't crash and we check the success of the workload.

252
0:19:11.880 --> 0:19:15.960
Like if Redis benchmark returns something different than zero for a reason, something

253
0:19:15.960 --> 0:19:16.960
went wrong.

254
0:19:16.960 --> 0:19:22.160
And then we are able to see, okay, so I'm currently trying to stop the Redis system

255
0:19:22.160 --> 0:19:27.520
call, is the application succeeded or not?

256
0:19:27.520 --> 0:19:34.000
So we'll use the database, let me check the time, okay.

257
0:19:34.000 --> 0:19:36.000
And we analyze the results.

258
0:19:36.000 --> 0:19:42.960
So these results are made in a relatively small database of about 12 highly popular,

259
0:19:42.960 --> 0:19:45.720
sorry, 15 highly popular cloud applications.

260
0:19:45.720 --> 0:19:48.920
So this is just a subset.

261
0:19:48.920 --> 0:19:57.040
So what you have on the Y axis is a number of system calls that are identified by static

262
0:19:57.040 --> 0:20:06.360
analysis in purple on the binary, on the sources in yellow and then dynamic analysis.

263
0:20:06.360 --> 0:20:12.560
And we run for each of these application both the standard benchmarks that we Redis benchmark

264
0:20:12.560 --> 0:20:16.920
for Redis, WRC for engineings and so on.

265
0:20:16.920 --> 0:20:18.920
And we also run the entire test suite.

266
0:20:18.920 --> 0:20:26.080
So the key idea with the test suite is if you support, I mean, if you measure what's

267
0:20:26.080 --> 0:20:31.680
going on during the entire test suite, you get a very good idea of what are all the possible

268
0:20:31.680 --> 0:20:34.320
system calls that could be done by the application.

269
0:20:34.320 --> 0:20:39.320
Obviously, you need to assume that the test suite has a good coverage, but it is the case

270
0:20:39.320 --> 0:20:42.880
with these very popular applications.

271
0:20:42.880 --> 0:20:49.120
And what we see is, first of all, you know, static analysis overestimates.

272
0:20:49.120 --> 0:20:50.120
This is not very surprising.

273
0:20:50.120 --> 0:20:54.400
The amount of syscalls that is identified by static analysis is relatively high compared

274
0:20:54.400 --> 0:20:57.320
to what we get with dynamic analysis.

275
0:20:57.320 --> 0:21:02.440
And if something interesting too is that the amount of syscalls that can be stubbed or

276
0:21:02.440 --> 0:21:09.400
faked, so the green bits on the dynamic analysis path, it is actually quite, it's non-legligible,

277
0:21:09.400 --> 0:21:10.400
right?

278
0:21:10.400 --> 0:21:15.840
So what this means is that if you want to support Redis with a Redis benchmark where

279
0:21:15.840 --> 0:21:22.240
binary level static analysis tells you that you should implement 100 system calls, if

280
0:21:22.240 --> 0:21:25.960
you just want to run the Redis benchmark to get performance numbers for your paper, you

281
0:21:25.960 --> 0:21:29.360
actually need to implement just 20, right?

282
0:21:29.360 --> 0:21:33.560
So that's what, like, divided by 5, right?

283
0:21:33.560 --> 0:21:39.320
And if you want to pass the entire test suite of Redis, you need to implement about 40.

284
0:21:39.320 --> 0:21:43.800
It's still like half what static analysis is telling you.

285
0:21:43.800 --> 0:21:48.160
So it's kind of a message of hope, right, for building compatibility layers and for

286
0:21:48.160 --> 0:21:51.260
developing custom operating systems in general.

287
0:21:51.260 --> 0:21:57.160
So yes, static analysis overestimates a lot the engineering effort to support a NAP.

288
0:21:57.160 --> 0:22:03.720
And even naive dynamic analysis does measure much more syscalls than what is actually required

289
0:22:03.720 --> 0:22:09.240
if you know that you can stub and fake syscalls.

290
0:22:09.240 --> 0:22:13.040
Another view of these results can be seen here.

291
0:22:13.040 --> 0:22:19.560
So for each of the system calls, you know, 0 is red, 1 is right, 2 is open, I guess,

292
0:22:19.560 --> 0:22:29.160
and so on, among a dataset of about 15 apps, how many of these apps require the implementation

293
0:22:29.160 --> 0:22:32.680
of the system calling question, right?

294
0:22:32.680 --> 0:22:38.320
And then so you have here the result for static analysis at the binary level.

295
0:22:38.320 --> 0:22:45.880
At the source level, this is estray, so without counting which system calls you can stub or

296
0:22:45.880 --> 0:22:49.600
fake, and this is what is actually required.

297
0:22:49.600 --> 0:22:53.960
So if you consider that you will not implement what you stub or fake, this is what you actually

298
0:22:53.960 --> 0:22:54.960
need to implement.

299
0:22:54.960 --> 0:23:00.760
And as you can see, it's much, much, much, much less engineering effort versus what static

300
0:23:00.760 --> 0:23:04.880
analysis is telling you.

301
0:23:04.880 --> 0:23:07.520
Why does stubbing and faking work?

302
0:23:07.520 --> 0:23:11.520
So here you get some code snippet from Redis.

303
0:23:11.520 --> 0:23:19.560
So if you stub, get our limit, the C library wrapper will return minus 1.

304
0:23:19.560 --> 0:23:24.240
And as you can see, Redis will actually fall back on some kind of safe value.

305
0:23:24.240 --> 0:23:30.440
You know, so I'm not able to understand the maximum of files that I can open, so I'm going

306
0:23:30.440 --> 0:23:35.520
to fall back on 100, sorry, 1000.

307
0:23:35.520 --> 0:23:44.120
And the fact that faking works is actually that you have quite a bunch of system calls.

308
0:23:44.120 --> 0:23:50.240
So this is for each system call and each app in our data set, what is the percentage of

309
0:23:50.240 --> 0:23:56.440
apps that are actually checking the return value of the system calls?

310
0:23:56.440 --> 0:24:02.240
And some system calls are almost never checked their return value.

311
0:24:02.240 --> 0:24:03.320
It kind of makes sense, right?

312
0:24:03.320 --> 0:24:08.560
When you see this, why check the return value of close?

313
0:24:08.560 --> 0:24:13.720
And this is why faking work in many cases.

314
0:24:13.720 --> 0:24:24.520
Another question that we asked is, okay, so when you speak about providing binary compatibility

315
0:24:24.520 --> 0:24:29.440
and you don't do porting anymore, basically all the effort of supporting apps is on you,

316
0:24:29.440 --> 0:24:31.080
the operating system developer.

317
0:24:31.080 --> 0:24:33.760
And this is how it should be, in my opinion.

318
0:24:33.760 --> 0:24:38.920
But how much effort does that mean in the long term?

319
0:24:38.920 --> 0:24:45.880
So we had a look at versions of Redis, and Genix, and Apache over the last 10 years,

320
0:24:45.880 --> 0:24:51.680
and what are these calls that actually needs to be implemented in purple.

321
0:24:51.680 --> 0:24:55.160
And we saw that this number does not change very much, right?

322
0:24:55.160 --> 0:25:03.600
So once you make an app and you make it work, it actually means that you need to keep up

323
0:25:03.600 --> 0:25:08.320
to date with the most recent version of this app that are coming up.

324
0:25:08.320 --> 0:25:14.760
But it doesn't necessarily mean a very big engineering effort either.

325
0:25:14.760 --> 0:25:16.560
And these are the support plans.

326
0:25:16.560 --> 0:25:25.040
So we had a look at UniCraft, Fuchsia, which are some operating systems that have already

327
0:25:25.040 --> 0:25:28.520
a relatively good support for a good number of system calls.

328
0:25:28.520 --> 0:25:32.040
And we look at Kerala, so Kerala is another UniCare mail written in Rust.

329
0:25:32.040 --> 0:25:36.720
And it's very, I wouldn't say immature, but it doesn't have support for a lot of system

330
0:25:36.720 --> 0:25:38.040
calls.

331
0:25:38.040 --> 0:25:43.400
And for our set of 15 apps that we had in the database, we derived a support plan.

332
0:25:43.400 --> 0:25:49.680
So for UniCraft, for example, in its current state, it's already supporting most of the

333
0:25:49.680 --> 0:25:52.560
apps of our dataset.

334
0:25:52.560 --> 0:25:55.840
If you want to support an additional app, what you need to do is to implement system

335
0:25:55.840 --> 0:26:03.120
call number 290 and stub these, and then you'll get memcached.

336
0:26:03.120 --> 0:26:07.400
And next, if you implement this C-score, you get H2O.

337
0:26:07.400 --> 0:26:12.160
And then you need to implement these two C-scores, and then you stub that, and you get MongoDB.

338
0:26:12.160 --> 0:26:15.240
So same thing for Fuchsia and Kerala.

339
0:26:15.240 --> 0:26:19.640
Obviously, it's a bit more interesting because this one doesn't support many applications

340
0:26:19.640 --> 0:26:22.680
out of the box.

341
0:26:22.680 --> 0:26:25.240
And I believe I have time to do a quick demo.

342
0:26:25.240 --> 0:26:31.920
Okay, I'm going to do it real quick.

343
0:26:31.920 --> 0:26:36.360
So I'm going to do a test with LS, which is like the simplest test because we don't have

344
0:26:36.360 --> 0:26:38.920
a lot of time.

345
0:26:38.920 --> 0:26:47.560
In the Dockerfile, I just copy a test that I'm going to show you, and then I call, this

346
0:26:47.560 --> 0:26:53.560
is kind of the top level script of loop with a few options that don't matter that much.

347
0:26:53.560 --> 0:26:58.760
And I say, okay, the binary that we are going to instrument is slash bin slash ls, and this

348
0:26:58.760 --> 0:26:59.760
is the parameter.

349
0:26:59.760 --> 0:27:05.760
So I'm going to do ls slash, and we are going to check if it works or not with every possible

350
0:27:05.760 --> 0:27:08.720
C-score that can be invoked by ls.

351
0:27:08.720 --> 0:27:17.000
And the test, which should be there, the test that we are going to run after each execution

352
0:27:17.000 --> 0:27:19.640
of ls to see if things have worked.

353
0:27:19.640 --> 0:27:25.800
So this shell script will take the standard output of ls as parameters, and to make things

354
0:27:25.800 --> 0:27:30.040
simple, I'm just checking that ls actually output something.

355
0:27:30.040 --> 0:27:32.960
I'm doing ls slash, so something should be outputted.

356
0:27:32.960 --> 0:27:36.280
If nothing is output, there is a problem.

357
0:27:36.280 --> 0:27:41.360
And keep in mind that loop is also checking the return value of ls itself.

358
0:27:41.360 --> 0:27:48.720
So okay, so I'm launching loop like this, so it should work.

359
0:27:48.720 --> 0:27:54.320
So what happens under the hood is that we build a container that we've seen the Dockerfile

360
0:27:54.320 --> 0:27:55.600
for.

361
0:27:55.600 --> 0:27:58.280
We are starting two containers in parallel.

362
0:27:58.280 --> 0:28:05.320
Each one is running a full set of tests trying to stub and fake all the system calls, and

363
0:28:05.320 --> 0:28:13.960
we use this to check for differences between the replicas in case there is a problem.

364
0:28:13.960 --> 0:28:15.360
Most of the time, there is no differences.

365
0:28:15.360 --> 0:28:18.320
So it takes a bit of time.

366
0:28:18.320 --> 0:28:23.640
And then, okay, it's done.

367
0:28:23.640 --> 0:28:31.120
So if we go to the database, so we have now much more than 12 apps.

368
0:28:31.120 --> 0:28:41.800
And if we go to ls, the most interesting result is this CSV file, which contains four hc

369
0:28:41.800 --> 0:28:45.120
calls, zero being read, one being write.

370
0:28:45.120 --> 0:28:49.080
Is it called by ls or not?

371
0:28:49.080 --> 0:28:50.600
Can we fake it?

372
0:28:50.600 --> 0:28:52.560
Can we stub it?

373
0:28:52.560 --> 0:28:56.720
Or can we both fake and stub it?

374
0:28:56.720 --> 0:29:00.200
Or it's more like, does the application work when it's faked?

375
0:29:00.200 --> 0:29:01.640
Does it work when it's stub?

376
0:29:01.640 --> 0:29:05.440
And does it work when it's both faked and stub?

377
0:29:05.440 --> 0:29:11.680
And as you can see, some syscalls, like 11, I don't know which one it is, can be both

378
0:29:11.680 --> 0:29:16.080
stub and fake, something for 12, something for 16.

379
0:29:16.080 --> 0:29:20.160
Some syscalls, like this is read, for example, it is called, but you can't stub or fake it,

380
0:29:20.160 --> 0:29:21.160
which kind of makes sense.

381
0:29:21.160 --> 0:29:24.880
And this wouldn't work if you can read.

382
0:29:24.880 --> 0:29:26.560
And yeah, that's pretty much it.

383
0:29:26.560 --> 0:29:33.200
So briefly, what we are currently working on is some more fine-grained measurement.

384
0:29:33.200 --> 0:29:39.560
Some syscalls have kind of sub features, like a lot of programs will require at least a

385
0:29:39.560 --> 0:29:45.000
map anonymous for a map to allocate memory, but not really to map a file.

386
0:29:45.000 --> 0:29:50.080
So we are looking at checking which flags can be stubbed or fake and things like that.

387
0:29:50.080 --> 0:29:55.280
And we are also looking at the virtual file system API.

388
0:29:55.280 --> 0:29:56.280
That's it.

389
0:29:56.280 --> 0:30:00.000
So building compatibility layers is important for custom operating system.

390
0:30:00.000 --> 0:30:28.440
It seems a bit scary, but actually it's not that much engineering effort.

