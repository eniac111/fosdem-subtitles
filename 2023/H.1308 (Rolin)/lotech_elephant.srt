1
0:00:00.000 --> 0:00:20.380
Great. Okay, my name is Noel Grandin. I'm talking about the work that I've been doing

2
0:00:20.380 --> 0:00:25.040
making Leber Office faster. We all know that Leber Office is a bit of an elephant. It's

3
0:00:25.040 --> 0:00:28.280
a cute elephant, but it's still a bit of an elephant. We've got 10 million odd lines

4
0:00:28.280 --> 0:00:33.400
of code, some of which is 20 years old. Practices that were perfectly acceptable 20 years ago

5
0:00:33.400 --> 0:00:39.720
are a little bit trickier these days. Optimizations that made perfect sense back in the day are

6
0:00:39.720 --> 0:00:45.720
not great anymore, and things have just changed around. For example, CPU memory bandwidth has

7
0:00:45.720 --> 0:00:51.720
dramatically changed. When we ran about the error of the 486DX2, we suddenly saw a dramatic

8
0:00:51.720 --> 0:00:57.120
shift in the speeds of main RAM versus the speeds of your main CPU. Up until that point,

9
0:00:57.120 --> 0:01:02.520
you were looking at CPUs that could touch location and memory at about the same speed

10
0:01:02.520 --> 0:01:10.200
as they could touch local cached memory. You wrote your algorithms around those sorts of

11
0:01:10.200 --> 0:01:13.960
things, the DX2 changed things, and it only got dramatically worse after that. To the

12
0:01:13.960 --> 0:01:19.960
point now where touching something in an L1 cache versus touching main memory can be anywhere

13
0:01:19.960 --> 0:01:26.320
up to 50 times slower. Cache-friendly algorithms are now the new game in town. We now have

14
0:01:26.320 --> 0:01:31.240
multiple cores. Everybody has multiple cores. It used to be that only people with vast sums

15
0:01:31.240 --> 0:01:37.320
of money had machines with multiple cores or L1 CPU. Now, everybody has one. Locking

16
0:01:37.320 --> 0:01:41.920
algorithms that made perfect sense because they only got touched by a handful of people

17
0:01:41.920 --> 0:01:48.960
suddenly get exercised by everybody, and all sorts of interesting flaws come up. An interesting

18
0:01:48.960 --> 0:01:53.160
note, the locking algorithms that were written back then, nobody actually knew if they were

19
0:01:53.160 --> 0:02:00.160
truly solid or not because Intel's own engineers refused to commit to a cache-coherency protocol

20
0:02:00.160 --> 0:02:08.140
until somewhere around the Pentium era. Up until then, they ruthlessly refused to say

21
0:02:08.140 --> 0:02:12.240
anything about it, and if queried, they would just say that it wasn't something they had

22
0:02:12.240 --> 0:02:19.680
locked down yet. You were kind of in the dark, so we end up where we end up. The good news,

23
0:02:19.680 --> 0:02:25.240
I've been at this for a little while, and the worst and ugliest of the stuff is largely

24
0:02:25.240 --> 0:02:29.440
gone. I mean, there's lots of performance issues left in LibreOffice, but the stuff

25
0:02:29.440 --> 0:02:34.200
that used to hang LibreOffice up for minutes at a time, I've managed to nail most of that

26
0:02:34.200 --> 0:02:39.440
down. There's still lots of places where it's slow, but it's not like heinously slow like

27
0:02:39.440 --> 0:02:46.000
it used to be. The bad news is that the remaining stuff is really, really ugly. It's typically

28
0:02:46.000 --> 0:02:49.640
the stuff that I just couldn't get my head around the first time, and some of it I've

29
0:02:49.640 --> 0:02:53.560
looked at again, and I still can't get my head around it. Some of it I've been beating

30
0:02:53.560 --> 0:02:59.840
on for a while, and I'm not making a lot of progress. But anyhow, the point of this exercise

31
0:02:59.840 --> 0:03:05.200
is that you keep beating on things, and eventually either you break or it breaks. Hopefully,

32
0:03:05.200 --> 0:03:10.200
get the right thing. So what worked out nicely? I'm going to talk today about what worked

33
0:03:10.200 --> 0:03:14.680
out well and what worked out worse, because I think it's important that we share both

34
0:03:14.680 --> 0:03:19.000
the successes and the failures, because I think you learn equally from both. So what

35
0:03:19.000 --> 0:03:23.760
worked out nicely is reducing allocations, primarily with things like using standard

36
0:03:23.760 --> 0:03:30.520
optional. So for example, if we were allocating a temporary data structure inside a subroutine,

37
0:03:30.520 --> 0:03:37.600
I would switch that to declare that thing using standard optional, which does have the

38
0:03:37.600 --> 0:03:43.120
downside that it takes up space on the stack even if you're not using it. But on the upside,

39
0:03:43.120 --> 0:03:48.800
using the stack is about 100 times faster than allocating from main memory. The reasons

40
0:03:48.800 --> 0:03:52.600
for that are because any time you touch main, any time you have to allocate out of the heap,

41
0:03:52.600 --> 0:03:56.480
you have to take a lock. That leads to locking contention, et cetera, et cetera. You have

42
0:03:56.480 --> 0:04:00.000
to talk to main memory because you're doing all sorts of stuff there, whereas the stack

43
0:04:00.000 --> 0:04:05.600
is pretty much guaranteed to be in L1 cache. And so allocating out of there and deallocating

44
0:04:05.600 --> 0:04:10.000
out of there is just ridiculously fast. So you can allocate quite large amounts of memory

45
0:04:10.000 --> 0:04:14.200
before you start falling out of L1 cache from talking up to a couple of kilobytes here.

46
0:04:14.200 --> 0:04:19.040
So throwing this sort of stuff at it worked well. The other thing I did was I've been

47
0:04:19.040 --> 0:04:24.580
converting the usages of our internal OSL, colon, colon, mutex class to the standard

48
0:04:24.580 --> 0:04:28.880
mutex class. Now, despite the naming here, we're talking about two different kinds of

49
0:04:28.880 --> 0:04:36.520
mutexes. Our internal mutex is what's normally called standard recursive mutex, whereas

50
0:04:36.520 --> 0:04:42.920
standard mutex is a non-recursive mutex, and it is considerably faster. In the uncontended

51
0:04:42.920 --> 0:04:49.360
case, standard mutex is about 20 to 50 times faster than standard recursive mutex. In the

52
0:04:49.360 --> 0:04:54.000
contended case, they tend to fall back to roughly being about the same cost. So given

53
0:04:54.000 --> 0:04:59.920
that we run most of the time with very, very little concurrency, except for an occasional

54
0:04:59.920 --> 0:05:05.760
use case here and there, standard mutex is a major win. The downside of standard mutex

55
0:05:05.760 --> 0:05:12.760
is that if you use it and you try and lock that mutex a second time, you get a dead lock.

56
0:05:12.760 --> 0:05:18.360
So you have to write your code quite carefully. Most of our usages converted very easily.

57
0:05:18.360 --> 0:05:22.760
I did make a couple of mistakes, so there were a couple of regressions I had to fix,

58
0:05:22.760 --> 0:05:25.720
and in a couple of cases, the regressions were simply unfixable, and so we backed it

59
0:05:25.720 --> 0:05:31.520
back out again. But in general, it's a win. And as a side effect, standard mutex is allocation

60
0:05:31.520 --> 0:05:37.880
free. It's literally just a single word in memory, and the common unattended case of

61
0:05:37.880 --> 0:05:42.880
taking a standard mutex is what's called a CAS operation, compare and swap. So it's

62
0:05:42.880 --> 0:05:49.800
really fast, and it doesn't use even less memory than OSL mutex, so a win all round.

63
0:05:49.800 --> 0:05:55.320
What didn't work out? So we've got this SVL shared string pool. It's a really nice thing.

64
0:05:55.320 --> 0:06:01.080
We use it in spreadsheets primarily, because spreadsheets often have many tens of thousands

65
0:06:01.080 --> 0:06:05.800
of strings which are all identical across all the cells, so we store references to those

66
0:06:05.800 --> 0:06:11.880
strings in the shared pool. Now, this shared pool gets hit very hard at load time, because

67
0:06:11.880 --> 0:06:18.120
other people just implemented concurrent loading of spreadsheets. So we have this nice stuff

68
0:06:18.120 --> 0:06:23.320
where we fire up multiple threads and load a bunch of different chunks of the spreadsheet

69
0:06:23.320 --> 0:06:28.880
at the same time. This is great, speeds things up. But it was bottlenecking very hard in

70
0:06:28.880 --> 0:06:34.640
SVL shared string pool. So I thought, oh, it's great. This is a hash map. Best case

71
0:06:34.640 --> 0:06:39.040
scenario, we can stick a concurrent lock free hash map in there. I found this great

72
0:06:39.040 --> 0:06:43.200
cuckoo hash map written by some very bright people, much smarter than myself. I stuck

73
0:06:43.200 --> 0:06:48.560
it in there. Oh, it was great. It worked out brilliantly. It completely destroyed the bottleneck.

74
0:06:48.560 --> 0:06:52.360
Spreadsheet loading went up by a factor of two or three, and I thought I had a great

75
0:06:52.360 --> 0:06:57.160
win. And then the first bug came in where there was an edge case where it wasn't quite

76
0:06:57.160 --> 0:07:01.680
the locking, because we were doing concurrent locking now. It was getting some weird edge

77
0:07:01.680 --> 0:07:07.400
cases. We had two different maps. They were both talking in the same thing. So I then

78
0:07:07.400 --> 0:07:12.000
had to fix those edge cases. I reworked it. We fixed those bugs. We reworked it again.

79
0:07:12.000 --> 0:07:19.720
I fixed the bugs, and eventually got it working. And then another bright guy came in, Lubwash

80
0:07:19.720 --> 0:07:27.800
came in and said, but wait. String pool is indeed a hotspot, but the hotspot is not actually

81
0:07:27.800 --> 0:07:33.200
the hash map inside string pool, the hotspot is the upper case conversion. He improved

82
0:07:33.200 --> 0:07:37.320
the upper case conversion. And by the time he was done improving the upper case conversion,

83
0:07:37.320 --> 0:07:44.480
my concurrent hash map was making no difference whatsoever. So I had identified a hotspot,

84
0:07:44.480 --> 0:07:50.460
but I hadn't identified the hotspot. So we backed it out, because there's no point in

85
0:07:50.460 --> 0:07:54.240
keeping a highly complicated piece of equipment like that around. So I was very sad to see

86
0:07:54.240 --> 0:07:59.840
it go. But I learned some stuff in the process. So no harm, no foul. And we backed it out

87
0:07:59.840 --> 0:08:05.560
again, and we were back to being faster than we were before. Red line processing in writer,

88
0:08:05.560 --> 0:08:11.320
which is our document editor section, is often very, very slow, especially if there's a ton

89
0:08:11.320 --> 0:08:16.920
of red lines in a big document. And it's slow both loading and slow at runtime, because

90
0:08:16.920 --> 0:08:21.320
when we're doing red lines, we often are doing massive amounts of adding and deleting to

91
0:08:21.320 --> 0:08:25.960
data structures as we internally render and stuff. And so I thought I'd try and speed that

92
0:08:25.960 --> 0:08:32.240
up. However, this is writer. Writer is the most cache-unfriendly program known to mankind.

93
0:08:32.240 --> 0:08:40.160
It just wants to chase pointers all over RAM. It's data set once you have more than a sort

94
0:08:40.160 --> 0:08:45.120
of 10-page document. You are completely falling out of L1 cache. So you have no chance of

95
0:08:45.120 --> 0:08:50.280
getting cached algorithms. The data structures are inherently very, very complicated. Human

96
0:08:50.280 --> 0:08:55.220
languages and documents just are. Consequences are we do pointer chasing. So we're constantly

97
0:08:55.220 --> 0:08:59.600
loading a pointer and then following it to some other location in memory, which is guaranteed

98
0:08:59.600 --> 0:09:04.120
to not to be an L1 cache. And then we're following that again to something else, which is also

99
0:09:04.120 --> 0:09:07.560
not an L1 cache. And in the process, we've now blown apart our cache. So if we need the

100
0:09:07.560 --> 0:09:12.280
first pointer again, that's also not an L1 cache. So we just take our tails around, enable

101
0:09:12.280 --> 0:09:17.760
very slow processing. I did my best to fix this. And that involves trying to speed up

102
0:09:17.760 --> 0:09:25.080
something called a node index and a content index. And I failed. I am now three levels

103
0:09:25.080 --> 0:09:30.160
deep, fixing different things related to that. No end in sight. And I'm currently bottlenecked

104
0:09:30.160 --> 0:09:34.960
on a piece of processing that I just don't understand. So that didn't work out. But in

105
0:09:34.960 --> 0:09:41.960
the process, I now know a lot more about writer. And so I consider that a word. Thank you.

106
0:10:04.960 --> 0:10:09.960
Thank you.

