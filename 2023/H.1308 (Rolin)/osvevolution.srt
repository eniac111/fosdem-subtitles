1
0:00:00.000 --> 0:00:07.400
So, once again, hello, everybody.

2
0:00:07.400 --> 0:00:10.520
Welcome to my talk.

3
0:00:10.520 --> 0:00:19.680
This talk is going to be about OSV, evolution of OSV towards greater modularity and possibility.

4
0:00:19.680 --> 0:00:21.320
Thanks to Aslan for introducing me.

5
0:00:21.320 --> 0:00:27.280
So, I've been contributing to OSV since 2016.

6
0:00:27.280 --> 0:00:34.040
Like a year ago, in 2015, I heard about OSV in one of the conferences.

7
0:00:34.040 --> 0:00:40.000
And then, a couple of years later, I was nominated to be one of its committers.

8
0:00:40.000 --> 0:00:47.000
And my greatest contributions to OSV include making OSV run on Firecracker and significantly

9
0:00:47.000 --> 0:00:53.240
improving ARG64 port, among other things.

10
0:00:53.240 --> 0:00:57.640
So I'm not sure if you can tell it, but OSV is actually my hobby.

11
0:00:57.640 --> 0:01:04.300
So I'm not like a real current developer like many of previous speakers are.

12
0:01:04.300 --> 0:01:10.240
So it's actually, you know, I work on it in my night when I feel it.

13
0:01:10.240 --> 0:01:14.560
And I have a day job, so I don't represent my company that I work for.

14
0:01:14.560 --> 0:01:24.960
So this is all my personal contribution to the project.

15
0:01:24.960 --> 0:01:32.080
So in today's presentation, I will talk about enhancements introduced by the latest release

16
0:01:32.080 --> 0:01:40.400
of OSV, 057, with a focus on greater modularity and composability.

17
0:01:40.400 --> 0:01:48.480
But I will also discuss other interesting enhancements, like lazy stock, novel ways

18
0:01:48.480 --> 0:01:54.160
to build ZFS images, and improvements to the ARM port.

19
0:01:54.160 --> 0:02:03.440
Finally, I will also cover an interesting use case of OSV, C-WIDFS, running on OSV,

20
0:02:03.440 --> 0:02:07.120
which is a distributed file system.

21
0:02:07.120 --> 0:02:16.280
So as you can see in this talk, besides the title, modularity, I will actually try to

22
0:02:16.280 --> 0:02:23.080
give you state of the art, where OSV is, how it has changed recently, and a little bit

23
0:02:23.080 --> 0:02:28.440
of where it's going, hopefully.

24
0:02:28.440 --> 0:02:35.040
So I know there are probably many definitions of unicurns, and each of them is a little

25
0:02:35.040 --> 0:02:43.800
bit different, and I'm sure most of you understand what unicurnls are, but just a quick recap,

26
0:02:43.800 --> 0:02:47.200
with emphasis on how OSV is a little bit different.

27
0:02:47.200 --> 0:02:52.960
So OSV is a unicurnl that was designed to run single and modified Linux application

28
0:02:52.960 --> 0:02:58.440
on top of Hypervisor, whereas traditional operating systems were originally designed

29
0:02:58.440 --> 0:03:02.180
to run on a vast range of physical machines.

30
0:03:02.180 --> 0:03:10.280
But simply speaking, OSV is an OS designed to run single application without isolation

31
0:03:10.280 --> 0:03:18.240
between application and kernel, or it can be thought as a way to run highly isolated

32
0:03:18.240 --> 0:03:23.320
process without ability to make system calls to the host OS.

33
0:03:23.320 --> 0:03:34.760
Finally, OSV can run on both 64-bit x86 and ARMv8 architectures.

34
0:03:34.760 --> 0:03:37.160
Now a little bit of history.

35
0:03:37.160 --> 0:03:46.240
So OSV, for those that don't know, OSV was started in late 2012 by the company called

36
0:03:46.240 --> 0:03:55.960
Cloud Use Systems, and they built pretty strong team of 10, 20 developers, I think.

37
0:03:55.960 --> 0:04:03.520
I wasn't one of them, but they pretty much wrote most of OSV, but at some point they

38
0:04:03.520 --> 0:04:07.480
basically realized they have to make money, I'm guessing.

39
0:04:07.480 --> 0:04:15.840
So they basically moved on and started working on this product you may have known, CillaDB,

40
0:04:15.840 --> 0:04:21.080
which is this high performance database, but I think they took some learning.

41
0:04:21.080 --> 0:04:27.640
And after that, basically, I think OSV did receive some grant from European Union, so

42
0:04:27.640 --> 0:04:33.720
there was some project on that, and I think there may have been some companies also using

43
0:04:33.720 --> 0:04:40.440
OSV, but honestly, since then, it's been really maintained by volunteers.

44
0:04:40.440 --> 0:04:52.040
So like me, there's still some people from CillaDB, Nadav Harrell, and others that contribute

45
0:04:52.040 --> 0:04:53.040
to the project.

46
0:04:53.040 --> 0:04:59.040
I would just single out 46 and Aqis, which actually was the one that implemented Virtay

47
0:04:59.040 --> 0:05:04.480
OFS as a very interesting contribution to OSV.

48
0:05:04.480 --> 0:05:10.680
And obviously, I would like to take this opportunity to invite more people to become part of our

49
0:05:10.680 --> 0:05:17.880
community, because honestly, you may not realize it, but our community is very small, so it's

50
0:05:17.880 --> 0:05:28.200
just really me, Nadav, and a couple of other people that contribute to the project.

51
0:05:28.200 --> 0:05:34.760
So I hope we're going to grow as a community after this talk.

52
0:05:34.760 --> 0:05:41.520
So a quick recap of a little bit of how OSV looks like, what the design is.

53
0:05:41.520 --> 0:05:49.480
So in this slide, you can see major components of OSV across layers, starting with G-LIPC,

54
0:05:49.480 --> 0:05:56.680
the top, which is greatly based actually on Mosul, then core layer in the middle, comprised

55
0:05:56.680 --> 0:06:06.200
of ELF, dynamic linker, of VFS, virtual file system, and then networking stack, thread scheduler,

56
0:06:06.200 --> 0:06:15.760
page cache, RCU, read, copy, update, page table management, and L1, L2, pools to manage

57
0:06:15.760 --> 0:06:16.760
memory.

58
0:06:16.760 --> 0:06:24.120
And then you have a layer of device drivers, where OSV implements Virtay O layer based

59
0:06:24.120 --> 0:06:34.960
virtual devices on both over PCI transport and MMIL transport, and then Zen and VMware,

60
0:06:34.960 --> 0:06:37.560
among others.

61
0:06:37.560 --> 0:06:44.440
And obviously, and one more thing, so OSV can run on KVM based hypervisors like QEMUIAC,

62
0:06:44.440 --> 0:06:45.840
like Firecracker.

63
0:06:45.840 --> 0:06:56.080
I did test also OSV on cloud hypervisor, which is I think Intel's hypervisor written in Rust.

64
0:06:56.080 --> 0:07:03.960
And then I personally didn't really run OSV on Zen, so I know that the Zen support is

65
0:07:03.960 --> 0:07:07.840
a little bit dated probably, and I'm not sure how much it has been tested.

66
0:07:07.840 --> 0:07:15.360
I did test on VMware vbox, virtual box, and I think on Hyperkit at some point.

67
0:07:15.360 --> 0:07:22.240
So I will just, I won't go into more detail about this diagram, but I will leave it with

68
0:07:22.240 --> 0:07:28.720
use just as a reference later.

69
0:07:28.720 --> 0:07:37.320
So in the first part of this presentation, I will, about modularity and composability,

70
0:07:37.320 --> 0:07:44.800
I will focus on new experimental modes to hide the non-G-LIP-C symbols and standard

71
0:07:44.800 --> 0:07:46.200
C++ library.

72
0:07:46.200 --> 0:07:53.160
I will also discuss how ZFS code was extracted out of the kernel in form of a dynamically

73
0:07:53.160 --> 0:07:54.440
linked library.

74
0:07:54.440 --> 0:08:02.200
And finally, I will also explain another new build option to tailor the kernel to a set

75
0:08:02.200 --> 0:08:03.760
of specific drivers.

76
0:08:03.760 --> 0:08:06.120
I call them driver profiles.

77
0:08:06.120 --> 0:08:12.800
And another new mechanism to allow building a version of kernel with a subset of G-LIP-C

78
0:08:12.800 --> 0:08:23.880
symbols needed to support a specific application, which I think is quite interesting.

79
0:08:23.880 --> 0:08:34.600
So by design, OSV has always been a fat unicernel, which has been some sort of, some other criticism.

80
0:08:34.600 --> 0:08:40.320
And by default provided a large subset of G-LIP-C functionality has included full standard

81
0:08:40.320 --> 0:08:49.840
C++ library and ZFS implementation drivers for many devices and has supported many hypervisors.

82
0:08:49.840 --> 0:08:58.000
So on one hand, it makes running arbitrary application on any hypervisor very easy using

83
0:08:58.000 --> 0:09:00.880
a single universal kernel.

84
0:09:00.880 --> 0:09:07.480
But on another hand, such universality comes with the price of bloated kernel with many

85
0:09:07.480 --> 0:09:13.480
symbols and drivers and possibly ZFS that is unused.

86
0:09:13.480 --> 0:09:23.080
That's causing inefficient memory usage, longer boot time, and potential security vulnerabilities.

87
0:09:23.080 --> 0:09:30.920
In addition, C++ application linked against one version of LIP-SDC++ different than the

88
0:09:30.920 --> 0:09:35.960
version the kernel was linked against may simply not work.

89
0:09:35.960 --> 0:09:42.920
For example, that happened to me when I was testing OSV with.NET.

90
0:09:42.920 --> 0:09:54.320
And yeah, the only way to make it work was to hide basically the C++ standard library

91
0:09:54.320 --> 0:10:03.760
and use the one that was part of the.NET app.

92
0:10:03.760 --> 0:10:10.120
So one way to lower memory utilization of the guest is to minimize the kernel size.

93
0:10:10.120 --> 0:10:17.540
By default, OSV comes with a universal kernel that provides quite large spectrum of G-LIP-C

94
0:10:17.540 --> 0:10:25.920
library and full standard C++ library and exposes over a total of 17,000 symbols.

95
0:10:25.920 --> 0:10:33.420
And most of those are very long, as you know, C++ symbols that make up the symbol table.

96
0:10:33.420 --> 0:10:41.000
So the question may be posed, why not have a mechanism where we can build a kernel with

97
0:10:41.000 --> 0:10:50.080
all known G-LIP-C symbols hidden and all unneeded code that is unused garbage collected.

98
0:10:50.080 --> 0:10:56.240
So the extra benefit of fewer exported symbols is increased security that stems from the

99
0:10:56.240 --> 0:11:03.760
fact that there's simply less potential code that is left, that is harmful, that could

100
0:11:03.760 --> 0:11:05.360
be harmful.

101
0:11:05.360 --> 0:11:15.360
And also, that way we can achieve better compatibility as any potential symbol collisions, for example,

102
0:11:15.360 --> 0:11:26.680
and mismatch standard C++ library, which I mentioned can be avoided.

103
0:11:26.680 --> 0:11:37.280
So the release 57, 057 added a new build option called conf-hide-symbols to hide those known

104
0:11:37.280 --> 0:11:42.480
G-LIP-C symbols and the standard C++ library symbols.

105
0:11:42.480 --> 0:11:50.080
These are enabled, if enabled, in essence most files in a source tree of OSV, except

106
0:11:50.080 --> 0:11:55.920
the ones under LIP-C and MOSL directories, would be compiled with the flags of visibility

107
0:11:55.920 --> 0:12:02.080
hidden and only if that build flag is enabled.

108
0:12:02.080 --> 0:12:08.040
On the other hand, the symbols to be exposed as public, like the G-LIP-C one, would be

109
0:12:08.040 --> 0:12:14.880
annotated with OSV asterisk API macros that translate basically to attribute visibility

110
0:12:14.880 --> 0:12:22.800
default and the standard C++ library is linked with the flag, no whole archive.

111
0:12:22.800 --> 0:12:30.240
Those S-V asterisk API macros basically would be like OSV LIP-C API or OSV P-threads API,

112
0:12:30.240 --> 0:12:33.680
OSV LIP-M API and so on.

113
0:12:33.680 --> 0:12:41.400
Basically that match all then I think around 10 libraries that OSV dynamic linker exposes.

114
0:12:41.400 --> 0:12:53.200
Finally, the list of public symbols exported by the kernel is enforced during the build

115
0:12:53.200 --> 0:12:59.440
process based on the symbol list files for each advertised library.

116
0:12:59.440 --> 0:13:08.600
Like for example LIP-C SO6 and is maintained under the directory exported symbols.

117
0:13:08.600 --> 0:13:14.720
These files are basically a list of symbols that are concatenated using the script called

118
0:13:14.720 --> 0:13:21.840
generate version script, which goes into version script file and then is fed to the linker

119
0:13:21.840 --> 0:13:28.600
as an argument to the version script flag.

120
0:13:28.600 --> 0:13:35.600
In order to now remove all unneeded code, basically garbage, all files will be compiled

121
0:13:35.600 --> 0:13:42.880
with the flag function sections and data sections and then they would be linked with the flag

122
0:13:42.880 --> 0:13:44.840
GC section.

123
0:13:44.840 --> 0:13:51.600
Now any code that needs to stay, like for example the bootstrap start points or dynamically

124
0:13:51.600 --> 0:14:03.040
enabled code like the optimal MAMCPY implementation or trace point patch size is retained by putting

125
0:14:03.040 --> 0:14:11.640
relevant kept directives and relevant sections in the linker script.

126
0:14:11.640 --> 0:14:21.400
The kernel L file built with most symbols hidden is roughly 4.3 megabytes in size compared

127
0:14:21.400 --> 0:14:27.280
to 6.7, which is reduction of around 40%.

128
0:14:27.280 --> 0:14:34.440
This great reduction stems from the fact that the standard C++ library is no longer linked

129
0:14:34.440 --> 0:14:36.400
with whole archive.

130
0:14:36.400 --> 0:14:43.840
The symbol table is way smaller and unused code is garbage collected.

131
0:14:43.840 --> 0:14:53.640
Please note that the resulting kernel is still universal as it exports all julipsy symbols

132
0:14:53.640 --> 0:14:56.440
and includes all the device drivers.

133
0:14:56.440 --> 0:15:04.120
As the result of this size reduction, kernel boots also a little bit faster.

134
0:15:04.120 --> 0:15:06.040
These all sound great.

135
0:15:06.040 --> 0:15:13.720
One may ask why not hide most symbols and standard C++ library by default.

136
0:15:13.720 --> 0:15:22.240
The problem is that there are around 35 unit tests and some also application that were

137
0:15:22.240 --> 0:15:33.400
written in the past that rely on C++ symbols and they basically would not run if we hide

138
0:15:33.400 --> 0:15:35.520
all of those symbols.

139
0:15:35.520 --> 0:15:45.280
Those are basically used to, they were implemented in the past and it was done sometimes out

140
0:15:45.280 --> 0:15:49.600
of convenience, sometimes basically out of necessity.

141
0:15:49.600 --> 0:15:57.680
To address this specific problem, we will need to expose some of those OSVC++ symbols

142
0:15:57.680 --> 0:16:02.640
as C basically, as API expressed in C.

143
0:16:02.640 --> 0:16:14.640
We will basically define very simple C wrapper functions that will call those C++ code.

144
0:16:14.640 --> 0:16:21.600
I can use this one.

145
0:16:21.600 --> 0:16:28.840
A good example of modularity improvements made in the release 0.57 is extracting ZFS

146
0:16:28.840 --> 0:16:35.080
code out of kernel as a dynamically linked library, libsolarisso, which effectively is

147
0:16:35.080 --> 0:16:37.440
a new module.

148
0:16:37.440 --> 0:16:45.360
To accomplish that, we changed the main OSV makefile to build new artifact, libsolarisso

149
0:16:45.360 --> 0:16:53.280
out of ZFS and solaris file sets in the makefile, which basically used to be linked into kernel.

150
0:16:53.280 --> 0:17:02.320
The new library has to be linked with a bind now flag and OSV specific OSV MLock node to

151
0:17:02.320 --> 0:17:10.360
force OSV dynamic linker to resolve symbols eagerly and populate the mappings eagerly

152
0:17:10.360 --> 0:17:11.700
as well.

153
0:17:11.700 --> 0:17:18.000
This basically is done to prevent page faults that would lead to potential deadlocks as

154
0:17:18.000 --> 0:17:22.520
the libraries load it and initialize.

155
0:17:22.520 --> 0:17:31.220
The init function ZFS initialize called upon the library is loaded creates necessary thread

156
0:17:31.220 --> 0:17:38.780
pulls and registers various callbacks so that the page cache arc, which is adaptive replacement

157
0:17:38.780 --> 0:17:50.480
cache from ZFS, and ZFS dev driver can interact with relevant code in the ZFS library.

158
0:17:50.480 --> 0:17:58.880
On another hand, the OSV kernel needs to expose some around 100 symbols that provide some

159
0:17:58.880 --> 0:18:05.920
internal FreeBSD originating functionality that libsolarisso depends on.

160
0:18:05.920 --> 0:18:12.600
Actually, OSV borrowed some code from FreeBSD and actually a good chunk of this code was

161
0:18:12.600 --> 0:18:18.040
actually implementation of ZFS, which right now is outside of the kernel.

162
0:18:18.040 --> 0:18:27.360
Finally, the virtual file system bootstrap code needs to dynamically load libsolarisso

163
0:18:27.360 --> 0:18:37.680
from bootfs or read-onlyfs using dlopen before mounting ZFS file system.

164
0:18:37.680 --> 0:18:44.120
There are at least three advantages of moving ZFS to a separate library.

165
0:18:44.120 --> 0:18:50.760
Most of ZFS can be optionally loaded from another file system like bootfs or read-onlyfs, partition

166
0:18:50.760 --> 0:18:53.760
on the same disk or another disk.

167
0:18:53.760 --> 0:19:00.080
I will actually discuss that in more detail in one of the upcoming slides later.

168
0:19:00.080 --> 0:19:08.040
Then also kernel gets smaller by around 800 kilobytes and effectively becomes 3.6 megabytes

169
0:19:08.040 --> 0:19:09.040
in size.

170
0:19:09.040 --> 0:19:15.160
Finally, there are at least 10 fewer threads that are needed to run non-ZFS image.

171
0:19:15.160 --> 0:19:31.960
For example, when you run read-onlyfs image on OSV, with one CPU, it only requires 25

172
0:19:31.960 --> 0:19:37.000
threads.

173
0:19:37.000 --> 0:19:45.280
The regular Linux GLLipsi apps should run fine on kernel with most symbols and standard

174
0:19:45.280 --> 0:19:47.440
C++ library hidden.

175
0:19:47.440 --> 0:19:55.800
Unfortunately, many unit tests, which I mentioned, and various internal OSV apps, which are written

176
0:19:55.800 --> 0:20:02.520
mostly in C++, so-called modules, do not, as they had been coded in the past to use

177
0:20:02.520 --> 0:20:07.760
those internal C++ symbols from the kernel.

178
0:20:07.760 --> 0:20:12.960
We have to do something to deal with that problem.

179
0:20:12.960 --> 0:20:23.320
In the release 0.57, we introduced some of the CRAPR API, which are basically in C-style

180
0:20:23.320 --> 0:20:28.160
convention.

181
0:20:28.160 --> 0:20:38.920
We changed those modules to use those CRAPR function instead of C++ code.

182
0:20:38.920 --> 0:20:44.880
The benefit is that down the road, we might have some newer apps or some newer modules

183
0:20:44.880 --> 0:20:50.440
that would use those CRAPR functions.

184
0:20:50.440 --> 0:20:54.520
It also may make OSV more modular.

185
0:20:54.520 --> 0:21:00.960
As you can see, one of those examples is, for example, OSV gets all threads, which is

186
0:21:00.960 --> 0:21:12.680
basically a function that gives a thread-safe way to color to basically iterate over threads,

187
0:21:12.680 --> 0:21:22.840
which for example is used in an HTTP monitoring module to list all the threads.

188
0:21:22.840 --> 0:21:32.720
A good example of OSV-specific modules that uses some internal C++ symbols is HTTP server

189
0:21:32.720 --> 0:21:33.720
monitoring.

190
0:21:33.720 --> 0:21:41.720
We modify the HTTP monitoring module to stop using internal kernel C++ API.

191
0:21:41.720 --> 0:21:47.480
We do it by replacing some of the calls to internal C++ symbols with this new module

192
0:21:47.480 --> 0:21:54.360
C-style API, symbols from the slide which you saw on the slide before, for example,

193
0:21:54.360 --> 0:21:59.840
SCAD with all threads with this new OSV getAllThreads function.

194
0:21:59.840 --> 0:22:04.760
In other scenarios, we fall back to standard G-LipC API.

195
0:22:04.760 --> 0:22:12.200
For example, the monitoring app used to call OSV current mount.

196
0:22:12.200 --> 0:22:29.400
Right now, it uses basically getNT and function and related ones.

197
0:22:29.400 --> 0:22:37.480
The release 0.57 introduced another build mechanism that allows creating a custom kernel

198
0:22:37.480 --> 0:22:43.000
with a specific list of drivers intended to target given hypervisor.

199
0:22:43.000 --> 0:22:50.800
Obviously such kernel benefits from even smaller size and better security as all unneeded code,

200
0:22:50.800 --> 0:22:55.480
all unneeded drivers are basically excluded during the build process.

201
0:22:55.480 --> 0:23:02.360
In essence, we introduce a new build script and make file parameter, driver's profile.

202
0:23:02.360 --> 0:23:09.240
This new parameter is intended to specify a driver profile which is simply a list of

203
0:23:09.240 --> 0:23:19.320
device drivers to be linked into the kernel and some extra functionality like PCI or ACPI

204
0:23:19.320 --> 0:23:21.760
these drivers depend on.

205
0:23:21.760 --> 0:23:30.640
Each profile is specified in a tiny include files with the MK extension under conf profiles

206
0:23:30.640 --> 0:23:39.400
arch directory and included by the main make file as requested by the driver profile parameter.

207
0:23:39.400 --> 0:23:48.200
The main make file has a number of basically if expressions and adds a conditionally given

208
0:23:48.200 --> 0:23:57.800
driver object to the linked object list depending on the value of 0 or 1 of the given conf driver's

209
0:23:57.800 --> 0:24:02.960
parameter specified in that include file.

210
0:24:02.960 --> 0:24:10.680
The benefit of using drivers are most profound when they are used with when you build kernel

211
0:24:10.680 --> 0:24:17.680
and hide most of the symbols as I talked about in one of the previous slides.

212
0:24:17.680 --> 0:24:24.440
It's also possible to enable or disable individual drivers on top of profiles.

213
0:24:24.440 --> 0:24:30.640
Files are basically list of the drivers but there are number of configuration parameters

214
0:24:30.640 --> 0:24:35.880
that where you can specifically for example include which I'm going to be actually showing

215
0:24:35.880 --> 0:24:36.880
here.

216
0:24:36.880 --> 0:24:42.120
You can include specific driver.

217
0:24:42.120 --> 0:24:47.320
One may ask a question why not use something more standard like menu config like for example

218
0:24:47.320 --> 0:24:49.880
what Unicraft does.

219
0:24:49.880 --> 0:24:58.240
Actually OSV has this specific build system and I didn't want to basically now introduce

220
0:24:58.240 --> 0:25:01.960
another way of doing things.

221
0:25:01.960 --> 0:25:09.760
That's where we basically script the build uses the various effectively parameters to

222
0:25:09.760 --> 0:25:21.520
for example take to hide symbols or specify specific driver profile or list of other parameters.

223
0:25:21.520 --> 0:25:38.640
As you can see in the first example we built default kernel with all symbols hidden and

224
0:25:38.640 --> 0:25:46.360
the resulting kernel is around 36 3.6 megabytes.

225
0:25:46.360 --> 0:25:56.800
In the next example we actually use we built kernel with the VITIO over PCI profiles which

226
0:25:56.800 --> 0:26:06.560
is like 300 kilobytes smaller and then we in the third one we built kernel which is

227
0:26:06.560 --> 0:26:20.480
intended to for example for Firecracker when we include only VITIO block device and networking

228
0:26:20.480 --> 0:26:23.840
driver over MMIO transport.

229
0:26:23.840 --> 0:26:31.040
And then just to see basically in a fourth one just to see how large the drivers code

230
0:26:31.040 --> 0:26:37.400
in OSV is when you basically use driver profiles base which is basically nothing no drivers

231
0:26:37.400 --> 0:26:46.880
you can see that roughly 600 kilobytes of the drivers code is roughly 600 kilobytes

232
0:26:46.880 --> 0:26:48.360
in size.

233
0:26:48.360 --> 0:26:54.560
And then in the last one actually option is where you can specify you use basically drivers

234
0:26:54.560 --> 0:27:02.280
profile and then you explicitly say which specific drivers or driver like the capability

235
0:27:02.280 --> 0:27:14.840
like in this case ACPI, VITIOFS and VITIO net and PV panic devices you want to use.

236
0:27:14.840 --> 0:27:21.960
Actually with the new version with new release of OSV 0.57 we started publishing new versions

237
0:27:21.960 --> 0:27:29.840
of new variations effectively of OSV kernel that correspond to this I thought you know

238
0:27:29.840 --> 0:27:36.600
interesting build configuration that I just mentioned and in this example the OSV loader

239
0:27:36.600 --> 0:27:45.600
hidden artifacts are effectively the versions of OSV kernel built with most symbols hidden

240
0:27:45.600 --> 0:27:59.280
and then for example which will be at the top for both ARM and x86 and then for example

241
0:27:59.280 --> 0:28:05.400
right here in the second and the third and fourth artifacts basically version of the

242
0:28:05.400 --> 0:28:12.240
kernel built for micro VM profile which is effectively something that you would use to

243
0:28:12.240 --> 0:28:25.040
run OSV on firecracker which only has VITIO over MMIO transport.

244
0:28:25.040 --> 0:28:32.400
Now the release 0.57 introduced yet another built mechanism and that allows creation of

245
0:28:32.400 --> 0:28:38.400
a custom kernel by exporting only symbols required by a specific application.

246
0:28:38.400 --> 0:28:45.240
The extra such kernel benefits from the fact that again it's a little bit smaller and thus

247
0:28:45.240 --> 0:28:53.400
offers better security as in essence all unneeded code by that specific application is removed.

248
0:28:53.400 --> 0:28:59.800
This new mechanism relies on two scripts that analyze the built manifest, detect application

249
0:28:59.800 --> 0:29:08.320
L files, identify symbols required from OSV kernel and finally produce the application-specific

250
0:29:08.320 --> 0:29:14.320
version script under app version script.

251
0:29:14.320 --> 0:29:21.400
The generate app version script iterates over the manifest files produced by list manifest

252
0:29:21.400 --> 0:29:29.560
files pi, identifies undefined symbols in the L files using object dump that are also

253
0:29:29.560 --> 0:29:35.560
exported by OSV kernel and finally generates basically the app version script.

254
0:29:35.560 --> 0:29:40.840
So please note that this functionality only works when you build kernel with most symbols

255
0:29:40.840 --> 0:29:43.480
hidden.

256
0:29:43.480 --> 0:29:52.320
So I think what is kind of interesting, well interesting, worth noting in that approach

257
0:29:52.320 --> 0:29:56.680
is that you basically run a built script against given application twice.

258
0:29:56.680 --> 0:30:02.960
Basically first time to identify all symbols that application needs from OSV kernel and

259
0:30:02.960 --> 0:30:15.080
then actually second time we do is to build the kernel for that specific app.

260
0:30:15.080 --> 0:30:24.000
In this example we actually generate kernel specific to run a simple going app on OSV

261
0:30:24.000 --> 0:30:38.520
and when you actually build kernel with symbols around I think 30 symbols in by going pi example,

262
0:30:38.520 --> 0:30:49.600
the kernel is effectively by around half a megabyte smaller and it's around 3.2 megabytes.

263
0:30:49.600 --> 0:31:00.600
So this approach has obviously some limitations.

264
0:31:00.600 --> 0:31:07.280
Some applications obviously use for example DLS then to dynamically resolve symbols and

265
0:31:07.280 --> 0:31:10.480
those would be missed by this technique.

266
0:31:10.480 --> 0:31:16.440
So in this scenario basically for now you have to manually find those symbols and add

267
0:31:16.440 --> 0:31:20.200
them to the app version script file.

268
0:31:20.200 --> 0:31:27.440
Conversely a lot of jilipsy functionality is still in OSV in the Linux CC where all

269
0:31:27.440 --> 0:31:37.640
the system calls are actually implemented is still basically references all the code

270
0:31:37.640 --> 0:31:40.680
in some of the parts of the lips implementation.

271
0:31:40.680 --> 0:31:43.240
So this obviously also would not be removed.

272
0:31:43.240 --> 0:31:54.080
So obviously we could think of ways of finding some kind of built mechanism that could for

273
0:31:54.080 --> 0:32:04.560
example find all the usages of Cisco instruction or SVC on ARM and analyze and find all these

274
0:32:04.560 --> 0:32:09.520
only code that is needed.

275
0:32:09.520 --> 0:32:14.400
In the future we may componentize other functional elements of the kernel for example the DHCP

276
0:32:14.400 --> 0:32:19.720
lookup code could be either loaded from a separate library or compiled out depending

277
0:32:19.720 --> 0:32:25.120
on some build option to improve compatibility while also planning to add support of statically

278
0:32:25.120 --> 0:32:33.280
linked executables which would require implementing at least clone, BRK and arch PRCTL syscalls.

279
0:32:33.280 --> 0:32:38.680
We may also introduce ability to swap built in version of jilipsy libraries with third

280
0:32:38.680 --> 0:32:45.920
party ones for example the subset of libm so that is provided by OSV kernel could be

281
0:32:45.920 --> 0:32:55.240
possibly hidden with the mechanism that is discussed and we could use different implementation

282
0:32:55.240 --> 0:32:57.160
of that library.

283
0:32:57.160 --> 0:33:04.600
Finally we are considering to expand standard procfs and syscfs and OSV specific parts of

284
0:33:04.600 --> 0:33:10.600
syscfs that would better support statically linked executables but also allow regular

285
0:33:10.600 --> 0:33:13.800
apps to interact with OSV.

286
0:33:13.800 --> 0:33:20.640
A good example of it could be implementation of netstat like type of capability application

287
0:33:20.640 --> 0:33:31.680
that could expose the networking internals of OSV better during runtime.

288
0:33:31.680 --> 0:33:35.920
In the next part of the presentation I will discuss the other interesting enhancements

289
0:33:35.920 --> 0:33:39.360
introduced as part of the latest 0.57 release.

290
0:33:39.360 --> 0:33:45.800
More specifically I will talk about lazy stack and new ways to build ZFS images and finally

291
0:33:45.800 --> 0:33:51.680
the improvements to the AR-64 port.

292
0:33:51.680 --> 0:34:00.120
The lazy stack which by the way is actually the idea that was felt off by Nadav Harrell

293
0:34:00.120 --> 0:34:06.800
which maybe is listening to this presentation effectively allows to save substantial amount

294
0:34:06.800 --> 0:34:12.440
of memory if an application spawns many p threads with large stack by letting stack

295
0:34:12.440 --> 0:34:17.840
grow dynamically as needed instead of getting pre-populated ahead of time which is normally

296
0:34:17.840 --> 0:34:19.760
the case right now with OSV.

297
0:34:19.760 --> 0:34:26.080
So on OSV right now all kernel threads and all application threads have stacks that are

298
0:34:26.080 --> 0:34:31.200
automatically pre-populated which is obviously not very memory efficient.

299
0:34:31.200 --> 0:34:36.420
Now the crux of the solution is based on observation that OSV page fault handler requires that

300
0:34:36.420 --> 0:34:45.880
both interrupts and preemption must be enabled when fault is triggered and therefore if stack

301
0:34:45.880 --> 0:34:51.720
is dynamically mapped we need to make sure that the stack page fault never happens in

302
0:34:51.720 --> 0:34:56.920
these relatively few places where the kernel code that executes with either interrupts

303
0:34:56.920 --> 0:35:00.160
or preemption disabled.

304
0:35:00.160 --> 0:35:06.200
And we basically satisfied this requirement by pre-faulting the stack by reading one byte,

305
0:35:06.200 --> 0:35:14.260
one page down per stack pointer just before preemption or interrupts are disabled.

306
0:35:14.260 --> 0:35:18.760
So a good example of that code would be in a scheduler right when OSV scheduler is trying

307
0:35:18.760 --> 0:35:29.120
to figure out what the next threads to switch to and obviously that code has preemption

308
0:35:29.120 --> 0:35:38.240
and interrupts disabled and we wouldn't obviously want to have page fault happen at that moment.

309
0:35:38.240 --> 0:35:44.960
So there are relatively few places when that happens and this idea is to basically pre-fault

310
0:35:44.960 --> 0:35:48.280
this code.

311
0:35:48.280 --> 0:35:54.760
So to achieve that we basically analyze OSV code to find all the places where the IRQ

312
0:35:54.760 --> 0:36:01.160
disabled and pre-empt disabled is called directly or indirectly sometimes and pre-fault the

313
0:36:01.160 --> 0:36:03.800
stack there if necessary.

314
0:36:03.800 --> 0:36:07.880
As we analyze all call sites we need to follow basically five rules.

315
0:36:07.880 --> 0:36:13.480
The first one do nothing if the call in question executes always on the kernel thread because

316
0:36:13.480 --> 0:36:15.480
it has pre-populated stack.

317
0:36:15.480 --> 0:36:19.800
There's no chance that page fault is going to happen.

318
0:36:19.800 --> 0:36:26.960
Second one is do nothing if the call site executes on other type of pre-populated stack.

319
0:36:26.960 --> 0:36:33.760
The good example of that would be the interrupt and exception stack or syscall stack which

320
0:36:33.760 --> 0:36:37.320
are all pre-populated.

321
0:36:37.320 --> 0:36:44.560
And the number three rule is do nothing if the call site executes when we know that either

322
0:36:44.560 --> 0:36:50.800
interrupts or pre-emptions are disabled because we don't need to somebody already probably

323
0:36:50.800 --> 0:36:52.400
pre-faulted that.

324
0:36:52.400 --> 0:36:57.760
And then pre-fault unconditionally if we know that both pre-emptions and interrupts are

325
0:36:57.760 --> 0:37:00.000
about to be enabled.

326
0:37:00.000 --> 0:37:07.280
And otherwise pre-fault stack by determining dynamically basically by calling the pre-emptable

327
0:37:07.280 --> 0:37:11.320
is pre-emptable and IRQ enabled functions.

328
0:37:11.320 --> 0:37:20.200
Now the idea basically if we only always if we did if we followed only rule number five

329
0:37:20.200 --> 0:37:24.200
which actually this is what I tried to do in the very beginning the first attempt to

330
0:37:24.200 --> 0:37:28.680
implement like syscall it would be actually pretty inefficient.

331
0:37:28.680 --> 0:37:36.840
I mean I saw pretty significant degradation of for example you know context switch and

332
0:37:36.840 --> 0:37:46.360
other parts of the OSV when I dynamically checked if preemption and interrupts were

333
0:37:46.360 --> 0:37:47.360
disabled.

334
0:37:47.360 --> 0:37:52.080
So this access was pretty painful to basically analyze the code but I think it was worth

335
0:37:52.080 --> 0:37:55.400
it.

336
0:37:55.400 --> 0:38:00.680
As you remember from the modularity slides the ZFS file system has been extracted from

337
0:38:00.680 --> 0:38:07.640
the kernel as a separate shared library called libsolyzeso which can be loaded from the different

338
0:38:07.640 --> 0:38:11.480
file system before ZFS file system can be mounted.

339
0:38:11.480 --> 0:38:15.680
This allows for three ways ZFS can be mounted by OSV.

340
0:38:15.680 --> 0:38:21.680
The first and original way assumes that ZFS is mounted at the root from the first partition

341
0:38:21.680 --> 0:38:23.320
of the first disk.

342
0:38:23.320 --> 0:38:29.120
The second one involves mounting ZFS from the second partition of the first disk and

343
0:38:29.120 --> 0:38:34.680
at an arbitrary non-root point for example slash data.

344
0:38:34.680 --> 0:38:39.960
Similarly the third way involves mounting ZFS from the first partition of the second

345
0:38:39.960 --> 0:38:46.200
or higher disk at an arbitrary non-root point as well.

346
0:38:46.200 --> 0:38:52.000
Please note that the second and third options assume that the root file system is non-ZFS

347
0:38:52.000 --> 0:38:59.480
obviously and which could be like read-onlyFS or bootFS.

348
0:38:59.480 --> 0:39:09.080
This slide shows you the build command and how OSV runs when we follow the original and

349
0:39:09.080 --> 0:39:13.320
default method of building and mounting ZFS.

350
0:39:13.320 --> 0:39:23.120
For those that have done it, there's nothing really interesting here.

351
0:39:23.120 --> 0:39:31.320
This is a new method, actually the first of the two new ones where we actually allow ZFS

352
0:39:31.320 --> 0:39:38.040
to be mounted at a non-root mount point like beta for example and mixed with another file

353
0:39:38.040 --> 0:39:39.520
system on the same disk.

354
0:39:39.520 --> 0:39:45.920
Please note that libsolaris as always plays on the root file system, typically read-onlyFS

355
0:39:45.920 --> 0:39:50.600
under usr.libfs and load it from it automatically.

356
0:39:50.600 --> 0:40:00.280
The build script will automatically add the relevant mount point time to its ZFS style.

357
0:40:00.280 --> 0:40:06.240
The last method is basically similar to the one before but this time we allow ZFS to be

358
0:40:06.240 --> 0:40:13.480
mounted from the partition from the second disk or another one.

359
0:40:13.480 --> 0:40:20.320
Actually what happens with this option, I noticed that OSV would actually mount ZFS

360
0:40:20.320 --> 0:40:27.560
file system by around 30 to 40 milliseconds faster.

361
0:40:27.560 --> 0:40:38.920
Now there's another new feature we used to run in order to build ZFS images and file

362
0:40:38.920 --> 0:40:43.760
system we would use OSV itself to do it.

363
0:40:43.760 --> 0:40:50.360
With this new release there's a specialized version of the kernel called ZFS loader which

364
0:40:50.360 --> 0:40:57.840
basically delegates to this utility like ZPUL, ZFS and so on to mount OSV.

365
0:40:57.840 --> 0:41:07.840
But there's also now a new script called ZFS image on host that can be used to mount OSV

366
0:41:07.840 --> 0:41:13.040
ZFS images provided you have open ZFS functionality on your host system.

367
0:41:13.040 --> 0:41:21.800
Which is actually quite nice because you can mount basically OSV disk and introspect it.

368
0:41:21.800 --> 0:41:30.560
You can also modify it using standard Linux tools and unmount it and use it on OSV again.

369
0:41:30.560 --> 0:41:38.720
Here's some help on how this new script can be used.

370
0:41:38.720 --> 0:41:43.640
Now I think I don't have much time left but I will try.

371
0:41:43.640 --> 0:41:49.080
Also I will focus a little bit on the AR64 improvements.

372
0:41:49.080 --> 0:41:53.200
I will focus on three things that I think are worth mentioning.

373
0:41:53.200 --> 0:42:04.320
The changes to dynamically map the kernel during the boot from the second basically

374
0:42:04.320 --> 0:42:09.720
gigabyte of visual memory to the 63rd gigabyte of memory.

375
0:42:09.720 --> 0:42:16.640
Additional enhancements to handle system calls and then also handle exceptions on a dedicated

376
0:42:16.640 --> 0:42:19.960
stock.

377
0:42:19.960 --> 0:42:25.400
As far as the moving memory, visual memory to the 63rd gigabyte, so I'm not sure if you

378
0:42:25.400 --> 0:42:30.640
realize OSV kernel is actually position dependent.

379
0:42:30.640 --> 0:42:37.320
But obviously the kernel itself may be loaded in different parts of physical memory.

380
0:42:37.320 --> 0:42:40.640
It used to be before that release that you would have to build different versions for

381
0:42:40.640 --> 0:42:44.440
Firecracker or for the QEMU.

382
0:42:44.440 --> 0:42:52.040
Basically in this release we changed the logic in the assembly in a boot loader where we

383
0:42:52.040 --> 0:43:02.520
basically OSV detects itself where it is in a physical memory and in essence builds dynamically

384
0:43:02.520 --> 0:43:12.600
the early mapping tables to then eventually bootstrap to the right place in the positional

385
0:43:12.600 --> 0:43:15.240
code.

386
0:43:15.240 --> 0:43:19.200
So now basically you don't need to... You can use the same version of the kernel to

387
0:43:19.200 --> 0:43:21.800
on any hypervisor.

388
0:43:21.800 --> 0:43:30.640
Now to add system calls on ARM we have to handle the SVC instruction.

389
0:43:30.640 --> 0:43:39.440
There's not really much interesting if you know how that works.

390
0:43:39.440 --> 0:43:46.480
What is maybe a little bit more interesting was the change that I made to make all exceptions,

391
0:43:46.480 --> 0:43:49.040
including system calls to work on a dedicated stock.

392
0:43:49.040 --> 0:43:55.800
So before that change all exceptions would be handled on the same stock as the application

393
0:43:55.800 --> 0:44:03.880
which costs all kinds of problems.

394
0:44:03.880 --> 0:44:08.560
For example, that would effectively prevent implementation of the lazy stock.

395
0:44:08.560 --> 0:44:21.720
So to support basically that, in OSV which runs in EL1 in a kernel mode we would basically

396
0:44:21.720 --> 0:44:30.720
take advantage of the stack selector register and we would basically use both stack pointer

397
0:44:30.720 --> 0:44:34.040
register SPL0 and SPL1.

398
0:44:34.040 --> 0:44:47.320
So normally OSV uses SPL1 register to point to the stock for each spread.

399
0:44:47.320 --> 0:44:53.320
With the new implementation what basically we would do before the exception was taken

400
0:44:53.320 --> 0:45:01.800
basically we would switch the stack pointer selector to SPL0.

401
0:45:01.800 --> 0:45:09.720
Once basically the exception was handled it would basically go back to normal which was

402
0:45:09.720 --> 0:45:12.720
SPL1.

403
0:45:12.720 --> 0:45:24.280
I think I'm going to skip with the fast because we're running a little bit of time left but

404
0:45:24.280 --> 0:45:28.280
you can read it on that.

405
0:45:28.280 --> 0:45:39.400
We've also added Netlink support and we've made quite many improvements to VFS layer.

406
0:45:39.400 --> 0:45:47.480
So both actually of those Netlink and VFS improvements were done to support C-WitFS.

407
0:45:47.480 --> 0:45:53.120
So there are basically more gaps that have been filled by trying to run this new use

408
0:45:53.120 --> 0:45:56.120
case.

409
0:45:56.120 --> 0:46:01.880
So just briefly as we are pretty much at the end of the presentation I think in the next

410
0:46:01.880 --> 0:46:07.800
releases of OSV whenever they're going to happen I would like us to focus on supporting

411
0:46:07.800 --> 0:46:13.720
statically linked executables, adding proper support of spin locks because OSV for example

412
0:46:13.720 --> 0:46:20.920
Mutex right now is lock less but under high contention it would actually make sense to

413
0:46:20.920 --> 0:46:25.840
use spin locks and we have actually a prototype on that on the mailing list.

414
0:46:25.840 --> 0:46:31.000
And then supporting ASLR, refreshing Capstan which is a build tool which hasn't been really

415
0:46:31.000 --> 0:46:35.600
out because we don't have enough volunteers, improved for a long time and then even the

416
0:46:35.600 --> 0:46:41.280
website and there are many other interesting ones.

417
0:46:41.280 --> 0:46:48.440
So as a last slide I would like to basically use this as occasion to thank basically organizer

418
0:46:48.440 --> 0:46:56.760
Rasvan for inviting me and everybody else from the community of UNI kernels and I would

419
0:46:56.760 --> 0:47:07.200
also want to thank SillaDB for supporting me and Dorlauer and Nadav Harrell for reviewing

420
0:47:07.200 --> 0:47:14.240
all the patches and his other improvements and I also want to thank all other contributors

421
0:47:14.240 --> 0:47:21.680
to OSV and I also would like to invite you to join us because there are not many of us

422
0:47:21.680 --> 0:47:27.800
and if you want to have OSV alive we definitely need you.

423
0:47:27.800 --> 0:47:35.520
So there are some resources about OSV, there's my P99 presentation here as well and if you

424
0:47:35.520 --> 0:47:37.840
guys have any questions I'm happy to answer them.

425
0:47:37.840 --> 0:47:38.840
Thank you.

426
0:47:38.840 --> 0:47:39.840
Thank you Voldemort.

427
0:47:39.840 --> 0:47:40.840
Thank you.

428
0:47:40.840 --> 0:47:46.280
So any questions for Voldemort?

429
0:47:46.280 --> 0:47:48.800
Yeah please, just ask it's going to be a bit too.

430
0:47:48.800 --> 0:47:49.800
I have two questions.

431
0:47:49.800 --> 0:48:01.040
First when you have spoken about symbols, about the GLC symbols and the CLC symbols,

432
0:48:01.040 --> 0:48:08.040
do I understand it correctly that the problem is that a kernel might be using some GLC functions

433
0:48:08.040 --> 0:48:15.400
and some applications might be linked to its own GLC and some symbols clash with it?

434
0:48:15.400 --> 0:48:21.880
Well not really, they would use the same version and there's no problem with for example malloc,

435
0:48:21.880 --> 0:48:28.240
like malloc we don't want to expose malloc but there is a good chunk of OSVs implemented

436
0:48:28.240 --> 0:48:35.440
in C++ and all of those symbols don't need to be exposed because they inflate the symbols

437
0:48:35.440 --> 0:48:42.400
stable a lot and they are not, they shouldn't be really available to visible to others.

438
0:48:42.400 --> 0:48:51.560
And now I think OSV exposes if you build with that option around I think 16 hundreds of

439
0:48:51.560 --> 0:48:52.560
symbols instead of 17,000.

440
0:48:52.560 --> 0:48:57.240
So it's really about the binary size there?

441
0:48:57.240 --> 0:49:05.360
Yeah, yeah basically binary size and with in case of C++ library avoiding a collision

442
0:49:05.360 --> 0:49:11.560
where you build OSV with different version of C++ library versus the application that.

443
0:49:11.560 --> 0:49:16.280
Yeah okay so this is the case I am interested in.

444
0:49:16.280 --> 0:49:22.280
So have you thought about maybe renaming the symbols in the kernel image during the link

445
0:49:22.280 --> 0:49:29.280
time, maybe adding some prefixes to all the symbols so that you can have them visible

446
0:49:29.280 --> 0:49:31.360
but they would not clash?

447
0:49:31.360 --> 0:49:34.360
That's interesting idea, I haven't thought about it yet.

448
0:49:34.360 --> 0:49:35.360
And Marti the second question?

449
0:49:35.360 --> 0:49:36.360
Yeah, yeah just a quick second question.

450
0:49:36.360 --> 0:49:37.360
So when you have spoken about the lazy stack you said that you pre-fold the stack to avoid

451
0:49:37.360 --> 0:49:38.360
the problematic case when it drops and preemptions disabled.

452
0:49:38.360 --> 0:49:39.360
So basically when I am thinking about it you still need to have some kind of upper bound

453
0:49:39.360 --> 0:49:40.360
of the size of the stack so that you know that you pre-folded large enough to not get

454
0:49:40.360 --> 0:49:41.360
into the inter-dependence of the stack.

455
0:49:41.360 --> 0:49:51.520
So what I am thinking about is that you have to have a kind of a

456
0:49:51.520 --> 0:50:19.520
upper bound for the whole kernel.

457
0:50:19.520 --> 0:50:24.280
Well I mean this is a technique for applications, threads only.

458
0:50:24.280 --> 0:50:30.480
So for application stacks where the kernel threads would still have the pre-populated

459
0:50:30.480 --> 0:50:33.200
fixed size stack.

460
0:50:33.200 --> 0:50:36.440
Because I mean there are many applications like good example is Java that would start

461
0:50:36.440 --> 0:50:42.600
like 200 threads and all of them right now are pre-folded like 1 megabyte and you would

462
0:50:42.600 --> 0:50:45.480
all of a sudden need like 200.

463
0:50:45.480 --> 0:50:46.480
So this is just for application.

464
0:50:46.480 --> 0:50:54.720
Okay so basically my understanding is wrong so you have obviously yeah so you have the

465
0:50:54.720 --> 0:50:57.840
user stack and the kernel stack is the same stack.

466
0:50:57.840 --> 0:51:07.240
Well no it is in the same you know virtual memory but yeah there are I mean when I say

467
0:51:07.240 --> 0:51:10.880
kernel stack I mean in OSV basically there are two types of threads.

468
0:51:10.880 --> 0:51:14.240
There are kernel threads and there are application threads.

469
0:51:14.240 --> 0:51:18.160
So basically application threads use their own stack but.

470
0:51:18.160 --> 0:51:25.680
But when they enter the kernels so to speak they are still using the original.

471
0:51:25.680 --> 0:51:31.440
I mean application threads use application stack and kernel use a kernel and when I say

472
0:51:31.440 --> 0:51:39.800
like some kernel code obviously as a, well obviously because uni-kernel as the code executes

473
0:51:39.800 --> 0:51:45.600
in an application it runs on application stack but it may execute some kernel code as well.

474
0:51:45.600 --> 0:51:46.600
Which yeah.

475
0:51:46.600 --> 0:51:47.600
Yeah.

476
0:51:47.600 --> 0:51:48.600
Thank you.

477
0:51:48.600 --> 0:51:49.600
Any other question?

478
0:51:49.600 --> 0:51:50.600
Okay thank you so much.

479
0:51:50.600 --> 0:52:11.240
Let's move on.

