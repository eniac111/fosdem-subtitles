WEBVTT

00:00.000 --> 00:10.420
Okay, thank you. Thank you, Rasaan. Actually, after hearing your talk, I'm kind of

00:10.420 --> 00:13.920
concerned I should join the UniCraft community. Sounds to be fun there.

00:13.920 --> 00:17.020
It's a treasure there, Kim.

00:17.020 --> 00:18.020
Yeah, I see.

00:18.020 --> 00:20.020
You don't have to test.

00:20.020 --> 00:26.100
Okay, so my name is Sihan Kunza, as you now heard. I'm the lead maintainer, also the

00:26.100 --> 00:31.720
original person that started that UniCraft project while being still a researcher at

00:31.720 --> 00:38.760
NEC Labs Europe. In the meantime, we spinned off. We have now a startup also called UniCraft,

00:38.760 --> 00:45.080
so it's the UniCraft GmbH, and I'm their CTO and co-founder. And yeah, we're building

00:45.080 --> 00:49.200
a community and a startup at the same time.

00:49.200 --> 00:58.080
So first question into the room, who has used UniCraft before, I would like to know. Okay.

00:58.080 --> 01:07.400
Who has maybe a more theoretical background, what our key concepts in UniCraft are? Okay.

01:07.400 --> 01:15.080
So then, yeah, I'll have some background slides to bring everybody on the same stage. And

01:15.080 --> 01:22.440
then we jump directly into the binary compatibility topic, but I won't spend too much time here.

01:22.440 --> 01:32.720
Okay. So with this picture, I usually start that. You see on the left side the traditional

01:32.720 --> 01:38.880
setup when you have virtual machines and your applications running on them, so I mean, stuff

01:38.880 --> 01:46.640
that you know since 20 years now. Then you have a setup which is more recent, the more

01:46.640 --> 01:53.760
popular is using containers, where you basically run a hostOS on your hardware and then you

01:53.760 --> 02:01.740
use isolation primitives on your host kernel to separate your containers from each other.

02:01.740 --> 02:07.200
And then there's UniCernels. I don't know, is this interrupted somewhere? Okay.

02:07.200 --> 02:14.440
It seems to be okay. Okay. So we think this could be a different execution environment,

02:14.440 --> 02:20.200
especially for the container setup, bringing kind of maraging what you had before with

02:20.200 --> 02:25.680
virtual machines with strong isolation and really more minimal hypervisors underneath

02:25.680 --> 02:32.920
that are much more secure as well. And you don't need to do a shared host base which

02:32.920 --> 02:39.240
can become an attack surface. And then you want the flexibility of containers, and this

02:39.240 --> 02:47.560
is where we think a UniCernel can come in, so where you build a kernel per application.

02:47.560 --> 02:54.800
So the thing is, since you know the application that you run on, you can also give up a lot

02:54.800 --> 03:01.380
of principles you had in standard operating systems and do simplifications, which is totally

03:01.380 --> 03:08.600
okay because it's not hitting your attack vector actually. So if you say one application,

03:08.600 --> 03:12.060
you can go for a flat and single address space because that kernel that you have underneath

03:12.060 --> 03:20.060
is just for your application for nothing else. We in UniCraft build a single monolithic

03:20.060 --> 03:26.360
binary usually, so it's your application plus the kernel layers, and everything ends up

03:26.360 --> 03:37.360
in function calls into drivers. And you get then further benefits, first by this simple

03:37.360 --> 03:42.840
setup but also since you know your environment, you know where you run on, you know what you

03:42.840 --> 03:49.640
run, so you can specialize the kernel layers that you need underneath. So you put only

03:49.640 --> 03:54.240
drivers that you need to run on your target hypervisor. You build a separate image if you

03:54.240 --> 03:59.160
run that application with a different hypervisor. So floppy drivers, forget it, you don't need

03:59.160 --> 04:06.840
it. Virta.io only for KVM guests. Zen.netfront, for instance, only for Zen guests. And you

04:06.840 --> 04:12.120
know the application, you have knowledge which features of the OS is needed, and that way

04:12.120 --> 04:18.600
you can also from the top down specialize the operating system to just provide that

04:18.600 --> 04:27.880
what you need. So this makes us also slightly different from the other UniCrun projects

04:27.880 --> 04:34.880
maybe that you had heard of, so we are for sure not the first ones. But we claim we are

04:34.880 --> 04:41.880
the ones that follow at least this principle as most strongest because we build it from

04:41.880 --> 04:47.640
the beginning with that in mind, which is specialization. So everything that we implement

04:47.640 --> 04:56.720
should never dictate any design principles. The concept is you know what you need for

04:56.720 --> 05:04.120
your application, you know what you need to run your UniCrunals, so I want to give you

05:04.120 --> 05:12.160
a highly customizable base where you pick and choose and configure of components and

05:12.160 --> 05:23.320
specialize the kernel for you. So that led us to this principle, everything is a microlibrary,

05:23.320 --> 05:28.600
which means for us even OS primitives are microlibraries, meaning a scheduler is a microlibrary,

05:28.600 --> 05:33.560
a specific scheduler implementation is a microlibrary, so like a cooperative scheduler

05:33.560 --> 05:40.080
or some schedulers that do a preemptive scheduling are different libraries, memory allocators,

05:40.080 --> 05:45.640
also things like VFS, network stacks, the architectures, the platform supports and the

05:45.640 --> 05:52.040
drivers are all libraries and because we are also going up the stack, the application interfaces.

05:52.040 --> 05:58.520
So everything that has to do with POSIX, even that is split into multiple POSIX subsystem

05:58.520 --> 06:05.480
libraries, the Linux system called ABI, which you will see in this talk now, and even language

06:05.480 --> 06:17.240
run times, if you let's say run a JavaScript UniCrunal, you can build it with a JS engine.

06:17.240 --> 06:25.160
And the project consists basically of a K config based configuration system and a build

06:25.160 --> 06:31.720
system also make based to not come up with yet another build system, to make actually

06:31.720 --> 06:36.440
entrance easy when people are familiar with Linux before.

06:36.440 --> 06:40.800
And our library pool actually.

06:40.800 --> 06:47.920
And to give you a rough idea of how this library pool is organized, I find this diagram nice,

06:47.920 --> 06:52.040
so let's see if this works at this point.

06:52.040 --> 06:59.800
So we divide roughly, so you don't find it that way in the repos, but we write roughly

06:59.800 --> 07:06.320
the libraries into these different categories, so you have like here on the bottom, the platform

07:06.320 --> 07:12.040
layer which basically includes drivers and platform support where you run on.

07:12.040 --> 07:16.600
Then we have this OS primitives layer, these are then libraries that implement like a TCP

07:16.600 --> 07:23.680
IP stack or file systems or something regarding scheduling, memory allocation, etc., etc.

07:23.680 --> 07:30.480
And then always in mind there's like first the opportunity for you to replace components

07:30.480 --> 07:34.200
in here and also that we provide also alternatives.

07:34.200 --> 07:38.120
So you don't need to stick with light may IP if you don't like it, so you can provide

07:38.120 --> 07:44.760
your own network stack here as well and reuse the rest of the stack too.

07:44.760 --> 07:49.560
Then we have this POSIX compatibility layer, this is basically things here, FDTA, this

07:49.560 --> 07:55.040
is for instance file descriptor handling as you know it, POSIX process has then aspects

07:55.040 --> 08:01.400
about process IDs, process handling, etc., pthread API of course.

08:01.400 --> 08:09.400
And then we have a libc layer where we also have at the moment actually three libcs, muscle

08:09.400 --> 08:15.160
which is becoming our main thing now, a new lib that we had in the past to actually provide

08:15.160 --> 08:19.880
all the libc functionally to the application but also actually for the other layers too.

08:19.880 --> 08:28.880
It provides also things like mem copy which is like all over the place used.

08:28.880 --> 08:36.480
Then Linux application compatibility, that was now a big topic for this release.

08:36.480 --> 08:39.320
Why do we do application compatibility?

08:39.320 --> 08:47.400
It's actually for us for adoption, to drive the adoption because most cloud software is

08:47.400 --> 08:50.160
developed for Linux.

08:50.160 --> 08:54.920
People are used to their software so we don't feel confident to ask them to use something

08:54.920 --> 08:59.240
new or rewrite stuff from scratch.

08:59.240 --> 09:04.440
And if you provide something like Linux compatibility you remove all the obstacles that people start

09:04.440 --> 09:10.360
using UniCraft because they can run their application with UniCraft.

09:10.360 --> 09:16.800
And our vision behind the project is to give seamless application support.

09:16.800 --> 09:25.960
So the users that say they tell you I use that in that web server and it should be like

09:25.960 --> 09:32.040
with the push of a button, so including with some tooling that we provide that we can run

09:32.040 --> 09:37.680
that on UniCraft as they run it before on Linux and they benefit from all these nice

09:37.680 --> 09:42.600
UniColonal properties which are lower boot times, less memory consumption and also improved

09:42.600 --> 09:50.480
performance.

09:50.480 --> 09:59.440
So now speaking about which possibilities you have for supporting Linux compatibility,

09:59.440 --> 10:06.560
we divide actually compatibility into two main tracks.

10:06.560 --> 10:13.120
One track is so-called native which means that we have the application sources and we

10:13.120 --> 10:18.000
compile that together and link that together with the UniCraft build system.

10:18.000 --> 10:23.800
And then we have on the other side the binary compatibility mode where the story is that

10:23.800 --> 10:31.000
the application is built externally and we just get binary artifacts or the final image

10:31.000 --> 10:33.720
event.

10:33.720 --> 10:38.760
And then actually you can subdivide these two tracks.

10:38.760 --> 10:45.440
On the native side we have which we did actually quite a lot until recently this UniCraft driven

10:45.440 --> 10:51.920
compilation which basically meant that when you have your application you had to port

10:51.920 --> 10:58.800
or mimic the application's original build system with the UniCraft build system and

10:58.800 --> 11:01.680
then you compile all the sources with UniCraft.

11:01.680 --> 11:07.520
Has the benefit that you're then staying in one universe and don't have potential conflicts

11:07.520 --> 11:17.440
with compiler flags or things that influence your calling conventions between objects.

11:17.440 --> 11:22.480
And then there is this way that you probably have also seen for instance with RAM kernels.

11:22.480 --> 11:30.000
They did it a lot using an instrumented way where you actually utilize the build system

11:30.000 --> 11:36.720
of an application with the cross compile feature and then you hook in and that's your entry

11:36.720 --> 11:45.400
point into replacing the compile calls and make it fit for your UniCraft.

11:45.400 --> 11:50.560
And then on the binary compatibility side we have so let's start here because that's

11:50.560 --> 11:51.720
easier.

11:51.720 --> 12:00.320
So of course so externally built and this means basically you have elf files so like

12:00.320 --> 12:04.480
a shared library or an elf application.

12:04.480 --> 12:08.240
What you need here is basically just to support loading that and get that into your address

12:08.240 --> 12:11.380
space and then run it.

12:11.380 --> 12:18.240
And then there's also this flavor of let's say build time linking which means that you

12:18.240 --> 12:25.880
take some build artifacts from the original application build system like the intermediate

12:25.880 --> 12:32.120
object files before it does the final link to the application image and you link those

12:32.120 --> 12:36.160
together with the UniCraft system.

12:36.160 --> 12:44.320
And I call it here binary compatible because you interface it on an API and not on the

12:44.320 --> 12:50.240
API level like in the native cases.

12:50.240 --> 12:58.680
So and here this is just a little mark that in the UniCraft project you will mostly find

12:58.680 --> 13:04.080
these three modes in the project that people are working on.

13:04.080 --> 13:09.800
So here that we never tried with UniCraft in fact but I mean some tooling and this should

13:09.800 --> 13:13.960
work too actually.

13:13.960 --> 13:22.060
So as you may have noticed native is about API compatibility so really on the programming

13:22.060 --> 13:27.600
interface and binary compatibility is about the application binary interface so really

13:27.600 --> 13:34.760
the compiled artifacts and how you have calling conventions here etc.

13:34.760 --> 13:40.160
Where are your arguments in which register or how is your stack layout etc.

13:40.160 --> 13:47.180
And this is here on a programming language level.

13:47.180 --> 13:57.160
So the requirements for providing you let's say a native experience is POSIX and POSIX.

13:57.160 --> 14:04.920
Most applications are written for POSIX so we have to do POSIX no excuse right.

14:04.920 --> 14:13.800
So libc's will mostly cover that but yeah it's all about POSIX.

14:13.800 --> 14:21.560
And the second point is that you also need to port the libraries that your application

14:21.560 --> 14:28.160
additionally uses let's say yeah let's take engine access and web so right you have then

14:28.160 --> 14:34.480
tons of library dependencies for instance for cryptographic things like setting up HTTPS

14:34.480 --> 14:37.080
tunnels or doing some other things.

14:37.080 --> 14:46.680
So those libraries you need also port here and add them so that you have the application

14:46.680 --> 14:50.800
sources available during the build right.

14:50.800 --> 14:57.360
On the binary compatibility side the requirements are you need to understand the else format,

14:57.360 --> 15:04.400
shared libraries or binaries depending on which level you are driving it and then since

15:04.400 --> 15:12.840
this stuff got built for Linux you must be aware that it can happen that that binary

15:12.840 --> 15:17.960
will do directly a system call so it's instrumented because it got built together with a libc or

15:17.960 --> 15:24.960
something like that to do an syscall assembly instruction which means on our side we need

15:24.960 --> 15:28.840
to be able to handle those system calls as well.

15:28.840 --> 15:35.760
And if we speak about shared library support we need to support all this library function

15:35.760 --> 15:39.280
or library symbol linking actually right.

15:39.280 --> 15:46.000
And additionally of course each data that is exchanged needs to be in the same representation.

15:46.000 --> 15:52.920
This means because this is ABI right now imagine you have a C struct and here it's fine to

15:52.920 --> 15:57.200
move some fields around because if you use the same definition for your compilation is

15:57.200 --> 16:01.480
all fine you can sort the fields in the struct will all work.

16:01.480 --> 16:08.680
Here you can't because your application they got built externally that layout of that struct

16:08.680 --> 16:15.120
that binary layout that must fit otherwise you will read different fields right obviously.

16:15.120 --> 16:21.120
And then for both modes which is important for us as an operating system we have of course

16:21.120 --> 16:25.160
also some things that we need to provide to the application which are things that the

16:25.160 --> 16:31.800
application just requires because it is that way on Linux meaning providing a procfs or

16:31.800 --> 16:39.280
syscfs entries or files in slash etc or something like that right for because you know they

16:39.280 --> 16:43.600
do they do sometimes silly things just to figure out in which time soon there are so

16:43.600 --> 16:52.800
they go to slash etc and figure out what is configured and also the locales and etc.

16:52.800 --> 16:58.040
So in closing that let's say this high level view up so that we have the full understanding

16:58.040 --> 17:03.920
let's speak a bit about the pros and cons between these two modes.

17:03.920 --> 17:10.400
The native side what is really nice here which is a really interesting pro so if you've

17:10.400 --> 17:19.360
got everything put together you have quite of a natural way to change code in the application

17:19.360 --> 17:28.000
to change code in the kernel to make maybe shortcuts between the application kernel interaction

17:28.000 --> 17:34.920
and can use that for driving your specialization even further right and performance tune your

17:34.920 --> 17:40.880
unique kernel for that application. The disadvantages you always need the source

17:40.880 --> 17:47.480
codes because we are compiling everything here together and which is also let's say

17:47.480 --> 17:57.000
for newcomers a bit difficult is if you require them that the application they have and they

17:57.000 --> 18:01.320
say okay I have the source codes and I just run make and then it compiles but I have the

18:01.320 --> 18:05.880
source code you need to instrument either the build system of the application as we

18:05.880 --> 18:13.560
just saw with the instrument to build that also prompted or you actually we must say

18:13.560 --> 18:19.600
okay sorry you can't use that build system now you need to mimic and write and uni-craft

18:19.600 --> 18:30.800
make file equivalent to build your application. So this is why binary compatibility is actually

18:30.800 --> 18:35.720
interesting really interesting for let's say newcomers because you don't need the source

18:35.720 --> 18:41.040
code they can compile the application that they're interested in so if they need to compile

18:41.040 --> 18:46.960
it let's say right the way as they usually do they don't need to care about uni-craft at all and

18:46.960 --> 18:53.640
normally also no modifications to the application is needed obviously you can still do things here

18:53.640 --> 19:03.160
but it's not a requirement. The risk that we saw by doing the work is at least for the for let's

19:03.160 --> 19:10.520
say on the uni-kernel side is that you get into a risk that you need to implement things the way

19:10.520 --> 19:17.880
how Linux does it and one really stupid example I get a bit nuts on that is providing an

19:17.880 --> 19:25.360
implementation for Netlink sockets because if you have like a web application or you know any

19:25.360 --> 19:30.960
application that does some networking and that application wants to figure out which networking

19:30.960 --> 19:35.440
interfaces are configured and what are the IP addresses there so it will likely lose the libc

19:35.440 --> 19:43.440
function get if address and that is implemented with a Netlink socket so this goes back here

19:43.440 --> 19:47.920
right here I can just provide a get if address which is highly optimized in that sense right

19:47.920 --> 19:53.640
which just returns in that struct all the interfaces but if I go binary compatible and if

19:53.640 --> 20:02.840
I do it really don't an extreme means because that libc which is part of your binary here maybe opens

20:02.840 --> 20:07.660
a socket which is address family Netlink and starts communicating about a socket with the

20:07.660 --> 20:14.400
kernel to figure out the interface addresses which can be really silly right for a uni-kernel

20:14.400 --> 20:20.880
right to do and then also it's maybe it's less opportunities but also a bit harder to specialize

20:20.880 --> 20:27.040
in tune the kernel application interaction right because assuming you don't have access to the

20:27.040 --> 20:34.240
source code of the application there's nothing you can do on the application side right so to give

20:34.240 --> 20:41.400
you a rough idea what that means in performance because it uni-craft let's say the the second

20:41.400 --> 20:50.960
important thing for us is always performance performance performance here we we just show

20:50.960 --> 20:58.600
you nginx here compiled as a as a native version so meaning it uses the uni-craft build system to

20:58.600 --> 21:06.040
build the nginx sources versus we run nginx on we call it elf loader so this is actually our

21:06.040 --> 21:15.080
uni-craft application to load elf binaries and then a comparison here with a standard Linux and here

21:15.080 --> 21:22.680
this is the same binary what that means in performance so it's this quick test we have

21:22.680 --> 21:30.720
just the index page the standard default of any nginx installation served and this is like that the

21:30.720 --> 21:39.520
performance numbers the takeaway here is if you just you know don't go into any special performance

21:39.520 --> 21:45.280
tuning yes and start just you know getting the thing compiled and run you will end up in a similar

21:45.280 --> 21:55.120
performance as if you just take the you know the elf loader to to run that application in binary

21:55.120 --> 22:02.240
compatibility mode that is interesting because you don't need to see necessarily huge performance

22:02.240 --> 22:09.120
drops the only thing that you lose is the potential to further optimize in this mode if you go for

22:09.120 --> 22:16.400
this one but this the nice thing is you can still see benefits right running your application on

22:16.400 --> 22:26.760
uni-craft right and to just give you an impression so this is here a go HTTP application where we go

22:26.760 --> 22:32.520
a bit crazy about we are optimizing and specializing the interaction between the go application and

22:32.520 --> 22:39.840
uni-craft yeah we can get more out of this we can really performance to increase stuff out of it

22:39.840 --> 22:52.960
okay so now in the next slides I go over how we implement these modes with uni-craft because as

22:52.960 --> 22:59.960
I said we we don't want to target just one mode we want to target multiple modes and it has also

22:59.960 --> 23:08.600
some implementation challenges because as an engineer you also want to reuse code as much as

23:08.600 --> 23:20.080
possible so we'll talk about the structure here okay so to give you an overview so this doesn't

23:20.080 --> 23:26.120
mean now this that these applications run at the same time could be also be possible but it's just

23:26.120 --> 23:33.480
to show you how the components get involved in our ecosystem so if you take just the left part the

23:33.480 --> 23:43.840
native port of application we settle now on muscle to provide all the libc functionality that the

23:43.840 --> 23:51.040
application needs and we have we have a library called syscall shim which is actually the heart

23:51.040 --> 24:00.160
of our application compatibility and this is actually you can imagine this is a bit of a

24:00.160 --> 24:06.720
registry where you know it knows where in which sub library a system called handler is implemented

24:06.720 --> 24:13.840
and it can forward then the muscle calls to those places on the binary compatibility side you have

24:13.840 --> 24:19.480
a library called this elf loader which is the the library that loads an elf binary into memory and

24:19.480 --> 24:28.640
then here's the syscall shim taking care of handling binary system calls and now I will go

24:28.640 --> 24:35.080
into the individual items to show a bit bit more zoomed in view what's happening there and we of

24:35.080 --> 24:46.320
course you start with the heart with the core the syscall shim so here we have I mean some macros

24:46.320 --> 24:53.920
when so when you develop like VFS core is our VFS library actually ported from OSB or POSIX

24:53.920 --> 25:00.200
process where you do some some process functionality like get PID or something like that we have some

25:00.200 --> 25:06.520
some macros that help you to define and a system call handler and it's really a system

25:06.520 --> 25:12.840
kind of it's just a function that is defined at that point and you will register this to the

25:12.840 --> 25:22.400
syscall shim then the the shim provides you two options how that system called handler can be

25:22.400 --> 25:31.320
reached one is that compile time this is like macros macros and preprocessor which allows you

25:31.320 --> 25:37.520
when you have a native application that does or they actually it's on the on the muscle side to

25:37.520 --> 25:44.560
call a system call it will replace those calls or will return at compile time the function of

25:44.560 --> 25:54.000
that library that implements that system call then it was also a runtime handler which is provided

25:54.000 --> 26:02.400
here which does you know the typical syscall trap and running that function and behind the scenes

26:02.400 --> 26:12.400
yeah and our aim as I mentioned we want to use we use code as much as possible so the idea is

26:12.400 --> 26:18.760
that we implement that function for that system called just once and the syscall shim is helping

26:18.760 --> 26:26.800
us depending on the modes doing a link or provided as binary compatible so let's go back to the

26:26.800 --> 26:33.080
overview and then you will see it a bit more concrete with muscle but probably I said everything

26:33.080 --> 26:42.400
already so we have muscle natively compiled with a unicraft build system now imagine you have the

26:42.400 --> 26:47.520
application you have a write goes to muscle and muscle does then a UK syscall r write which is

26:47.520 --> 26:54.720
then actually the symbol that is provided by the actual library that's implementing it and the

26:54.720 --> 27:02.560
the rewriting happens as I said with the macros at compile time in libmuscle so what we did for

27:02.560 --> 27:11.320
that is to replace that syscall muscle internal function with our syscall macro which then kicks

27:11.320 --> 27:19.960
in the whole machinery to map a system call request to the direct function call the thing is that in

27:19.960 --> 27:27.600
muscle not all but most of the system call requests have a static argument with the system

27:27.600 --> 27:33.840
call number first so so so this this let's say write is an MC wrapper and internally there they're

27:33.840 --> 27:39.720
setting preparing the arguments just maybe some checks before they go to the kernel and then they

27:39.720 --> 27:45.440
have this syscall function with the number of the system call and then the arguments hand it over

27:45.440 --> 27:53.560
and as soon that number is a const static you know just just written down in your code literally we

27:53.560 --> 27:59.080
can do a direct mapping so that that right will directly do a function call with you kiss is cool

27:59.080 --> 28:06.280
alright if if it's not static which is really happening only on two three places if I remember

28:06.280 --> 28:11.440
correctly then of course we can provide an intermediate function that then does a switch

28:11.440 --> 28:18.560
case and switch and jumps them to the actual system call handler and the thing is since everything

28:18.560 --> 28:24.840
is configurable means I can have a build where be this core is not part of the build or POSIX

28:24.840 --> 28:30.360
process is not part of the build then the syscall symbol automatically also with all this macro

28:30.360 --> 28:38.680
magic that we do replace calls to non-existing system call handles with an inosys stop so that

28:38.680 --> 28:47.760
for the applications look like a functional to implement it yeah and exactly so it's runtime

28:47.760 --> 28:52.560
this is called shim is for that point out out of the game so everything happens at the compartment

28:52.560 --> 29:02.160
so for the binary compatibility side that's unfortunately runtime thing and we have actually

29:02.160 --> 29:09.960
two components here as I was mentioning the elf load itself which loads the the elf application

29:09.960 --> 29:17.160
what we support today is static pies so if you have a static position independent executable

29:17.160 --> 29:26.320
compiled you can run that and what also works is using your let's say with your libc together

29:26.320 --> 29:32.200
provided dynamic linker meaning if you use glibc with the application you can use that dynamic

29:32.200 --> 29:40.640
linkers so LD or SO and also run dynamically linked applications with that what it needs

29:40.640 --> 29:47.280
is POSIX mapp as a library which implements all these mapp and unapp and protect functions

29:47.280 --> 29:56.840
on the on the system call there then system calls are trapped here this is called shim and yeah I

29:56.840 --> 30:04.120
think I said that then the library is not selected it's replaced with inosys so this is called shim

30:04.120 --> 30:10.320
knows which system calls are available which which are not then there's a bit of a specialty

30:10.320 --> 30:20.600
for handling a system call so the the system called trap handler so we provide it with a

30:20.600 --> 30:27.860
system call shim and we don't need to do an domain switch so we have still a single address

30:27.860 --> 30:38.720
space a single what's called not forget the word so it's all kernel privilege yeah so we have done

30:38.720 --> 30:43.640
it's the same privilege domain exactly so we don't have a privilege domain switch as well right now

30:43.640 --> 30:54.520
we have it good good good you learn something but we are sliding a different environment that

30:54.520 --> 31:00.520
will show you later in the slide exactly what this means we have some different assumptions

31:00.520 --> 31:08.240
that you have on the Linux system called API which requires us to do some extra steps unfortunately

31:08.240 --> 31:15.520
so the first thing is Linux does not use extended register or if they use it they they guard it

31:15.520 --> 31:24.040
meaning extended registers are floating point units vector units MMX SSE you know we do

31:24.040 --> 31:28.560
unfortunately so we need to save that state because that's unexpected for an application

31:28.560 --> 31:34.680
that was compiled for Linux before that these units could screw up when coming back from a

31:34.680 --> 31:44.400
system call and the second thing is we don't have a TLS you know in the Linux kernel but

31:44.400 --> 31:51.280
unfortunately on uni craft we have so we use the same even unfortunately the same TLS register so

31:51.280 --> 31:57.560
we also need to save and restore that so that the application keeps its TLS and all the uni craft

31:57.560 --> 32:07.400
functions operate on the uni craft TLS good before I continue and give you some let's say lessons

32:07.400 --> 32:14.080
learned while implementing all these things I would like to give you a short demo and then

32:14.080 --> 32:22.640
I give you speak a bit about what was tricky during the implementation and what our special

32:22.640 --> 32:33.140
considerations that we had to do so then let's hope that this works so this is a super fresh

32:33.140 --> 32:41.040
demo is don't touch it you will burn your fingers my colleagues so thank you mark for getting that

32:41.040 --> 32:51.040
work just you know half an hour before the talk yeah he's amazing yeah okay so in this demo I have

32:51.040 --> 32:57.120
actually engine X as a web server with a standard file system I'll show you with the files around

32:57.120 --> 33:03.160
I have it once compiled natively and once compiled as a Linux application will run it with the

33:03.160 --> 33:10.680
off-loader and you will see that the result is the same right so let's start with the native one

33:10.680 --> 33:17.440
so I'm actually already so probably I need to increase a bit the size right that you can read

33:17.440 --> 33:27.920
it the background is that good yeah that's to any hit you so I hope you can also in the last row

33:27.920 --> 33:41.480
you can read perfect so yeah you have here the the engine X app checked out so we have my new

33:41.480 --> 33:52.840
config so you can oh this the windows somehow wider no wait just one second no it's better

33:52.840 --> 34:03.880
okay so you see the the application is here as an library here the engine X and then you have here

34:03.880 --> 34:12.360
the configuration you know of of you know all this HTTP modules that engine X provides and you can

34:12.360 --> 34:19.880
select and choose like this is really the uni craft way to do things because it builds a while and

34:19.880 --> 34:32.320
for that that my laptop is not the fastest I built it already so you see here the result of the the

34:32.320 --> 34:38.680
build directory you see each individual library that because of dependencies we're coming in and

34:38.680 --> 34:50.320
we're compiled so like for instance POSIX, Feutex, POSIX socket, RAMFS, in memory file system and the

34:50.320 --> 35:05.120
where is it now the application here that's the application image uncompressed so wait I can do

35:05.120 --> 35:19.480
let you see how big it is so it's here 1.1 megabyte so this is like it a full image of engine X

35:19.480 --> 35:29.080
including muscle including all the kernel code and driver to run on a chemo KVM x-rated machine

35:29.080 --> 35:41.880
yeah and then let's let's run it to see what what happens so exactly it's already up and running to

35:41.880 --> 35:50.640
show you these were roughly the arguments so we have in the meantime because I found chemo system

35:50.640 --> 35:57.320
sometimes a bit brutal with command line arguments a wrapper script that shortens a few things but

35:57.320 --> 36:05.120
in the end I mean this is this is running a chemo system and then you know it's attaching to this

36:05.120 --> 36:15.120
virtual bridge take that kernel image load that in ID file system because we we serve a file from

36:15.120 --> 36:21.240
from that RAMFS and here's also some parameters to set the IP address and that mask for that guest

36:21.240 --> 36:32.520
and here down there so we can check actually see here set IPV4 that's the address where the unicorn

36:32.520 --> 36:43.960
is up and yeah you see here with this w get line that yeah I get the page served and to prove that

36:43.960 --> 36:53.720
this is real let kill let us kill this now the guest is gone and this is dead so no response

36:53.720 --> 37:03.440
anymore good so now let's go to the elf loader which is also treated as an application that can

37:03.440 --> 37:16.000
run other applications also here in the build directly let's do the same thing so has also

37:16.000 --> 37:22.520
like similar dependencies of course it's prepared to run nginx so POSIX socket is there etc etc

37:22.520 --> 37:33.080
where's G here so here's the image it's a bit smaller is now 526 kilobytes which provides

37:33.080 --> 37:37.720
your environment to run and Linux elf of course the nginx image is not included here anymore right

37:37.720 --> 37:47.720
so that is part of the rule file system and if I run this now so I so on purpose I enabled now

37:47.720 --> 37:53.960
some debug output so that you see the proof that it does system calls well if you scroll up so the

37:53.960 --> 38:00.120
initialization phase looks a bit different also sets the IP address here it extracting the the

38:00.120 --> 38:10.840
the in it RD and here is starting to load the nginx binary the Linux binary from the in it ID and then

38:10.840 --> 38:17.240
from that point on the elf loader was jumping into the application and you see every system call

38:17.240 --> 38:25.520
that the application was doing and you can even see that you know some stuff probably this is

38:25.520 --> 38:32.960
first GLC initialization here for instance etc local time it's trying to open and find some

38:32.960 --> 38:38.560
configuration of course we don't have it we could provide one but it's still fine it's continuous

38:38.560 --> 38:54.320
booting affinity we don't have so but whatever it continues yeah exactly and there's tons of mmaps

38:54.320 --> 39:01.520
and you know EDC password etc so those files we had provide so you get a file descriptor returned

39:01.520 --> 39:09.400
back otherwise would have stopped etc and then you know configuration and so forth and now you

39:09.400 --> 39:16.560
should see that some system will happen when I access the access the page and you saw it happened

39:16.560 --> 39:24.720
index was opened file descriptor 7 and here is there should be a right to the socket you know

39:24.720 --> 39:31.800
over here this is probably the socket number four yeah I mean you get you get the impression what's

39:31.800 --> 39:40.440
going on so it's working the same way okay how much time I do I have left five minutes five minutes

39:40.440 --> 39:46.280
okay then actually three minutes just to leave some room for questions yeah exactly okay so let's

39:46.280 --> 40:01.960
get quickly back so we had some learned lessons learned lessons for the native mode I mean the

40:01.960 --> 40:07.320
thing is we have we have also this model like you heard the noise V we want to use just one libc in

40:07.320 --> 40:12.400
our build right so meaning all the kernel implementation and all everything that the

40:12.400 --> 40:19.560
application needs is one libc we provide multiple implementations of libc because muscle might be

40:19.560 --> 40:24.720
for some users cases too thick still or too big so we have some and the alternative like no libc

40:24.720 --> 40:31.760
and originally we had new lib and we need also what we want as well in our project is to keep

40:31.760 --> 40:37.040
the libc as vanilla as possible like upstream as possible because we want to keep the maintenance

40:37.040 --> 40:45.000
effort for updating the libc versions low but these causes then I mean let's just list them

40:45.000 --> 40:52.120
I've speak just about one of these items some things that that you stumble on and one was quite

40:52.120 --> 40:59.040
interesting was this get dense 64 issue that cost us some headache was many rust when fixing it

40:59.040 --> 41:04.720
which caused or required actually a patch I've always fixing it yeah I've required a patch to

41:04.720 --> 41:11.480
muscle the thing what happened here is that in this dear end dot H muscle is providing a mark

41:11.480 --> 41:19.120
and alias right to use the non 64 version for get dense and if it finds code with using get that get

41:19.120 --> 41:25.480
dense 64 because of this large file system support thing that was happening it maps it to get dense

41:25.480 --> 41:32.200
right on the other side on the vfs of the first course side so this is the VFS implementation

41:32.200 --> 41:37.840
where we provide the system call we need to provide both obviously we need to provide the

41:37.840 --> 41:44.640
non 64 version and the 64 version and guess what we include dear end because we need a struct

41:44.640 --> 41:51.720
definition here and then you can imagine so if you're familiar with C and preprocessor there's

41:51.720 --> 41:58.280
a little hint with this thunder of course I mean this gets replaced and then you have two times

41:58.280 --> 42:07.080
the same symbol and you're like what the hell is going on here all right yeah so let's keep this

42:07.080 --> 42:17.200
because of time upcoming features Russell was telling a bit already especially for this topic

42:17.200 --> 42:21.960
for application compatibility we will further improve it so this will be now our first release

42:21.960 --> 42:30.400
to officially release out loader and an updated muscle version we want to make that more seamless

42:30.400 --> 42:38.480
which requires a bit more under the hood libraries for that support you should also watch out for

42:38.480 --> 42:42.960
features that are coming up for a seamless integration of unicraft into your Kubernetes

42:42.960 --> 42:50.600
deployment no pressure Alex running unicraft on your infrastructure provider for instance AWS

42:50.600 --> 42:59.600
Google Cloud etc and automatically packaging of your applications right and it would love or

42:59.600 --> 43:04.840
actually all of us everyone with a new culture will off to hear also your feedback and what you

43:04.840 --> 43:12.680
think about you know turning the cloud with unicorns to the next level yeah any feedback to me please

43:12.680 --> 43:37.840
send to Simone right and these are again the project resources if you're interested you can just scan the QR code I think that's it okay Simone right we can take a couple of questions you can also address me to listen to me I mean yeah yeah so any quick yeah please first here and then on the back

43:42.800 --> 43:49.640
yeah thanks a lot both of you for your talks I have a question regarding dynamically linked applications yeah

43:49.640 --> 44:09.920
Linux as far as I can see you only use muscle and how does this work out if my application is linked against GWC and I want to run it with yeah I've loaded what do I have to do because in Linux world when I think against GWC and I only have muscle nothing works right right no so I'm so I'm assuming we

44:09.920 --> 44:20.840
speak you know about the binary compatibility mode in the end what you just need to do is providing the muscle loader if you have compiled with your application with muscle or the G.

44:20.840 --> 44:36.680
Lipsy loaded and then both works the thing is in that setup in memory there is actually two lip sees there's the lip see on the uni craft side and there's the lip see with your application so that's why it works seamless actually okay thank you

44:36.680 --> 45:06.280
so just add to that when you build your unique kernel for binary compatibility you don't use muscle I mean you can if you want I mean because the entire lip see is provided by the application either by the application of course that the binary or the application plus it's lip see inside the root file system and it's loaded from there there's no need to have anything like that yeah please yeah yeah so the question is about the

45:06.280 --> 45:35.280
API you spoke about the POSIX API yeah you also add a diagram showing a direct link to unique kernel so the question is is there some variable next diagram perhaps one of the next day at the other arm okay is it a variable use case yes this one there is a link

45:35.280 --> 46:05.240
directly from the native application to the unique kernel yeah yeah this is what it shows you is like how the calls are going it can happen because some system calls don't have a provided lip see wrapper yeah it's like for that completeness this error is here for instance the few takes call if you do if you use few takes directly from your application there is no wrapper function in lip see you need to do a system call directly and you can do that by also using

46:05.240 --> 46:23.640
the syscall macro then or actually I mean the syscall shim will replace that with the direct function call then to actually POSIX few takes no is it valuable to have kind of application that you develop specially for a unique unique

46:23.640 --> 46:52.760
kernel and native API are you yes yes that's for sure that's for sure so this talk is just about how we get application compatibility in case you have your application already but if you write it anyway from scratch I recommend forget everything about POSIX and speak the native API's you get much more performance and more directly connected to your driver layers and API's that you know POSIX has some implications right there's a lot of things like read write

46:52.760 --> 47:21.320
imply there's a mem copy happening and with these lower level API's you can do way quicker transfers because just because you can do a zero copy one sense yeah sure of course of course yeah have you looked into patching the binary to remove the syscall overhead patching the binary to remove for a couple

47:21.320 --> 47:35.960
of the now with the syscalls do you have to emulate the syscalls have you looked into patching the binary itself yeah instead of running the EL doing it at runtime handling the syscalls at runtime yeah let's say this is at least we thought about that but we didn't do it I mean the

47:35.960 --> 47:58.840
hardware talks that is the other exactly he's sitting in front of it they were doing some experiments with that that works too so you can patch it but yeah I mean this is just we didn't do it okay in regards to memory usage obviously

47:58.840 --> 48:25.200
the unicorn all lowers it but what if I ran multiple unicorn alls multiple VMs how how do you support member loaning or something like that or is it like just over provision and yeah I mean the idea is to have member loaning it's but it's not upstream yet of course there's also some really interesting research project and we should mention that works on memory

48:25.200 --> 48:43.200
deed application so if you run the same unicorn the same like a hundred times you can share VM memory pages right on the hypervisor side but you need hypervisor support for them okay thank you so much Simone let's end it here we're going to ask yeah yeah and get some stickers

48:43.200 --> 48:58.680
Anastasia is for the next talk on VXL so please so please get some stickers yeah stickers they're they're free don't have to pay for now next year 100 euro each
