WEBVTT

00:00.000 --> 00:08.520
At our final talk for this session, we have Pierre here.

00:08.520 --> 00:14.560
He's going to discuss about Loop, a tool that, well, he and we have been using to measure

00:14.560 --> 00:16.160
compatibility for different OSes.

00:16.160 --> 00:17.680
Pierre, you have the floor.

00:17.680 --> 00:22.040
Thank you, Rizvan, and thanks everyone for attending my talk.

00:22.040 --> 00:28.080
This is joint work with a bunch of colleagues and students, including Hugo, my PhD student.

00:28.080 --> 00:30.320
He's a key player behind his work.

00:30.320 --> 00:35.880
I'm just getting all the media suggestion, but he has built all this stuff, so all the

00:35.880 --> 00:39.120
credits go to him.

00:39.120 --> 00:46.240
In this brief talk, I want to speak a bit about application compatibility for custom

00:46.240 --> 00:49.080
operating systems.

00:49.080 --> 00:55.840
I guess most of you don't need to be convinced that we still need custom operating systems

00:55.840 --> 00:56.840
today.

00:56.840 --> 01:05.720
We still need both research operating systems and prototype operating systems from the industry.

01:05.720 --> 01:12.720
The thinking that Linux has solved everything is not true, in my opinion.

01:12.720 --> 01:17.640
We still need things like UniCraft if we want to go fast or if we want to specialize like

01:17.640 --> 01:18.640
crazy.

01:18.640 --> 01:23.320
We still need things like Resty-R-Met if we want security or SCL4.

01:23.320 --> 01:28.280
We still need custom operating systems.

01:28.280 --> 01:32.840
The thing is, with this operating system, they're only as good as the applications that

01:32.840 --> 01:36.840
they can run.

01:36.840 --> 01:39.200
Compatibility is key.

01:39.200 --> 01:42.920
Compatibility with existing applications is extremely important.

01:42.920 --> 01:47.880
If you want to build a community, you want your user to go to your website, compile your

01:47.880 --> 01:54.440
custom OS and then try some of their favorite applications or try some of the highly popular

01:54.440 --> 01:59.680
applications in a given application domain like engineering or Redis for cloud.

01:59.680 --> 02:05.800
If you want to attract sponsors or investors, or even if you, like me, are a scientist,

02:05.800 --> 02:10.920
you want to gather some early numbers to make a publication, well, you need to do that on

02:10.920 --> 02:14.480
standard applications.

02:14.480 --> 02:19.360
Compatibility is important.

02:19.360 --> 02:24.960
Another argument would be like how many times did you hear the one POSIX spoken today?

02:24.960 --> 02:29.800
There were some slides, there was POSIX like three or four times written in the single

02:29.800 --> 02:32.080
slide.

02:32.080 --> 02:40.240
Compatibility is important and it can be achieved in a few different ways as we have seen with

02:40.240 --> 02:41.240
Simon.

02:41.240 --> 02:47.360
But one important thing to note is, in my opinion, porting is not sustainable.

02:47.360 --> 02:49.720
Porting is what many of us do.

02:49.720 --> 02:54.440
We build a custom operating system and then we take Redis and obviously it doesn't work

02:54.440 --> 02:57.360
as this with our operating system.

02:57.360 --> 03:03.400
We modify Redis a bit, we disable some features because we know that they make our OS crash.

03:03.400 --> 03:07.240
And then we have Redis like a version customized for our operating system.

03:07.240 --> 03:12.560
This is not sustainable because you can't maintain a branch of Redis for every operating

03:12.560 --> 03:13.560
system out there.

03:13.560 --> 03:17.520
In the long term, it doesn't work so well.

03:17.520 --> 03:26.440
So porting also basically means that you as the OS developer, you ask the users of your

03:26.440 --> 03:31.520
application to make some effort to the application developers, they need to make some effort

03:31.520 --> 03:33.600
to be compatible with your operating system.

03:33.600 --> 03:38.440
This doesn't work, nobody is ready to make that kind of effort.

03:38.440 --> 03:45.320
Maybe if you give them 10x performance speed up, but this is unrealistic.

03:45.320 --> 03:53.560
What you want to do is, once again in my opinion, as an OS developer, you want to provide compatibility

03:53.560 --> 03:56.480
as transparently as possible.

03:56.480 --> 04:05.080
And this means you emulate a popular operating system, for example Linux or a popular abstraction

04:05.080 --> 04:08.960
like POSIX or the standard C library.

04:08.960 --> 04:13.400
And then you can be compatible at three different levels.

04:13.400 --> 04:17.600
The first level is source level or API level compatibility.

04:17.600 --> 04:29.400
So you ask the user to compile the application code against the sources of your kernel in

04:29.400 --> 04:31.280
the case of a unique kernel.

04:31.280 --> 04:35.800
So you're still asking some effort from the users.

04:35.800 --> 04:38.760
In many scenarios, you don't have access to sources.

04:38.760 --> 04:46.080
If you have proprietary binary or pre-compiled binary, you can't have source level compatibility.

04:46.080 --> 04:54.360
So it's not perfect and binary compatibility is a pure version of compatibility.

04:54.360 --> 04:56.520
And there are many ways to achieve it.

04:56.520 --> 05:01.760
You can do that at the level of the standard C library like OSV.

05:01.760 --> 05:11.600
So you will dynamically link your kernel plus a standard C library against your application

05:11.600 --> 05:17.160
compiled as a position independent executable or as a shared library itself.

05:17.160 --> 05:24.000
This is great, but if the application is making directly system calls to the kernel without

05:24.000 --> 05:28.960
going through the C standard library, well, once again, it doesn't work.

05:28.960 --> 05:34.560
And as a matter of fact, I have counted more than 500 executables in the Debian repository

05:34.560 --> 05:36.360
that contains the C score instruction.

05:36.360 --> 05:39.400
So they make C scores directly to the kernel.

05:39.400 --> 05:41.880
They don't go through the C library.

05:41.880 --> 05:47.880
Go, for example, is making most of its C score put directly to the kernel and not through

05:47.880 --> 05:48.880
the C library.

05:48.880 --> 05:53.000
So what you want to do is to be compatible at the level of the system call.

05:53.000 --> 05:58.560
So your kernel needs to emulate the C score API that Linux is providing.

05:58.560 --> 06:03.040
This is the most transparent way of achieving compatibility.

06:03.040 --> 06:06.720
Now this is scary, right?

06:06.720 --> 06:11.720
Linux has more than 350 system calls we need to implement them all.

06:11.720 --> 06:17.640
Will we be and aren't we going to re-implement Linux by doing so?

06:17.640 --> 06:21.120
And some of them are extremely scary by themselves, right?

06:21.120 --> 06:28.680
You have like hundreds of IO controls and each of them require its own implementation.

06:28.680 --> 06:32.520
The Linux API even goes beyond system calls.

06:32.520 --> 06:40.720
You have things like slash proc slash dev that are used by many applications.

06:40.720 --> 06:48.440
The first thing a muscle binary does when it runs is to look in, I believe it's slash

06:48.440 --> 06:51.400
proc or slash sys to get to the size of the terminal, right?

06:51.400 --> 06:55.480
So you need to have emulation for this part of the API too.

06:55.480 --> 07:02.480
And this, because it seems like a big engineering effort, this creates, it hinders the development

07:02.480 --> 07:04.120
of custom operating systems.

07:04.120 --> 07:12.880
So this is inspired by the keynote by Eti Mati Roscoe at ATC and OSDI 2021.

07:12.880 --> 07:14.800
We looked at all the papers.

07:14.800 --> 07:19.120
So these are top tier operating systems conferences.

07:19.120 --> 07:28.080
And we look over the past 10 years over a total of more than 1,000 papers, how many were

07:28.080 --> 07:32.600
about proposing a new operating system as opposed to things like security or machine

07:32.600 --> 07:34.160
learning?

07:34.160 --> 07:39.920
And among them, how many were just hacking Linux versus proposing an actual operating

07:39.920 --> 07:42.160
system implemented from scratch?

07:42.160 --> 07:46.680
And the numbers are similar to what we saw earlier, right?

07:46.680 --> 07:52.600
You have just a very, very, very few papers proposing a new operating system because it's

07:52.600 --> 07:55.800
a significant engineering effort.

07:55.800 --> 08:01.480
And part of the effort is providing compatibility to run applications like Apache or Redis to

08:01.480 --> 08:04.400
get a few numbers at the end of the paper, right?

08:04.400 --> 08:07.440
So this is a problem.

08:07.440 --> 08:15.640
Now the particular problem that I want to talk about is how I'm sure several people

08:15.640 --> 08:22.080
in this room have attempted to build some form of compatibility layer for an operating

08:22.080 --> 08:23.320
systems.

08:23.320 --> 08:28.640
And we are all kind of working on the same thing in parallel with some form of ad hoc

08:28.640 --> 08:32.240
processes that may benefit from some optimization.

08:32.240 --> 08:40.200
So I've just listed here a few projects that have a Cisco level binary compatibility layers,

08:40.200 --> 08:43.800
but actually there are many more.

08:43.800 --> 08:48.920
And from what I understand, it is a very organic process.

08:48.920 --> 08:50.880
So first of all, it is application driven, right?

08:50.880 --> 08:56.280
People have a few sets of application in mind that they want to support.

08:56.280 --> 09:01.720
If you are doing cloud, you want to support the user and suspect, Redis, Apache, whatever.

09:01.720 --> 09:05.040
And the process basically looks like that.

09:05.040 --> 09:10.480
You take an app, you try to run it on top of your operating system.

09:10.480 --> 09:12.240
Obviously it fails.

09:12.240 --> 09:17.320
You investigate, you're like, oh, I'm missing the implementation from this system code.

09:17.320 --> 09:22.520
So you implement whatever operating system features are required to fix that particular

09:22.520 --> 09:27.080
issue, rinse and repeat until the app is working.

09:27.080 --> 09:29.440
And then you go to the next app.

09:29.440 --> 09:33.280
So it's a very intuitive and organic process.

09:33.280 --> 09:38.040
So when I built the Almetix, this is exactly what I was doing.

09:38.040 --> 09:46.280
So something that comes to mind is, can't we have some form of generic compatibility

09:46.280 --> 09:50.000
layer that we could plug?

09:50.000 --> 09:55.600
Something a bit like Newlib that would provide generic interface.

09:55.600 --> 10:00.440
And I believe it's not really possible because most of this implementation to support the

10:00.440 --> 10:05.600
system code is very specific to whatever operating system you are using.

10:05.600 --> 10:11.560
And it's not clear if a generic compatibility layer can be achieved.

10:11.560 --> 10:17.120
But can we still somehow optimize that process?

10:17.120 --> 10:19.480
Some have tried static analysis.

10:19.480 --> 10:26.000
So they take the binaries of the applications they want to support and they look, okay,

10:26.000 --> 10:29.680
so what are the system codes that are made by these applications?

10:29.680 --> 10:38.120
So this has been done in the best paper in Erosys 2016, analyzed all the binaries from

10:38.120 --> 10:39.120
Ubuntu.

10:39.120 --> 10:43.720
I believe it was 1404 repositories.

10:43.720 --> 10:49.960
And they concluded that every Ubuntu installation, including the smallest one, require more than

10:49.960 --> 10:56.440
200 system codes, and 200 IO controls, five controls, and PRCTL codes, and hundreds of

10:56.440 --> 10:57.920
pseudo-fives.

10:57.920 --> 10:59.680
So this doesn't help.

10:59.680 --> 11:01.440
It is still quite scary.

11:01.440 --> 11:08.240
It's still represent like a gigantic engineering effort.

11:08.240 --> 11:18.720
But do we want full compatibility with Ubuntu installation?

11:18.720 --> 11:23.280
In the end, especially in the early stage of the development of an operating system,

11:23.280 --> 11:26.560
you just want to get a few applications up and running.

11:26.560 --> 11:30.040
And do you even want 100% compatibility?

11:30.040 --> 11:33.840
When I write a paper, I don't really care if everything is stable.

11:33.840 --> 11:38.000
I just want to get some numbers.

11:38.000 --> 11:41.840
So isn't there a better way?

11:41.840 --> 11:45.800
And obviously, maybe you think about, yeah, let's do dynamic analysis, right?

11:45.800 --> 11:50.400
Let's run the applications that we want to support.

11:50.400 --> 11:55.920
We send them some input that we want to support, like I'm running an engineering and I'm submitting

11:55.920 --> 11:58.280
some HTTP or something like that.

11:58.280 --> 12:00.560
And then we trace the system codes that are done, right?

12:00.560 --> 12:05.880
So this is going to give us a subset of the system codes that can be identified through

12:05.880 --> 12:09.360
static analysis that has a tendency to overestimate.

12:09.360 --> 12:15.080
So with this trace, the engineering effort to support an application and a set of input

12:15.080 --> 12:16.960
is a bit lower.

12:16.960 --> 12:21.160
But it's still not a panacea because it's not taking into account two things that we

12:21.160 --> 12:23.880
do when we implement compatibility layers.

12:23.880 --> 12:25.280
So this is my code.

12:25.280 --> 12:27.080
Don't judge me.

12:27.080 --> 12:34.040
One thing that I did from Hermitax was at some point, it was a map that was calling

12:34.040 --> 12:38.320
MNCORE to check if some page of memory wasn't swapped or not.

12:38.320 --> 12:43.840
It's as actually, there is no swap in most unique channels, so it really didn't matter

12:43.840 --> 12:45.640
to implement this.

12:45.640 --> 12:50.000
So inosis means operation not supported, right?

12:50.000 --> 12:53.480
So stubbing a system code is just saying, yeah, we don't support it.

12:53.480 --> 12:58.000
And you cross your finger that the application has some kind of fallback pass to do something

12:58.000 --> 13:00.600
else if the syscall fails.

13:00.600 --> 13:02.480
And it works in some cases.

13:02.480 --> 13:05.120
Then we can do something even more nasty.

13:05.120 --> 13:06.600
Don't judge me again.

13:06.600 --> 13:09.840
You can fake the success of a system code, right?

13:09.840 --> 13:19.200
Surprisingly, in some situation, returning a success code, even if the system code doesn't

13:19.200 --> 13:23.440
have any implementation in your operating system, it's going to work in some cases.

13:23.440 --> 13:27.320
I'll tell a bit more about why this works sometimes.

13:27.320 --> 13:32.240
So stubbing and faking lets you implement even less system codes than what you would

13:32.240 --> 13:35.200
trace with a trace.

13:35.200 --> 13:40.360
So in the end, if you want to support an app or a set of applications in your custom operating

13:40.360 --> 13:46.760
system, the amount of system codes that you actually need to implement, so obviously it's

13:46.760 --> 13:50.240
more than the entire Linux syscall API.

13:50.240 --> 13:55.520
Static binary analysis will, on the binaries of the applications you want to support, will

13:55.520 --> 13:58.240
identify a subset of that, still pretty big.

13:58.240 --> 14:00.240
It's another estimate.

14:00.240 --> 14:07.000
Glass analysis gets you more precise results, but it is pretty hard to achieve, and it is

14:07.000 --> 14:10.280
still overestimating.

14:10.280 --> 14:13.240
S trace will give you, once again, a subset.

14:13.240 --> 14:14.640
Things start to look better.

14:14.640 --> 14:19.440
And among these trace by S trace, you actually don't need to implement everything.

14:19.440 --> 14:22.720
You can stub and fake some of the syscalls.

14:22.720 --> 14:26.360
So can we measure that?

14:26.360 --> 14:28.120
Yes, with loop.

14:28.120 --> 14:31.920
So loop means magnifying glass in French.

14:31.920 --> 14:36.600
It's a tool that was built by Hugo, my student, and it's some kind of super S trace that is

14:36.600 --> 14:41.440
measuring the syscalls that are required to support an application, and that can also

14:41.440 --> 14:47.320
tell you which one you can stub and which one you can fake.

14:47.320 --> 14:56.120
So we used it to build a database of measurement for a relatively large set of applications.

14:56.120 --> 15:01.560
And with loop, if you give me a description of your operating system, basically the list

15:01.560 --> 15:05.880
of syscalls that you already support, and you give me the list of applications that

15:05.880 --> 15:11.440
you would like to support, we run them through loop, and loop can derive a support plan,

15:11.440 --> 15:17.600
which basically will tell you, okay, for this set of target applications, and given the

15:17.600 --> 15:23.120
set of syscalls that you already support, what is the optimized order of syscalls to

15:23.120 --> 15:26.560
implement to support as many applications as soon as possible?

15:26.560 --> 15:33.040
Okay, so I will give you an example of support plan by the end of the presentation.

15:33.040 --> 15:37.720
So from the user point of view, loop needs two things to perform its measurement on a

15:37.720 --> 15:38.720
given application.

15:38.720 --> 15:45.880
You give it a Docker file that is describing how you want to build and run the application

15:45.880 --> 15:50.200
for which you want to measure the syscalls needed.

15:50.200 --> 15:54.800
And optionally, you may need an input workload, think about a web server, it's not going to

15:54.800 --> 16:00.680
call many system calls until you actually start to send requests to it.

16:00.680 --> 16:06.400
Loop will instantiate the application, launch it on a standard Linux kernel, and analyze

16:06.400 --> 16:12.520
the syscalls that have done, and with a few tricks, we'll be able to know which ones can

16:12.520 --> 16:15.200
be faked or stubbed.

16:15.200 --> 16:19.280
The results are, it's basically just a CSV file.

16:19.280 --> 16:25.800
For a syscall that is made by the application, can it be faked?

16:25.800 --> 16:26.800
Can it be stubbed?

16:26.800 --> 16:29.720
Or does it require a full implementation?

16:29.720 --> 16:36.840
We start that in a database, and later, so we populate the database with as many measurements

16:36.840 --> 16:43.720
as possible, and this database can, given the list of syscalls that is already supported

16:43.720 --> 16:49.080
by your operating systems, give you some form of optimized super plan given which of the

16:49.080 --> 16:53.080
applications you want to support.

16:53.080 --> 16:56.280
So how does it work?

16:56.280 --> 17:02.440
When Loop runs the application, first it does a quick pass of a trace to measure all the

17:02.440 --> 17:09.080
system calls that are done by the application, and then for each system call that we identify,

17:09.080 --> 17:17.240
we use seccomp to hook into the execution of each of the system calls, and rather than

17:17.240 --> 17:24.880
actually executing them through the Linux kernel, we emulate the fact that the syscall

17:24.880 --> 17:25.880
is stubbed.

17:25.880 --> 17:29.040
So we just return in assist without executing the syscall.

17:29.040 --> 17:34.960
We can also emulate the fact that the syscall is faked, return zero, and then we check

17:34.960 --> 17:42.480
if the application works or not, following the stubbing or the faking of this particular

17:42.480 --> 17:43.480
syscall.

17:43.480 --> 17:48.480
We use that for each system call that we have identified with this trace.

17:48.480 --> 17:54.040
How do we actually check for the success of the execution of the application?

17:54.040 --> 17:56.600
So we identify two types of apps.

17:56.600 --> 17:58.480
Some we call them run to completion.

17:58.480 --> 18:02.880
There will be something like fio, when you start fio, it runs for one minute and then

18:02.880 --> 18:08.400
it exits outputting some stuff on the external output.

18:08.400 --> 18:14.880
So with run to completion apps, we run the app instrumented with loop, we check its exit

18:14.880 --> 18:15.880
code.

18:15.880 --> 18:19.320
If it's different from zero, we consider that the run was a failure, could have been killed

18:19.320 --> 18:22.640
by a signal or things like that.

18:22.640 --> 18:28.600
And we can also run a script optionally in addition to that, after each run of the application

18:28.600 --> 18:35.880
to check its standard output, we can grab error values, we can grab for success printing,

18:35.880 --> 18:40.880
something like 50 requests per second have been achieved.

18:40.880 --> 18:46.040
The files that may have been created by the application and so on.

18:46.040 --> 18:49.440
And then another type of application is client servers.

18:49.440 --> 18:55.760
So with client servers, we run the app instrumented by loop and in parallel, we run a workload,

18:55.760 --> 19:01.080
could be WRC, HTTP, the Redis benchmark for Redis and so on.

19:01.080 --> 19:03.680
And we check the success of both.

19:03.680 --> 19:08.120
We check that the app doesn't crash, we generally say others are not supposed to exit.

19:08.120 --> 19:11.880
So we check that it doesn't crash and we check the success of the workload.

19:11.880 --> 19:15.960
Like if Redis benchmark returns something different than zero for a reason, something

19:15.960 --> 19:16.960
went wrong.

19:16.960 --> 19:22.160
And then we are able to see, okay, so I'm currently trying to stop the Redis system

19:22.160 --> 19:27.520
call, is the application succeeded or not?

19:27.520 --> 19:34.000
So we'll use the database, let me check the time, okay.

19:34.000 --> 19:36.000
And we analyze the results.

19:36.000 --> 19:42.960
So these results are made in a relatively small database of about 12 highly popular,

19:42.960 --> 19:45.720
sorry, 15 highly popular cloud applications.

19:45.720 --> 19:48.920
So this is just a subset.

19:48.920 --> 19:57.040
So what you have on the Y axis is a number of system calls that are identified by static

19:57.040 --> 20:06.360
analysis in purple on the binary, on the sources in yellow and then dynamic analysis.

20:06.360 --> 20:12.560
And we run for each of these application both the standard benchmarks that we Redis benchmark

20:12.560 --> 20:16.920
for Redis, WRC for engineings and so on.

20:16.920 --> 20:18.920
And we also run the entire test suite.

20:18.920 --> 20:26.080
So the key idea with the test suite is if you support, I mean, if you measure what's

20:26.080 --> 20:31.680
going on during the entire test suite, you get a very good idea of what are all the possible

20:31.680 --> 20:34.320
system calls that could be done by the application.

20:34.320 --> 20:39.320
Obviously, you need to assume that the test suite has a good coverage, but it is the case

20:39.320 --> 20:42.880
with these very popular applications.

20:42.880 --> 20:49.120
And what we see is, first of all, you know, static analysis overestimates.

20:49.120 --> 20:50.120
This is not very surprising.

20:50.120 --> 20:54.400
The amount of syscalls that is identified by static analysis is relatively high compared

20:54.400 --> 20:57.320
to what we get with dynamic analysis.

20:57.320 --> 21:02.440
And if something interesting too is that the amount of syscalls that can be stubbed or

21:02.440 --> 21:09.400
faked, so the green bits on the dynamic analysis path, it is actually quite, it's non-legligible,

21:09.400 --> 21:10.400
right?

21:10.400 --> 21:15.840
So what this means is that if you want to support Redis with a Redis benchmark where

21:15.840 --> 21:22.240
binary level static analysis tells you that you should implement 100 system calls, if

21:22.240 --> 21:25.960
you just want to run the Redis benchmark to get performance numbers for your paper, you

21:25.960 --> 21:29.360
actually need to implement just 20, right?

21:29.360 --> 21:33.560
So that's what, like, divided by 5, right?

21:33.560 --> 21:39.320
And if you want to pass the entire test suite of Redis, you need to implement about 40.

21:39.320 --> 21:43.800
It's still like half what static analysis is telling you.

21:43.800 --> 21:48.160
So it's kind of a message of hope, right, for building compatibility layers and for

21:48.160 --> 21:51.260
developing custom operating systems in general.

21:51.260 --> 21:57.160
So yes, static analysis overestimates a lot the engineering effort to support a NAP.

21:57.160 --> 22:03.720
And even naive dynamic analysis does measure much more syscalls than what is actually required

22:03.720 --> 22:09.240
if you know that you can stub and fake syscalls.

22:09.240 --> 22:13.040
Another view of these results can be seen here.

22:13.040 --> 22:19.560
So for each of the system calls, you know, 0 is red, 1 is right, 2 is open, I guess,

22:19.560 --> 22:29.160
and so on, among a dataset of about 15 apps, how many of these apps require the implementation

22:29.160 --> 22:32.680
of the system calling question, right?

22:32.680 --> 22:38.320
And then so you have here the result for static analysis at the binary level.

22:38.320 --> 22:45.880
At the source level, this is estray, so without counting which system calls you can stub or

22:45.880 --> 22:49.600
fake, and this is what is actually required.

22:49.600 --> 22:53.960
So if you consider that you will not implement what you stub or fake, this is what you actually

22:53.960 --> 22:54.960
need to implement.

22:54.960 --> 23:00.760
And as you can see, it's much, much, much, much less engineering effort versus what static

23:00.760 --> 23:04.880
analysis is telling you.

23:04.880 --> 23:07.520
Why does stubbing and faking work?

23:07.520 --> 23:11.520
So here you get some code snippet from Redis.

23:11.520 --> 23:19.560
So if you stub, get our limit, the C library wrapper will return minus 1.

23:19.560 --> 23:24.240
And as you can see, Redis will actually fall back on some kind of safe value.

23:24.240 --> 23:30.440
You know, so I'm not able to understand the maximum of files that I can open, so I'm going

23:30.440 --> 23:35.520
to fall back on 100, sorry, 1000.

23:35.520 --> 23:44.120
And the fact that faking works is actually that you have quite a bunch of system calls.

23:44.120 --> 23:50.240
So this is for each system call and each app in our data set, what is the percentage of

23:50.240 --> 23:56.440
apps that are actually checking the return value of the system calls?

23:56.440 --> 24:02.240
And some system calls are almost never checked their return value.

24:02.240 --> 24:03.320
It kind of makes sense, right?

24:03.320 --> 24:08.560
When you see this, why check the return value of close?

24:08.560 --> 24:13.720
And this is why faking work in many cases.

24:13.720 --> 24:24.520
Another question that we asked is, okay, so when you speak about providing binary compatibility

24:24.520 --> 24:29.440
and you don't do porting anymore, basically all the effort of supporting apps is on you,

24:29.440 --> 24:31.080
the operating system developer.

24:31.080 --> 24:33.760
And this is how it should be, in my opinion.

24:33.760 --> 24:38.920
But how much effort does that mean in the long term?

24:38.920 --> 24:45.880
So we had a look at versions of Redis, and Genix, and Apache over the last 10 years,

24:45.880 --> 24:51.680
and what are these calls that actually needs to be implemented in purple.

24:51.680 --> 24:55.160
And we saw that this number does not change very much, right?

24:55.160 --> 25:03.600
So once you make an app and you make it work, it actually means that you need to keep up

25:03.600 --> 25:08.320
to date with the most recent version of this app that are coming up.

25:08.320 --> 25:14.760
But it doesn't necessarily mean a very big engineering effort either.

25:14.760 --> 25:16.560
And these are the support plans.

25:16.560 --> 25:25.040
So we had a look at UniCraft, Fuchsia, which are some operating systems that have already

25:25.040 --> 25:28.520
a relatively good support for a good number of system calls.

25:28.520 --> 25:32.040
And we look at Kerala, so Kerala is another UniCare mail written in Rust.

25:32.040 --> 25:36.720
And it's very, I wouldn't say immature, but it doesn't have support for a lot of system

25:36.720 --> 25:38.040
calls.

25:38.040 --> 25:43.400
And for our set of 15 apps that we had in the database, we derived a support plan.

25:43.400 --> 25:49.680
So for UniCraft, for example, in its current state, it's already supporting most of the

25:49.680 --> 25:52.560
apps of our dataset.

25:52.560 --> 25:55.840
If you want to support an additional app, what you need to do is to implement system

25:55.840 --> 26:03.120
call number 290 and stub these, and then you'll get memcached.

26:03.120 --> 26:07.400
And next, if you implement this C-score, you get H2O.

26:07.400 --> 26:12.160
And then you need to implement these two C-scores, and then you stub that, and you get MongoDB.

26:12.160 --> 26:15.240
So same thing for Fuchsia and Kerala.

26:15.240 --> 26:19.640
Obviously, it's a bit more interesting because this one doesn't support many applications

26:19.640 --> 26:22.680
out of the box.

26:22.680 --> 26:25.240
And I believe I have time to do a quick demo.

26:25.240 --> 26:31.920
Okay, I'm going to do it real quick.

26:31.920 --> 26:36.360
So I'm going to do a test with LS, which is like the simplest test because we don't have

26:36.360 --> 26:38.920
a lot of time.

26:38.920 --> 26:47.560
In the Dockerfile, I just copy a test that I'm going to show you, and then I call, this

26:47.560 --> 26:53.560
is kind of the top level script of loop with a few options that don't matter that much.

26:53.560 --> 26:58.760
And I say, okay, the binary that we are going to instrument is slash bin slash ls, and this

26:58.760 --> 26:59.760
is the parameter.

26:59.760 --> 27:05.760
So I'm going to do ls slash, and we are going to check if it works or not with every possible

27:05.760 --> 27:08.720
C-score that can be invoked by ls.

27:08.720 --> 27:17.000
And the test, which should be there, the test that we are going to run after each execution

27:17.000 --> 27:19.640
of ls to see if things have worked.

27:19.640 --> 27:25.800
So this shell script will take the standard output of ls as parameters, and to make things

27:25.800 --> 27:30.040
simple, I'm just checking that ls actually output something.

27:30.040 --> 27:32.960
I'm doing ls slash, so something should be outputted.

27:32.960 --> 27:36.280
If nothing is output, there is a problem.

27:36.280 --> 27:41.360
And keep in mind that loop is also checking the return value of ls itself.

27:41.360 --> 27:48.720
So okay, so I'm launching loop like this, so it should work.

27:48.720 --> 27:54.320
So what happens under the hood is that we build a container that we've seen the Dockerfile

27:54.320 --> 27:55.600
for.

27:55.600 --> 27:58.280
We are starting two containers in parallel.

27:58.280 --> 28:05.320
Each one is running a full set of tests trying to stub and fake all the system calls, and

28:05.320 --> 28:13.960
we use this to check for differences between the replicas in case there is a problem.

28:13.960 --> 28:15.360
Most of the time, there is no differences.

28:15.360 --> 28:18.320
So it takes a bit of time.

28:18.320 --> 28:23.640
And then, okay, it's done.

28:23.640 --> 28:31.120
So if we go to the database, so we have now much more than 12 apps.

28:31.120 --> 28:41.800
And if we go to ls, the most interesting result is this CSV file, which contains four hc

28:41.800 --> 28:45.120
calls, zero being read, one being write.

28:45.120 --> 28:49.080
Is it called by ls or not?

28:49.080 --> 28:50.600
Can we fake it?

28:50.600 --> 28:52.560
Can we stub it?

28:52.560 --> 28:56.720
Or can we both fake and stub it?

28:56.720 --> 29:00.200
Or it's more like, does the application work when it's faked?

29:00.200 --> 29:01.640
Does it work when it's stub?

29:01.640 --> 29:05.440
And does it work when it's both faked and stub?

29:05.440 --> 29:11.680
And as you can see, some syscalls, like 11, I don't know which one it is, can be both

29:11.680 --> 29:16.080
stub and fake, something for 12, something for 16.

29:16.080 --> 29:20.160
Some syscalls, like this is read, for example, it is called, but you can't stub or fake it,

29:20.160 --> 29:21.160
which kind of makes sense.

29:21.160 --> 29:24.880
And this wouldn't work if you can read.

29:24.880 --> 29:26.560
And yeah, that's pretty much it.

29:26.560 --> 29:33.200
So briefly, what we are currently working on is some more fine-grained measurement.

29:33.200 --> 29:39.560
Some syscalls have kind of sub features, like a lot of programs will require at least a

29:39.560 --> 29:45.000
map anonymous for a map to allocate memory, but not really to map a file.

29:45.000 --> 29:50.080
So we are looking at checking which flags can be stubbed or fake and things like that.

29:50.080 --> 29:55.280
And we are also looking at the virtual file system API.

29:55.280 --> 29:56.280
That's it.

29:56.280 --> 30:00.000
So building compatibility layers is important for custom operating system.

30:00.000 --> 30:28.440
It seems a bit scary, but actually it's not that much engineering effort.
