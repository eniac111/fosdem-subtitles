1
0:00:00.000 --> 0:00:07.320
So, hello everyone.

2
0:00:07.320 --> 0:00:09.760
Thanks for your patience.

3
0:00:09.760 --> 0:00:14.640
We're going to start with a slight delay, but I think it will be all right in the end.

4
0:00:14.640 --> 0:00:21.680
Can I please welcome to the stage Andreas Scharbaum, who's going to talk about tour

5
0:00:21.680 --> 0:00:25.600
the data types.

6
0:00:25.600 --> 0:00:30.600
Okay, thank you.

7
0:00:30.600 --> 0:00:31.600
Good morning.

8
0:00:31.600 --> 0:00:33.600
I hope you had a good time yesterday in Borsets.

9
0:00:33.600 --> 0:00:36.600
How was the restaurant?

10
0:00:36.600 --> 0:00:39.600
Well, we got some food.

11
0:00:39.600 --> 0:00:44.600
This is not switching.

12
0:00:44.600 --> 0:00:49.600
So, I have the pleasure to talk to you today about data types in Postgres.

13
0:00:49.600 --> 0:00:52.600
I actually have two talks about this.

14
0:00:52.600 --> 0:00:57.600
This is a, I would say, basic talk about regular data types we have.

15
0:00:57.600 --> 0:01:01.600
I also have an advanced data types talk, but let's focus on this one.

16
0:01:01.600 --> 0:01:03.600
So, my name is Andreas.

17
0:01:03.600 --> 0:01:07.600
I work with Postgres Science about 97, 98.

18
0:01:07.600 --> 0:01:10.600
I'm one of the founding members of Postgres Europe.

19
0:01:10.600 --> 0:01:16.600
V1 conferences like Post and PG Day on Friday here, Postgres Europe, which is in Prague

20
0:01:16.600 --> 0:01:22.600
this year in December, Postgres comes Germany, Belgium, Postgres comes Malta in April, so

21
0:01:22.600 --> 0:01:27.600
if you're interested in a couple more.

22
0:01:27.600 --> 0:01:31.600
Currently, I work for a company called Adjust in Berlin.

23
0:01:31.600 --> 0:01:36.600
We have about a thousand Postgres databases running everything from a couple gigabyte up

24
0:01:36.600 --> 0:01:41.600
to like 30 terabytes in Postgres.

25
0:01:41.600 --> 0:01:46.600
Okay, if you already know everything about data types in Postgres, if you're regularly

26
0:01:46.600 --> 0:01:50.600
reading hackers' mailing list, this is maybe not the right talk for you.

27
0:01:50.600 --> 0:01:58.600
I don't want to occupy your time, but otherwise, have a seat and we start.

28
0:01:58.600 --> 0:02:04.600
So, quick poll, how many data types do we have in Postgres?

29
0:02:04.600 --> 0:02:06.600
Any idea?

30
0:02:06.600 --> 0:02:12.600
40?

31
0:02:12.600 --> 0:02:13.600
40?

32
0:02:13.600 --> 0:02:16.600
Anyone else?

33
0:02:16.600 --> 0:02:23.600
I see you're still sleeping.

34
0:02:23.600 --> 0:02:29.600
And we talk about each and everyone today.

35
0:02:29.600 --> 0:02:33.600
So, first of all, we have to exclude a number of data types because every time you create

36
0:02:33.600 --> 0:02:39.600
a table in Postgres, you also create an according data type which matches the structure of the

37
0:02:39.600 --> 0:02:40.600
table.

38
0:02:40.600 --> 0:02:42.600
So, we don't want this.

39
0:02:42.600 --> 0:02:46.600
If we look at regular data types, we have about 80 depending on which Postgres version

40
0:02:46.600 --> 0:02:47.600
you are.

41
0:02:47.600 --> 0:02:52.600
It's still more than you expected.

42
0:02:52.600 --> 0:02:56.600
There's things like Boolean, you know this one, or text, or time stamps.

43
0:02:56.600 --> 0:03:01.600
There's also things like trigger and void and cardinal number, which you never heard

44
0:03:01.600 --> 0:03:04.600
about and you never need.

45
0:03:04.600 --> 0:03:10.600
And we focus on mostly the ones I marked highlighted here.

46
0:03:10.600 --> 0:03:16.600
At any time, you can also go and check the Postgres documentation on Postgres.org.

47
0:03:16.600 --> 0:03:20.600
And it has a very long list of all the data types here in Postgres.

48
0:03:20.600 --> 0:03:30.600
The basic ones, the advanced ones, all the ones I listed in the slides before, it's all there.

49
0:03:30.600 --> 0:03:31.600
Okay.

50
0:03:31.600 --> 0:03:36.600
How many different data types are you using in your application?

51
0:03:36.600 --> 0:03:37.600
Sweet.

52
0:03:37.600 --> 0:03:38.600
Which ones?

53
0:03:38.600 --> 0:03:46.600
What about numbers?

54
0:03:46.600 --> 0:03:50.600
So, every single is workshop.

55
0:03:50.600 --> 0:03:51.600
Okay.

56
0:03:51.600 --> 0:03:52.600
Anyone else?

57
0:03:52.600 --> 0:04:00.600
I'm a doctor.

58
0:04:00.600 --> 0:04:08.600
I mean, yeah, it fits you.

59
0:04:08.600 --> 0:04:09.600
Okay.

60
0:04:09.600 --> 0:04:14.600
No, that's too much.

61
0:04:14.600 --> 0:04:16.600
Better?

62
0:04:16.600 --> 0:04:17.600
Good.

63
0:04:17.600 --> 0:04:24.600
We are going over this basic type, so numeric, so when you come back, you can rewrite your

64
0:04:24.600 --> 0:04:27.600
application and finally you start using numbers.

65
0:04:27.600 --> 0:04:35.600
Text types, maybe XML if anyone of you is still using it, JSON, Booleans, and a couple

66
0:04:35.600 --> 0:04:36.600
more.

67
0:04:36.600 --> 0:04:39.600
So, text types.

68
0:04:39.600 --> 0:04:43.600
We basically have one text type in Postgres under the hood.

69
0:04:43.600 --> 0:04:44.600
It's all the same.

70
0:04:44.600 --> 0:04:50.280
So, we have watcher and char and text.

71
0:04:50.280 --> 0:04:52.960
Text types in Postgres by default are case sensitive.

72
0:04:52.960 --> 0:04:57.200
So, if you want to compare two texts, it's always case sensitive.

73
0:04:57.200 --> 0:05:02.680
If you want to compare it case insensitive, you have to use something like lower or higher

74
0:05:02.680 --> 0:05:07.440
to make a string uppercase or lowercase.

75
0:05:07.440 --> 0:05:11.840
A string can Postgres and hold up to a gigabyte of text, roughly.

76
0:05:11.840 --> 0:05:13.600
It cannot hold binary data.

77
0:05:13.600 --> 0:05:19.320
So, if you want to store images in text, that's not going to work.

78
0:05:19.320 --> 0:05:26.360
If you specify a length, like this larger n here, this is where Postgres stores up to

79
0:05:26.360 --> 0:05:37.120
these number of bytes or characters in a text data type, excluding white spaces at the end.

80
0:05:37.120 --> 0:05:40.760
A char by default is only one byte.

81
0:05:40.760 --> 0:05:43.040
That's most likely not what you want.

82
0:05:43.040 --> 0:05:47.040
So, for char, you always want to specify a length.

83
0:05:47.040 --> 0:05:51.800
And then, of course, if you say text, it's just the text you cannot specify your length

84
0:05:51.800 --> 0:05:52.800
here.

85
0:05:52.800 --> 0:05:53.800
Hi, everyone.

86
0:05:53.800 --> 0:06:00.120
How does char and watcher differentiate?

87
0:06:00.120 --> 0:06:03.240
Mostly how they handle white spaces.

88
0:06:03.240 --> 0:06:13.440
So, we have a watcher 1 and 5 here and 10 here, and I cast five white spaces to both

89
0:06:13.440 --> 0:06:17.960
watcher 1, 5 and 10 and char 1, 5 and 10.

90
0:06:17.960 --> 0:06:22.360
And as you can see, we get different lengths here.

91
0:06:22.360 --> 0:06:29.560
So, our five white spaces here, if we cast them to watcher 10, we only get five at the

92
0:06:29.560 --> 0:06:33.480
end because we only have five white spaces in it.

93
0:06:33.480 --> 0:06:41.080
What char is doing, char will fill up the entire string to the lengths we specified.

94
0:06:41.080 --> 0:06:48.520
If you say char 10, we get a string back with 10 characters in it.

95
0:06:48.520 --> 0:06:51.960
That's mostly the difference for char and watcher.

96
0:06:51.960 --> 0:06:56.600
How it's saying that if you specify your lengths in char, it will actually give you that many

97
0:06:56.600 --> 0:06:59.200
bytes back, including white spaces.

98
0:06:59.200 --> 0:07:04.080
If you use a watcher, it will only give you the string back, excluding white spaces at

99
0:07:04.080 --> 0:07:05.080
the end.

100
0:07:05.080 --> 0:07:12.160
So, it will cut off white spaces.

101
0:07:12.160 --> 0:07:15.040
We have a sink in process which is called a page.

102
0:07:15.040 --> 0:07:18.600
By default, it's 8 kilobyte.

103
0:07:18.600 --> 0:07:20.600
That's where we store all the data in it.

104
0:07:20.600 --> 0:07:22.080
So, we have one page header.

105
0:07:22.080 --> 0:07:29.880
Then we store rows in it, and a row can be anything from just one column up to 1,000,

106
0:07:29.880 --> 0:07:34.560
1,500 columns.

107
0:07:34.560 --> 0:07:36.760
As I said, by default, it's 8 kilobyte.

108
0:07:36.760 --> 0:07:40.560
You can increase it, but almost no one is doing this.

109
0:07:40.560 --> 0:07:43.160
How does Postgres store a text?

110
0:07:43.160 --> 0:07:45.520
A text can be up to one gigabyte.

111
0:07:45.520 --> 0:07:50.320
So, we cannot store one gigabyte in 8 kilobyte.

112
0:07:50.320 --> 0:07:54.040
I mean, you can if you compress, but usually it doesn't work.

113
0:07:54.040 --> 0:08:00.080
Any data type in Postgres which has a variable length is only a pointer into what we call

114
0:08:00.080 --> 0:08:03.400
a toast table.

115
0:08:03.400 --> 0:08:04.920
Sorry.

116
0:08:04.920 --> 0:08:13.480
So, in our regular table, we have a 4-byte pointer, which is a pointer into the toast

117
0:08:13.480 --> 0:08:14.480
table.

118
0:08:14.480 --> 0:08:19.520
And in the toast table, we have as many rows as we need just towards this gigabyte of text.

119
0:08:19.520 --> 0:08:25.920
So, it's 4 bytes here, and as many rows as we need here.

120
0:08:25.920 --> 0:08:27.720
Which brings me to my one question.

121
0:08:27.720 --> 0:08:33.240
Why are so many people using char 255?

122
0:08:33.240 --> 0:08:39.360
Just two weeks ago, I've seen a customer who does this all over the place, street names,

123
0:08:39.360 --> 0:08:43.440
customer names, everything is char 255.

124
0:08:43.440 --> 0:08:45.240
Does it make any sense?

125
0:08:45.240 --> 0:08:51.160
Well, there are certain databases where this might make sense, but it doesn't make any

126
0:08:51.160 --> 0:08:54.040
sense in Postgres.

127
0:08:54.040 --> 0:09:00.800
When Postgres doesn't make a difference if you use 10 bytes, 200 bytes, 1 kilobyte, you

128
0:09:00.800 --> 0:09:11.520
always end up, almost always end up with a text pointer into your toast table.

129
0:09:11.520 --> 0:09:17.240
There are other databases that make sense, like if you look at one of the competition

130
0:09:17.240 --> 0:09:27.520
in the market, they can only have 255 bytes in an index, or just 255 characters if you

131
0:09:27.520 --> 0:09:31.320
use UTF-8 up to 700 something bytes.

132
0:09:31.320 --> 0:09:33.440
That's what they can use in the index zone.

133
0:09:33.440 --> 0:09:37.120
That might make sense to use 255 as a char.

134
0:09:37.120 --> 0:09:44.120
But every time you see this construct in Postgres where someone says, char 255, go and question

135
0:09:44.120 --> 0:09:46.360
why is the reasoning behind this?

136
0:09:46.360 --> 0:09:48.560
Why did someone say 255?

137
0:09:48.560 --> 0:09:53.000
Not 200, not 1 kilobyte.

138
0:09:53.000 --> 0:09:57.960
Technically it doesn't make sense.

139
0:09:57.960 --> 0:09:58.960
Numeric ties.

140
0:09:58.960 --> 0:10:02.880
Pay attention, please.

141
0:10:02.880 --> 0:10:04.860
So we have integers.

142
0:10:04.860 --> 0:10:06.520
We have floating point numbers.

143
0:10:06.520 --> 0:10:11.480
We have numeric, and we have sequences.

144
0:10:11.480 --> 0:10:15.520
Integers, we have small int storing 2 bytes.

145
0:10:15.520 --> 0:10:19.720
We have integers, regular integers by default, it's 4 bytes.

146
0:10:19.720 --> 0:10:23.440
So anything from minus 2 billion to plus 2 billion.

147
0:10:23.440 --> 0:10:26.240
And then we have big int using 8 bytes.

148
0:10:26.240 --> 0:10:28.720
This is 9.

149
0:10:28.720 --> 0:10:32.080
No idea how much this is.

150
0:10:32.080 --> 0:10:37.840
You might get some small problems if you store something, okay, small int integer, small

151
0:10:37.840 --> 0:10:44.320
int integer, because the compiler will actually go and say, okay, we need to have some space

152
0:10:44.320 --> 0:10:49.800
because we need to align the next integer on a 32-bit boundary or 64-bit boundary depending

153
0:10:49.800 --> 0:10:51.520
on your operating system.

154
0:10:51.520 --> 0:10:57.680
So what you really want to do is have all of your big data types in first in your table.

155
0:10:57.680 --> 0:11:02.840
There was things which is like text and big int, and then start with your regular 4-byte

156
0:11:02.840 --> 0:11:08.600
types and then 2-byte types at the end so you can compress the table a little bit more.

157
0:11:08.600 --> 0:11:15.680
It doesn't really bring you much if you only have a small table, a couple million rows.

158
0:11:15.680 --> 0:11:19.680
But if you're talking about billion rows, it's actually quite a big saving you get if

159
0:11:19.680 --> 0:11:27.880
you can compress and save like 2 bytes, 4 bytes per row.

160
0:11:27.880 --> 0:11:35.280
Then we have floating point numbers as wheel and as double precision with 4 and 8 bytes.

161
0:11:35.280 --> 0:11:40.120
Keep in mind a floating point number, even though post was my choice, accurate number

162
0:11:40.120 --> 0:11:43.400
you have, is always rounded internally.

163
0:11:43.400 --> 0:11:45.880
It's always a base and an exponent.

164
0:11:45.880 --> 0:11:50.520
I want to give you an example here.

165
0:11:50.520 --> 0:11:57.960
We have 6 digits here, 100,001, and you see if I return this, Postgres still shows me

166
0:11:57.960 --> 0:12:03.000
the 100,001, even though internally it's already rounded.

167
0:12:03.000 --> 0:12:08.240
If I expand this to 7 digits, you see I get a rounded number here.

168
0:12:08.240 --> 0:12:13.560
The 4 bytes can no longer store the precision required to store this 1 at the end.

169
0:12:13.560 --> 0:12:18.520
So you only get 1 million here.

170
0:12:18.520 --> 0:12:27.600
The same is true if I have my 100,001.5 as a 4 byte floating point number, Postgres starts

171
0:12:27.600 --> 0:12:30.680
rounding it internally.

172
0:12:30.680 --> 0:12:36.320
So if you want to store anything like money or data where you really need precision, please

173
0:12:36.320 --> 0:12:42.320
do not use floating point numbers.

174
0:12:42.320 --> 0:12:48.440
But same example for floating point with double positions of 8 bytes.

175
0:12:48.440 --> 0:12:54.520
If I have 15 digits here, it still looks okay, but once you start expanding the number a

176
0:12:54.520 --> 0:13:03.440
little bit or add DCMA digits here, you see it starts rounding the number.

177
0:13:03.440 --> 0:13:09.080
So if you really want to store floating point numbers, but you cannot use integers, what

178
0:13:09.080 --> 0:13:13.880
else can we use?

179
0:13:13.880 --> 0:13:20.000
Ah, this one comes later.

180
0:13:20.000 --> 0:13:22.600
We can use numeric.

181
0:13:22.600 --> 0:13:30.560
There's data type code numeric, which stores up to 1,000 digits of precision in Postgres

182
0:13:30.560 --> 0:13:33.320
square.

183
0:13:33.320 --> 0:13:38.080
Basically any number you need to store, you can store in numeric.

184
0:13:38.080 --> 0:13:40.960
Keep in mind there's no real headdress support.

185
0:13:40.960 --> 0:13:47.400
If you add two integers, in the end, the CPU will load one integer into one register and

186
0:13:47.400 --> 0:13:51.840
the other integer into another register and just use one operation internally.

187
0:13:51.840 --> 0:13:52.840
Everything is fast.

188
0:13:52.840 --> 0:13:54.560
That's not how it works with numeric.

189
0:13:54.560 --> 0:13:57.240
So it will be a little bit slower.

190
0:13:57.240 --> 0:14:05.120
Not much, but if you have to use it all the time, you might see it.

191
0:14:05.120 --> 0:14:10.120
There's also a type called money in Postgres.

192
0:14:10.120 --> 0:14:13.120
Don't use it.

193
0:14:13.120 --> 0:14:15.520
Never.

194
0:14:15.520 --> 0:14:21.120
Internally it's a big int, so we have the same position here, eight bytes.

195
0:14:21.120 --> 0:14:25.360
However, you only have one currency.

196
0:14:25.360 --> 0:14:31.960
Whatever you assign as LC monetary in your environment, this is the currency Postgres

197
0:14:31.960 --> 0:14:35.440
is using to show you this number.

198
0:14:35.440 --> 0:14:39.320
You cannot say I want to store two currencies in my database.

199
0:14:39.320 --> 0:14:41.680
You cannot store like an exchange rate.

200
0:14:41.680 --> 0:14:45.600
All of this is not working.

201
0:14:45.600 --> 0:14:51.040
Money was deprecated I think two times, three times, something like this.

202
0:14:51.040 --> 0:14:54.480
There's always one user who comes back, oh, I really need it.

203
0:14:54.480 --> 0:14:58.720
It's still around, but please don't use it.

204
0:14:58.720 --> 0:15:08.280
Is anyone from India here?

205
0:15:08.280 --> 0:15:13.480
Ever met him?

206
0:15:13.480 --> 0:15:15.480
You know the name?

207
0:15:15.480 --> 0:15:25.320
Would be surprised if you know the name.

208
0:15:25.320 --> 0:15:26.320
Anyone knows this game?

209
0:15:26.320 --> 0:15:30.160
Okay.

210
0:15:30.160 --> 0:15:36.720
How many, the story is that he did something for his king and the king asked, okay, what

211
0:15:36.720 --> 0:15:37.720
can I give you in return?

212
0:15:37.720 --> 0:15:41.160
And he said, okay, give me some waste wings.

213
0:15:41.160 --> 0:15:45.000
One on the first chest feed and a doubly number.

214
0:15:45.000 --> 0:15:48.520
How many waste wings are we talking about?

215
0:15:48.520 --> 0:15:54.280
Yeah, it's hidden.

216
0:15:54.280 --> 0:15:59.840
So we can actually use numeric for this.

217
0:15:59.840 --> 0:16:04.120
So it's two to the power of 64 minus one.

218
0:16:04.120 --> 0:16:05.120
So we have 64 fields.

219
0:16:05.120 --> 0:16:07.800
We start with one, so it's minus one.

220
0:16:07.800 --> 0:16:12.760
If we use floating point for this double position here, you see we don't get an exact number.

221
0:16:12.760 --> 0:16:15.720
It's way too big for floating point.

222
0:16:15.720 --> 0:16:19.120
If we use numeric, it's just 20 digits.

223
0:16:19.120 --> 0:16:21.600
We have 980 left.

224
0:16:21.600 --> 0:16:25.720
We can store much more in numeric than just this number.

225
0:16:25.720 --> 0:16:35.840
I did a mess at some point, and it's like 1,000 times the speed we are currently using

226
0:16:35.840 --> 0:16:40.160
or producing on Earth per year.

227
0:16:40.160 --> 0:16:48.600
So 1,000 years of waste production on Earth will happen to solve this problem.

228
0:16:48.600 --> 0:16:54.240
Okay, we also have sequences in Postgres.

229
0:16:54.240 --> 0:17:00.800
Internally these are just integers which are used as a default type in a table.

230
0:17:00.800 --> 0:17:06.680
We have small serial which gives you two bytes, 32K plus minus.

231
0:17:06.680 --> 0:17:13.280
Then we have regular serial data type which is four bytes from zero to or from one to

232
0:17:13.280 --> 0:17:20.240
two billion, and then we have big end for everything which needs larger numbers.

233
0:17:20.240 --> 0:17:24.760
If you create a data type or a table with a data type serial, what Postgres will do

234
0:17:24.760 --> 0:17:30.120
internally, it will create this data type in a table.

235
0:17:30.120 --> 0:17:34.840
If you look into the table, it's actually an integer in four and eight.

236
0:17:34.840 --> 0:17:40.120
It will create a sequence for you, and it will make this sequence a default value for

237
0:17:40.120 --> 0:17:41.880
this color.

238
0:17:41.880 --> 0:17:47.400
So every time you insert something into this table and you don't specify this color, Postgres

239
0:17:47.400 --> 0:17:50.240
will use the next value from a sequence.

240
0:17:50.240 --> 0:17:54.480
If you specify something for this color, it will not use the sequence.

241
0:17:54.480 --> 0:17:59.200
So if this is your primary key and you're mixing values you are inserting and values

242
0:17:59.200 --> 0:18:02.480
from a sequence, at some point you will have collisions.

243
0:18:02.480 --> 0:18:06.200
Please don't do this.

244
0:18:06.200 --> 0:18:09.400
Also sequences are not transactional.

245
0:18:09.400 --> 0:18:14.000
If you roll back a transaction, a sequence will not roll back.

246
0:18:14.000 --> 0:18:17.600
Sequences just mean to provide you a unique number.

247
0:18:17.600 --> 0:18:23.640
That's all.

248
0:18:23.640 --> 0:18:30.440
So we can ask the current value of a sequence, select current or my sequence name here.

249
0:18:30.440 --> 0:18:36.120
This will only work if you already used the sequence in a current session, like you did

250
0:18:36.120 --> 0:18:38.040
an insert into a table.

251
0:18:38.040 --> 0:18:44.000
This will tell you what is the last value this current session I have inserted into

252
0:18:44.000 --> 0:18:45.000
this table.

253
0:18:45.000 --> 0:18:50.760
So if you have five different sessions inserting data, it will always show you what you, the

254
0:18:50.760 --> 0:18:53.680
current session inserted.

255
0:18:53.680 --> 0:18:58.040
You can also ask for the next value by using next wall.

256
0:18:58.040 --> 0:19:02.400
Keep in mind it will not roll back if you roll back the transaction.

257
0:19:02.400 --> 0:19:08.480
It will only ever move forward.

258
0:19:08.480 --> 0:19:13.360
You can also set a sequence by just saying set wall and then specify your key, what is

259
0:19:13.360 --> 0:19:16.760
my new value I want to have.

260
0:19:16.760 --> 0:19:21.640
If you do this on a table, where you use the sequence on a table as primary key and you

261
0:19:21.640 --> 0:19:27.320
reset it to a previous value, you may run into collisions again because it will reuse

262
0:19:27.320 --> 0:19:32.600
the same values again.

263
0:19:32.600 --> 0:19:35.280
Okay.

264
0:19:35.280 --> 0:19:40.600
A sequence internally in Postgres is just another object, another table.

265
0:19:40.600 --> 0:19:44.320
So what you can say, select star for my sequence.

266
0:19:44.320 --> 0:19:47.880
It will show you all the data about a sequence.

267
0:19:47.880 --> 0:19:49.720
So what's the sequence name?

268
0:19:49.720 --> 0:19:51.880
What's the last value we used here?

269
0:19:51.880 --> 0:19:56.440
What's the minimum maximum value with the sequence cycle around so when it comes to

270
0:19:56.440 --> 0:19:58.200
an end, will it start again?

271
0:19:58.200 --> 0:20:00.240
Will it wrap over or not?

272
0:20:00.240 --> 0:20:05.240
So by default it will not wrap over because otherwise you will end up with the same numbers

273
0:20:05.240 --> 0:20:09.360
again.

274
0:20:09.360 --> 0:20:11.880
Just another object.

275
0:20:11.880 --> 0:20:13.840
Good.

276
0:20:13.840 --> 0:20:21.640
Any questions about numeric types?

277
0:20:21.640 --> 0:20:24.640
What is this?

278
0:20:24.640 --> 0:20:29.480
Come again.

279
0:20:29.480 --> 0:20:35.560
South pole.

280
0:20:35.560 --> 0:20:39.160
South pole sounds good.

281
0:20:39.160 --> 0:20:45.440
How did you figure it out?

282
0:20:45.440 --> 0:20:46.440
That's one way.

283
0:20:46.440 --> 0:20:52.200
Yeah, it tells you here 90 degree south latitude.

284
0:20:52.200 --> 0:20:55.720
This is south pole.

285
0:20:55.720 --> 0:21:02.120
What time is it there right now?

286
0:21:02.120 --> 0:21:06.520
That's one valid answer all the times.

287
0:21:06.520 --> 0:21:10.080
Let's see if we can answer this question.

288
0:21:10.080 --> 0:21:13.640
So we have a couple of date and time types in Postgres.

289
0:21:13.640 --> 0:21:19.760
So we have time stamp without time zone and time stamp with time zone.

290
0:21:19.760 --> 0:21:24.520
Depending on your use case, make a good choice which one you use.

291
0:21:24.520 --> 0:21:29.960
Any time you just want to store a time, date and a time, you want to use a time stamp without

292
0:21:29.960 --> 0:21:30.960
time zone.

293
0:21:30.960 --> 0:21:36.160
If you work with multiple time zones, we really use with time zone.

294
0:21:36.160 --> 0:21:42.040
Internally Postgres will store the time as UTC time, but it will make sure to handle

295
0:21:42.040 --> 0:21:43.440
the transformation for you.

296
0:21:43.440 --> 0:21:46.400
We will see a couple examples.

297
0:21:46.400 --> 0:21:54.720
We also have time without time zone and time with time zone.

298
0:21:54.720 --> 0:21:58.000
And we have date and interval.

299
0:21:58.000 --> 0:22:02.040
Postgres will not know about any kind of leap seconds.

300
0:22:02.040 --> 0:22:06.280
So occasionally we have a year which is a second longer with the leap seconds.

301
0:22:06.280 --> 0:22:11.920
I think they are planning one because earth is going slower so we use it.

302
0:22:11.920 --> 0:22:13.800
I don't know how this will go.

303
0:22:13.800 --> 0:22:18.360
But anyway, Postgres doesn't know about leap seconds because the time zone database doesn't

304
0:22:18.360 --> 0:22:21.960
know about it.

305
0:22:21.960 --> 0:22:26.240
So we have something which looks like a date.

306
0:22:26.240 --> 0:22:28.880
I cast it to a time stamp.

307
0:22:28.880 --> 0:22:33.400
And we see that Postgres makes it this date midnight.

308
0:22:33.400 --> 0:22:37.680
So if you don't specify your time, it's always midnight.

309
0:22:37.680 --> 0:22:43.240
We can also say, okay, January 5th, that's a format Americans using months first.

310
0:22:43.240 --> 0:22:52.000
We also see it's midnight here.

311
0:22:52.000 --> 0:22:55.760
We can also specify a time zone.

312
0:22:55.760 --> 0:23:02.240
So I have here 325 afternoon in UTC time zone.

313
0:23:02.240 --> 0:23:05.200
I cast this to a time stamp.

314
0:23:05.200 --> 0:23:06.200
What does it say?

315
0:23:06.200 --> 0:23:07.200
525.

316
0:23:07.200 --> 0:23:11.720
Any idea?

317
0:23:11.720 --> 0:23:17.120
Postgres will always return times in my current time zone which is set on my system.

318
0:23:17.120 --> 0:23:21.680
So many companies use servers which are set to UTC.

319
0:23:21.680 --> 0:23:27.560
My laptop which I use for this example is set to Berlin time which in summer is two

320
0:23:27.560 --> 0:23:29.840
hours before UTC.

321
0:23:29.840 --> 0:23:34.280
So we see I specify a time zone as UTC here.

322
0:23:34.280 --> 0:23:38.680
And in August I get 525 back.

323
0:23:38.680 --> 0:23:43.960
So it transforms the time I specify to my local time.

324
0:23:43.960 --> 0:23:45.360
Same in winter.

325
0:23:45.360 --> 0:23:47.360
This is December here.

326
0:23:47.360 --> 0:23:49.320
1023 UTC.

327
0:23:49.320 --> 0:23:54.680
I get 1123 back as my time.

328
0:23:54.680 --> 0:23:58.880
This can be very convenient but also very inconvenient depending on what you're working

329
0:23:58.880 --> 0:24:01.480
on.

330
0:24:01.480 --> 0:24:08.560
We can also say any time zone as any time zone your computer knows about.

331
0:24:08.560 --> 0:24:12.560
So we have a time zone database in computers.

332
0:24:12.560 --> 0:24:16.720
And we can use any name from there to specify as a time zone.

333
0:24:16.720 --> 0:24:22.800
We can also say just plus or minus the number for the time zone.

334
0:24:22.800 --> 0:24:28.480
Obviously if you specify a time zone as a name Postgres knows about summertime.

335
0:24:28.480 --> 0:24:34.560
If you just say plus four it never knows about summertime because you just say plus four.

336
0:24:34.560 --> 0:24:39.440
The example I'm using here is because at some point Russia just said okay we are no longer

337
0:24:39.440 --> 0:24:42.000
doing this dance with winter and summertime.

338
0:24:42.000 --> 0:24:52.640
They just stopped at some point and said okay year round it's one time zone.

339
0:24:52.640 --> 0:24:57.280
In Postgres time is stopped if you start a transaction.

340
0:24:57.280 --> 0:24:58.920
So we start a transaction here.

341
0:24:58.920 --> 0:25:04.680
If you say select now multiple times you always get the same time back.

342
0:25:04.680 --> 0:25:06.800
That's the transaction time.

343
0:25:06.800 --> 0:25:08.920
So I'm using this here.

344
0:25:08.920 --> 0:25:13.480
Now I'm setting my time zone to Europe Moscow.

345
0:25:13.480 --> 0:25:14.480
What changed?

346
0:25:14.480 --> 0:25:16.120
My output changed.

347
0:25:16.120 --> 0:25:18.520
This one is my brilliant time.

348
0:25:18.520 --> 0:25:20.000
This one is my Moscow time.

349
0:25:20.000 --> 0:25:30.760
Two time zones difference.

350
0:25:30.760 --> 0:25:38.960
What I can also say any time stamp I have in Postgres at a specific time zone.

351
0:25:38.960 --> 0:25:46.720
So previously everything here was always returned in my own time zone which is set on my computer.

352
0:25:46.720 --> 0:25:54.520
I could say change everything to this specific time zone or I can just say format one specific

353
0:25:54.520 --> 0:25:58.680
time stamp I have at a given time zone.

354
0:25:58.680 --> 0:26:05.000
Then of course you can say okay select now at Berlin comma now at New York comma now

355
0:26:05.000 --> 0:26:08.720
at Buenos Aires in one single query.

356
0:26:08.720 --> 0:26:13.280
So we can return as many time zones or time stamps you have at as many different time

357
0:26:13.280 --> 0:26:19.720
zones you have.

358
0:26:19.720 --> 0:26:20.880
A couple more examples.

359
0:26:20.880 --> 0:26:28.920
So Postgres does not know about leap seconds but it knows about leap years.

360
0:26:28.920 --> 0:26:35.920
So we have two thousand fifths of January minus two thousand first of January.

361
0:26:35.920 --> 0:26:38.000
This is four days difference.

362
0:26:38.000 --> 0:26:42.280
It's an interval now of four days.

363
0:26:42.280 --> 0:26:47.040
We have two thousand first of January minus fourth of January.

364
0:26:47.040 --> 0:26:53.200
It's minus three days.

365
0:26:53.200 --> 0:26:54.400
We get an interval back.

366
0:26:54.400 --> 0:27:00.000
So if we have time stamps Postgres will do all the calculation for us between the two

367
0:27:00.000 --> 0:27:06.880
time stamps including years dates including leap dates everything.

368
0:27:06.880 --> 0:27:13.200
And we can use this to figure out of a specific year is a leap year.

369
0:27:13.200 --> 0:27:22.200
Two thousand twenty eight of February plus one day interval gives me two thousand twenty

370
0:27:22.200 --> 0:27:24.520
ninth of February.

371
0:27:24.520 --> 0:27:30.400
Of course if I do this in 2001 I only get three hundred sixty five days back.

372
0:27:30.400 --> 0:27:33.920
In 2000 I get two hundred sixty six days back.

373
0:27:33.920 --> 0:27:36.640
It's a leap year.

374
0:27:36.640 --> 0:27:39.080
Makes all of this into account.

375
0:27:39.080 --> 0:27:42.400
Which brings me back to my initial question.

376
0:27:42.400 --> 0:27:45.600
What time is it at South Pole?

377
0:27:45.600 --> 0:27:50.080
This station at South Pole is operated by the Americans.

378
0:27:50.080 --> 0:27:55.520
It's called Scott Amundsen station which is however supplied from New Zealand.

379
0:27:55.520 --> 0:27:59.960
That's the closest airport they have for all the planes they're operating.

380
0:27:59.960 --> 0:28:06.000
And by now we know how to figure out the time in New Zealand right.

381
0:28:06.000 --> 0:28:10.080
Right now at time zone in New Zealand.

382
0:28:10.080 --> 0:28:17.440
That would be cheating because your operating system actually knows about Antarctica.

383
0:28:17.440 --> 0:28:24.800
So every station which humans have on Antarctica got its own time zone which is usually aligned

384
0:28:24.800 --> 0:28:28.560
to the country operating the station.

385
0:28:28.560 --> 0:28:34.480
Because this one is well operated by Americans but supplied from New Zealand conveniently

386
0:28:34.480 --> 0:28:38.320
they're using the same time zone as New Zealand.

387
0:28:38.320 --> 0:28:47.320
Select now at time zone and Antarctica South Pole gives you the time at the South Pole.

388
0:28:47.320 --> 0:28:49.920
Anyone here using XML?

389
0:28:49.920 --> 0:28:51.880
Fine.

390
0:28:51.880 --> 0:28:56.720
Let's.

391
0:28:56.720 --> 0:29:01.440
What's your use case?

392
0:29:01.440 --> 0:29:07.520
Some data feeds.

393
0:29:07.520 --> 0:29:14.360
Where there are some basic support in Postgres for XML.

394
0:29:14.360 --> 0:29:16.960
You can set encoding.

395
0:29:16.960 --> 0:29:20.160
You cannot search directly in XML.

396
0:29:20.160 --> 0:29:24.520
So Postgres stores the XML as it is but you cannot really search in it.

397
0:29:24.520 --> 0:29:25.680
There's no support for this.

398
0:29:25.680 --> 0:29:30.320
What you could do is cast it to text and then try to make some sense out of it.

399
0:29:30.320 --> 0:29:33.000
But that's about it.

400
0:29:33.000 --> 0:29:34.560
We have two different types.

401
0:29:34.560 --> 0:29:37.920
We can say it's a document type here.

402
0:29:37.920 --> 0:29:44.560
So I specify a text and I tell Postgres, okay, pass this as a document in XML.

403
0:29:44.560 --> 0:29:46.640
It will fail if it's not proper XML.

404
0:29:46.640 --> 0:29:50.320
And tell me, okay, this doesn't work.

405
0:29:50.320 --> 0:29:58.720
So I get back an XML document here with all the formatting and the entire tree.

406
0:29:58.720 --> 0:30:01.280
I can also say I don't want to have an entire document.

407
0:30:01.280 --> 0:30:07.960
I just want to have a piece of XML content that's working with XML paths and content.

408
0:30:07.960 --> 0:30:10.600
But then again, I cannot search in it.

409
0:30:10.600 --> 0:30:14.440
There's no support for it.

410
0:30:14.440 --> 0:30:19.760
I can serialize this and unserialize this if you want to store some larger XML documents

411
0:30:19.760 --> 0:30:20.760
in Postgres.

412
0:30:20.760 --> 0:30:21.760
That's working.

413
0:30:21.760 --> 0:30:25.240
So this one is a text now.

414
0:30:25.240 --> 0:30:27.480
No longer an XML document.

415
0:30:27.480 --> 0:30:32.400
So if you really want to search in something and say, okay, does this specific text appear

416
0:30:32.400 --> 0:30:40.320
in my XML document, go and serialize it and then maybe apply some like or wakeups on it.

417
0:30:40.320 --> 0:30:49.760
But then again, if you search for name, you will find plenty of this.

418
0:30:49.760 --> 0:30:56.440
Reality is if you want to store something like XML, go for JSON.

419
0:30:56.440 --> 0:31:00.240
We have two different JSON data types in Postgres.

420
0:31:00.240 --> 0:31:04.520
One is the older one is called JSON.

421
0:31:04.520 --> 0:31:07.080
JSON as it is stores the data as it comes in.

422
0:31:07.080 --> 0:31:12.320
So it basically takes the entire JSON blob, stores it as it is.

423
0:31:12.320 --> 0:31:18.120
And then later on, if you work on the data type, then it does all the parsing.

424
0:31:18.120 --> 0:31:24.160
So at insertion time, it doesn't really know if your JSON object is valid or not.

425
0:31:24.160 --> 0:31:27.840
It only figures out when you try to operate on it.

426
0:31:27.840 --> 0:31:30.960
And then at some point, there came a better JSON type.

427
0:31:30.960 --> 0:31:32.840
It's called JSONB.

428
0:31:32.840 --> 0:31:39.000
I think one of the big mistakes this project made was not to duplicate JSON and say, okay,

429
0:31:39.000 --> 0:31:41.880
the new one is the JSON type.

430
0:31:41.880 --> 0:31:46.640
So I called it JSONB and now we have to live with it.

431
0:31:46.640 --> 0:31:49.920
The new one is better in almost every way.

432
0:31:49.920 --> 0:31:55.960
So it does parse the JSON when you insert into the database, when you create this type.

433
0:31:55.960 --> 0:31:57.920
You can create an index on it.

434
0:31:57.920 --> 0:32:02.480
It already tells you on creation time if it's valid or not.

435
0:32:02.480 --> 0:32:08.120
And then you have a decomposed object, a JSON object in your database, which you can work

436
0:32:08.120 --> 0:32:09.120
on.

437
0:32:09.120 --> 0:32:17.320
All of this is using regular transactions, like some other databases using JSON.

438
0:32:17.320 --> 0:32:19.640
You don't get JSON support on this.

439
0:32:19.640 --> 0:32:26.000
And Postgres, it's one more data type we use supporting transactions, everything supporting

440
0:32:26.000 --> 0:32:27.160
replication.

441
0:32:27.160 --> 0:32:33.120
How do we use this?

442
0:32:33.120 --> 0:32:36.440
This is a regular text here.

443
0:32:36.440 --> 0:32:42.440
I need to quote my text in JSON with a double quote to make it a JSON text.

444
0:32:42.440 --> 0:32:47.800
Then I have my single quotes for Postgres telling it this is a string.

445
0:32:47.800 --> 0:33:01.680
And I cast this string to JSON and I get my JSON text back.

446
0:33:01.680 --> 0:33:05.520
We can use arrays and lists and hashes in JSON.

447
0:33:05.520 --> 0:33:07.000
So here we have an array.

448
0:33:07.000 --> 0:33:12.200
As you can see, we have several text types, JSON text types in the array.

449
0:33:12.200 --> 0:33:14.800
Then we create this array.

450
0:33:14.800 --> 0:33:20.280
And we make this a Postgres string.

451
0:33:20.280 --> 0:33:25.480
It's a JSON type and we see, okay, I still have my JSON text types here.

452
0:33:25.480 --> 0:33:26.480
My JSON array.

453
0:33:26.480 --> 0:33:33.040
And for Postgres, all of this, one string passed as a JSON type.

454
0:33:33.040 --> 0:33:35.280
The same works with key value pairs.

455
0:33:35.280 --> 0:33:43.120
So we have key and value here as JSON, make it a hash, and then make it a string in Postgres,

456
0:33:43.120 --> 0:33:49.880
cast this string to JSONP.

457
0:33:49.880 --> 0:33:54.720
Of course, since we passed the JSON, I already Postgres knows what's in there.

458
0:33:54.720 --> 0:33:59.600
So we can say, okay, I only want to have this key number two.

459
0:33:59.600 --> 0:34:01.880
Here my text is D, E, F.

460
0:34:01.880 --> 0:34:09.240
I can access whatever is in my JSON value.

461
0:34:09.240 --> 0:34:15.720
I can ask if the white element is in the list on the left.

462
0:34:15.720 --> 0:34:18.120
GHA is actually in there.

463
0:34:18.120 --> 0:34:19.120
So I get a tool back.

464
0:34:19.120 --> 0:34:29.640
It's a yes, no question in Postgres.

465
0:34:29.640 --> 0:34:30.760
Same for the keys.

466
0:34:30.760 --> 0:34:31.760
So here's the value.

467
0:34:31.760 --> 0:34:33.840
Is this value in this list?

468
0:34:33.840 --> 0:34:35.720
Is this key in this list?

469
0:34:35.720 --> 0:34:42.600
So any of the questions you usually have from an application using JSON, like, is a specific

470
0:34:42.600 --> 0:34:43.600
value there?

471
0:34:43.600 --> 0:34:45.560
Does a specific field exist?

472
0:34:45.560 --> 0:34:49.200
All of this you can answer directly in Postgres.

473
0:34:49.200 --> 0:34:54.320
You don't have to extract the entire data type, transfer it into your application, and

474
0:34:54.320 --> 0:34:56.360
then try to make sense of it.

475
0:34:56.360 --> 0:35:03.200
All of this can be answered directly in Postgres in one query.

476
0:35:03.200 --> 0:35:09.320
On top of it, because Postgres already knows what is in the JSON, we can have an index

477
0:35:09.320 --> 0:35:11.040
support on this.

478
0:35:11.040 --> 0:35:17.920
This only works on JSON B, by the way, not on the old JSON type.

479
0:35:17.920 --> 0:35:21.000
So what I'm doing here is I have an index.

480
0:35:21.000 --> 0:35:25.240
On my table, I have to use the GIN type.

481
0:35:25.240 --> 0:35:31.920
And I only create this index on one specific field in my JSON object.

482
0:35:31.920 --> 0:35:34.880
This is not indexing the entire JSON field.

483
0:35:34.880 --> 0:35:37.920
It's just one field in my object here, the name field.

484
0:35:37.920 --> 0:35:43.520
And then I can use this index to answer queries.

485
0:35:43.520 --> 0:35:51.480
So if you have this typical web application, we are storing 20, 50, 100 JSON values in

486
0:35:51.480 --> 0:35:55.240
one object, you don't want to index all of them.

487
0:35:55.240 --> 0:35:59.320
You maybe just want to have an index on one or two of the fields.

488
0:35:59.320 --> 0:36:03.640
This is possible in Postgres.

489
0:36:03.640 --> 0:36:06.240
Booleans.

490
0:36:06.240 --> 0:36:10.200
We have a real Boolean type in Postgres.

491
0:36:10.200 --> 0:36:13.320
So we can say two faults.

492
0:36:13.320 --> 0:36:16.480
We have a couple of alternatives you can specify.

493
0:36:16.480 --> 0:36:20.320
So for two, you can say it's one or two or yes.

494
0:36:20.320 --> 0:36:25.200
For faults, you can say it's faults or no or n.

495
0:36:25.200 --> 0:36:30.720
Can you please be a little bit silent back then?

496
0:36:30.720 --> 0:36:36.960
In the end, what Postgres does, it transforms all of these values into the two or fours

497
0:36:36.960 --> 0:36:39.680
value for the Boolean.

498
0:36:39.680 --> 0:36:46.880
We have a couple of other databases in the market which say, yeah, we have a Boolean.

499
0:36:46.880 --> 0:36:50.080
Under the root, it's maybe just an integer.

500
0:36:50.080 --> 0:36:55.440
Then you can insert not only zero and one but also five, eight, 50 and then try to make

501
0:36:55.440 --> 0:36:57.960
sense out of this.

502
0:36:57.960 --> 0:37:05.320
Here we only get two and fours back.

503
0:37:05.320 --> 0:37:11.080
So two, cast as a Boolean, we see it's getting two.

504
0:37:11.080 --> 0:37:17.520
And fours, we get an F back.

505
0:37:17.520 --> 0:37:20.800
And we can use this in queries.

506
0:37:20.800 --> 0:37:23.440
So I have one table here.

507
0:37:23.440 --> 0:37:28.960
Let's say that's a table where you store log file messages.

508
0:37:28.960 --> 0:37:31.560
And yeah, we have some content here.

509
0:37:31.560 --> 0:37:37.400
And we have one field which tells me, okay, this log entry is an error.

510
0:37:37.400 --> 0:37:40.960
Usually we don't have a lot of errors.

511
0:37:40.960 --> 0:37:46.040
But this is an entry in our table that we are mostly interested in.

512
0:37:46.040 --> 0:37:54.320
So maybe one, 2% of the queries of the entries in this table will have this flag set.

513
0:37:54.320 --> 0:38:02.240
What I'm doing here, I create a million rows and about 2% of them have this flag set just

514
0:38:02.240 --> 0:38:07.240
to have some basic test data here.

515
0:38:07.240 --> 0:38:13.280
And when I go and say, okay, I only want to see all the entries in my table where the

516
0:38:13.280 --> 0:38:19.320
error is true, because that's what I'm interested in, you see by default, process has to scan

517
0:38:19.320 --> 0:38:21.840
the entire table.

518
0:38:21.840 --> 0:38:25.760
It's quite expensive.

519
0:38:25.760 --> 0:38:31.680
What I can say, I create an index and make this a conditional index.

520
0:38:31.680 --> 0:38:36.880
And only every row where error is true goes into this index.

521
0:38:36.880 --> 0:38:42.920
So the 98% of the table where there is no error I'm not interested in, because the

522
0:38:42.920 --> 0:38:44.960
cardinality is not high enough.

523
0:38:44.960 --> 0:38:49.880
I will never see this data in my index.

524
0:38:49.880 --> 0:38:56.360
Only the 2% here go into my index and now suddenly the cost of the query came down from

525
0:38:56.360 --> 0:39:03.720
16,000 something to 68.

526
0:39:03.720 --> 0:39:07.080
Very fast now.

527
0:39:07.080 --> 0:39:16.840
There's one use case for Boolean index.

528
0:39:16.840 --> 0:39:25.400
So for the cost of this, I have another index now on the entire column and you can see the

529
0:39:25.400 --> 0:39:35.800
entire index needs 2,700 pages and my Boolean index only needs 57 pages on this.

530
0:39:35.800 --> 0:39:42.360
So much, much faster.

531
0:39:42.360 --> 0:39:46.520
We have a bit type in Postgres, so not only Boolean, we can also say we want to store

532
0:39:46.520 --> 0:39:48.520
bits.

533
0:39:48.520 --> 0:39:53.000
Or you can specify how many bits you want to have.

534
0:39:53.000 --> 0:39:58.040
The interesting part is you can tell Postgres by using the B in front.

535
0:39:58.040 --> 0:40:03.400
This is binary value and it does all the transformation for you.

536
0:40:03.400 --> 0:40:23.800
From this which looks like a string with bits in it to sorry.

537
0:40:23.800 --> 0:40:27.480
Postgres will do the transformation of the bits for us.

538
0:40:27.480 --> 0:40:35.600
So we don't have to do the middle calculation which bit is which value.

539
0:40:35.600 --> 0:40:39.560
We can also do bit operations on these values.

540
0:40:39.560 --> 0:40:48.080
So I have some data in my table and I want to only select where this bit is set.

541
0:40:48.080 --> 0:40:52.600
Or specify that's a binary value here, a bit value here.

542
0:40:52.600 --> 0:40:57.000
And that's a logical end for a logical OR.

543
0:40:57.000 --> 0:41:02.080
And then of course because OR gives me any value which the bit is set on the left or

544
0:41:02.080 --> 0:41:07.960
the right side, I get everything back here.

545
0:41:07.960 --> 0:41:10.400
We can have exclusive OR on bits.

546
0:41:10.400 --> 0:41:15.520
We can also do mathematical operations on bits like shifting left and right for multiplied

547
0:41:15.520 --> 0:41:18.000
by two or divided by two.

548
0:41:18.000 --> 0:41:23.160
All of this works.

549
0:41:23.160 --> 0:41:24.720
We can also search in bits.

550
0:41:24.720 --> 0:41:30.360
It looks a bit complicated because we need to make this a logical operation.

551
0:41:30.360 --> 0:41:37.200
So I want to search everything where this bit is set and because it gives me a value

552
0:41:37.200 --> 0:41:43.440
back I need to say okay this is greater now than I get to result.

553
0:41:43.440 --> 0:41:48.440
If I search for everything where the second bit from the right is set, there is no value

554
0:41:48.440 --> 0:41:57.840
in it so I don't get anything back from my database.

555
0:41:57.840 --> 0:42:02.640
I can cast bits to any value.

556
0:42:02.640 --> 0:42:03.840
Like that's an integer.

557
0:42:03.840 --> 0:42:08.960
I can cast it to bits and get my bit value back any other way around.

558
0:42:08.960 --> 0:42:15.200
I can cast from bits to integers to any other value.

559
0:42:15.200 --> 0:42:18.000
Good.

560
0:42:18.000 --> 0:42:23.120
Anyone of you storing binary data in a database?

561
0:42:23.120 --> 0:42:26.920
Good for you.

562
0:42:26.920 --> 0:42:31.440
We have a byte A type which can do this.

563
0:42:31.440 --> 0:42:35.680
All I want to say about this is please use the functions of your programming language

564
0:42:35.680 --> 0:42:39.960
to transfer the data in and out of the database.

565
0:42:39.960 --> 0:42:45.720
Please don't write your own code to try and transform this data.

566
0:42:45.720 --> 0:42:51.400
Every language we have which supports Postgres has functions for transferring binary data

567
0:42:51.400 --> 0:42:57.840
in and out of the database.

568
0:42:57.840 --> 0:42:59.920
We have two different formats.

569
0:42:59.920 --> 0:43:05.240
One is called the old escape format which is hard to pass.

570
0:43:05.240 --> 0:43:10.840
And the new one is the hex format so this will always store hex values for binary data

571
0:43:10.840 --> 0:43:15.960
in a database.

572
0:43:15.960 --> 0:43:21.440
We also have network types in Postgres.

573
0:43:21.440 --> 0:43:27.360
So E-Net can store any IPv4 or IPv6 address.

574
0:43:27.360 --> 0:43:33.000
The network type CDIR can store the networks.

575
0:43:33.000 --> 0:43:37.320
And we can store mega address in Postgres.

576
0:43:37.320 --> 0:43:49.160
So any time you work with network addresses, please don't store them as a text.

577
0:43:49.160 --> 0:43:55.160
Use the proper data type for it because you can go and have an index on it or you can

578
0:43:55.160 --> 0:44:02.920
go and ask Postgres is this IP address in this network?

579
0:44:02.920 --> 0:44:12.080
I created a table with a couple of IP addresses in here.

580
0:44:12.080 --> 0:44:19.520
And then I can ask Postgres give me every IP address which is in this network.

581
0:44:19.520 --> 0:44:26.920
So you don't have to do all the manual handling of IP addresses in text matching and whatever

582
0:44:26.920 --> 0:44:31.080
people do to find IP addresses.

583
0:44:31.080 --> 0:44:33.720
And on top of that, it supports an index.

584
0:44:33.720 --> 0:44:36.400
It gets very, very fast.

585
0:44:36.400 --> 0:44:41.440
Any kind of address and IP address parsing can be very fast in Postgres.

586
0:44:41.440 --> 0:44:48.080
I'm almost running out of time because we started late.

587
0:44:48.080 --> 0:44:50.440
You can create your own data types in Postgres.

588
0:44:50.440 --> 0:44:53.720
So we have enums and a couple more.

589
0:44:53.720 --> 0:44:57.120
We have Postgres as extension.

590
0:44:57.120 --> 0:45:00.600
This is part of another talk I have.

591
0:45:00.600 --> 0:45:05.040
Not here, not today.

592
0:45:05.040 --> 0:45:07.720
So I keep this part.

593
0:45:07.720 --> 0:45:11.720
Any questions you have?

594
0:45:11.720 --> 0:45:20.800
Jimmy, do we have any questions?

595
0:45:20.800 --> 0:45:43.880
During the part about the timestamp or timestamp C data type, you said that the database handles

596
0:45:43.880 --> 0:45:47.480
the conversion from UTC type.

597
0:45:47.480 --> 0:45:55.000
Does it store the UTC type and also the offset in time zone or does it always convert to

598
0:45:55.000 --> 0:45:59.120
the current time zone of the database, I guess?

599
0:45:59.120 --> 0:46:08.080
Whatever you insert into the data type and you don't specify a time zone, it will assume

600
0:46:08.080 --> 0:46:09.760
your local time zone.

601
0:46:09.760 --> 0:46:16.000
If you specify a time zone and use this time zone, it always converts it to UTC internally.

602
0:46:16.000 --> 0:46:20.520
When you select it, it returns in your time zone you specify.

603
0:46:20.520 --> 0:46:22.520
Thank you.

604
0:46:22.520 --> 0:46:26.320
Nice talk.

605
0:46:26.320 --> 0:46:33.640
So, different type, particularly since interesting, what do you say, like, can I use Postgres for

606
0:46:33.640 --> 0:46:38.760
my non-intellectual use purposes or something like MongoDB?

607
0:46:38.760 --> 0:46:41.400
I think you're in a wrong room.

608
0:46:41.400 --> 0:46:44.920
The answer is yes.

609
0:46:44.920 --> 0:46:51.960
Almost everything you can use MongoDB for, you can also do in Postgres.

610
0:46:51.960 --> 0:46:53.960
Hi.

611
0:46:53.960 --> 0:47:00.120
Is there ever any hope of getting an unsigned integer type in PostgresQL?

612
0:47:00.120 --> 0:47:04.640
I think there's an extension which can do that, but it's not there by default.

613
0:47:04.640 --> 0:47:09.120
If you check the Postgres extension network, PGXN, there are a couple more data types you

614
0:47:09.120 --> 0:47:21.000
can use for this.

615
0:47:21.000 --> 0:47:23.640
I didn't get it quite right.

616
0:47:23.640 --> 0:47:27.760
You recommended JSONB data type over JSON.

617
0:47:27.760 --> 0:47:28.760
Yes.

618
0:47:28.760 --> 0:47:29.760
Okay.

619
0:47:29.760 --> 0:47:30.760
I got it.

620
0:47:30.760 --> 0:47:35.520
Basically, everything you can do in JSON, you can also do in JSONB, but it gives you

621
0:47:35.520 --> 0:47:41.800
more functionality in JSONB.

622
0:47:41.800 --> 0:47:46.680
It usually is, yes.

623
0:47:46.680 --> 0:47:50.320
Okay.

624
0:47:50.320 --> 0:48:05.960
Thanks, Jimmy.

