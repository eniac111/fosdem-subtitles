1
0:00:00.000 --> 0:00:07.440
So, hello everyone.

2
0:00:07.440 --> 0:00:11.560
Thanks for joining us today to the Postgres Dev Room.

3
0:00:11.560 --> 0:00:16.020
Our next speaker is Chris Travers, who flew all the way from Indonesia.

4
0:00:16.020 --> 0:00:17.020
Thank you.

5
0:00:17.020 --> 0:00:22.440
And he's going to talk about why database teams need human factor training.

6
0:00:22.440 --> 0:00:29.480
Thank you.

7
0:00:29.480 --> 0:00:31.520
Thank you very much for coming to this talk.

8
0:00:31.520 --> 0:00:39.040
I think it's certainly one of the topics I'm most excited about when it comes to database

9
0:00:39.040 --> 0:00:42.960
related topics, actually, even though I'm very, very much into Postgres.

10
0:00:42.960 --> 0:00:45.720
This topic really excites me.

11
0:00:45.720 --> 0:00:49.520
So just introducing myself a bit for those who don't know me.

12
0:00:49.520 --> 0:00:55.920
I have a bit over 24 years of experience in Postgres, so almost 25 years.

13
0:00:55.920 --> 0:00:58.200
I've built accounting software on it.

14
0:00:58.200 --> 0:01:05.440
I have worked as a database administrator on very large databases, built database teams,

15
0:01:05.440 --> 0:01:07.840
managed infrastructure, a lot of that sort of thing.

16
0:01:07.840 --> 0:01:09.640
So I have a wide range of experience.

17
0:01:09.640 --> 0:01:14.240
I have submitted several patches in for Postgres, of which one has been accepted.

18
0:01:14.240 --> 0:01:19.440
And I'll probably be submitting more patches at some point in the future.

19
0:01:19.440 --> 0:01:24.560
So I absolutely love Postgres for its extensibility.

20
0:01:24.560 --> 0:01:30.120
And with that, of course, comes some complexity, some difficulties in kind of maintaining our

21
0:01:30.120 --> 0:01:33.840
mental models about how things are actually working.

22
0:01:33.840 --> 0:01:40.640
And especially if you're working at things in scale, it's really easy for your mental

23
0:01:40.640 --> 0:01:42.680
model not to match what's actually happening.

24
0:01:42.680 --> 0:01:45.360
And then sometimes we make mistakes and things happen.

25
0:01:45.360 --> 0:01:53.320
So this talk is basically going to be about two things.

26
0:01:53.320 --> 0:02:00.640
The first thing is something our industry doesn't do very well.

27
0:02:00.640 --> 0:02:09.240
And that is how we look at human error and how we can possibly do that better.

28
0:02:09.240 --> 0:02:14.680
I kind of want to talk a little bit about how we can improve and what the benefits are

29
0:02:14.680 --> 0:02:21.360
that we can expect from some of the early steps that we can take as an industry.

30
0:02:21.360 --> 0:02:24.920
So this is very much a talk about database people.

31
0:02:24.920 --> 0:02:27.000
It's a talk about us.

32
0:02:27.000 --> 0:02:31.480
It's much less a talk about, like, a specific technology.

33
0:02:31.480 --> 0:02:35.800
But a lot of the same technical approaches apply.

34
0:02:35.800 --> 0:02:43.640
So I want to give a few thanks, first of all, timescale for paying me to come and do this.

35
0:02:43.640 --> 0:02:47.280
Wouldn't really be feasible for me to fly from an issue without them.

36
0:02:47.280 --> 0:02:50.640
But I also really want to thank two of my prior employers.

37
0:02:50.640 --> 0:02:57.400
I want to thank Adjust where we were actually able to bring in aviation training on these

38
0:02:57.400 --> 0:02:58.600
human factors.

39
0:02:58.600 --> 0:03:05.400
So we brought in a company that did training for pilots as well as doctors.

40
0:03:05.400 --> 0:03:09.440
And a lot of the training was really eye-opening, and it allowed us to do some things that we

41
0:03:09.440 --> 0:03:11.760
couldn't do before.

42
0:03:11.760 --> 0:03:20.160
This was really a grand experiment, and it had a number of really big, tangible benefits.

43
0:03:20.160 --> 0:03:25.640
And then, of course, I also want to thank deliverycuro who I worked after that, where

44
0:03:25.640 --> 0:03:30.120
I was able to kind of work with people and evaluate both the successes and the shortcomings

45
0:03:30.120 --> 0:03:35.000
of what we had done at Adjust and further develop some of these ideas.

46
0:03:35.000 --> 0:03:38.240
So these are important areas.

47
0:03:38.240 --> 0:03:45.600
And I would also say that I'm involved in trying to help implement some of these things

48
0:03:45.600 --> 0:03:48.240
also at timescale.

49
0:03:48.240 --> 0:03:54.120
And introduction.

50
0:03:54.120 --> 0:03:59.440
So just as a – this is a completely rhetorical question.

51
0:03:59.440 --> 0:04:02.040
You don't have to raise your hand if you don't feel comfortable doing so.

52
0:04:02.040 --> 0:04:07.480
But how many of us have been on a team where somebody has been working on a production

53
0:04:07.480 --> 0:04:09.480
database while they're drunk?

54
0:04:09.480 --> 0:04:11.040
Let's see.

55
0:04:11.040 --> 0:04:17.000
I mean, as we go through our career, almost every single one of us will probably have

56
0:04:17.000 --> 0:04:21.320
that experience, right?

57
0:04:21.320 --> 0:04:27.160
And yet, how many times does it cause a major problem?

58
0:04:27.160 --> 0:04:28.160
Almost never.

59
0:04:28.160 --> 0:04:30.560
At least, I've never seen it cause a major problem.

60
0:04:30.560 --> 0:04:31.560
Okay?

61
0:04:31.560 --> 0:04:34.560
Now, part of that may be the context in which it happens.

62
0:04:34.560 --> 0:04:38.960
Like, you know, the subject matter experts been out partying, was not expecting to – was

63
0:04:38.960 --> 0:04:43.200
not really on call and has now been called in in an escalation.

64
0:04:43.200 --> 0:04:48.560
Somebody else may be handling a lot of the sort of wider incident strategy stuff where

65
0:04:48.560 --> 0:04:52.600
maybe alcohol might be a bigger problem.

66
0:04:52.600 --> 0:05:03.000
But at least in these contexts, alcohol doesn't seem to be a big factor in the further disruption

67
0:05:03.000 --> 0:05:05.560
of things once stuff's going on.

68
0:05:05.560 --> 0:05:07.460
But let me ask another question.

69
0:05:07.460 --> 0:05:14.480
How many people here have seen a case where a major incident or outage happened because

70
0:05:14.480 --> 0:05:20.200
somebody made a mistake because that person was tired?

71
0:05:20.200 --> 0:05:22.440
See?

72
0:05:22.440 --> 0:05:28.400
So we valorize the thing that causes us problems.

73
0:05:28.400 --> 0:05:32.440
While we demonize something that probably does cause some problems, no doubt, and maybe

74
0:05:32.440 --> 0:05:38.240
the demonization helps prevent more problems, but we valorize something that causes a lot

75
0:05:38.240 --> 0:05:39.240
more problems.

76
0:05:39.240 --> 0:05:40.240
Okay?

77
0:05:40.240 --> 0:05:43.240
Why is it we do that?

78
0:05:43.240 --> 0:05:48.080
How is it that we should stop doing that and actually rethink our priorities?

79
0:05:48.080 --> 0:05:56.240
Now, on one side, this is a good example of human error, right?

80
0:05:56.240 --> 0:06:01.720
We can talk about all the factors that go into that prioritization.

81
0:06:01.720 --> 0:06:09.320
But on the other side, it's also partly because we don't understand human error in our field,

82
0:06:09.320 --> 0:06:10.320
right?

83
0:06:10.320 --> 0:06:16.800
When we do a postmortem, if somebody made a mistake, we just say, oh, human error, and

84
0:06:16.800 --> 0:06:17.800
that's it.

85
0:06:17.800 --> 0:06:21.480
I'm going to come back to that point in a few minutes.

86
0:06:21.480 --> 0:06:25.200
So drunkenness versus fatigue.

87
0:06:25.200 --> 0:06:31.320
Now if one person drinks, say, a bottle of wine, and another person, one group of people

88
0:06:31.320 --> 0:06:36.760
drinks each a bottle of wine, and the other group of the people has, say, their sleep

89
0:06:36.760 --> 0:06:41.440
disrupted so they're only sleeping four hours, and then, you know, get up, and a few hours

90
0:06:41.440 --> 0:06:44.800
later they're in the other group.

91
0:06:44.800 --> 0:06:48.560
Give both of them complex tasks to perform.

92
0:06:48.560 --> 0:06:53.640
Who's going to perform worse?

93
0:06:53.640 --> 0:06:59.920
Sleep deprivation causes heavier cognitive deficiencies.

94
0:06:59.920 --> 0:07:09.680
Four hours of sleep, missing sleep, is worse than four drinks.

95
0:07:09.680 --> 0:07:15.320
Now obviously, you know, there are some tasks where that's not the case, like, you know,

96
0:07:15.320 --> 0:07:18.920
driving a car or something because you also have coordination problems induced by the

97
0:07:18.920 --> 0:07:19.920
alcohol.

98
0:07:19.920 --> 0:07:29.880
But from a pure information processing standpoint, having four hours of sleep only is worse than

99
0:07:29.880 --> 0:07:33.840
drinking a bottle of wine.

100
0:07:33.840 --> 0:07:40.440
And it's going to last at least the next day.

101
0:07:40.440 --> 0:07:49.400
So totally, totally worth thinking about that.

102
0:07:49.400 --> 0:07:56.360
So now that I've talked about, like, one aspect of human error, one thing that can induce

103
0:07:56.360 --> 0:08:01.920
a lot of human error, I want to talk about a brief history of why this field became really

104
0:08:01.920 --> 0:08:04.040
big in aviation.

105
0:08:04.040 --> 0:08:14.240
So back in the 1950s, 1960s, 80% of the aircraft accidents, for instance, were blamed on pilot

106
0:08:14.240 --> 0:08:15.240
error.

107
0:08:15.240 --> 0:08:17.240
Notice I didn't say human error, I said pilot.

108
0:08:17.240 --> 0:08:22.080
I'm going to come back to that distinction in a moment.

109
0:08:22.080 --> 0:08:26.320
In fact, I think the number might have been closer to 90%, okay?

110
0:08:26.320 --> 0:08:35.920
Our incident and accident rates in airlines are well over 100 times lower than they were

111
0:08:35.920 --> 0:08:38.320
at that point.

112
0:08:38.320 --> 0:08:43.960
So if you think about it, improvements in the technology of the airplanes could only

113
0:08:43.960 --> 0:08:48.240
account for maybe 10% of that improvement.

114
0:08:48.240 --> 0:08:54.840
All the rest of this is due to much better understanding of the question of human error.

115
0:08:54.840 --> 0:09:00.160
And there's been a shift from focusing on pilot error to focusing on human error.

116
0:09:00.160 --> 0:09:04.960
And when the aviation industry talks about human error, they don't mean somebody made

117
0:09:04.960 --> 0:09:08.280
a mistake, and that's where they leave it, right?

118
0:09:08.280 --> 0:09:14.240
They have a rich taxonomy of understanding kinds of human error, causes of each of these

119
0:09:14.240 --> 0:09:22.360
particular types of errors, and sort of practices to try to mitigate them.

120
0:09:22.360 --> 0:09:28.480
So the way I would actually describe this difference is that if you're debugging software

121
0:09:28.480 --> 0:09:32.080
and it's connecting to the database, every time, you know, let's say you have an error

122
0:09:32.080 --> 0:09:36.400
in your query or something the database can't fulfill your request, it just says something

123
0:09:36.400 --> 0:09:37.400
went wrong.

124
0:09:37.400 --> 0:09:42.480
You know, you're not going to be able to debug that software at all, and you're probably

125
0:09:42.480 --> 0:09:47.240
going to, you know, have a lot of trouble.

126
0:09:47.240 --> 0:09:50.280
That's kind of what we do currently when we say human error.

127
0:09:50.280 --> 0:09:56.080
We just simply say the person made a mistake and that's as far usually as we look.

128
0:09:56.080 --> 0:10:00.640
The aircraft industry has actually come up with something with a much richer understanding

129
0:10:00.640 --> 0:10:07.720
of this topic and sort of richer system of almost like error codes that they use when

130
0:10:07.720 --> 0:10:11.520
they talk about these issues.

131
0:10:11.520 --> 0:10:14.200
Reason is it's a very unforgiving environment, you know?

132
0:10:14.200 --> 0:10:18.480
You make a mistake, you might or might not be able to recover.

133
0:10:18.480 --> 0:10:26.360
So you have a lot of training on this, and now, you know, the chance of a massive disaster

134
0:10:26.360 --> 0:10:36.800
is down, you know, probably, you know, one error disaster per billion takeoffs, which

135
0:10:36.800 --> 0:10:39.480
is really impressive, right?

136
0:10:39.480 --> 0:10:43.440
We'd love to have that.

137
0:10:43.440 --> 0:10:45.480
So they've made this shift.

138
0:10:45.480 --> 0:10:49.440
They've also made a shift that we've already made, and it's worth pointing this out, and

139
0:10:49.440 --> 0:10:54.800
that's that they've made a shift from individual responsibility to collective responsibility.

140
0:10:54.800 --> 0:10:58.160
In our terms, we call it blameless culture, right?

141
0:10:58.160 --> 0:10:59.160
Somebody makes a mistake.

142
0:10:59.160 --> 0:11:00.160
We don't blame them.

143
0:11:00.160 --> 0:11:01.960
We don't go, hey, stop making mistakes.

144
0:11:01.960 --> 0:11:05.680
We try to find some way to keep that mistake from happening, right?

145
0:11:05.680 --> 0:11:09.920
But because we don't have a clear understanding of this topic, we try to solve this in ways

146
0:11:09.920 --> 0:11:14.920
that maybe aren't as effective as it could be.

147
0:11:14.920 --> 0:11:19.360
I want to give one really good example of sort of a watershed moment.

148
0:11:19.360 --> 0:11:24.400
Actually, before I talk about that, let me just discuss David Beatty's contribution quickly.

149
0:11:24.400 --> 0:11:35.640
Beatty was a cognitive psychologist and pilot in the UK, and in 1969, he wrote a seminal

150
0:11:35.640 --> 0:11:41.480
book called The Human Factor in Aircraft Accidents, where he basically looked at the kinds of

151
0:11:41.480 --> 0:11:46.920
mistakes that happen in the kinds of circumstances that lead to those mistakes.

152
0:11:46.920 --> 0:11:51.400
There are newer versions of that book out now.

153
0:11:51.400 --> 0:11:58.480
It's actually worth reading, but probably not best to read if you're a nervous flyer.

154
0:11:58.480 --> 0:12:09.240
But as a good description of how we break down error, it was the industry starting point.

155
0:12:09.240 --> 0:12:14.600
And 10 years after that, there was, I think, the watershed moment in how this became really

156
0:12:14.600 --> 0:12:18.280
big within aviation, and that was the Tenerife disaster.

157
0:12:18.280 --> 0:12:24.640
Tenerife disaster was the most deadly aircraft accident still in history.

158
0:12:24.640 --> 0:12:32.400
It happened on the ground in Tenerife due to a variety of factors, and I'm not sure how

159
0:12:32.400 --> 0:12:36.760
much detail I should go into in this talk, but the end result was basically that one

160
0:12:36.760 --> 0:12:44.520
747 tried to take off down a runway with limited visibility without a proper takeoff clearance,

161
0:12:44.520 --> 0:12:49.040
and they hit another 747 on the ground.

162
0:12:49.040 --> 0:12:54.880
Clear case of human error, and the Spanish report on this more or less blamed it on pilot

163
0:12:54.880 --> 0:12:55.880
error.

164
0:12:55.880 --> 0:12:57.120
This guy tried to take off.

165
0:12:57.120 --> 0:12:58.120
He didn't have the clearance.

166
0:12:58.120 --> 0:13:01.960
It was his fault.

167
0:13:01.960 --> 0:13:11.880
The Dutch report, which is often criticized in some documentaries that I've seen on this,

168
0:13:11.880 --> 0:13:13.120
was very, very different.

169
0:13:13.120 --> 0:13:16.880
What they actually did was they asked why did he try to take off without clearance?

170
0:13:16.880 --> 0:13:18.640
What was going through?

171
0:13:18.640 --> 0:13:21.240
How did that mistake happen?

172
0:13:21.240 --> 0:13:24.200
And the thing was, he was an extremely experienced pilot.

173
0:13:24.200 --> 0:13:27.400
He was their chief pilot.

174
0:13:27.400 --> 0:13:29.200
He actually didn't fly airplanes that much.

175
0:13:29.200 --> 0:13:31.080
He was mostly sitting in simulators.

176
0:13:31.080 --> 0:13:37.080
And the thing was, at that time, when you were the senior pilot in the simulator, you

177
0:13:37.080 --> 0:13:39.840
were giving the clearances.

178
0:13:39.840 --> 0:13:44.320
So stressful situation, visibility is slow.

179
0:13:44.320 --> 0:13:48.120
There's pressure to take off.

180
0:13:48.120 --> 0:13:53.280
Stressful situation, he goes back to what he's used to doing, which is assuming he has

181
0:13:53.280 --> 0:13:58.760
the clearance because he's used to giving them to himself.

182
0:13:58.760 --> 0:14:04.840
Airlines don't do that anymore in their simulators for obvious reasons.

183
0:14:04.840 --> 0:14:10.720
But the Dutch report actually became the basis for how the aviation industry has started

184
0:14:10.720 --> 0:14:14.200
to look at human error ever since.

185
0:14:14.200 --> 0:14:18.800
And as a result, what we've seen is we've seen this massive, massive improvement in

186
0:14:18.800 --> 0:14:20.920
safety.

187
0:14:20.920 --> 0:14:24.520
Every pilot in every airline gets this sort of training.

188
0:14:24.520 --> 0:14:31.040
And it has made our flights much, much, much, much safer.

189
0:14:31.040 --> 0:14:34.960
So the question is, can we benefit from the same thing?

190
0:14:34.960 --> 0:14:35.960
And the answer is yes.

191
0:14:35.960 --> 0:14:41.040
And we actually can get a lot more benefits from it than just reducing incidents and recovering

192
0:14:41.040 --> 0:14:43.400
from them better.

193
0:14:43.400 --> 0:14:50.200
In fact, if you look at the standard definition that people give of crew resource management,

194
0:14:50.200 --> 0:14:56.240
it's the use of all available resources to ensure the safe and efficient operation of

195
0:14:56.240 --> 0:14:57.240
the aircraft.

196
0:14:57.240 --> 0:15:04.800
Well, if we can use all of our resources to improve both safety and efficiency, that's

197
0:15:04.800 --> 0:15:06.200
going to make our jobs better.

198
0:15:06.200 --> 0:15:08.240
We're going to be more productive.

199
0:15:08.240 --> 0:15:11.000
We're going to be happier.

200
0:15:11.000 --> 0:15:17.360
So this is actually a really, really important field that I think that we need to improve

201
0:15:17.360 --> 0:15:19.040
on.

202
0:15:19.040 --> 0:15:26.600
So now I'm going to talk about how we look at human error in the industry.

203
0:15:26.600 --> 0:15:34.240
Human error, typically in the DevOps and SRE systems, we have one answer to human error.

204
0:15:34.240 --> 0:15:37.920
And what that is, automation.

205
0:15:37.920 --> 0:15:38.920
Somebody made a mistake.

206
0:15:38.920 --> 0:15:39.920
We're going to automate that away.

207
0:15:39.920 --> 0:15:41.920
We're just going to add more automation.

208
0:15:41.920 --> 0:15:42.920
We're going to add more automation.

209
0:15:42.920 --> 0:15:45.920
We're going to add more automation.

210
0:15:45.920 --> 0:15:48.640
Seems like a great idea, right?

211
0:15:48.640 --> 0:15:52.480
Computers are infallible, we're fallible, so we're just going to use the computers to

212
0:15:52.480 --> 0:15:55.200
prevent the mistake.

213
0:15:55.200 --> 0:15:57.200
Problem with this.

214
0:15:57.200 --> 0:16:04.080
IEEE has done a bunch of research on something they call the automation paradox.

215
0:16:04.080 --> 0:16:17.960
The automation paradox is that the more reliable the automation is, the less opportunities

216
0:16:17.960 --> 0:16:24.360
humans have to contribute to the overall success of that.

217
0:16:24.360 --> 0:16:27.160
And I think I'm going to take a little bit of time here to talk about why that is the

218
0:16:27.160 --> 0:16:28.160
case.

219
0:16:28.160 --> 0:16:34.360
And that will get reinforced in the next section when we talk about why we make mistakes.

220
0:16:34.360 --> 0:16:44.000
But basic, to start with a basic summary, you know, obviously we need automation because

221
0:16:44.000 --> 0:16:48.120
there are certain kinds of tasks that we're actually very bad at following.

222
0:16:48.120 --> 0:16:55.920
And there are certain kinds of requirements where automation can really save us a lot

223
0:16:55.920 --> 0:16:58.640
of safety considerations.

224
0:16:58.640 --> 0:17:04.640
So steps that have to be done together really should be automated so that they happen together.

225
0:17:04.640 --> 0:17:07.640
But automation is just done reflexively.

226
0:17:07.640 --> 0:17:14.680
At least according to a lot of the research that's come out of the IEEE as well as many

227
0:17:14.680 --> 0:17:22.200
of the aviation study groups on this is that simply throwing automation at a problem can

228
0:17:22.200 --> 0:17:26.400
actually make human error more common and can make human error more severe.

229
0:17:26.400 --> 0:17:32.280
And then when things are out of whack, you have no possibility at all of saving, of preventing

230
0:17:32.280 --> 0:17:35.960
a major incident.

231
0:17:35.960 --> 0:17:46.920
And part of the reason here is that we process all of what we see through a mental model.

232
0:17:46.920 --> 0:17:56.040
And so when we add more complexity, when we add more automation around a lot of this,

233
0:17:56.040 --> 0:18:02.400
we make it really, really, really, really, really hard for us to keep that mental model

234
0:18:02.400 --> 0:18:05.720
reasonably in sync with reality.

235
0:18:05.720 --> 0:18:10.200
And then when something goes wrong, we can spend a lot of time and effort struggling

236
0:18:10.200 --> 0:18:16.080
to understand what's going on or we may reflexively react in ways which actually make the problem

237
0:18:16.080 --> 0:18:19.360
worse.

238
0:18:19.360 --> 0:18:23.480
So automation isn't the answer.

239
0:18:23.480 --> 0:18:27.560
It is part of an answer.

240
0:18:27.560 --> 0:18:34.920
And reflexive automation, oh, we had a problem that's automated away, is not the answer.

241
0:18:34.920 --> 0:18:44.360
Now I mentioned just a moment ago this issue of mental models.

242
0:18:44.360 --> 0:18:49.960
We humans, we operate in a world that's very different from the way computers operate.

243
0:18:49.960 --> 0:19:00.160
Computers are basically systems that mathematically process inputs and produce outputs.

244
0:19:00.160 --> 0:19:05.800
And therefore computing programs basically operate in a closed world.

245
0:19:05.800 --> 0:19:08.720
We humans don't operate in closed worlds.

246
0:19:08.720 --> 0:19:10.240
We operate in open worlds.

247
0:19:10.240 --> 0:19:15.740
We have the situation where we know we don't know everything.

248
0:19:15.740 --> 0:19:18.600
We know we don't know some things.

249
0:19:18.600 --> 0:19:23.680
We know, well, and then we don't know other things that we don't know we don't know.

250
0:19:23.680 --> 0:19:29.600
Some cases we know we don't know what we don't know.

251
0:19:29.600 --> 0:19:33.680
But in order to function, we have to maintain these mental models.

252
0:19:33.680 --> 0:19:38.000
And those mental models are necessarily a simplification of reality.

253
0:19:38.000 --> 0:19:42.680
And so when something's going wrong, we have to dig into how we think the system works

254
0:19:42.680 --> 0:19:45.600
and we have to kind of go through that.

255
0:19:45.600 --> 0:19:55.880
And the more complexity we throw into automation, the harder that process becomes.

256
0:19:55.880 --> 0:20:01.440
So automation, as I say, is an important tool.

257
0:20:01.440 --> 0:20:06.520
I'm going to talk in a few moments about good automation versus bad automation.

258
0:20:06.520 --> 0:20:11.720
But it's something that we can't rely on to solve the human error problem.

259
0:20:11.720 --> 0:20:16.240
So I mentioned that I talked about good automation versus bad automation.

260
0:20:16.240 --> 0:20:21.140
I think this is really, really, really important here.

261
0:20:21.140 --> 0:20:27.760
So oftentimes what I've seen happen is that you end up with large automated systems, whether

262
0:20:27.760 --> 0:20:35.160
they're something like Ansible or Rex or Kubernetes or whatever.

263
0:20:35.160 --> 0:20:46.920
And oftentimes there isn't a clear understanding of how these things work underneath.

264
0:20:46.920 --> 0:20:52.080
Now if people do understand all of that and they've built in a lot of these things, then

265
0:20:52.080 --> 0:20:54.800
a lot of that's going to be a lot easier.

266
0:20:54.800 --> 0:21:00.520
So good automation is basically going to be a deliberate and engineered process.

267
0:21:00.520 --> 0:21:06.200
Rather than something that's thrown together in the course of the messy world of operations,

268
0:21:06.200 --> 0:21:12.120
it is a deliberate process which is designed around two factors and three factors actually.

269
0:21:12.120 --> 0:21:15.260
The first factor is the system.

270
0:21:15.260 --> 0:21:16.920
The second factor is the people.

271
0:21:16.920 --> 0:21:19.120
And we usually forget that.

272
0:21:19.120 --> 0:21:24.960
And then the last one is that we actually need to be thinking about the human-machine

273
0:21:24.960 --> 0:21:28.200
interaction.

274
0:21:28.200 --> 0:21:34.060
So good automation takes the people into account.

275
0:21:34.060 --> 0:21:41.980
Good automation is something which has built-in decision points where the person can actually

276
0:21:41.980 --> 0:21:47.000
sit there and say, hmm, this isn't going right.

277
0:21:47.000 --> 0:21:50.880
We're not going to proceed.

278
0:21:50.880 --> 0:22:04.200
And good automation is sort of then a well-understood process.

279
0:22:04.200 --> 0:22:12.880
So the other thing that is really important as we look at automation is this issue of

280
0:22:12.880 --> 0:22:14.040
feedback, right?

281
0:22:14.040 --> 0:22:18.360
Because the more we automate, typically the more we insulate the individual from the feedback

282
0:22:18.360 --> 0:22:22.360
of the individual steps that would be right.

283
0:22:22.360 --> 0:22:30.400
So it's really super important to sit down and think about what's the person going to

284
0:22:30.400 --> 0:22:31.400
see?

285
0:22:31.400 --> 0:22:33.680
What's the human going to see?

286
0:22:33.680 --> 0:22:36.400
How's the human going to be able to interpret this?

287
0:22:36.400 --> 0:22:39.200
How much feedback do we want to send?

288
0:22:39.200 --> 0:22:41.440
Do we want to send everything that we got?

289
0:22:41.440 --> 0:22:45.280
Do we want to send some summary of it?

290
0:22:45.280 --> 0:22:49.880
And those are going to be decisions that have to be made deliberately based upon the context

291
0:22:49.880 --> 0:22:56.140
of what we're doing, as well as a clear understanding of what the failure case is of the automation.

292
0:22:56.140 --> 0:22:59.160
And then, of course, people actually need to be trained on what the automation is actually

293
0:22:59.160 --> 0:23:04.160
doing under the hood so that they understand it, rather than just simply saying, oh, push

294
0:23:04.160 --> 0:23:09.840
button, okay, everything is good.

295
0:23:09.840 --> 0:23:18.920
So the way I always look at it is a lot of people think automation, basically a lot of

296
0:23:18.920 --> 0:23:22.640
people think checklists are a step towards automation.

297
0:23:22.640 --> 0:23:27.240
I think that automation should be a step towards a checklist.

298
0:23:27.240 --> 0:23:29.600
Okay?

299
0:23:29.600 --> 0:23:33.320
The relationship should actually be something around on the other side so that you're thinking

300
0:23:33.320 --> 0:23:37.360
about how do I want the human to interact with this?

301
0:23:37.360 --> 0:23:39.440
How do I want the human to perform these?

302
0:23:39.440 --> 0:23:44.600
Where do I want the human to be able to say this isn't going while we are stopping?

303
0:23:44.600 --> 0:23:49.000
And those are the sorts of questions and designs that we have to think about when we're dealing

304
0:23:49.000 --> 0:23:53.680
with especially these sorts of critical systems like the databases where if the database is

305
0:23:53.680 --> 0:24:05.840
down, you know, the business may be down.

306
0:24:05.840 --> 0:24:13.280
Now I want to talk a little bit about why we make mistakes.

307
0:24:13.280 --> 0:24:20.560
Now I mentioned before computers operate in the closed world, right?

308
0:24:20.560 --> 0:24:23.000
They get inputs from us.

309
0:24:23.000 --> 0:24:29.240
In due processing, they give us outputs, right?

310
0:24:29.240 --> 0:24:34.120
We live in an open world.

311
0:24:34.120 --> 0:24:36.800
We experience things, we perceive things.

312
0:24:36.800 --> 0:24:45.440
What we perceive is not a complete model of or it's not even complete aspect of what our

313
0:24:45.440 --> 0:24:46.560
mental models are.

314
0:24:46.560 --> 0:24:51.760
We make inferences based on incomplete data, okay?

315
0:24:51.760 --> 0:24:57.920
And in order to function in this world, we have had to adapt and develop certain kinds

316
0:24:57.920 --> 0:25:00.400
of cognitive biases, okay?

317
0:25:00.400 --> 0:25:05.680
And a lot of times people look at this and they go, oh, it's not good to be biased.

318
0:25:05.680 --> 0:25:07.320
Bias is a bad word.

319
0:25:07.320 --> 0:25:09.240
We don't like biases.

320
0:25:09.240 --> 0:25:13.080
But the fact of the matter is that if you could get rid of all of your cognitive biases,

321
0:25:13.080 --> 0:25:18.280
you would be unable to function, okay?

322
0:25:18.280 --> 0:25:22.360
Continuation bias, of course, is one thing that we tend to be aware of.

323
0:25:22.360 --> 0:25:25.280
But here's another one, continuation bias.

324
0:25:25.280 --> 0:25:31.920
Continuation bias is the tendency to continue to follow a plan that you've put in motion,

325
0:25:31.920 --> 0:25:38.800
even when you're starting to get good indications that that's not a good idea, okay?

326
0:25:38.800 --> 0:25:43.920
If you didn't have continuation bias, you might have to sit down and rethink your plan

327
0:25:43.920 --> 0:25:49.440
continuously over and over and over again, right?

328
0:25:49.440 --> 0:25:51.880
That wouldn't be very helpful.

329
0:25:51.880 --> 0:25:56.360
So continuation bias, just like confirmation bias, actually helps us function in the real

330
0:25:56.360 --> 0:25:58.200
world.

331
0:25:58.200 --> 0:26:02.540
Problem is it can also lead us into situations where we do the wrong thing.

332
0:26:02.540 --> 0:26:09.760
And so understanding these biases, understanding their implications is very clear, is a very

333
0:26:09.760 --> 0:26:17.720
important step to being able to notice when they're causing problems and start to trap

334
0:26:17.720 --> 0:26:18.880
those sorts of problems.

335
0:26:18.880 --> 0:26:25.600
So rather than trying to eliminate our biases, which is, I think, a way in which I see people

336
0:26:25.600 --> 0:26:32.700
typically trying to do this, it's better to think about what kinds of problems the biases

337
0:26:32.700 --> 0:26:38.760
can cause and how we can detect and trap those problems, right?

338
0:26:38.760 --> 0:26:42.520
And there are a large number of these biases, right?

339
0:26:42.520 --> 0:26:44.200
Expectation bias.

340
0:26:44.200 --> 0:26:47.240
Expectation bias is also related to confirmation bias.

341
0:26:47.240 --> 0:26:55.480
It's the tendency to filter out perceptions that don't match your expectations, right?

342
0:26:55.480 --> 0:26:58.320
This happens today in a lot of environments.

343
0:26:58.320 --> 0:27:00.640
It happens in our industry.

344
0:27:00.640 --> 0:27:05.080
It obviously still happens in aviation, fortunately, usually not with serious problems.

345
0:27:05.080 --> 0:27:11.120
The most common problem it causes there is that the plane comes up to the gate, the pilot

346
0:27:11.120 --> 0:27:13.460
says disarm doors and cross-check.

347
0:27:13.460 --> 0:27:17.280
Somebody misses the door that's going to be opened, the other person cross-checks, and

348
0:27:17.280 --> 0:27:21.840
expectation bias kicks in and they don't notice that the door is still armed.

349
0:27:21.840 --> 0:27:24.280
Go to open the door and guess what happens?

350
0:27:24.280 --> 0:27:29.640
Emergency slide deploys doesn't harm anybody on the airplane, but it's going to make a

351
0:27:29.640 --> 0:27:37.640
bunch of people unhappy because the next leg on the airplane's flight is going to get canceled.

352
0:27:37.640 --> 0:27:39.800
That's usually the worst that happens.

353
0:27:39.800 --> 0:27:46.720
But these are important things and we have to recognize that these sorts of biases are

354
0:27:46.720 --> 0:27:55.240
going to happen and that our ability to maintain a situation awareness in the course of these

355
0:27:55.240 --> 0:28:08.200
biases is very much tied to how or where we are of the kinds of problems that they can

356
0:28:08.200 --> 0:28:10.840
cause, right?

357
0:28:10.840 --> 0:28:15.200
Because we form this mental model, we're going to interpret things according to that mental

358
0:28:15.200 --> 0:28:22.000
model, we're going to continue our existing plans and things like that, and when somebody

359
0:28:22.000 --> 0:28:25.760
says hey, wait, maybe this isn't right, then that's suddenly an opportunity to go hey,

360
0:28:25.760 --> 0:28:28.920
my bias is maybe leading me astray.

361
0:28:28.920 --> 0:28:34.280
Let's sit down and figure out what's going on and verify.

362
0:28:34.280 --> 0:28:39.880
Human factors training actually tends to include exercises or training specifically aimed at

363
0:28:39.880 --> 0:28:45.200
doing that.

364
0:28:45.200 --> 0:28:55.720
So second major issue is reversion to prior behavior under stress.

365
0:28:55.720 --> 0:29:02.560
Something that happens to all of us when we're under stress, our focus narrows, right?

366
0:29:02.560 --> 0:29:07.560
We start filtering things out and we start resorting to habit.

367
0:29:07.560 --> 0:29:13.120
What this also means is that in a database team when there's an outage, if we're not

368
0:29:13.120 --> 0:29:18.960
careful we will resort to the things that we're used to doing, even if we have decided

369
0:29:18.960 --> 0:29:22.080
that they're not maybe the best ways forward.

370
0:29:22.080 --> 0:29:28.600
And I've watched cases where incidents happen and if a company has been really trying to

371
0:29:28.600 --> 0:29:33.800
move towards a more collaborative approach to incidents, that suddenly when the incident

372
0:29:33.800 --> 0:29:37.720
happens people are getting stressed out and they're going back to this like hyper-individualistic

373
0:29:37.720 --> 0:29:41.160
cowboy incident response.

374
0:29:41.160 --> 0:29:44.440
And a lot of that is just simply due to stress.

375
0:29:44.440 --> 0:29:48.120
It's a very well-documented part of the stress response.

376
0:29:48.120 --> 0:29:54.360
One thing that we got at adjust with the human factors training was a strong understanding

377
0:29:54.360 --> 0:30:02.120
of that problem as well as good understandings of how to measure the stress so that we could

378
0:30:02.120 --> 0:30:06.760
actually kind of keep an eye on it.

379
0:30:06.760 --> 0:30:13.320
Another major point that causes problems and I've alluded to this before is fatigue.

380
0:30:13.320 --> 0:30:18.440
How often do we see people who have a rough on call night and come back in the next day

381
0:30:18.440 --> 0:30:21.000
and start working on stuff?

382
0:30:21.000 --> 0:30:26.280
How often are we willing to say to that person, no, go home, get some rest, I don't want you

383
0:30:26.280 --> 0:30:34.760
working on this stuff right now?

384
0:30:34.760 --> 0:30:42.080
How often have we seen people who are on call for an extended time period and a rough shift

385
0:30:42.080 --> 0:30:48.720
make mistakes after several days of continuous sleep interruptions?

386
0:30:48.720 --> 0:30:54.140
Do we start to think about the question of maybe when this happens we should be switching

387
0:30:54.140 --> 0:30:57.480
these people out more frequently?

388
0:30:57.480 --> 0:31:04.480
In the airlines before any flight happens the flight crew get together and they check

389
0:31:04.480 --> 0:31:07.480
out how each other are doing, right?

390
0:31:07.480 --> 0:31:12.240
And there is an expectation that there is a standby flight crew so that if you're not

391
0:31:12.240 --> 0:31:19.960
feeling your best you can say, hey, I didn't sleep well last night, I don't want to fly.

392
0:31:19.960 --> 0:31:25.920
And that's another thing which has really helped the increase of the safety, something

393
0:31:25.920 --> 0:31:29.280
we should probably think about doing, you know?

394
0:31:29.280 --> 0:31:36.560
You're getting tired from the on call, time to switch you out.

395
0:31:36.560 --> 0:31:37.560
Do we?

396
0:31:37.560 --> 0:31:41.320
I have never worked anywhere that did.

397
0:31:41.320 --> 0:31:54.960
So a final major point on how and why we make mistakes has to do with a term in human factors

398
0:31:54.960 --> 0:31:56.800
lingo called workload.

399
0:31:56.800 --> 0:32:03.240
Now, I don't like this term in this context because when we say workload in here everybody

400
0:32:03.240 --> 0:32:08.200
is thinking, oh, I have so many things I need to get done this month.

401
0:32:08.200 --> 0:32:13.320
But in the human factors side workload doesn't mean over the next month or over the next

402
0:32:13.320 --> 0:32:16.960
week, although planning that can be helpful.

403
0:32:16.960 --> 0:32:24.440
What it really means is how many tasks are you having to pay attention to right now?

404
0:32:24.440 --> 0:32:28.920
How many people here can actually listen to and understand through two conversations at

405
0:32:28.920 --> 0:32:31.320
the same time?

406
0:32:31.320 --> 0:32:33.320
Nobody?

407
0:32:33.320 --> 0:32:35.160
Maybe possible for some people to train that.

408
0:32:35.160 --> 0:32:41.880
But our brains don't, our brains can't, there are certain kinds of things that our brains

409
0:32:41.880 --> 0:32:46.160
can't parallelize very well.

410
0:32:46.160 --> 0:32:49.920
Understanding where those boundaries are.

411
0:32:49.920 --> 0:32:53.240
Switching and flipping between tasks.

412
0:32:53.240 --> 0:32:57.480
How much can we reduce that workload?

413
0:32:57.480 --> 0:33:01.680
That's actually really important because one of the things I've seen happen is your standard

414
0:33:01.680 --> 0:33:06.800
run book and the way most people write the run books is you have step, explanation, discussion

415
0:33:06.800 --> 0:33:09.880
of output, next step.

416
0:33:09.880 --> 0:33:14.920
What happens at three in the morning if you've never done this particular process is step.

417
0:33:14.920 --> 0:33:18.240
Yes, it did what I expected it to.

418
0:33:18.240 --> 0:33:21.600
Where is the next step?

419
0:33:21.600 --> 0:33:31.120
It becomes really, really, really easy to miss the next step in your checklist or to

420
0:33:31.120 --> 0:33:37.240
miss critical details that are kind of obscured in the fact that now you're having to read

421
0:33:37.240 --> 0:33:42.720
through paragraphs at three in the morning while troubleshooting a broken system.

422
0:33:42.720 --> 0:33:53.760
One of the things that I did while I was at Adjust is I started writing some of our, I

423
0:33:53.760 --> 0:33:56.880
guess I would call them unusual procedure checklists.

424
0:33:56.880 --> 0:34:00.360
A non-normal procedure checklist.

425
0:34:00.360 --> 0:34:04.440
Things that happen when, things that you do when something goes wrong.

426
0:34:04.440 --> 0:34:08.360
Things that you might have to do at three in the morning without doing them for any

427
0:34:08.360 --> 0:34:11.520
of the previous three months.

428
0:34:11.520 --> 0:34:15.160
What I ended up doing in this case and it was actually, this is a good opportunity to

429
0:34:15.160 --> 0:34:28.600
talk about some of the main benefits of this sort of training, is that we talked about,

430
0:34:28.600 --> 0:34:33.440
we talked about basically what we did was we did the following format.

431
0:34:33.440 --> 0:34:35.720
It's a bullet point.

432
0:34:35.720 --> 0:34:41.240
Here's what you can ideally copy and paste into the terminal.

433
0:34:41.240 --> 0:34:50.480
With output, warning signs, all in bullet points and then back, unindented again the

434
0:34:50.480 --> 0:34:51.960
next bullet point.

435
0:34:51.960 --> 0:34:56.680
So they're hierarchical, it's easy to scan, but then your main points are all really,

436
0:34:56.680 --> 0:34:57.760
really, really short.

437
0:34:57.760 --> 0:35:01.480
And then all of the major description that would be in those paragraphs would be moved

438
0:35:01.480 --> 0:35:02.480
into footnotes.

439
0:35:02.480 --> 0:35:04.440
Those would all be hyperlinked.

440
0:35:04.440 --> 0:35:09.360
So you run a test, you run a step.

441
0:35:09.360 --> 0:35:10.360
Something doesn't look quite right.

442
0:35:10.360 --> 0:35:13.960
If you want to see the longer description, you click that hyperlink, you come down to

443
0:35:13.960 --> 0:35:18.240
the footnote, you read the whole thing.

444
0:35:18.240 --> 0:35:21.480
Decide if you want to proceed or not.

445
0:35:21.480 --> 0:35:23.200
And then decide.

446
0:35:23.200 --> 0:35:30.240
And what this allowed us to do was to take like the standard platform team people who

447
0:35:30.240 --> 0:35:38.400
are on call and actually have them do aerospike maintenance at three in the morning on, as

448
0:35:38.400 --> 0:35:44.280
I say, the super critical high speed database system.

449
0:35:44.280 --> 0:35:51.320
And before that, every time there was an aerospike issue, it was an automatic escalation.

450
0:35:51.320 --> 0:35:54.360
And it was an automatic escalation because we didn't trust that they would be able to

451
0:35:54.360 --> 0:35:56.760
do it or make proper decisions around it.

452
0:35:56.760 --> 0:36:01.600
But since we formalized it into checklists and we offered some training on them and we

453
0:36:01.600 --> 0:36:07.120
tried to make sure that people kind of understood the overall considerations of the processes,

454
0:36:07.120 --> 0:36:16.320
then they could do some basic stuff and then call us if there were questions that weren't

455
0:36:16.320 --> 0:36:20.800
obviously answered by the documentation.

456
0:36:20.800 --> 0:36:26.040
Very, very, very good tangible benefit meant that instead of several people waking up in

457
0:36:26.040 --> 0:36:32.520
the middle of the night, they could be done by the on call engineer.

458
0:36:32.520 --> 0:36:38.880
So that's a really good example of the benefits that come out of paying attention to that

459
0:36:38.880 --> 0:36:43.640
workload issue and the sensory overload that happens that's much more serious at three

460
0:36:43.640 --> 0:36:50.000
in the morning than at three in the afternoon.

461
0:36:50.000 --> 0:36:56.560
So this point, it's really important to recognize that at this point, we're no longer really

462
0:36:56.560 --> 0:37:03.120
talking about human error being somebody made a mistake, right?

463
0:37:03.120 --> 0:37:07.800
Instead we're talking about the need to be able to debug the person and why they made

464
0:37:07.800 --> 0:37:12.440
the mistake, right?

465
0:37:12.440 --> 0:37:18.680
And this is something which very often times we don't even try to do in our industry, but

466
0:37:18.680 --> 0:37:20.480
we should.

467
0:37:20.480 --> 0:37:31.160
This requires that we have a really good taxonomy of types of mistakes, right?

468
0:37:31.160 --> 0:37:38.680
That we can say, okay, situation awareness laps because of sensory overload from too

469
0:37:38.680 --> 0:37:42.080
many monitoring alerts going off, right?

470
0:37:42.080 --> 0:37:44.800
Very common one that happens in our industry.

471
0:37:44.800 --> 0:37:51.400
That's also something that's caused airplane issues.

472
0:37:51.400 --> 0:37:58.480
So if we understand that, we know, okay, they've lost their situation awareness.

473
0:37:58.480 --> 0:38:01.420
They couldn't understand where the problem was.

474
0:38:01.420 --> 0:38:05.640
This happened because they had too many alerts they were trying to focus on.

475
0:38:05.640 --> 0:38:08.640
Now the question is, are we actually throwing too many alerts?

476
0:38:08.640 --> 0:38:11.720
Do we need to think about maybe prioritizing things differently?

477
0:38:11.720 --> 0:38:15.640
We need to rethink how we do alerting, right?

478
0:38:15.640 --> 0:38:20.720
And suddenly we have a dimension for looking at these problems that we currently don't

479
0:38:20.720 --> 0:38:21.720
have.

480
0:38:21.720 --> 0:38:27.200
Instead, currently what happens most places I've worked is, okay, something went wrong.

481
0:38:27.200 --> 0:38:31.720
We didn't spot it, therefore let's add another alert over this, right?

482
0:38:31.720 --> 0:38:40.320
But when I was at Delivery Hero, we actually had a major incident where, you know, somebody,

483
0:38:40.320 --> 0:38:45.160
again, missed a problem relating to a database, relating to a Postgres instance, I believe,

484
0:38:45.160 --> 0:38:50.240
if I remember right, despite the fact that it was well-alerted.

485
0:38:50.240 --> 0:38:54.720
Okay, I was talking to somebody afterwards and he says, do you know what the false positivity

486
0:38:54.720 --> 0:38:55.720
rate of our alerts are?

487
0:38:55.720 --> 0:38:59.240
And I'm like, no, it's like 99.8%.

488
0:38:59.240 --> 0:39:05.160
How do you expect somebody to spot the problem when almost all the time our alerts don't

489
0:39:05.160 --> 0:39:07.320
mean there's a real problem?

490
0:39:07.320 --> 0:39:09.320
Okay.

491
0:39:09.320 --> 0:39:12.920
Now what he meant by false positivity isn't what I would mean by it.

492
0:39:12.920 --> 0:39:17.000
I mean, there were problems that the alerts were alerting about, but they weren't like

493
0:39:17.000 --> 0:39:21.880
customer-facing problems, right?

494
0:39:21.880 --> 0:39:32.040
So the second thing is we need a really good understanding of our cognitive biases and

495
0:39:32.040 --> 0:39:36.400
the functions that they provide to us and also the problems that they can lead us into,

496
0:39:36.400 --> 0:39:37.400
right?

497
0:39:37.400 --> 0:39:45.240
So one of the good examples is, hey, look, you know, I know you're about to do this.

498
0:39:45.240 --> 0:39:47.000
I'm not sure that's what the problem is.

499
0:39:47.000 --> 0:39:48.480
Can we think about this first?

500
0:39:48.480 --> 0:39:49.480
Right?

501
0:39:49.480 --> 0:39:52.480
And as soon as somebody says that, that means that they're saying my mental model is not

502
0:39:52.480 --> 0:39:53.840
the same as your mental model.

503
0:39:53.840 --> 0:39:54.840
One of us is wrong.

504
0:39:54.840 --> 0:39:59.040
We should probably figure that out before we proceed.

505
0:39:59.040 --> 0:40:03.560
Figuring out how to do that's really, really important, especially when we talk about social

506
0:40:03.560 --> 0:40:05.120
factors involved, right?

507
0:40:05.120 --> 0:40:08.360
It's one thing to do that with your peer when you're on an incident call and there are two

508
0:40:08.360 --> 0:40:09.960
of you there.

509
0:40:09.960 --> 0:40:14.320
Something very different to do when the person typing the words is very senior and you're

510
0:40:14.320 --> 0:40:20.840
very junior, and there's somebody C-level popping into the call to ask for an update.

511
0:40:20.840 --> 0:40:25.280
I've been there, I've done that, and yes, no, I have not raised the issue and I should

512
0:40:25.280 --> 0:40:30.400
have, right?

513
0:40:30.400 --> 0:40:37.080
You know, figuring out how to make these sorts of interventions and how to understand the

514
0:40:37.080 --> 0:40:40.680
intervention and how to respond to it, those are things that we actually need training

515
0:40:40.680 --> 0:40:43.040
on, right?

516
0:40:43.040 --> 0:40:45.240
We also need training on the social factors.

517
0:40:45.240 --> 0:40:48.280
We need to understand how power distance affects these.

518
0:40:48.280 --> 0:40:51.180
What happens when there's, you know, the C-level person in the call?

519
0:40:51.180 --> 0:40:53.520
How does that change your social interactions?

520
0:40:53.520 --> 0:40:58.880
How does that change your interactions in terms of debugging, right?

521
0:40:58.880 --> 0:41:05.520
Those are important things and that's one thing that we can get some really big improvements

522
0:41:05.520 --> 0:41:09.240
on relating to this.

523
0:41:09.240 --> 0:41:15.720
Finally, it's really important for us to be able to get to the point where we can contextualize

524
0:41:15.720 --> 0:41:16.720
the person.

525
0:41:16.720 --> 0:41:24.920
In other words, since we operate as humans in a relatively heuristic manner, right, we

526
0:41:24.920 --> 0:41:31.720
need to understand what the situation the human was in when the mistake happened.

527
0:41:31.720 --> 0:41:39.440
That's another thing that these sorts of trainings can help with.

528
0:41:39.440 --> 0:41:44.400
I've talked a little bit about social factors here.

529
0:41:44.400 --> 0:41:49.660
Power distance is what it sounds like, you know, how big the difference is between the

530
0:41:49.660 --> 0:41:53.960
most powerful person in the interaction and the least powerful person in the interaction

531
0:41:53.960 --> 0:41:58.760
where we want it to be kind of, you know, not quite equal but much closer instead of

532
0:41:58.760 --> 0:42:03.000
like this, maybe more like this.

533
0:42:03.000 --> 0:42:09.960
And, you know, figuring out how to structure things so that power distance doesn't cause

534
0:42:09.960 --> 0:42:10.960
a problem.

535
0:42:10.960 --> 0:42:17.760
That also means giving people good training on how to intervene when they see somebody

536
0:42:17.760 --> 0:42:21.640
much more senior about to make a mistake.

537
0:42:21.640 --> 0:42:26.920
You want to intervene in a way which is not threatening and in the event where there's

538
0:42:26.920 --> 0:42:32.480
somebody even higher in the call isn't going to be perceived as humiliating.

539
0:42:32.480 --> 0:42:38.920
Having good training on this and how to communicate in those cases is really, really important.

540
0:42:38.920 --> 0:42:44.800
And a lot of this ends up playing out into trying to create a work relationship between

541
0:42:44.800 --> 0:42:51.240
the people on the team which is very heavily mutually supportive.

542
0:42:51.240 --> 0:42:59.280
And also kind of helps prevent or check some traps the kinds of mistakes that each of

543
0:42:59.280 --> 0:43:07.280
us can make.

544
0:43:07.280 --> 0:43:20.480
So let's talk a little bit about the ideal role of humans in database operations.

545
0:43:20.480 --> 0:43:26.480
Now we kind of need to understand this well.

546
0:43:26.480 --> 0:43:28.480
Okay.

547
0:43:28.480 --> 0:43:30.480
10?

548
0:43:30.480 --> 0:43:31.480
Okay.

549
0:43:31.480 --> 0:43:32.480
Who's checking?

550
0:43:32.480 --> 0:43:33.480
Five?

551
0:43:33.480 --> 0:43:34.480
Okay, perfect.

552
0:43:34.480 --> 0:43:39.160
We kind of need to understand this.

553
0:43:39.160 --> 0:43:40.720
Humans need to be in control.

554
0:43:40.720 --> 0:43:41.880
We need to be the decision makers.

555
0:43:41.880 --> 0:43:46.840
We need to be the people who can say, this is what I think is going on.

556
0:43:46.840 --> 0:43:49.480
Let's go ahead and try this process.

557
0:43:49.480 --> 0:43:52.840
And halfway through that process go, this is not going well.

558
0:43:52.840 --> 0:43:57.720
Let's back off, rethink and make another decision.

559
0:43:57.720 --> 0:44:06.160
Partly because we're also operating heuristically, we can do things that computers can't.

560
0:44:06.160 --> 0:44:08.640
We need to maintain really good situation awareness.

561
0:44:08.640 --> 0:44:12.140
This means we need to have transparency in our complex automation.

562
0:44:12.140 --> 0:44:18.480
We need the automation to be built around helping us, not replacing us.

563
0:44:18.480 --> 0:44:26.080
And to do this, we need to be well rested.

564
0:44:26.080 --> 0:44:31.520
We need to be a clear peak capability, ideally when we're in the middle of an incident.

565
0:44:31.520 --> 0:44:37.920
Now we may not be able to completely manage that last part, but if we can take steps towards

566
0:44:37.920 --> 0:44:43.080
it and we can try to improve, we can do better.

567
0:44:43.080 --> 0:44:45.080
Right?

568
0:44:45.080 --> 0:44:51.880
So a lot of this training is, at least what I've gotten out of it, is really important.

569
0:44:51.880 --> 0:44:57.340
What I think is really important about this, I'll just talk quickly about how to go about

570
0:44:57.340 --> 0:45:04.480
doing it, is that if we can get the organizational leverage behind the training, then we can

571
0:45:04.480 --> 0:45:08.320
actually turn the promise of the training into the reality.

572
0:45:08.320 --> 0:45:11.720
Sometimes you can't just like teach people something and then have the management abandon

573
0:45:11.720 --> 0:45:15.240
them and that doesn't work.

574
0:45:15.240 --> 0:45:30.840
So as an industry, we treat human error the way pilot error was treated in the 1950s.

575
0:45:30.840 --> 0:45:36.440
We have a whole lot to learn from aviation.

576
0:45:36.440 --> 0:45:42.160
Those lessons are already being played out in medicine and many other fields today.

577
0:45:42.160 --> 0:45:46.200
We need to do what we can to learn from it also.

578
0:45:46.200 --> 0:45:52.080
And it's really important to recognize that we can get really good improvements in reliability,

579
0:45:52.080 --> 0:45:58.560
efficiency, speed of development, all these sorts of things, if we can better work with

580
0:45:58.560 --> 0:46:00.440
the human side of things.

581
0:46:00.440 --> 0:46:01.440
Right?

582
0:46:01.440 --> 0:46:05.920
And I'm not talking about like management, human managers, you know, rating performance.

583
0:46:05.920 --> 0:46:10.520
I'm talking about like people on the team understanding performance for themselves and

584
0:46:10.520 --> 0:46:13.200
others.

585
0:46:13.200 --> 0:46:17.880
I just want to say that the three pieces of this are trying to get, you know, trainers

586
0:46:17.880 --> 0:46:20.080
in who have experience.

587
0:46:20.080 --> 0:46:28.920
Also an organizational commitment to make it happen and then internally building your

588
0:46:28.920 --> 0:46:34.480
own programs and your own recurring trainings and your own training for new people so that

589
0:46:34.480 --> 0:46:38.360
internally you have a big culture around it and you have experts who can think about it

590
0:46:38.360 --> 0:46:40.800
when it comes to being post-mortem.

591
0:46:40.800 --> 0:46:41.800
So that's what I have.

592
0:46:41.800 --> 0:46:42.800
Any questions?

593
0:46:42.800 --> 0:46:43.800
All right.

594
0:46:43.800 --> 0:46:44.800
Thank you.

595
0:46:44.800 --> 0:46:49.800
That was an amazing talk.

596
0:46:49.800 --> 0:47:07.840
Do you have any recommendations for further reading if you can't bring in experts?

597
0:47:07.840 --> 0:47:11.560
So yeah.

598
0:47:11.560 --> 0:47:18.600
So like this is a field which in aviation has a massive textbook industry.

599
0:47:18.600 --> 0:47:23.000
I think probably the best, sort of the most accessible book I would recommend starting

600
0:47:23.000 --> 0:47:29.360
with is the more recent versions of David Beatty's Human Factors and Aircraft Accidents.

601
0:47:29.360 --> 0:47:33.520
I think the most recent version is called The Naked Pilot of Human Factors and Aircraft

602
0:47:33.520 --> 0:47:34.520
Accidents.

603
0:47:34.520 --> 0:47:39.840
It's just referring to, you know, exposing the inner workings of the human piece of

604
0:47:39.840 --> 0:47:45.080
the aircraft piece there.

605
0:47:45.080 --> 0:47:51.240
But again, if you're a nervous flier, probably look for a crew resource management textbook

606
0:47:51.240 --> 0:47:59.320
instead because it may be less nerve-wracking, it may be less intimidating, but it will have

607
0:47:59.320 --> 0:48:09.120
information there too.

608
0:48:09.120 --> 0:48:13.920
Do you have any recommendations for testing or drilling your processes like those checklists?

609
0:48:13.920 --> 0:48:16.640
Yes, I do.

610
0:48:16.640 --> 0:48:22.800
One thing that I think we really should figure out how to do as an industry, and I completely

611
0:48:22.800 --> 0:48:28.840
believe in this, obviously, like the Chaos Monkey idea and Netflix could be exploited

612
0:48:28.840 --> 0:48:32.920
to do this if you can also build war games around it.

613
0:48:32.920 --> 0:48:37.800
But the thing is, it's really important to have drills, which means oftentimes you've

614
0:48:37.800 --> 0:48:44.440
actually got to probably simulate or create some sort of a potential incident that you

615
0:48:44.440 --> 0:48:46.800
have to come together and resolve.

616
0:48:46.800 --> 0:48:52.440
Now, ideally, you need to figure out how to do this without threatening your customer-oriented

617
0:48:52.440 --> 0:48:53.440
services.

618
0:48:53.440 --> 0:48:58.440
In some cases, maybe the cloud's a really good opportunity for that.

619
0:48:58.440 --> 0:49:04.680
But having those sorts of drills, maybe once a quarter or twice a year or something, can

620
0:49:04.680 --> 0:49:11.440
really give you an opportunity to spot problems, figure out improvements, and actually go figure

621
0:49:11.440 --> 0:49:17.480
out what to do about those.

622
0:49:17.480 --> 0:49:22.240
Just kind of building on that last point is, how do you justify the expense in time or

623
0:49:22.240 --> 0:49:23.240
money?

624
0:49:23.240 --> 0:49:26.680
And given that if this is successful, then nothing goes wrong.

625
0:49:26.680 --> 0:49:31.400
So it can sometimes be the outcome of success is that you're spending a lot of effort on

626
0:49:31.400 --> 0:49:32.400
apparently doing nothing.

627
0:49:32.400 --> 0:49:35.720
I don't believe that, but that's a reasonable thing that gets asked.

628
0:49:35.720 --> 0:49:41.800
How do you go about justifying the time or the money on this after they're successful?

629
0:49:41.800 --> 0:49:49.160
So what I've usually done in the past is I make my points about, yes, we're going to

630
0:49:49.160 --> 0:49:53.120
improve our incident response.

631
0:49:53.120 --> 0:49:54.440
This will reduce our time of recovery.

632
0:49:54.440 --> 0:49:56.920
It'll improve our reliability, et cetera.

633
0:49:56.920 --> 0:50:02.240
It'll maybe even improve our throughput organizationally.

634
0:50:02.240 --> 0:50:04.840
But then usually people don't listen.

635
0:50:04.840 --> 0:50:07.040
And then usually there are more incidents.

636
0:50:07.040 --> 0:50:12.320
And then you can come in and say, you know, these are specific problems that we had here

637
0:50:12.320 --> 0:50:15.480
where this training would help.

638
0:50:15.480 --> 0:50:20.440
And I usually find that after two or three of those, people start listening and go, oh,

639
0:50:20.440 --> 0:50:23.440
really, maybe there is something to this.

640
0:50:23.440 --> 0:50:32.800
Thank you.

