WEBVTT

00:00.000 --> 00:07.320
So, hello everyone.

00:07.320 --> 00:09.760
Thanks for your patience.

00:09.760 --> 00:14.640
We're going to start with a slight delay, but I think it will be all right in the end.

00:14.640 --> 00:21.680
Can I please welcome to the stage Andreas Scharbaum, who's going to talk about tour

00:21.680 --> 00:25.600
the data types.

00:25.600 --> 00:30.600
Okay, thank you.

00:30.600 --> 00:31.600
Good morning.

00:31.600 --> 00:33.600
I hope you had a good time yesterday in Borsets.

00:33.600 --> 00:36.600
How was the restaurant?

00:36.600 --> 00:39.600
Well, we got some food.

00:39.600 --> 00:44.600
This is not switching.

00:44.600 --> 00:49.600
So, I have the pleasure to talk to you today about data types in Postgres.

00:49.600 --> 00:52.600
I actually have two talks about this.

00:52.600 --> 00:57.600
This is a, I would say, basic talk about regular data types we have.

00:57.600 --> 01:01.600
I also have an advanced data types talk, but let's focus on this one.

01:01.600 --> 01:03.600
So, my name is Andreas.

01:03.600 --> 01:07.600
I work with Postgres Science about 97, 98.

01:07.600 --> 01:10.600
I'm one of the founding members of Postgres Europe.

01:10.600 --> 01:16.600
V1 conferences like Post and PG Day on Friday here, Postgres Europe, which is in Prague

01:16.600 --> 01:22.600
this year in December, Postgres comes Germany, Belgium, Postgres comes Malta in April, so

01:22.600 --> 01:27.600
if you're interested in a couple more.

01:27.600 --> 01:31.600
Currently, I work for a company called Adjust in Berlin.

01:31.600 --> 01:36.600
We have about a thousand Postgres databases running everything from a couple gigabyte up

01:36.600 --> 01:41.600
to like 30 terabytes in Postgres.

01:41.600 --> 01:46.600
Okay, if you already know everything about data types in Postgres, if you're regularly

01:46.600 --> 01:50.600
reading hackers' mailing list, this is maybe not the right talk for you.

01:50.600 --> 01:58.600
I don't want to occupy your time, but otherwise, have a seat and we start.

01:58.600 --> 02:04.600
So, quick poll, how many data types do we have in Postgres?

02:04.600 --> 02:06.600
Any idea?

02:06.600 --> 02:12.600
40?

02:12.600 --> 02:13.600
40?

02:13.600 --> 02:16.600
Anyone else?

02:16.600 --> 02:23.600
I see you're still sleeping.

02:23.600 --> 02:29.600
And we talk about each and everyone today.

02:29.600 --> 02:33.600
So, first of all, we have to exclude a number of data types because every time you create

02:33.600 --> 02:39.600
a table in Postgres, you also create an according data type which matches the structure of the

02:39.600 --> 02:40.600
table.

02:40.600 --> 02:42.600
So, we don't want this.

02:42.600 --> 02:46.600
If we look at regular data types, we have about 80 depending on which Postgres version

02:46.600 --> 02:47.600
you are.

02:47.600 --> 02:52.600
It's still more than you expected.

02:52.600 --> 02:56.600
There's things like Boolean, you know this one, or text, or time stamps.

02:56.600 --> 03:01.600
There's also things like trigger and void and cardinal number, which you never heard

03:01.600 --> 03:04.600
about and you never need.

03:04.600 --> 03:10.600
And we focus on mostly the ones I marked highlighted here.

03:10.600 --> 03:16.600
At any time, you can also go and check the Postgres documentation on Postgres.org.

03:16.600 --> 03:20.600
And it has a very long list of all the data types here in Postgres.

03:20.600 --> 03:30.600
The basic ones, the advanced ones, all the ones I listed in the slides before, it's all there.

03:30.600 --> 03:31.600
Okay.

03:31.600 --> 03:36.600
How many different data types are you using in your application?

03:36.600 --> 03:37.600
Sweet.

03:37.600 --> 03:38.600
Which ones?

03:38.600 --> 03:46.600
What about numbers?

03:46.600 --> 03:50.600
So, every single is workshop.

03:50.600 --> 03:51.600
Okay.

03:51.600 --> 03:52.600
Anyone else?

03:52.600 --> 04:00.600
I'm a doctor.

04:00.600 --> 04:08.600
I mean, yeah, it fits you.

04:08.600 --> 04:09.600
Okay.

04:09.600 --> 04:14.600
No, that's too much.

04:14.600 --> 04:16.600
Better?

04:16.600 --> 04:17.600
Good.

04:17.600 --> 04:24.600
We are going over this basic type, so numeric, so when you come back, you can rewrite your

04:24.600 --> 04:27.600
application and finally you start using numbers.

04:27.600 --> 04:35.600
Text types, maybe XML if anyone of you is still using it, JSON, Booleans, and a couple

04:35.600 --> 04:36.600
more.

04:36.600 --> 04:39.600
So, text types.

04:39.600 --> 04:43.600
We basically have one text type in Postgres under the hood.

04:43.600 --> 04:44.600
It's all the same.

04:44.600 --> 04:50.280
So, we have watcher and char and text.

04:50.280 --> 04:52.960
Text types in Postgres by default are case sensitive.

04:52.960 --> 04:57.200
So, if you want to compare two texts, it's always case sensitive.

04:57.200 --> 05:02.680
If you want to compare it case insensitive, you have to use something like lower or higher

05:02.680 --> 05:07.440
to make a string uppercase or lowercase.

05:07.440 --> 05:11.840
A string can Postgres and hold up to a gigabyte of text, roughly.

05:11.840 --> 05:13.600
It cannot hold binary data.

05:13.600 --> 05:19.320
So, if you want to store images in text, that's not going to work.

05:19.320 --> 05:26.360
If you specify a length, like this larger n here, this is where Postgres stores up to

05:26.360 --> 05:37.120
these number of bytes or characters in a text data type, excluding white spaces at the end.

05:37.120 --> 05:40.760
A char by default is only one byte.

05:40.760 --> 05:43.040
That's most likely not what you want.

05:43.040 --> 05:47.040
So, for char, you always want to specify a length.

05:47.040 --> 05:51.800
And then, of course, if you say text, it's just the text you cannot specify your length

05:51.800 --> 05:52.800
here.

05:52.800 --> 05:53.800
Hi, everyone.

05:53.800 --> 06:00.120
How does char and watcher differentiate?

06:00.120 --> 06:03.240
Mostly how they handle white spaces.

06:03.240 --> 06:13.440
So, we have a watcher 1 and 5 here and 10 here, and I cast five white spaces to both

06:13.440 --> 06:17.960
watcher 1, 5 and 10 and char 1, 5 and 10.

06:17.960 --> 06:22.360
And as you can see, we get different lengths here.

06:22.360 --> 06:29.560
So, our five white spaces here, if we cast them to watcher 10, we only get five at the

06:29.560 --> 06:33.480
end because we only have five white spaces in it.

06:33.480 --> 06:41.080
What char is doing, char will fill up the entire string to the lengths we specified.

06:41.080 --> 06:48.520
If you say char 10, we get a string back with 10 characters in it.

06:48.520 --> 06:51.960
That's mostly the difference for char and watcher.

06:51.960 --> 06:56.600
How it's saying that if you specify your lengths in char, it will actually give you that many

06:56.600 --> 06:59.200
bytes back, including white spaces.

06:59.200 --> 07:04.080
If you use a watcher, it will only give you the string back, excluding white spaces at

07:04.080 --> 07:05.080
the end.

07:05.080 --> 07:12.160
So, it will cut off white spaces.

07:12.160 --> 07:15.040
We have a sink in process which is called a page.

07:15.040 --> 07:18.600
By default, it's 8 kilobyte.

07:18.600 --> 07:20.600
That's where we store all the data in it.

07:20.600 --> 07:22.080
So, we have one page header.

07:22.080 --> 07:29.880
Then we store rows in it, and a row can be anything from just one column up to 1,000,

07:29.880 --> 07:34.560
1,500 columns.

07:34.560 --> 07:36.760
As I said, by default, it's 8 kilobyte.

07:36.760 --> 07:40.560
You can increase it, but almost no one is doing this.

07:40.560 --> 07:43.160
How does Postgres store a text?

07:43.160 --> 07:45.520
A text can be up to one gigabyte.

07:45.520 --> 07:50.320
So, we cannot store one gigabyte in 8 kilobyte.

07:50.320 --> 07:54.040
I mean, you can if you compress, but usually it doesn't work.

07:54.040 --> 08:00.080
Any data type in Postgres which has a variable length is only a pointer into what we call

08:00.080 --> 08:03.400
a toast table.

08:03.400 --> 08:04.920
Sorry.

08:04.920 --> 08:13.480
So, in our regular table, we have a 4-byte pointer, which is a pointer into the toast

08:13.480 --> 08:14.480
table.

08:14.480 --> 08:19.520
And in the toast table, we have as many rows as we need just towards this gigabyte of text.

08:19.520 --> 08:25.920
So, it's 4 bytes here, and as many rows as we need here.

08:25.920 --> 08:27.720
Which brings me to my one question.

08:27.720 --> 08:33.240
Why are so many people using char 255?

08:33.240 --> 08:39.360
Just two weeks ago, I've seen a customer who does this all over the place, street names,

08:39.360 --> 08:43.440
customer names, everything is char 255.

08:43.440 --> 08:45.240
Does it make any sense?

08:45.240 --> 08:51.160
Well, there are certain databases where this might make sense, but it doesn't make any

08:51.160 --> 08:54.040
sense in Postgres.

08:54.040 --> 09:00.800
When Postgres doesn't make a difference if you use 10 bytes, 200 bytes, 1 kilobyte, you

09:00.800 --> 09:11.520
always end up, almost always end up with a text pointer into your toast table.

09:11.520 --> 09:17.240
There are other databases that make sense, like if you look at one of the competition

09:17.240 --> 09:27.520
in the market, they can only have 255 bytes in an index, or just 255 characters if you

09:27.520 --> 09:31.320
use UTF-8 up to 700 something bytes.

09:31.320 --> 09:33.440
That's what they can use in the index zone.

09:33.440 --> 09:37.120
That might make sense to use 255 as a char.

09:37.120 --> 09:44.120
But every time you see this construct in Postgres where someone says, char 255, go and question

09:44.120 --> 09:46.360
why is the reasoning behind this?

09:46.360 --> 09:48.560
Why did someone say 255?

09:48.560 --> 09:53.000
Not 200, not 1 kilobyte.

09:53.000 --> 09:57.960
Technically it doesn't make sense.

09:57.960 --> 09:58.960
Numeric ties.

09:58.960 --> 10:02.880
Pay attention, please.

10:02.880 --> 10:04.860
So we have integers.

10:04.860 --> 10:06.520
We have floating point numbers.

10:06.520 --> 10:11.480
We have numeric, and we have sequences.

10:11.480 --> 10:15.520
Integers, we have small int storing 2 bytes.

10:15.520 --> 10:19.720
We have integers, regular integers by default, it's 4 bytes.

10:19.720 --> 10:23.440
So anything from minus 2 billion to plus 2 billion.

10:23.440 --> 10:26.240
And then we have big int using 8 bytes.

10:26.240 --> 10:28.720
This is 9.

10:28.720 --> 10:32.080
No idea how much this is.

10:32.080 --> 10:37.840
You might get some small problems if you store something, okay, small int integer, small

10:37.840 --> 10:44.320
int integer, because the compiler will actually go and say, okay, we need to have some space

10:44.320 --> 10:49.800
because we need to align the next integer on a 32-bit boundary or 64-bit boundary depending

10:49.800 --> 10:51.520
on your operating system.

10:51.520 --> 10:57.680
So what you really want to do is have all of your big data types in first in your table.

10:57.680 --> 11:02.840
There was things which is like text and big int, and then start with your regular 4-byte

11:02.840 --> 11:08.600
types and then 2-byte types at the end so you can compress the table a little bit more.

11:08.600 --> 11:15.680
It doesn't really bring you much if you only have a small table, a couple million rows.

11:15.680 --> 11:19.680
But if you're talking about billion rows, it's actually quite a big saving you get if

11:19.680 --> 11:27.880
you can compress and save like 2 bytes, 4 bytes per row.

11:27.880 --> 11:35.280
Then we have floating point numbers as wheel and as double precision with 4 and 8 bytes.

11:35.280 --> 11:40.120
Keep in mind a floating point number, even though post was my choice, accurate number

11:40.120 --> 11:43.400
you have, is always rounded internally.

11:43.400 --> 11:45.880
It's always a base and an exponent.

11:45.880 --> 11:50.520
I want to give you an example here.

11:50.520 --> 11:57.960
We have 6 digits here, 100,001, and you see if I return this, Postgres still shows me

11:57.960 --> 12:03.000
the 100,001, even though internally it's already rounded.

12:03.000 --> 12:08.240
If I expand this to 7 digits, you see I get a rounded number here.

12:08.240 --> 12:13.560
The 4 bytes can no longer store the precision required to store this 1 at the end.

12:13.560 --> 12:18.520
So you only get 1 million here.

12:18.520 --> 12:27.600
The same is true if I have my 100,001.5 as a 4 byte floating point number, Postgres starts

12:27.600 --> 12:30.680
rounding it internally.

12:30.680 --> 12:36.320
So if you want to store anything like money or data where you really need precision, please

12:36.320 --> 12:42.320
do not use floating point numbers.

12:42.320 --> 12:48.440
But same example for floating point with double positions of 8 bytes.

12:48.440 --> 12:54.520
If I have 15 digits here, it still looks okay, but once you start expanding the number a

12:54.520 --> 13:03.440
little bit or add DCMA digits here, you see it starts rounding the number.

13:03.440 --> 13:09.080
So if you really want to store floating point numbers, but you cannot use integers, what

13:09.080 --> 13:13.880
else can we use?

13:13.880 --> 13:20.000
Ah, this one comes later.

13:20.000 --> 13:22.600
We can use numeric.

13:22.600 --> 13:30.560
There's data type code numeric, which stores up to 1,000 digits of precision in Postgres

13:30.560 --> 13:33.320
square.

13:33.320 --> 13:38.080
Basically any number you need to store, you can store in numeric.

13:38.080 --> 13:40.960
Keep in mind there's no real headdress support.

13:40.960 --> 13:47.400
If you add two integers, in the end, the CPU will load one integer into one register and

13:47.400 --> 13:51.840
the other integer into another register and just use one operation internally.

13:51.840 --> 13:52.840
Everything is fast.

13:52.840 --> 13:54.560
That's not how it works with numeric.

13:54.560 --> 13:57.240
So it will be a little bit slower.

13:57.240 --> 14:05.120
Not much, but if you have to use it all the time, you might see it.

14:05.120 --> 14:10.120
There's also a type called money in Postgres.

14:10.120 --> 14:13.120
Don't use it.

14:13.120 --> 14:15.520
Never.

14:15.520 --> 14:21.120
Internally it's a big int, so we have the same position here, eight bytes.

14:21.120 --> 14:25.360
However, you only have one currency.

14:25.360 --> 14:31.960
Whatever you assign as LC monetary in your environment, this is the currency Postgres

14:31.960 --> 14:35.440
is using to show you this number.

14:35.440 --> 14:39.320
You cannot say I want to store two currencies in my database.

14:39.320 --> 14:41.680
You cannot store like an exchange rate.

14:41.680 --> 14:45.600
All of this is not working.

14:45.600 --> 14:51.040
Money was deprecated I think two times, three times, something like this.

14:51.040 --> 14:54.480
There's always one user who comes back, oh, I really need it.

14:54.480 --> 14:58.720
It's still around, but please don't use it.

14:58.720 --> 15:08.280
Is anyone from India here?

15:08.280 --> 15:13.480
Ever met him?

15:13.480 --> 15:15.480
You know the name?

15:15.480 --> 15:25.320
Would be surprised if you know the name.

15:25.320 --> 15:26.320
Anyone knows this game?

15:26.320 --> 15:30.160
Okay.

15:30.160 --> 15:36.720
How many, the story is that he did something for his king and the king asked, okay, what

15:36.720 --> 15:37.720
can I give you in return?

15:37.720 --> 15:41.160
And he said, okay, give me some waste wings.

15:41.160 --> 15:45.000
One on the first chest feed and a doubly number.

15:45.000 --> 15:48.520
How many waste wings are we talking about?

15:48.520 --> 15:54.280
Yeah, it's hidden.

15:54.280 --> 15:59.840
So we can actually use numeric for this.

15:59.840 --> 16:04.120
So it's two to the power of 64 minus one.

16:04.120 --> 16:05.120
So we have 64 fields.

16:05.120 --> 16:07.800
We start with one, so it's minus one.

16:07.800 --> 16:12.760
If we use floating point for this double position here, you see we don't get an exact number.

16:12.760 --> 16:15.720
It's way too big for floating point.

16:15.720 --> 16:19.120
If we use numeric, it's just 20 digits.

16:19.120 --> 16:21.600
We have 980 left.

16:21.600 --> 16:25.720
We can store much more in numeric than just this number.

16:25.720 --> 16:35.840
I did a mess at some point, and it's like 1,000 times the speed we are currently using

16:35.840 --> 16:40.160
or producing on Earth per year.

16:40.160 --> 16:48.600
So 1,000 years of waste production on Earth will happen to solve this problem.

16:48.600 --> 16:54.240
Okay, we also have sequences in Postgres.

16:54.240 --> 17:00.800
Internally these are just integers which are used as a default type in a table.

17:00.800 --> 17:06.680
We have small serial which gives you two bytes, 32K plus minus.

17:06.680 --> 17:13.280
Then we have regular serial data type which is four bytes from zero to or from one to

17:13.280 --> 17:20.240
two billion, and then we have big end for everything which needs larger numbers.

17:20.240 --> 17:24.760
If you create a data type or a table with a data type serial, what Postgres will do

17:24.760 --> 17:30.120
internally, it will create this data type in a table.

17:30.120 --> 17:34.840
If you look into the table, it's actually an integer in four and eight.

17:34.840 --> 17:40.120
It will create a sequence for you, and it will make this sequence a default value for

17:40.120 --> 17:41.880
this color.

17:41.880 --> 17:47.400
So every time you insert something into this table and you don't specify this color, Postgres

17:47.400 --> 17:50.240
will use the next value from a sequence.

17:50.240 --> 17:54.480
If you specify something for this color, it will not use the sequence.

17:54.480 --> 17:59.200
So if this is your primary key and you're mixing values you are inserting and values

17:59.200 --> 18:02.480
from a sequence, at some point you will have collisions.

18:02.480 --> 18:06.200
Please don't do this.

18:06.200 --> 18:09.400
Also sequences are not transactional.

18:09.400 --> 18:14.000
If you roll back a transaction, a sequence will not roll back.

18:14.000 --> 18:17.600
Sequences just mean to provide you a unique number.

18:17.600 --> 18:23.640
That's all.

18:23.640 --> 18:30.440
So we can ask the current value of a sequence, select current or my sequence name here.

18:30.440 --> 18:36.120
This will only work if you already used the sequence in a current session, like you did

18:36.120 --> 18:38.040
an insert into a table.

18:38.040 --> 18:44.000
This will tell you what is the last value this current session I have inserted into

18:44.000 --> 18:45.000
this table.

18:45.000 --> 18:50.760
So if you have five different sessions inserting data, it will always show you what you, the

18:50.760 --> 18:53.680
current session inserted.

18:53.680 --> 18:58.040
You can also ask for the next value by using next wall.

18:58.040 --> 19:02.400
Keep in mind it will not roll back if you roll back the transaction.

19:02.400 --> 19:08.480
It will only ever move forward.

19:08.480 --> 19:13.360
You can also set a sequence by just saying set wall and then specify your key, what is

19:13.360 --> 19:16.760
my new value I want to have.

19:16.760 --> 19:21.640
If you do this on a table, where you use the sequence on a table as primary key and you

19:21.640 --> 19:27.320
reset it to a previous value, you may run into collisions again because it will reuse

19:27.320 --> 19:32.600
the same values again.

19:32.600 --> 19:35.280
Okay.

19:35.280 --> 19:40.600
A sequence internally in Postgres is just another object, another table.

19:40.600 --> 19:44.320
So what you can say, select star for my sequence.

19:44.320 --> 19:47.880
It will show you all the data about a sequence.

19:47.880 --> 19:49.720
So what's the sequence name?

19:49.720 --> 19:51.880
What's the last value we used here?

19:51.880 --> 19:56.440
What's the minimum maximum value with the sequence cycle around so when it comes to

19:56.440 --> 19:58.200
an end, will it start again?

19:58.200 --> 20:00.240
Will it wrap over or not?

20:00.240 --> 20:05.240
So by default it will not wrap over because otherwise you will end up with the same numbers

20:05.240 --> 20:09.360
again.

20:09.360 --> 20:11.880
Just another object.

20:11.880 --> 20:13.840
Good.

20:13.840 --> 20:21.640
Any questions about numeric types?

20:21.640 --> 20:24.640
What is this?

20:24.640 --> 20:29.480
Come again.

20:29.480 --> 20:35.560
South pole.

20:35.560 --> 20:39.160
South pole sounds good.

20:39.160 --> 20:45.440
How did you figure it out?

20:45.440 --> 20:46.440
That's one way.

20:46.440 --> 20:52.200
Yeah, it tells you here 90 degree south latitude.

20:52.200 --> 20:55.720
This is south pole.

20:55.720 --> 21:02.120
What time is it there right now?

21:02.120 --> 21:06.520
That's one valid answer all the times.

21:06.520 --> 21:10.080
Let's see if we can answer this question.

21:10.080 --> 21:13.640
So we have a couple of date and time types in Postgres.

21:13.640 --> 21:19.760
So we have time stamp without time zone and time stamp with time zone.

21:19.760 --> 21:24.520
Depending on your use case, make a good choice which one you use.

21:24.520 --> 21:29.960
Any time you just want to store a time, date and a time, you want to use a time stamp without

21:29.960 --> 21:30.960
time zone.

21:30.960 --> 21:36.160
If you work with multiple time zones, we really use with time zone.

21:36.160 --> 21:42.040
Internally Postgres will store the time as UTC time, but it will make sure to handle

21:42.040 --> 21:43.440
the transformation for you.

21:43.440 --> 21:46.400
We will see a couple examples.

21:46.400 --> 21:54.720
We also have time without time zone and time with time zone.

21:54.720 --> 21:58.000
And we have date and interval.

21:58.000 --> 22:02.040
Postgres will not know about any kind of leap seconds.

22:02.040 --> 22:06.280
So occasionally we have a year which is a second longer with the leap seconds.

22:06.280 --> 22:11.920
I think they are planning one because earth is going slower so we use it.

22:11.920 --> 22:13.800
I don't know how this will go.

22:13.800 --> 22:18.360
But anyway, Postgres doesn't know about leap seconds because the time zone database doesn't

22:18.360 --> 22:21.960
know about it.

22:21.960 --> 22:26.240
So we have something which looks like a date.

22:26.240 --> 22:28.880
I cast it to a time stamp.

22:28.880 --> 22:33.400
And we see that Postgres makes it this date midnight.

22:33.400 --> 22:37.680
So if you don't specify your time, it's always midnight.

22:37.680 --> 22:43.240
We can also say, okay, January 5th, that's a format Americans using months first.

22:43.240 --> 22:52.000
We also see it's midnight here.

22:52.000 --> 22:55.760
We can also specify a time zone.

22:55.760 --> 23:02.240
So I have here 325 afternoon in UTC time zone.

23:02.240 --> 23:05.200
I cast this to a time stamp.

23:05.200 --> 23:06.200
What does it say?

23:06.200 --> 23:07.200
525.

23:07.200 --> 23:11.720
Any idea?

23:11.720 --> 23:17.120
Postgres will always return times in my current time zone which is set on my system.

23:17.120 --> 23:21.680
So many companies use servers which are set to UTC.

23:21.680 --> 23:27.560
My laptop which I use for this example is set to Berlin time which in summer is two

23:27.560 --> 23:29.840
hours before UTC.

23:29.840 --> 23:34.280
So we see I specify a time zone as UTC here.

23:34.280 --> 23:38.680
And in August I get 525 back.

23:38.680 --> 23:43.960
So it transforms the time I specify to my local time.

23:43.960 --> 23:45.360
Same in winter.

23:45.360 --> 23:47.360
This is December here.

23:47.360 --> 23:49.320
1023 UTC.

23:49.320 --> 23:54.680
I get 1123 back as my time.

23:54.680 --> 23:58.880
This can be very convenient but also very inconvenient depending on what you're working

23:58.880 --> 24:01.480
on.

24:01.480 --> 24:08.560
We can also say any time zone as any time zone your computer knows about.

24:08.560 --> 24:12.560
So we have a time zone database in computers.

24:12.560 --> 24:16.720
And we can use any name from there to specify as a time zone.

24:16.720 --> 24:22.800
We can also say just plus or minus the number for the time zone.

24:22.800 --> 24:28.480
Obviously if you specify a time zone as a name Postgres knows about summertime.

24:28.480 --> 24:34.560
If you just say plus four it never knows about summertime because you just say plus four.

24:34.560 --> 24:39.440
The example I'm using here is because at some point Russia just said okay we are no longer

24:39.440 --> 24:42.000
doing this dance with winter and summertime.

24:42.000 --> 24:52.640
They just stopped at some point and said okay year round it's one time zone.

24:52.640 --> 24:57.280
In Postgres time is stopped if you start a transaction.

24:57.280 --> 24:58.920
So we start a transaction here.

24:58.920 --> 25:04.680
If you say select now multiple times you always get the same time back.

25:04.680 --> 25:06.800
That's the transaction time.

25:06.800 --> 25:08.920
So I'm using this here.

25:08.920 --> 25:13.480
Now I'm setting my time zone to Europe Moscow.

25:13.480 --> 25:14.480
What changed?

25:14.480 --> 25:16.120
My output changed.

25:16.120 --> 25:18.520
This one is my brilliant time.

25:18.520 --> 25:20.000
This one is my Moscow time.

25:20.000 --> 25:30.760
Two time zones difference.

25:30.760 --> 25:38.960
What I can also say any time stamp I have in Postgres at a specific time zone.

25:38.960 --> 25:46.720
So previously everything here was always returned in my own time zone which is set on my computer.

25:46.720 --> 25:54.520
I could say change everything to this specific time zone or I can just say format one specific

25:54.520 --> 25:58.680
time stamp I have at a given time zone.

25:58.680 --> 26:05.000
Then of course you can say okay select now at Berlin comma now at New York comma now

26:05.000 --> 26:08.720
at Buenos Aires in one single query.

26:08.720 --> 26:13.280
So we can return as many time zones or time stamps you have at as many different time

26:13.280 --> 26:19.720
zones you have.

26:19.720 --> 26:20.880
A couple more examples.

26:20.880 --> 26:28.920
So Postgres does not know about leap seconds but it knows about leap years.

26:28.920 --> 26:35.920
So we have two thousand fifths of January minus two thousand first of January.

26:35.920 --> 26:38.000
This is four days difference.

26:38.000 --> 26:42.280
It's an interval now of four days.

26:42.280 --> 26:47.040
We have two thousand first of January minus fourth of January.

26:47.040 --> 26:53.200
It's minus three days.

26:53.200 --> 26:54.400
We get an interval back.

26:54.400 --> 27:00.000
So if we have time stamps Postgres will do all the calculation for us between the two

27:00.000 --> 27:06.880
time stamps including years dates including leap dates everything.

27:06.880 --> 27:13.200
And we can use this to figure out of a specific year is a leap year.

27:13.200 --> 27:22.200
Two thousand twenty eight of February plus one day interval gives me two thousand twenty

27:22.200 --> 27:24.520
ninth of February.

27:24.520 --> 27:30.400
Of course if I do this in 2001 I only get three hundred sixty five days back.

27:30.400 --> 27:33.920
In 2000 I get two hundred sixty six days back.

27:33.920 --> 27:36.640
It's a leap year.

27:36.640 --> 27:39.080
Makes all of this into account.

27:39.080 --> 27:42.400
Which brings me back to my initial question.

27:42.400 --> 27:45.600
What time is it at South Pole?

27:45.600 --> 27:50.080
This station at South Pole is operated by the Americans.

27:50.080 --> 27:55.520
It's called Scott Amundsen station which is however supplied from New Zealand.

27:55.520 --> 27:59.960
That's the closest airport they have for all the planes they're operating.

27:59.960 --> 28:06.000
And by now we know how to figure out the time in New Zealand right.

28:06.000 --> 28:10.080
Right now at time zone in New Zealand.

28:10.080 --> 28:17.440
That would be cheating because your operating system actually knows about Antarctica.

28:17.440 --> 28:24.800
So every station which humans have on Antarctica got its own time zone which is usually aligned

28:24.800 --> 28:28.560
to the country operating the station.

28:28.560 --> 28:34.480
Because this one is well operated by Americans but supplied from New Zealand conveniently

28:34.480 --> 28:38.320
they're using the same time zone as New Zealand.

28:38.320 --> 28:47.320
Select now at time zone and Antarctica South Pole gives you the time at the South Pole.

28:47.320 --> 28:49.920
Anyone here using XML?

28:49.920 --> 28:51.880
Fine.

28:51.880 --> 28:56.720
Let's.

28:56.720 --> 29:01.440
What's your use case?

29:01.440 --> 29:07.520
Some data feeds.

29:07.520 --> 29:14.360
Where there are some basic support in Postgres for XML.

29:14.360 --> 29:16.960
You can set encoding.

29:16.960 --> 29:20.160
You cannot search directly in XML.

29:20.160 --> 29:24.520
So Postgres stores the XML as it is but you cannot really search in it.

29:24.520 --> 29:25.680
There's no support for this.

29:25.680 --> 29:30.320
What you could do is cast it to text and then try to make some sense out of it.

29:30.320 --> 29:33.000
But that's about it.

29:33.000 --> 29:34.560
We have two different types.

29:34.560 --> 29:37.920
We can say it's a document type here.

29:37.920 --> 29:44.560
So I specify a text and I tell Postgres, okay, pass this as a document in XML.

29:44.560 --> 29:46.640
It will fail if it's not proper XML.

29:46.640 --> 29:50.320
And tell me, okay, this doesn't work.

29:50.320 --> 29:58.720
So I get back an XML document here with all the formatting and the entire tree.

29:58.720 --> 30:01.280
I can also say I don't want to have an entire document.

30:01.280 --> 30:07.960
I just want to have a piece of XML content that's working with XML paths and content.

30:07.960 --> 30:10.600
But then again, I cannot search in it.

30:10.600 --> 30:14.440
There's no support for it.

30:14.440 --> 30:19.760
I can serialize this and unserialize this if you want to store some larger XML documents

30:19.760 --> 30:20.760
in Postgres.

30:20.760 --> 30:21.760
That's working.

30:21.760 --> 30:25.240
So this one is a text now.

30:25.240 --> 30:27.480
No longer an XML document.

30:27.480 --> 30:32.400
So if you really want to search in something and say, okay, does this specific text appear

30:32.400 --> 30:40.320
in my XML document, go and serialize it and then maybe apply some like or wakeups on it.

30:40.320 --> 30:49.760
But then again, if you search for name, you will find plenty of this.

30:49.760 --> 30:56.440
Reality is if you want to store something like XML, go for JSON.

30:56.440 --> 31:00.240
We have two different JSON data types in Postgres.

31:00.240 --> 31:04.520
One is the older one is called JSON.

31:04.520 --> 31:07.080
JSON as it is stores the data as it comes in.

31:07.080 --> 31:12.320
So it basically takes the entire JSON blob, stores it as it is.

31:12.320 --> 31:18.120
And then later on, if you work on the data type, then it does all the parsing.

31:18.120 --> 31:24.160
So at insertion time, it doesn't really know if your JSON object is valid or not.

31:24.160 --> 31:27.840
It only figures out when you try to operate on it.

31:27.840 --> 31:30.960
And then at some point, there came a better JSON type.

31:30.960 --> 31:32.840
It's called JSONB.

31:32.840 --> 31:39.000
I think one of the big mistakes this project made was not to duplicate JSON and say, okay,

31:39.000 --> 31:41.880
the new one is the JSON type.

31:41.880 --> 31:46.640
So I called it JSONB and now we have to live with it.

31:46.640 --> 31:49.920
The new one is better in almost every way.

31:49.920 --> 31:55.960
So it does parse the JSON when you insert into the database, when you create this type.

31:55.960 --> 31:57.920
You can create an index on it.

31:57.920 --> 32:02.480
It already tells you on creation time if it's valid or not.

32:02.480 --> 32:08.120
And then you have a decomposed object, a JSON object in your database, which you can work

32:08.120 --> 32:09.120
on.

32:09.120 --> 32:17.320
All of this is using regular transactions, like some other databases using JSON.

32:17.320 --> 32:19.640
You don't get JSON support on this.

32:19.640 --> 32:26.000
And Postgres, it's one more data type we use supporting transactions, everything supporting

32:26.000 --> 32:27.160
replication.

32:27.160 --> 32:33.120
How do we use this?

32:33.120 --> 32:36.440
This is a regular text here.

32:36.440 --> 32:42.440
I need to quote my text in JSON with a double quote to make it a JSON text.

32:42.440 --> 32:47.800
Then I have my single quotes for Postgres telling it this is a string.

32:47.800 --> 33:01.680
And I cast this string to JSON and I get my JSON text back.

33:01.680 --> 33:05.520
We can use arrays and lists and hashes in JSON.

33:05.520 --> 33:07.000
So here we have an array.

33:07.000 --> 33:12.200
As you can see, we have several text types, JSON text types in the array.

33:12.200 --> 33:14.800
Then we create this array.

33:14.800 --> 33:20.280
And we make this a Postgres string.

33:20.280 --> 33:25.480
It's a JSON type and we see, okay, I still have my JSON text types here.

33:25.480 --> 33:26.480
My JSON array.

33:26.480 --> 33:33.040
And for Postgres, all of this, one string passed as a JSON type.

33:33.040 --> 33:35.280
The same works with key value pairs.

33:35.280 --> 33:43.120
So we have key and value here as JSON, make it a hash, and then make it a string in Postgres,

33:43.120 --> 33:49.880
cast this string to JSONP.

33:49.880 --> 33:54.720
Of course, since we passed the JSON, I already Postgres knows what's in there.

33:54.720 --> 33:59.600
So we can say, okay, I only want to have this key number two.

33:59.600 --> 34:01.880
Here my text is D, E, F.

34:01.880 --> 34:09.240
I can access whatever is in my JSON value.

34:09.240 --> 34:15.720
I can ask if the white element is in the list on the left.

34:15.720 --> 34:18.120
GHA is actually in there.

34:18.120 --> 34:19.120
So I get a tool back.

34:19.120 --> 34:29.640
It's a yes, no question in Postgres.

34:29.640 --> 34:30.760
Same for the keys.

34:30.760 --> 34:31.760
So here's the value.

34:31.760 --> 34:33.840
Is this value in this list?

34:33.840 --> 34:35.720
Is this key in this list?

34:35.720 --> 34:42.600
So any of the questions you usually have from an application using JSON, like, is a specific

34:42.600 --> 34:43.600
value there?

34:43.600 --> 34:45.560
Does a specific field exist?

34:45.560 --> 34:49.200
All of this you can answer directly in Postgres.

34:49.200 --> 34:54.320
You don't have to extract the entire data type, transfer it into your application, and

34:54.320 --> 34:56.360
then try to make sense of it.

34:56.360 --> 35:03.200
All of this can be answered directly in Postgres in one query.

35:03.200 --> 35:09.320
On top of it, because Postgres already knows what is in the JSON, we can have an index

35:09.320 --> 35:11.040
support on this.

35:11.040 --> 35:17.920
This only works on JSON B, by the way, not on the old JSON type.

35:17.920 --> 35:21.000
So what I'm doing here is I have an index.

35:21.000 --> 35:25.240
On my table, I have to use the GIN type.

35:25.240 --> 35:31.920
And I only create this index on one specific field in my JSON object.

35:31.920 --> 35:34.880
This is not indexing the entire JSON field.

35:34.880 --> 35:37.920
It's just one field in my object here, the name field.

35:37.920 --> 35:43.520
And then I can use this index to answer queries.

35:43.520 --> 35:51.480
So if you have this typical web application, we are storing 20, 50, 100 JSON values in

35:51.480 --> 35:55.240
one object, you don't want to index all of them.

35:55.240 --> 35:59.320
You maybe just want to have an index on one or two of the fields.

35:59.320 --> 36:03.640
This is possible in Postgres.

36:03.640 --> 36:06.240
Booleans.

36:06.240 --> 36:10.200
We have a real Boolean type in Postgres.

36:10.200 --> 36:13.320
So we can say two faults.

36:13.320 --> 36:16.480
We have a couple of alternatives you can specify.

36:16.480 --> 36:20.320
So for two, you can say it's one or two or yes.

36:20.320 --> 36:25.200
For faults, you can say it's faults or no or n.

36:25.200 --> 36:30.720
Can you please be a little bit silent back then?

36:30.720 --> 36:36.960
In the end, what Postgres does, it transforms all of these values into the two or fours

36:36.960 --> 36:39.680
value for the Boolean.

36:39.680 --> 36:46.880
We have a couple of other databases in the market which say, yeah, we have a Boolean.

36:46.880 --> 36:50.080
Under the root, it's maybe just an integer.

36:50.080 --> 36:55.440
Then you can insert not only zero and one but also five, eight, 50 and then try to make

36:55.440 --> 36:57.960
sense out of this.

36:57.960 --> 37:05.320
Here we only get two and fours back.

37:05.320 --> 37:11.080
So two, cast as a Boolean, we see it's getting two.

37:11.080 --> 37:17.520
And fours, we get an F back.

37:17.520 --> 37:20.800
And we can use this in queries.

37:20.800 --> 37:23.440
So I have one table here.

37:23.440 --> 37:28.960
Let's say that's a table where you store log file messages.

37:28.960 --> 37:31.560
And yeah, we have some content here.

37:31.560 --> 37:37.400
And we have one field which tells me, okay, this log entry is an error.

37:37.400 --> 37:40.960
Usually we don't have a lot of errors.

37:40.960 --> 37:46.040
But this is an entry in our table that we are mostly interested in.

37:46.040 --> 37:54.320
So maybe one, 2% of the queries of the entries in this table will have this flag set.

37:54.320 --> 38:02.240
What I'm doing here, I create a million rows and about 2% of them have this flag set just

38:02.240 --> 38:07.240
to have some basic test data here.

38:07.240 --> 38:13.280
And when I go and say, okay, I only want to see all the entries in my table where the

38:13.280 --> 38:19.320
error is true, because that's what I'm interested in, you see by default, process has to scan

38:19.320 --> 38:21.840
the entire table.

38:21.840 --> 38:25.760
It's quite expensive.

38:25.760 --> 38:31.680
What I can say, I create an index and make this a conditional index.

38:31.680 --> 38:36.880
And only every row where error is true goes into this index.

38:36.880 --> 38:42.920
So the 98% of the table where there is no error I'm not interested in, because the

38:42.920 --> 38:44.960
cardinality is not high enough.

38:44.960 --> 38:49.880
I will never see this data in my index.

38:49.880 --> 38:56.360
Only the 2% here go into my index and now suddenly the cost of the query came down from

38:56.360 --> 39:03.720
16,000 something to 68.

39:03.720 --> 39:07.080
Very fast now.

39:07.080 --> 39:16.840
There's one use case for Boolean index.

39:16.840 --> 39:25.400
So for the cost of this, I have another index now on the entire column and you can see the

39:25.400 --> 39:35.800
entire index needs 2,700 pages and my Boolean index only needs 57 pages on this.

39:35.800 --> 39:42.360
So much, much faster.

39:42.360 --> 39:46.520
We have a bit type in Postgres, so not only Boolean, we can also say we want to store

39:46.520 --> 39:48.520
bits.

39:48.520 --> 39:53.000
Or you can specify how many bits you want to have.

39:53.000 --> 39:58.040
The interesting part is you can tell Postgres by using the B in front.

39:58.040 --> 40:03.400
This is binary value and it does all the transformation for you.

40:03.400 --> 40:23.800
From this which looks like a string with bits in it to sorry.

40:23.800 --> 40:27.480
Postgres will do the transformation of the bits for us.

40:27.480 --> 40:35.600
So we don't have to do the middle calculation which bit is which value.

40:35.600 --> 40:39.560
We can also do bit operations on these values.

40:39.560 --> 40:48.080
So I have some data in my table and I want to only select where this bit is set.

40:48.080 --> 40:52.600
Or specify that's a binary value here, a bit value here.

40:52.600 --> 40:57.000
And that's a logical end for a logical OR.

40:57.000 --> 41:02.080
And then of course because OR gives me any value which the bit is set on the left or

41:02.080 --> 41:07.960
the right side, I get everything back here.

41:07.960 --> 41:10.400
We can have exclusive OR on bits.

41:10.400 --> 41:15.520
We can also do mathematical operations on bits like shifting left and right for multiplied

41:15.520 --> 41:18.000
by two or divided by two.

41:18.000 --> 41:23.160
All of this works.

41:23.160 --> 41:24.720
We can also search in bits.

41:24.720 --> 41:30.360
It looks a bit complicated because we need to make this a logical operation.

41:30.360 --> 41:37.200
So I want to search everything where this bit is set and because it gives me a value

41:37.200 --> 41:43.440
back I need to say okay this is greater now than I get to result.

41:43.440 --> 41:48.440
If I search for everything where the second bit from the right is set, there is no value

41:48.440 --> 41:57.840
in it so I don't get anything back from my database.

41:57.840 --> 42:02.640
I can cast bits to any value.

42:02.640 --> 42:03.840
Like that's an integer.

42:03.840 --> 42:08.960
I can cast it to bits and get my bit value back any other way around.

42:08.960 --> 42:15.200
I can cast from bits to integers to any other value.

42:15.200 --> 42:18.000
Good.

42:18.000 --> 42:23.120
Anyone of you storing binary data in a database?

42:23.120 --> 42:26.920
Good for you.

42:26.920 --> 42:31.440
We have a byte A type which can do this.

42:31.440 --> 42:35.680
All I want to say about this is please use the functions of your programming language

42:35.680 --> 42:39.960
to transfer the data in and out of the database.

42:39.960 --> 42:45.720
Please don't write your own code to try and transform this data.

42:45.720 --> 42:51.400
Every language we have which supports Postgres has functions for transferring binary data

42:51.400 --> 42:57.840
in and out of the database.

42:57.840 --> 42:59.920
We have two different formats.

42:59.920 --> 43:05.240
One is called the old escape format which is hard to pass.

43:05.240 --> 43:10.840
And the new one is the hex format so this will always store hex values for binary data

43:10.840 --> 43:15.960
in a database.

43:15.960 --> 43:21.440
We also have network types in Postgres.

43:21.440 --> 43:27.360
So E-Net can store any IPv4 or IPv6 address.

43:27.360 --> 43:33.000
The network type CDIR can store the networks.

43:33.000 --> 43:37.320
And we can store mega address in Postgres.

43:37.320 --> 43:49.160
So any time you work with network addresses, please don't store them as a text.

43:49.160 --> 43:55.160
Use the proper data type for it because you can go and have an index on it or you can

43:55.160 --> 44:02.920
go and ask Postgres is this IP address in this network?

44:02.920 --> 44:12.080
I created a table with a couple of IP addresses in here.

44:12.080 --> 44:19.520
And then I can ask Postgres give me every IP address which is in this network.

44:19.520 --> 44:26.920
So you don't have to do all the manual handling of IP addresses in text matching and whatever

44:26.920 --> 44:31.080
people do to find IP addresses.

44:31.080 --> 44:33.720
And on top of that, it supports an index.

44:33.720 --> 44:36.400
It gets very, very fast.

44:36.400 --> 44:41.440
Any kind of address and IP address parsing can be very fast in Postgres.

44:41.440 --> 44:48.080
I'm almost running out of time because we started late.

44:48.080 --> 44:50.440
You can create your own data types in Postgres.

44:50.440 --> 44:53.720
So we have enums and a couple more.

44:53.720 --> 44:57.120
We have Postgres as extension.

44:57.120 --> 45:00.600
This is part of another talk I have.

45:00.600 --> 45:05.040
Not here, not today.

45:05.040 --> 45:07.720
So I keep this part.

45:07.720 --> 45:11.720
Any questions you have?

45:11.720 --> 45:20.800
Jimmy, do we have any questions?

45:20.800 --> 45:43.880
During the part about the timestamp or timestamp C data type, you said that the database handles

45:43.880 --> 45:47.480
the conversion from UTC type.

45:47.480 --> 45:55.000
Does it store the UTC type and also the offset in time zone or does it always convert to

45:55.000 --> 45:59.120
the current time zone of the database, I guess?

45:59.120 --> 46:08.080
Whatever you insert into the data type and you don't specify a time zone, it will assume

46:08.080 --> 46:09.760
your local time zone.

46:09.760 --> 46:16.000
If you specify a time zone and use this time zone, it always converts it to UTC internally.

46:16.000 --> 46:20.520
When you select it, it returns in your time zone you specify.

46:20.520 --> 46:22.520
Thank you.

46:22.520 --> 46:26.320
Nice talk.

46:26.320 --> 46:33.640
So, different type, particularly since interesting, what do you say, like, can I use Postgres for

46:33.640 --> 46:38.760
my non-intellectual use purposes or something like MongoDB?

46:38.760 --> 46:41.400
I think you're in a wrong room.

46:41.400 --> 46:44.920
The answer is yes.

46:44.920 --> 46:51.960
Almost everything you can use MongoDB for, you can also do in Postgres.

46:51.960 --> 46:53.960
Hi.

46:53.960 --> 47:00.120
Is there ever any hope of getting an unsigned integer type in PostgresQL?

47:00.120 --> 47:04.640
I think there's an extension which can do that, but it's not there by default.

47:04.640 --> 47:09.120
If you check the Postgres extension network, PGXN, there are a couple more data types you

47:09.120 --> 47:21.000
can use for this.

47:21.000 --> 47:23.640
I didn't get it quite right.

47:23.640 --> 47:27.760
You recommended JSONB data type over JSON.

47:27.760 --> 47:28.760
Yes.

47:28.760 --> 47:29.760
Okay.

47:29.760 --> 47:30.760
I got it.

47:30.760 --> 47:35.520
Basically, everything you can do in JSON, you can also do in JSONB, but it gives you

47:35.520 --> 47:41.800
more functionality in JSONB.

47:41.800 --> 47:46.680
It usually is, yes.

47:46.680 --> 47:50.320
Okay.

47:50.320 --> 48:05.960
Thanks, Jimmy.
