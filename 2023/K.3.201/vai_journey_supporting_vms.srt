1
0:00:00.000 --> 0:00:08.000
So, hello everyone.

2
0:00:08.000 --> 0:00:09.340
A pleasure to be here.

3
0:00:09.340 --> 0:00:14.300
I'm Ithamar Holder and I'm a senior software engineer working for Red Hat.

4
0:00:14.300 --> 0:00:20.420
And this is a talk about the journey through supporting VMs with dedicated CPUs on Kubernetes.

5
0:00:20.420 --> 0:00:26.380
And the reason that there's a journey word in the subject is that this was a true journey

6
0:00:26.380 --> 0:00:27.420
for me.

7
0:00:27.420 --> 0:00:33.600
And I'm going to guide you through the journey until we reach the actual problems and solutions.

8
0:00:33.600 --> 0:00:34.760
And there are two reasons for that.

9
0:00:34.760 --> 0:00:40.360
So first reason is that we need to understand the problems and solutions.

10
0:00:40.360 --> 0:00:43.960
So we need to understand the background for it.

11
0:00:43.960 --> 0:00:48.960
But the second reason is that I've gained a lot of insights and takeaways during that

12
0:00:48.960 --> 0:00:49.960
journey.

13
0:00:49.960 --> 0:00:55.200
And I think that I hope that you could find this also valuable for your journeys.

14
0:00:55.200 --> 0:01:00.720
And that you can take the same takeaways for whatever you're doing and whatever you're

15
0:01:00.720 --> 0:01:02.120
interested in.

16
0:01:02.120 --> 0:01:08.480
So we're going to talk about all sorts of stuff like dedicated CPUs and CPU manager,

17
0:01:08.480 --> 0:01:13.680
C groups, pod isolation and name spaces, Kubernetes resource allocation.

18
0:01:13.680 --> 0:01:16.460
And so let's begin.

19
0:01:16.460 --> 0:01:19.560
So first of all, an introduction to Kubernetes.

20
0:01:19.560 --> 0:01:27.360
So Kubernetes is designed to run containers which are designed very differently than VMs.

21
0:01:27.360 --> 0:01:34.320
And running VMs on one platform and containers on another platform is not the best approach.

22
0:01:34.320 --> 0:01:37.560
And this is where Kubernetes comes into play.

23
0:01:37.560 --> 0:01:42.840
This is basically an add-on or extension to Kubernetes which lets you run VMs on top of

24
0:01:42.840 --> 0:01:47.920
Kubernetes as a first-class citizen, as a completely cloud native.

25
0:01:47.920 --> 0:01:53.120
And I'm not going to dive into all the architectural details here, but the trick is basically to

26
0:01:53.120 --> 0:01:59.880
run a VM within a container like this picture tries to illustrate.

27
0:01:59.880 --> 0:02:05.200
And that's basically what you need to know for this talk.

28
0:02:05.200 --> 0:02:08.040
So what's the deal with dedicated CPUs?

29
0:02:08.040 --> 0:02:13.920
So basically the key word here is avoiding preemption or context switches.

30
0:02:13.920 --> 0:02:14.920
Right?

31
0:02:14.920 --> 0:02:21.200
So this is crucial for certain use cases like real-time VMs or VMs that depend on very low

32
0:02:21.200 --> 0:02:22.200
latency.

33
0:02:22.200 --> 0:02:27.680
So as a naive example, let's think about a VM that hot loops over some condition.

34
0:02:27.680 --> 0:02:31.720
And when this condition becomes true, it has to react really, really fast.

35
0:02:31.720 --> 0:02:37.580
So if we context switch this workload out, then it would take more time because once

36
0:02:37.580 --> 0:02:42.040
the condition becomes true, it would take time to context switch back and only then

37
0:02:42.040 --> 0:02:45.040
the VM could react.

38
0:02:45.040 --> 0:02:48.200
So this is very crucial for some use cases.

39
0:02:48.200 --> 0:02:54.560
Also it's supported by most hypervisors and it's a pretty standard feature.

40
0:02:54.560 --> 0:02:58.920
And we aim to bring this also to Kubernetes.

41
0:02:58.920 --> 0:03:00.280
So a question to the crowd.

42
0:03:00.280 --> 0:03:02.120
Who recognizes this section?

43
0:03:02.120 --> 0:03:03.960
Who knows what this is?

44
0:03:03.960 --> 0:03:05.240
Okay.

45
0:03:05.240 --> 0:03:06.620
So most of you.

46
0:03:06.620 --> 0:03:12.720
And another question, who can say that he's confident about how this is implemented behind

47
0:03:12.720 --> 0:03:16.720
the scenes or how Kubernetes actually does that?

48
0:03:16.720 --> 0:03:18.760
A lot less of you, right?

49
0:03:18.760 --> 0:03:20.760
So that's good.

50
0:03:20.760 --> 0:03:23.960
It means that this is relevant, right?

51
0:03:23.960 --> 0:03:27.600
So obviously this is the part, this is taken from the Pause manifest.

52
0:03:27.600 --> 0:03:30.420
This is the place when we specify resources.

53
0:03:30.420 --> 0:03:33.200
We have of course requests and limits.

54
0:03:33.200 --> 0:03:41.040
We can specify CPU, memory, ephemeral storage and a bunch of other stuff.

55
0:03:41.040 --> 0:03:43.980
And so let's talk about containers for a second.

56
0:03:43.980 --> 0:03:50.240
So containers are actually a conceptual concept that could be implemented in many ways.

57
0:03:50.240 --> 0:03:56.560
So from the kernels, the Linux kernel perspective, there isn't such a thing as a container really.

58
0:03:56.560 --> 0:04:03.040
There are basically a couple of main kernel features that serve as the building blocks

59
0:04:03.040 --> 0:04:04.960
for containers.

60
0:04:04.960 --> 0:04:06.880
One of them is C groups.

61
0:04:06.880 --> 0:04:12.300
C groups is very important and is one of the main building blocks for containers.

62
0:04:12.300 --> 0:04:15.760
So let's talk about C groups a bit.

63
0:04:15.760 --> 0:04:22.280
So basically the idea is that the architecture is a tree of resources, right?

64
0:04:22.280 --> 0:04:26.600
We have the root C group, which is basically all of the resources on the node.

65
0:04:26.600 --> 0:04:29.000
So for example, 100 CPUs.

66
0:04:29.000 --> 0:04:31.160
And then we divide them into groups.

67
0:04:31.160 --> 0:04:37.480
Like for example, 70 CPUs, 20 CPUs, 10 CPUs, and so on.

68
0:04:37.480 --> 0:04:44.560
The idea is that every process on the system is attached to a C group.

69
0:04:44.560 --> 0:04:54.160
And that basically the C groups limits the resources for this group of processes.

70
0:04:54.160 --> 0:04:57.540
And in Kubernetes, there is usually one C group per container.

71
0:04:57.540 --> 0:05:01.240
This actually depends on the CRI that you're using.

72
0:05:01.240 --> 0:05:09.720
But the most common approach is to use one C group per container.

73
0:05:09.720 --> 0:05:14.660
So in Kubernetes, all of the values are always absolute, right?

74
0:05:14.660 --> 0:05:20.920
When we specify CPU, for example, we can specify 100M, which stands for mill CPUs, which is

75
0:05:20.920 --> 0:05:24.860
similar to 0.1 CPUs, 1.3, whatever.

76
0:05:24.860 --> 0:05:27.240
But these are always absolute values.

77
0:05:27.240 --> 0:05:29.920
In C groups, it's all relative, right?

78
0:05:29.920 --> 0:05:31.500
It's called CPU shares.

79
0:05:31.500 --> 0:05:34.560
The default is 1024, but it doesn't really matter.

80
0:05:34.560 --> 0:05:42.560
So if we will look on a very naive example again, let's say that we have a node with

81
0:05:42.560 --> 0:05:45.120
two pods running on the system, pod A and pod B.

82
0:05:45.120 --> 0:05:49.920
And let's say that pod A has one CPU share and pod B has two CPU shares.

83
0:05:49.920 --> 0:05:56.760
What it would mean is that pod B would have twice as CPU time as pod A.

84
0:05:56.760 --> 0:06:01.520
It doesn't really matter how many CPUs the nodes have, because this is all relative,

85
0:06:01.520 --> 0:06:02.520
right?

86
0:06:02.520 --> 0:06:10.200
So how does Kubernetes convert between the absolute values and the relative shares?

87
0:06:10.200 --> 0:06:17.320
So we can think about one CPU as 1024 shares, just because it's the default in C groups.

88
0:06:17.320 --> 0:06:20.680
So let's say that a pod asks for 200 mill CPUs.

89
0:06:20.680 --> 0:06:23.320
So this is actually a fifth of a CPU.

90
0:06:23.320 --> 0:06:32.960
So what we can do is divide 1024 by 5, and we get approximately 205 shares.

91
0:06:32.960 --> 0:06:36.780
And this would work, but remember that shares are still relative.

92
0:06:36.780 --> 0:06:44.320
So what happens, for example, if the node has 100 CPUs and one pod with 200 mill CPUs

93
0:06:44.320 --> 0:06:46.520
requests runs on that pod?

94
0:06:46.520 --> 0:06:50.100
Since it's relative, it would just use all of the nodes' resources, right?

95
0:06:50.100 --> 0:06:52.320
So this has a nice side effect.

96
0:06:52.320 --> 0:07:01.520
The spare resources on the node can be used by the pod relatively to their requests.

97
0:07:01.520 --> 0:07:05.800
So basically, the request is the minimum amount that is actually allocated, and all of the

98
0:07:05.800 --> 0:07:14.280
spare resources are being split relatively to the request.

99
0:07:14.280 --> 0:07:18.120
So let's talk about Kubernetes QoS for a second.

100
0:07:18.120 --> 0:07:21.280
There are three quality of service levels.

101
0:07:21.280 --> 0:07:23.080
The first one is best effort.

102
0:07:23.080 --> 0:07:25.060
That means that I don't specify anything.

103
0:07:25.060 --> 0:07:26.060
I don't have requests.

104
0:07:26.060 --> 0:07:30.760
I don't have limits, not for memory and not for CPU.

105
0:07:30.760 --> 0:07:34.060
The last one, guaranteed, is kind of the opposite from that.

106
0:07:34.060 --> 0:07:38.760
I specify both requests and limits to both CPUs and memory, and the requests and limits

107
0:07:38.760 --> 0:07:40.680
are equal.

108
0:07:40.680 --> 0:07:44.720
Now if you're not best effort and you're not guaranteed, you'd be burstable.

109
0:07:44.720 --> 0:07:50.400
So this is just an example, but the idea is that you can specify either only requests,

110
0:07:50.400 --> 0:07:53.040
only limits, you can specify them both, but they're not equal.

111
0:07:53.040 --> 0:08:00.120
So any other than best effort and guarantee.

112
0:08:00.120 --> 0:08:05.340
Now basically, the trade-off here is predictability in order to get stability.

113
0:08:05.340 --> 0:08:10.920
So basically, Kubernetes tells you, if you want me to guarantee you stability, you have

114
0:08:10.920 --> 0:08:12.760
to be predictable.

115
0:08:12.760 --> 0:08:16.960
Or if you will be more predictable, you'll gain more stability.

116
0:08:16.960 --> 0:08:22.400
Like one example for that, if we're talking about memory, for example, are node pressures.

117
0:08:22.400 --> 0:08:29.840
So when the node would have high memory pressure, it would evict guaranteed QoS pods less.

118
0:08:29.840 --> 0:08:35.640
And after that, it would get to burstable, and after that, it would get to best efforts.

119
0:08:35.640 --> 0:08:39.280
So this is true, by the way, as long as you keep your promises.

120
0:08:39.280 --> 0:08:43.360
If you say that you're limited to a certain amount of memory and then you exceed this

121
0:08:43.360 --> 0:08:53.200
memory, then Kubernetes is, on most CRIs, will just kill the pod.

122
0:08:53.200 --> 0:08:56.120
So can we use dedicated CPUs in Kubernetes?

123
0:08:56.120 --> 0:08:57.800
So the answer is yes.

124
0:08:57.800 --> 0:09:01.200
This is possible with CPU manager.

125
0:09:01.200 --> 0:09:04.800
And in order to do that, we have two requirements.

126
0:09:04.800 --> 0:09:09.640
Now, first of all, the pod needs to be of guaranteed QoS.

127
0:09:09.640 --> 0:09:14.080
First of all, the CPU request, which equals the limit because it's a guaranteed QoS, has

128
0:09:14.080 --> 0:09:15.280
to be an integer.

129
0:09:15.280 --> 0:09:19.440
It cannot be a floating point value.

130
0:09:19.440 --> 0:09:24.760
Also, an interesting fact is that only a single container or some of the containers in a pod

131
0:09:24.760 --> 0:09:34.080
can have dedicated CPUs, but the whole pod needs to be of a guaranteed QoS.

132
0:09:34.080 --> 0:09:36.880
So let's talk about namespaces for a second.

133
0:09:36.880 --> 0:09:39.440
So remember this little diagram from before.

134
0:09:39.440 --> 0:09:44.040
So namespaces is another building block for containers, and it basically is responsible

135
0:09:44.040 --> 0:09:46.000
for the isolation of the containers.

136
0:09:46.000 --> 0:09:50.160
So when I'm picturing a pod, this is what I think about.

137
0:09:50.160 --> 0:09:53.140
Like it's a box with some containers in it.

138
0:09:53.140 --> 0:09:57.680
The containers are absolutely isolated from one another.

139
0:09:57.680 --> 0:10:03.880
And as we said, container is a concept.

140
0:10:03.880 --> 0:10:11.080
So if we will take some of the namespaces out and we will break some of the isolations

141
0:10:11.080 --> 0:10:16.120
between the containers, are there still containers?

142
0:10:16.120 --> 0:10:20.760
How layers of isolation do we need to strip before it stops being a container?

143
0:10:20.760 --> 0:10:25.120
This is more of a philosophical question, but is it possible on Kubernetes?

144
0:10:25.120 --> 0:10:27.040
And the answer is yes.

145
0:10:27.040 --> 0:10:32.680
So for example, it's possible to share the PID namespace between containers or the process

146
0:10:32.680 --> 0:10:34.000
namespace between containers.

147
0:10:34.000 --> 0:10:39.600
And what it means is that inside a container, if you will do something like PS, you would

148
0:10:39.600 --> 0:10:42.480
see all of the processes from all of the containers.

149
0:10:42.480 --> 0:10:46.080
This isolation will not exist anymore.

150
0:10:46.080 --> 0:10:51.040
Another interesting fact is that as a side effect, the file systems are also shared.

151
0:10:51.040 --> 0:10:57.640
Now, they're not shared directly, but you can use Datric to use them indirectly.

152
0:10:57.640 --> 0:11:02.880
So, slash proc slash PID slash root will get you to the root file system of another process

153
0:11:02.880 --> 0:11:06.720
that now can be in another container.

154
0:11:06.720 --> 0:11:13.560
So to actually enable that, that's what you need to do in the pod, in the spec, share

155
0:11:13.560 --> 0:11:19.600
process namespace through, and that's it.

156
0:11:19.600 --> 0:11:21.920
So now a word about KVM.

157
0:11:21.920 --> 0:11:24.560
So who knows KVM, by the way?

158
0:11:24.560 --> 0:11:26.480
Oh, a lot of you.

159
0:11:26.480 --> 0:11:27.480
Okay.

160
0:11:27.480 --> 0:11:31.280
So we have a kernel model which turns the Linux into a hypervisor.

161
0:11:31.280 --> 0:11:34.920
Basically we have two kinds of hypervisors, type one and type two.

162
0:11:34.920 --> 0:11:39.380
Type one means that it's also called a bare metal hypervisor because it's being installed

163
0:11:39.380 --> 0:11:40.380
on a bare metal.

164
0:11:40.380 --> 0:11:42.760
There's no OS benefit.

165
0:11:42.760 --> 0:11:47.800
And what it means is that it's really fast, but the downside is that it has to implement

166
0:11:47.800 --> 0:11:52.060
stuff like the scheduler and virtual memory and a lot of stuff that already exists on

167
0:11:52.060 --> 0:11:53.720
every OS.

168
0:11:53.720 --> 0:11:58.280
Type two hypervisors are being installed on top of the OS, so they don't have to re-implement

169
0:11:58.280 --> 0:12:01.620
all of those stuff, but they're usually a lot slower.

170
0:12:01.620 --> 0:12:06.960
So KVM is really incredible because it turns Linux into a type one hypervisor.

171
0:12:06.960 --> 0:12:12.160
And this is what QVRT is using to gain near to native performance.

172
0:12:12.160 --> 0:12:18.520
Now, an interesting fact about the KVM is that its main purpose is CPU virtualization

173
0:12:18.520 --> 0:12:22.060
because this is a performance part.

174
0:12:22.060 --> 0:12:28.160
It's also backed by QEMU, which does things like IO and stuff like that, which are usually

175
0:12:28.160 --> 0:12:33.480
are less related to performance.

176
0:12:33.480 --> 0:12:36.660
So how does KVM actually work?

177
0:12:36.660 --> 0:12:41.980
So from the guest perspective, it will have, for example, four CPUs.

178
0:12:41.980 --> 0:12:44.160
But these aren't real CPUs, right?

179
0:12:44.160 --> 0:12:46.800
They are virtual CPUs or vCPUs.

180
0:12:46.800 --> 0:12:52.680
And from the kernel perspective, these are just threads, vCPU threads.

181
0:12:52.680 --> 0:12:58.840
So what the guest sees as a physical CPU is actually from the host perspective is just

182
0:12:58.840 --> 0:13:01.440
another thread on the system.

183
0:13:01.440 --> 0:13:07.000
Okay, so now back to QVRT after all of these introductions.

184
0:13:07.000 --> 0:13:09.400
In QVRT, we have the VRT launcher pod.

185
0:13:09.400 --> 0:13:11.560
It has some containers in it.

186
0:13:11.560 --> 0:13:14.360
The compute container is the main container.

187
0:13:14.360 --> 0:13:19.480
Inside the compute container, we run the QEMU process, which actually runs the guest.

188
0:13:19.480 --> 0:13:26.080
And this is the main container that we're using.

189
0:13:26.080 --> 0:13:29.080
So first attempt to support dedicated CPUs.

190
0:13:29.080 --> 0:13:37.240
So the idea was let's allocate the compute container that we talked about with dedicated

191
0:13:37.240 --> 0:13:38.240
CPUs.

192
0:13:38.240 --> 0:13:40.520
So this is possible with CPU manager, as we talked about.

193
0:13:40.520 --> 0:13:45.600
All we need is to do is to have a pod that's guaranteed QS and to have an integer amount

194
0:13:45.600 --> 0:13:51.360
of CPUs on the compute container.

195
0:13:51.360 --> 0:13:57.360
So by the way, is it a good approach, do you think?

196
0:13:57.360 --> 0:14:00.640
This is a problem, and let me explain you why.

197
0:14:00.640 --> 0:14:04.300
So let's zoom into the compute container for a second.

198
0:14:04.300 --> 0:14:09.160
The list here is all of the processes and threads that run inside the compute container.

199
0:14:09.160 --> 0:14:13.480
Now, you don't need to understand everything that's running here.

200
0:14:13.480 --> 0:14:15.440
But let me show you the interesting part.

201
0:14:15.440 --> 0:14:18.400
So you see the QEMU KVM process.

202
0:14:18.400 --> 0:14:22.040
All of the red ones are its threads.

203
0:14:22.040 --> 0:14:28.680
Now as you can see, we have two threads, which are the actual vCPU threads, like I said earlier.

204
0:14:28.680 --> 0:14:36.640
And so the problem is that we have a lot of threads with different priorities.

205
0:14:36.640 --> 0:14:44.880
And basically, if we let all of the compute container run with dedicated CPUs, these aren't

206
0:14:44.880 --> 0:14:50.080
really dedicated CPUs, because we said that the key word here is avoiding preemption.

207
0:14:50.080 --> 0:14:56.280
But with the previous setting, we will context switch out the vCPUs in order for other threads

208
0:14:56.280 --> 0:14:58.060
inside the compute containers.

209
0:14:58.060 --> 0:15:02.720
So the vCPUs aren't running on dedicated CPUs, really.

210
0:15:02.720 --> 0:15:05.400
We actually lied to the guest.

211
0:15:05.400 --> 0:15:09.880
So that's a problem.

212
0:15:09.880 --> 0:15:12.840
Now the second approach is called the housekeeping cgroup.

213
0:15:12.840 --> 0:15:18.960
And the idea is that we will make a child cgroup for all of the low priority threads

214
0:15:18.960 --> 0:15:21.360
or processes.

215
0:15:21.360 --> 0:15:22.640
So how would it work?

216
0:15:22.640 --> 0:15:26.480
So let's say that the user asks for x CPUs.

217
0:15:26.480 --> 0:15:30.520
We would actually allocate x plus 1 dedicated CPUs.

218
0:15:30.520 --> 0:15:36.280
And one dedicated CPU will serve all of the housekeeping tasks.

219
0:15:36.280 --> 0:15:44.240
And when I say housekeeping tasks, I basically mean everything but the vCPUs themselves.

220
0:15:44.240 --> 0:15:51.000
Then what we can do is move all of the threads that aren't vCPUs into the housekeeping cgroups.

221
0:15:51.000 --> 0:15:56.260
And then the vCPUs would be with true dedicated CPUs.

222
0:15:56.260 --> 0:15:57.260
So this is how it looks like.

223
0:15:57.260 --> 0:15:59.280
We have the BERT launcher pod.

224
0:15:59.280 --> 0:16:04.040
How do we have the compute container with x plus 1 dedicated CPUs?

225
0:16:04.040 --> 0:16:08.160
One dedicated CPU is for everything but the vCPUs themselves.

226
0:16:08.160 --> 0:16:14.640
And the x dedicated CPUs are for the vCPUs.

227
0:16:14.640 --> 0:16:20.480
So this approach is much better because this basically supports true dedicated CPUs for

228
0:16:20.480 --> 0:16:22.320
the vCPUs.

229
0:16:22.320 --> 0:16:24.660
But this also has a problem.

230
0:16:24.660 --> 0:16:33.000
So first problem is that we waste one dedicated CPU for stuff that are of low priority.

231
0:16:33.000 --> 0:16:36.600
This is a huge waste.

232
0:16:36.600 --> 0:16:43.240
Ideally we would have wanted to do something like give me a 4 or x amount of dedicated

233
0:16:43.240 --> 0:16:47.820
CPUs and another amount of shared CPUs for everything else.

234
0:16:47.820 --> 0:16:51.500
And this is actually possible on cgroups, but it's not possible on Kubernetes because

235
0:16:51.500 --> 0:16:52.840
of what we said earlier.

236
0:16:52.840 --> 0:16:57.360
If we're going to ask like 3.2 CPUs or something like that, they won't be dedicated.

237
0:16:57.360 --> 0:16:59.440
They would be all shared.

238
0:16:59.440 --> 0:17:03.120
So basically Kubernetes goes for an all or nothing approach.

239
0:17:03.120 --> 0:17:09.520
Either all of the CPUs are dedicated or all of the CPUs are shared.

240
0:17:09.520 --> 0:17:13.280
Another problem, which is more of a design problem, is that we're focused around the

241
0:17:13.280 --> 0:17:15.600
lowest priority processes.

242
0:17:15.600 --> 0:17:18.200
And this kind of should be reversed, right?

243
0:17:18.200 --> 0:17:25.600
I mean, we want to configure the vCPUs to have dedicated CPUs, so we configure everything

244
0:17:25.600 --> 0:17:27.800
that is not the vCPUs.

245
0:17:27.800 --> 0:17:35.080
And this is problematic because we would ideally want to only change the vCPU threads and leave

246
0:17:35.080 --> 0:17:42.080
everything as is to keep it open for extensions in the future and stuff like that.

247
0:17:42.080 --> 0:17:45.480
There are more problems related to cgroups v1 and v2.

248
0:17:45.480 --> 0:17:54.000
I'll not dive into details here, but two words about it is, for example, in v2, we have the

249
0:17:54.000 --> 0:17:57.600
threaded cgroup concept, which doesn't exist in v1.

250
0:17:57.600 --> 0:18:01.200
And in a threaded cgroup, we have a lot of limitations.

251
0:18:01.200 --> 0:18:06.500
Some of the controllers or some systems of cgroups cannot work at all.

252
0:18:06.500 --> 0:18:11.840
So just know that there are more problems with this solution.

253
0:18:11.840 --> 0:18:13.680
So a third attempt.

254
0:18:13.680 --> 0:18:17.400
This is, I'm calling it the dedicated CPU cgroup approach.

255
0:18:17.400 --> 0:18:18.400
And here's the idea.

256
0:18:18.400 --> 0:18:23.280
So the compute container stays completely as it is.

257
0:18:23.280 --> 0:18:24.940
We won't touch it at all.

258
0:18:24.940 --> 0:18:29.800
We would allocate it with CPUs that are not guaranteed, not dedicated, sorry, they are

259
0:18:29.800 --> 0:18:32.100
shared CPUs.

260
0:18:32.100 --> 0:18:38.200
But remember that the pod still needs to be with guaranteed QS, and I'll explain why.

261
0:18:38.200 --> 0:18:44.840
So what we will do is add another container, which is basically a blank container with

262
0:18:44.840 --> 0:18:46.540
x dedicated CPUs.

263
0:18:46.540 --> 0:18:49.600
And as I said, every container creates a new cgroup.

264
0:18:49.600 --> 0:18:51.940
So it will create a new cgroup for us.

265
0:18:51.940 --> 0:18:53.920
So what do I mean by a blank container?

266
0:18:53.920 --> 0:18:56.240
I mean, a container doesn't really run anything.

267
0:18:56.240 --> 0:19:01.200
It could run, for example, a slip forever process just to keep the container alive, but

268
0:19:01.200 --> 0:19:02.200
that's it.

269
0:19:02.200 --> 0:19:04.160
It would be entirely blank.

270
0:19:04.160 --> 0:19:10.520
And then what we'll do is move only the vCPUs into another container, right, into another

271
0:19:10.520 --> 0:19:11.800
cgroup.

272
0:19:11.800 --> 0:19:19.520
All of the compute containers stays as is, and only the vCPUs are configured.

273
0:19:19.520 --> 0:19:22.480
So this is how it looks like.

274
0:19:22.480 --> 0:19:27.440
So as the VM starts, or before it starts, we have the VRT launcher.

275
0:19:27.440 --> 0:19:29.320
Now we have two different containers.

276
0:19:29.320 --> 0:19:32.300
One of them is the compute container with Y shared CPUs.

277
0:19:32.300 --> 0:19:34.360
These are not dedicated CPUs.

278
0:19:34.360 --> 0:19:37.280
The other one is a container with dedicated CPUs.

279
0:19:37.280 --> 0:19:42.920
X dedicated CPUs, exactly the amount we need, not x plus 1 as before.

280
0:19:42.920 --> 0:19:49.100
So in the compute container, everything is being run when the VM is being started.

281
0:19:49.100 --> 0:19:53.880
But right after it's being started, or right before it's being started, what we will do

282
0:19:53.880 --> 0:19:58.080
is move the vCPUs into the different container.

283
0:19:58.080 --> 0:20:03.320
And that basically solves the problem, because now all of the housekeeping tasks are being

284
0:20:03.320 --> 0:20:05.160
run with shared CPUs.

285
0:20:05.160 --> 0:20:10.040
The vCPUs are running on dedicated CPUs.

286
0:20:10.040 --> 0:20:11.800
So can we actually do that?

287
0:20:11.800 --> 0:20:18.600
I mean, we actually moved some threads of a process to another container.

288
0:20:18.600 --> 0:20:21.160
This looks extremely weird, right?

289
0:20:21.160 --> 0:20:24.880
But this is possible because we shared the PID namespace.

290
0:20:24.880 --> 0:20:31.520
So you can think about it like the processes are not isolated anymore.

291
0:20:31.520 --> 0:20:36.200
They're not really being moved from one container to another, because the container does not

292
0:20:36.200 --> 0:20:38.160
isolate processes anymore.

293
0:20:38.160 --> 0:20:40.720
So we only change C groups.

294
0:20:40.720 --> 0:20:46.940
So from the vCPUs perspective, they just stay the same.

295
0:20:46.940 --> 0:20:52.780
They can communicate with all of the threads and processes in the system.

296
0:20:52.780 --> 0:20:55.800
So only the relevant threads are being configured, as I said.

297
0:20:55.800 --> 0:21:01.720
Shared CPUs for the housekeeping tasks, so we don't waste one dedicated core anymore.

298
0:21:01.720 --> 0:21:04.200
And we keep things open for extensions in the future.

299
0:21:04.200 --> 0:21:08.920
Maybe we would want to do more plays with C groups in the future.

300
0:21:08.920 --> 0:21:14.120
So we want everything in the compute container to stay completely as is.

301
0:21:14.120 --> 0:21:18.120
OK, so summary and takeaways.

302
0:21:18.120 --> 0:21:19.760
There were a lot of introductions here.

303
0:21:19.760 --> 0:21:25.720
And I've scratched the surface of a lot of cool facts and technologies that I've seen

304
0:21:25.720 --> 0:21:27.920
along the way.

305
0:21:27.920 --> 0:21:35.400
So we've seen CPU allocation, implementation in Kubernetes, how C group uses relative shares

306
0:21:35.400 --> 0:21:44.360
and not absolute values, and how the CPUs and the resources are being spread along the

307
0:21:44.360 --> 0:21:48.080
pods relatively to their requests.

308
0:21:48.080 --> 0:21:50.960
We've seen how to enable dedicated CPUs in Kubernetes.

309
0:21:50.960 --> 0:21:56.760
We've seen namespaces and how to break the isolation within a pod.

310
0:21:56.760 --> 0:22:05.680
We've talked a bit about KVM and how it uses threads to run the actual CPUs.

311
0:22:05.680 --> 0:22:07.320
And of course, we talked about Kubernetes.

312
0:22:07.320 --> 0:22:13.200
And again, I really hope that these takeaways will serve you in your journeys in the future.

313
0:22:13.200 --> 0:22:15.800
And I hope that you find this interesting.

314
0:22:15.800 --> 0:22:20.760
So thanks a lot, and you're always welcome to send questions or feedback or anything

315
0:22:20.760 --> 0:22:22.280
else to my mail.

316
0:22:22.280 --> 0:22:25.560
And I'll also be outside for questions.

317
0:22:25.560 --> 0:22:28.520
OK, so thank you.

318
0:22:28.520 --> 0:22:32.240
And if you have any questions.

319
0:22:32.240 --> 0:22:41.760
OK, so the question is, do we need to do a

320
0:22:41.760 --> 0:22:50.400
virtual answer pod, or is it being done by the virtual handler?

321
0:22:50.400 --> 0:22:53.480
And the answer is, it's being done by the virtual handler.

322
0:22:53.480 --> 0:22:55.040
So just a bit of context.

323
0:22:55.040 --> 0:22:59.000
Virtual handler is another pod that Kubernetes uses.

324
0:22:59.000 --> 0:23:04.000
It's a pod with high privileges, and therefore we don't need any extra privileges for it

325
0:23:04.000 --> 0:23:06.000
to configure that.

326
0:23:06.000 --> 0:23:07.000
Yeah.

327
0:23:07.000 --> 0:23:14.000
Does this allow for easier networking communication if you talk in service to service communication

328
0:23:14.000 --> 0:23:15.000
in the Kubernetes sense?

329
0:23:15.000 --> 0:23:18.000
You can do it VM to VM, presumably the exact same mechanism, so resolving service names

330
0:23:18.000 --> 0:23:19.000
from what VM is for another VM, which uses Kubernetes.

331
0:23:19.000 --> 0:23:20.000
Does that work at the moment?

332
0:23:20.000 --> 0:23:21.000
So that's a question about Kubernetes in general, right?

333
0:23:21.000 --> 0:23:22.000
In Kubernetes I can do it, but presumably I can do the same thing from a VM running

334
0:23:22.000 --> 0:23:23.000
inside the pod in the same fashion.

335
0:23:23.000 --> 0:23:24.000
Right.

336
0:23:24.000 --> 0:23:25.000
Yeah, so the answer is yes.

337
0:23:25.000 --> 0:23:37.000
Okay, cool.

338
0:23:37.000 --> 0:23:45.000
Any other question?

339
0:23:45.000 --> 0:23:51.000
Yeah.

340
0:23:51.000 --> 0:23:59.000
So this is an early IELTS.

341
0:23:59.000 --> 0:24:00.000
Right.

342
0:24:00.000 --> 0:24:05.000
So the question is what about IELTS threads or network related threads, what about them?

343
0:24:05.000 --> 0:24:09.000
And actually in the VM manifest we have configurations for that.

344
0:24:09.000 --> 0:24:13.000
So you can ask for example for an IELTS thread to run on a dedicated CPU.

345
0:24:13.000 --> 0:24:15.000
That is also supported.

346
0:24:15.000 --> 0:24:20.000
Yeah, I focused solely on the CPUs themselves, but this is entirely possible in Kubernetes

347
0:24:20.000 --> 0:24:21.000
today.

348
0:24:21.000 --> 0:24:22.000
Yes.

349
0:24:22.000 --> 0:24:38.000
So the question is can we support NUMA with this?

350
0:24:38.000 --> 0:24:39.000
The answer is yes.

351
0:24:39.000 --> 0:24:49.000
I'm not sure if it works right now outside of the box, but I think it should be possible

352
0:24:49.000 --> 0:24:53.000
especially with C groups V2, but this is an interesting question.

353
0:24:53.000 --> 0:25:00.000
I will have to think about it a little more, but I think it is possible.

354
0:25:00.000 --> 0:25:01.000
Anyone else?

355
0:25:01.000 --> 0:25:02.000
Time's up.

356
0:25:02.000 --> 0:25:05.000
Sorry, but I'll be out here if you want to ask further questions.

357
0:25:05.000 --> 0:25:06.000
Thank you.

358
0:25:06.000 --> 0:25:20.000
Thank you.

