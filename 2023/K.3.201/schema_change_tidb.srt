1
0:00:00.000 --> 0:00:08.140
I'm Matthias, I work at the Pincap.

2
0:00:08.140 --> 0:00:11.560
We are doing a distributed SQL database called the TIDB.

3
0:00:11.560 --> 0:00:13.440
It's MySQL compatible,

4
0:00:13.440 --> 0:00:16.800
so for the clients it's just looks the same.

5
0:00:16.800 --> 0:00:23.520
I do have a short talk about online schematics at scale in TIDB.

6
0:00:24.520 --> 0:00:27.360
Compared to MySQL,

7
0:00:27.360 --> 0:00:30.800
a distributed database is slightly different.

8
0:00:30.800 --> 0:00:33.160
MySQL does a metadata lock,

9
0:00:33.160 --> 0:00:35.920
so it basically needs to stop the world.

10
0:00:35.920 --> 0:00:39.240
No transaction can go through the metadata lock,

11
0:00:39.240 --> 0:00:41.840
just to change the metadata.

12
0:00:41.840 --> 0:00:47.920
That means that it's a short lock when you do any kind of DDL.

13
0:00:47.920 --> 0:00:51.000
It also means that when you're doing replication,

14
0:00:51.000 --> 0:00:54.480
these metadata locks actually stops replication a bit.

15
0:00:54.480 --> 0:00:57.440
So if it's not an instant DDL,

16
0:00:57.440 --> 0:01:01.600
you would start getting replication delay when the DDL goes through.

17
0:01:02.640 --> 0:01:06.880
So a distributed database is of course different.

18
0:01:06.880 --> 0:01:09.100
From the client perspective,

19
0:01:09.100 --> 0:01:11.440
you should just see a normal database.

20
0:01:11.440 --> 0:01:14.200
You should just expect that it to be transactional,

21
0:01:14.200 --> 0:01:19.280
it should be an asset compliant query with your normal SQL queries.

22
0:01:19.280 --> 0:01:21.200
For the user, you shouldn't see any changes.

23
0:01:21.200 --> 0:01:23.020
But of course, underneath,

24
0:01:23.020 --> 0:01:26.200
it's distributed on multiple nodes, etc.

25
0:01:26.200 --> 0:01:30.560
So if you take add index as an example of a DDL,

26
0:01:30.560 --> 0:01:34.000
we can't do the synchronous stop the world scenario

27
0:01:34.000 --> 0:01:37.800
with SMDL example in MySQL.

28
0:01:37.800 --> 0:01:40.520
So that's something that we need to solve.

29
0:01:40.520 --> 0:01:43.080
During that, we do need to copy and create

30
0:01:43.080 --> 0:01:49.240
all the index entries for creating a new index while normal traffic comes in.

31
0:01:49.240 --> 0:01:50.720
So in the beginning,

32
0:01:50.720 --> 0:01:54.440
MySQL did more or less stop the full table and

33
0:01:54.440 --> 0:01:57.280
copied everything over and then it released it again.

34
0:01:57.280 --> 0:02:01.560
Nowadays, they are much better on the online and only keeping the metadata lock.

35
0:02:01.560 --> 0:02:06.400
But that's something we need to do better in a distributed database.

36
0:02:06.840 --> 0:02:11.200
So the proposed solution is to version the schema.

37
0:02:11.200 --> 0:02:14.420
So every change you're doing to a schema or a table,

38
0:02:14.420 --> 0:02:17.200
you do with that as a specific version.

39
0:02:17.200 --> 0:02:22.760
You would need to allow sessions to use either the most up-to-date version,

40
0:02:22.760 --> 0:02:25.960
the current version of the schema or a previous version.

41
0:02:25.960 --> 0:02:30.880
So then you can do transitions in between these states or versions.

42
0:02:30.880 --> 0:02:33.600
We need to guarantee that

43
0:02:33.600 --> 0:02:39.360
the states between the previous version and the current version are compatible.

44
0:02:39.360 --> 0:02:44.040
So that basically means we need to create some kind of states from

45
0:02:44.040 --> 0:02:49.760
the before or the start states to the public state where it's usable.

46
0:02:51.640 --> 0:02:56.920
I think it's easy to go backwards to actually see what kind of states are needed.

47
0:02:56.920 --> 0:02:59.760
So here the VN,

48
0:02:59.760 --> 0:03:04.800
it's the current version and we start by the public.

49
0:03:04.800 --> 0:03:07.040
It's the end state, everyone sees the index.

50
0:03:07.040 --> 0:03:11.560
So select goes there, insert updates, everything goes there.

51
0:03:11.560 --> 0:03:16.040
The previous version, we can actually remove the selects,

52
0:03:16.040 --> 0:03:20.720
but it still needs to do all the updates, insert some deletes there.

53
0:03:21.360 --> 0:03:25.240
As you see, regardless,

54
0:03:25.240 --> 0:03:28.360
the time here can be a bit confusing.

55
0:03:28.360 --> 0:03:33.360
So the transactions are of course using the real-time, current time,

56
0:03:33.360 --> 0:03:36.600
but it might see a different version of the schema because we

57
0:03:36.600 --> 0:03:41.760
can't require all transactions to

58
0:03:41.760 --> 0:03:46.840
constantly check for the new schema and stop the world for that.

59
0:03:48.440 --> 0:03:52.480
Let's then move and say we are in this right only states.

60
0:03:52.480 --> 0:03:54.800
Before state for that,

61
0:03:54.800 --> 0:03:57.800
then of course we cannot do selects.

62
0:03:57.800 --> 0:04:01.920
Do we need to do inserts before to make them compatible?

63
0:04:01.920 --> 0:04:04.920
Well, we don't actually serve the reads.

64
0:04:04.920 --> 0:04:09.680
So we do not need to do the inserts in this state.

65
0:04:09.680 --> 0:04:11.720
Backfill will handle it.

66
0:04:11.720 --> 0:04:14.080
Then of course comes the question,

67
0:04:14.080 --> 0:04:16.440
how would backfill handle it?

68
0:04:16.440 --> 0:04:19.080
So for backfill to handle this correctly,

69
0:04:19.080 --> 0:04:22.400
we actually need to have another state between

70
0:04:22.400 --> 0:04:25.680
public and the right only state.

71
0:04:25.680 --> 0:04:29.720
As you see, state-wise for transactions,

72
0:04:29.720 --> 0:04:31.960
it doesn't really change anything,

73
0:04:31.960 --> 0:04:35.240
but it gives time for doing this backfilling.

74
0:04:35.240 --> 0:04:38.920
Because when we enter the right reorganization state,

75
0:04:38.920 --> 0:04:41.080
then we know that the state before,

76
0:04:41.080 --> 0:04:42.440
it's the right only.

77
0:04:42.440 --> 0:04:46.080
So all changes will be double right.

78
0:04:47.880 --> 0:04:51.880
That means that updates can be a bit tricky

79
0:04:51.880 --> 0:04:54.240
because we say we're not doing insert,

80
0:04:54.240 --> 0:04:56.000
but how do we handle updates?

81
0:04:56.000 --> 0:05:02.360
So we need to go a bit deeper to see how that's handled.

82
0:05:02.360 --> 0:05:05.200
In the add index example,

83
0:05:05.200 --> 0:05:08.000
we do have the table data that's public.

84
0:05:08.000 --> 0:05:09.760
So everyone should be able to read

85
0:05:09.760 --> 0:05:12.880
the directly from the table without the index.

86
0:05:12.880 --> 0:05:15.880
So let's say we have at time zero,

87
0:05:15.880 --> 0:05:20.600
a session that sees this new right only state,

88
0:05:20.600 --> 0:05:22.200
and it doesn't insert,

89
0:05:22.200 --> 0:05:25.480
it inserts into the road,

90
0:05:25.480 --> 0:05:28.840
into the table, and it updates the index.

91
0:05:28.840 --> 0:05:32.640
So you can find the row through the index.

92
0:05:33.000 --> 0:05:38.480
Then later on, another session comes in,

93
0:05:38.480 --> 0:05:40.560
but that session has not yet

94
0:05:40.560 --> 0:05:42.760
transitioned to this right only state.

95
0:05:42.760 --> 0:05:45.080
So it's in the state before,

96
0:05:45.080 --> 0:05:47.760
and it wants to update this.

97
0:05:47.760 --> 0:05:50.800
So it goes to the table and updates the row.

98
0:05:50.800 --> 0:05:53.680
That's public, so that's what it needs to do.

99
0:05:53.680 --> 0:05:56.520
But then how about the index?

100
0:05:56.520 --> 0:06:00.400
We don't actually need to insert into the index here,

101
0:06:00.400 --> 0:06:02.960
because that will be handled by

102
0:06:02.960 --> 0:06:05.760
the backfill in the right row organization stage.

103
0:06:05.760 --> 0:06:08.640
But the trick here is that we actually need to

104
0:06:08.640 --> 0:06:14.160
remove the old entry as a part of the update.

105
0:06:14.160 --> 0:06:18.640
So update actually means that we need to propagate

106
0:06:18.640 --> 0:06:21.360
the deletes into this new index object,

107
0:06:21.360 --> 0:06:24.520
but we do not need to do with inserts.

108
0:06:26.120 --> 0:06:30.920
So we need to propagate updates as delete only,

109
0:06:30.920 --> 0:06:33.800
and that also makes it easy to handle the deletes.

110
0:06:33.800 --> 0:06:37.280
So we do need to handle deletes in the new index.

111
0:06:37.280 --> 0:06:39.520
That also gives a name for the state,

112
0:06:39.520 --> 0:06:40.800
so delete only state.

113
0:06:40.800 --> 0:06:43.400
That's when you're reading this,

114
0:06:43.400 --> 0:06:47.920
it's inspired by a paper from Google about

115
0:06:47.920 --> 0:06:51.920
the online asynchronous schema changes in F1,

116
0:06:51.920 --> 0:06:53.520
so on top of Spanner.

117
0:06:53.520 --> 0:06:57.520
Then it takes some time before you understand exactly why

118
0:06:57.520 --> 0:06:59.360
you do need a delete state,

119
0:06:59.360 --> 0:07:02.440
but this is the reason to be

120
0:07:02.440 --> 0:07:05.640
able to move through the different stages.

121
0:07:05.640 --> 0:07:08.800
By not inserting the new row in the index,

122
0:07:08.800 --> 0:07:10.520
or the new entry in the index,

123
0:07:10.520 --> 0:07:12.640
does that not mean that nothing else in

124
0:07:12.640 --> 0:07:14.120
the system can use it because you

125
0:07:14.120 --> 0:07:15.960
have to wait for the backfill to complete?

126
0:07:15.960 --> 0:07:19.320
Yeah, so you don't read from the index until it goes public.

127
0:07:19.320 --> 0:07:20.840
It has to complete, okay, so you have to wait for it to

128
0:07:20.840 --> 0:07:22.160
read it so it doesn't matter either way.

129
0:07:22.160 --> 0:07:22.440
Yeah.

130
0:07:22.440 --> 0:07:24.040
It just delays that you could have done it at

131
0:07:24.040 --> 0:07:25.640
the same time while you're alternating.

132
0:07:25.640 --> 0:07:29.280
So if you would insert it,

133
0:07:29.280 --> 0:07:31.280
then it would more or less be overwritten by

134
0:07:31.280 --> 0:07:34.000
the reorg phase anyway because the reorg needs to

135
0:07:34.000 --> 0:07:38.160
read from a snapshot and take all that data.

136
0:07:38.160 --> 0:07:40.560
So a snapshot taken somewhere

137
0:07:40.560 --> 0:07:42.680
when everything were on the right only.

138
0:07:42.680 --> 0:07:44.280
Just simply to do it that way.

139
0:07:44.280 --> 0:07:46.600
So it would just be overhead to doing the insert.

140
0:07:46.600 --> 0:07:48.840
It wouldn't actually mess up anything.

141
0:07:48.840 --> 0:07:49.840
It will still be correct,

142
0:07:49.840 --> 0:07:53.640
but it will just be unnecessary.

143
0:07:54.400 --> 0:07:58.200
Then if we move on from the delete only state,

144
0:07:58.200 --> 0:08:00.760
the previous version can actually be the start state

145
0:08:00.760 --> 0:08:05.280
because as long as deletes are done,

146
0:08:05.280 --> 0:08:07.120
the previous version does not need to

147
0:08:07.120 --> 0:08:08.920
do anything with previous states.

148
0:08:08.920 --> 0:08:11.680
So there we have the different states

149
0:08:11.680 --> 0:08:13.840
that it needs to transition through.

150
0:08:13.840 --> 0:08:18.200
For keeping transactions running without being blocked.

151
0:08:18.200 --> 0:08:20.640
So here we do have the full part of

152
0:08:20.640 --> 0:08:26.240
the asynchronous DDL in online.

153
0:08:26.240 --> 0:08:29.040
That's done online in a distributed database.

154
0:08:29.040 --> 0:08:31.400
Do you support distributed transactions?

155
0:08:31.400 --> 0:08:36.200
If you do, what transactions in XA prepare state?

156
0:08:36.200 --> 0:08:40.000
So we do not support XA transactions right now.

157
0:08:40.000 --> 0:08:42.600
But of course, if you're connecting to

158
0:08:42.600 --> 0:08:47.120
different SQL nodes, it looks just like it is a master,

159
0:08:47.120 --> 0:08:48.680
or a primary wherever.

160
0:08:48.680 --> 0:08:54.480
So full read and write in however you connect.

161
0:08:54.480 --> 0:08:57.200
So I suppose all this happens

162
0:08:57.200 --> 0:08:59.840
when the remote transaction is committed.

163
0:08:59.840 --> 0:09:03.720
These changes are only then being applied to the.

164
0:09:03.720 --> 0:09:06.960
So transaction is a bit slightly different.

165
0:09:06.960 --> 0:09:10.280
So you cannot have transactions

166
0:09:10.280 --> 0:09:12.960
spanning more than two versions.

167
0:09:12.960 --> 0:09:17.000
So you need to either wait or you need to block,

168
0:09:17.000 --> 0:09:24.320
stop and fail transactions that are too long-running.

169
0:09:24.320 --> 0:09:26.560
Okay. These versions, you have

170
0:09:26.560 --> 0:09:28.600
several versions associated with

171
0:09:28.600 --> 0:09:31.360
a single all-night schema change.

172
0:09:31.360 --> 0:09:32.560
Did I get it right?

173
0:09:32.560 --> 0:09:36.440
Yes. So a single DDL goes through multiple stages.

174
0:09:36.440 --> 0:09:38.360
Okay. I'm calling the inversions.

175
0:09:38.360 --> 0:09:44.240
Thank you. Currently, I'm actually working with

176
0:09:44.240 --> 0:09:48.560
partitioning and for all the table,

177
0:09:48.560 --> 0:09:52.120
reorganized partition will take one set of partitions into a new set.

178
0:09:52.120 --> 0:09:55.200
Then there's another thing.

179
0:09:55.200 --> 0:09:59.560
So during the reorganized phase,

180
0:09:59.560 --> 0:10:01.720
when you're copying data,

181
0:10:01.720 --> 0:10:05.240
you do select from the old one.

182
0:10:05.240 --> 0:10:07.600
Then you go to public.

183
0:10:07.600 --> 0:10:09.000
So you select from this one,

184
0:10:09.000 --> 0:10:15.600
which means that if someone is actually on the right reorganization state,

185
0:10:15.600 --> 0:10:19.320
then they will select from that that's not updated in this one.

186
0:10:19.320 --> 0:10:23.560
So you need to add an additional state between

187
0:10:23.560 --> 0:10:29.120
the right reorganization and the public state just for moving the select.

188
0:10:29.120 --> 0:10:33.240
So it's a double right while moving the reads.

189
0:10:33.240 --> 0:10:38.040
All this is done in TIDB,

190
0:10:38.040 --> 0:10:41.720
and I'm not sure how many is familiar with TIDB.

191
0:10:41.720 --> 0:10:47.160
Okay. Good. Then let's do a quick introduction to this.

192
0:10:47.160 --> 0:10:54.360
TIDB is mainly architecture around three different components.

193
0:10:54.360 --> 0:10:57.120
You have PD, which stands for placement driver.

194
0:10:57.120 --> 0:11:00.440
It creates the timestamps for

195
0:11:00.440 --> 0:11:06.280
transaction handling and it knows about the data locations.

196
0:11:06.280 --> 0:11:10.280
So it knows on which nodes the data are.

197
0:11:10.280 --> 0:11:15.320
Then we have an SQL layer that is stateless.

198
0:11:15.320 --> 0:11:21.480
So it's very easy to spin up or scale in the different number of nodes.

199
0:11:21.480 --> 0:11:25.200
Here we have re-implemented the MySQL protocol.

200
0:11:25.200 --> 0:11:27.840
So this is actually written in Go,

201
0:11:27.840 --> 0:11:31.080
and all of it is in Apache 2 license.

202
0:11:31.080 --> 0:11:34.320
So we do not share any code from MySQL or Maria.

203
0:11:34.320 --> 0:11:40.040
It's completely new since 2015 when the project started.

204
0:11:40.040 --> 0:11:43.360
Then we have a storage layer.

205
0:11:43.360 --> 0:11:46.800
The base storage layer is TIDB,

206
0:11:46.800 --> 0:11:49.560
so it's a distributed key value store.

207
0:11:49.560 --> 0:11:53.840
We even have people that runs that as a distributed key value store and

208
0:11:53.840 --> 0:11:56.560
don't bother about the SQL part.

209
0:11:56.560 --> 0:11:58.840
So that's what you can do as well.

210
0:11:58.840 --> 0:12:03.600
Then we do also have an additional and optional way of

211
0:12:03.600 --> 0:12:06.800
storing the data in what we call TIE flash.

212
0:12:06.800 --> 0:12:08.480
That's a column store.

213
0:12:08.480 --> 0:12:11.840
So by connecting it here,

214
0:12:11.840 --> 0:12:16.080
you can actually do analytics like aggregations and so on,

215
0:12:16.080 --> 0:12:19.520
on the same data within the same transaction even.

216
0:12:19.520 --> 0:12:24.120
The optimizer here would choose what is the fastest way,

217
0:12:24.120 --> 0:12:28.120
what has the lowest cost for executing the query.

218
0:12:28.120 --> 0:12:33.120
So you don't have any ETL or anything like that in between.

219
0:12:33.120 --> 0:12:34.560
It's very easy to just add.

220
0:12:34.560 --> 0:12:39.400
You're doing all the tables and set the TIE flash replica equals one or two,

221
0:12:39.400 --> 0:12:41.800
or if you add more than one,

222
0:12:41.800 --> 0:12:48.360
then you also get the MPPs or massive parallel processing part of it.

223
0:12:48.360 --> 0:12:54.920
We do have, you can run Spark on it as well.

224
0:12:54.920 --> 0:13:01.560
Let's just go down slightly deeper on how we actually store the data.

225
0:13:01.560 --> 0:13:09.840
So we take all this data and split it into ranges about 100 megabytes,

226
0:13:09.840 --> 0:13:14.080
and each such range is stored in three,

227
0:13:14.080 --> 0:13:19.800
or it's configurable, let's say three copies in the TIE KV storage nodes.

228
0:13:19.800 --> 0:13:23.840
Each such region is forming a raft group.

229
0:13:23.840 --> 0:13:29.640
So that's how it keeps the HA and high availability.

230
0:13:30.240 --> 0:13:35.560
TIE KV is using RoxDB as a lower level storage.

231
0:13:35.560 --> 0:13:38.040
So it's an LSM tree.

232
0:13:38.040 --> 0:13:43.720
It's similar as MyRox in Percona or MariaDB.

233
0:13:43.720 --> 0:13:47.240
So it's not B-tree based.

234
0:13:47.520 --> 0:13:51.560
Through this raft protocol,

235
0:13:51.560 --> 0:13:55.040
that's how we also can connect the column store.

236
0:13:55.040 --> 0:13:58.440
So that's how we also have it.

237
0:13:58.440 --> 0:14:01.200
So you can run it in the same transaction.

238
0:14:01.200 --> 0:14:02.760
Even if you have a join,

239
0:14:02.760 --> 0:14:10.520
maybe it's faster to execute parts of it through an index in the row store,

240
0:14:10.520 --> 0:14:19.040
and then do some of the table scans and aggregation in TIE flash in the column store.

241
0:14:22.320 --> 0:14:26.480
This is optional, but this is not, this is the base.

242
0:14:26.480 --> 0:14:31.200
You always need to have the row store and you can have this as an option.

243
0:14:35.160 --> 0:14:38.240
There's a lot of tooling that works.

244
0:14:38.240 --> 0:14:42.000
So first of all, I would say that the data migration.

245
0:14:42.000 --> 0:14:48.920
So it's easy to have a TIE DB cluster to read the binary logs,

246
0:14:48.920 --> 0:14:53.560
or you have to set it up for dumping an upstream MySQL instance,

247
0:14:53.560 --> 0:14:57.320
or even several instances into the same cluster.

248
0:14:57.320 --> 0:15:00.480
So you can combine all the data back.

249
0:15:00.800 --> 0:15:06.920
We have the backup and restore very good dump story.

250
0:15:06.920 --> 0:15:09.400
I think that even works with MySQL, right?

251
0:15:09.400 --> 0:15:10.400
Yes.

252
0:15:10.400 --> 0:15:17.280
You have a tool for us to do a diff between the different instances,

253
0:15:17.280 --> 0:15:22.520
the change data capture that can go to either another TIE DB cluster,

254
0:15:22.520 --> 0:15:27.840
or MySQL instance go through Kafka as well if you want.

255
0:15:29.840 --> 0:15:36.160
TIE up, that's a way for managing and deploying TIE DB,

256
0:15:36.160 --> 0:15:38.480
and all components you want.

257
0:15:38.480 --> 0:15:42.080
You can even use it as a playground to start it in your laptop.

258
0:15:42.080 --> 0:15:44.040
It will download the binaries and start everything,

259
0:15:44.040 --> 0:15:45.440
including monitoring everything.

260
0:15:45.440 --> 0:15:48.160
So it's very easy to just try out.

261
0:15:50.160 --> 0:15:56.000
We have an operator if you want to run it in Kubernetes as well in the cloud.

262
0:15:56.000 --> 0:15:59.680
So we even have it as a cloud service.

263
0:15:59.680 --> 0:16:05.640
You can do anything from on-prem up to cloud service in many different ways.

264
0:16:05.640 --> 0:16:10.840
And we also have Lightning, which is an optimized import tool.

265
0:16:10.840 --> 0:16:14.640
And that's what I will actually use in the next slide soon.

266
0:16:17.600 --> 0:16:23.000
A year ago, we started a project because we heard and compared

267
0:16:23.000 --> 0:16:28.240
the add index performance in TIE DB cluster versus, for example,

268
0:16:28.240 --> 0:16:30.600
Cassandra or Aurora.

269
0:16:30.600 --> 0:16:36.680
And at that time, we were basically three times slower because we hadn't

270
0:16:36.680 --> 0:16:43.680
optimized that it was just stable, proven, and it worked, but it was not fast.

271
0:16:43.680 --> 0:16:49.320
And that's especially when you're doing proof of concept or loading the data.

272
0:16:49.320 --> 0:16:52.320
That's where it's really beneficial to speed it up.

273
0:16:52.320 --> 0:16:57.320
And the way it worked, it would just do a data copying through small transaction

274
0:16:57.320 --> 0:16:58.960
batches, more or less.

275
0:16:58.960 --> 0:17:03.680
So that also creates a lot of overhead with transaction handling, etc.

276
0:17:03.680 --> 0:17:09.040
That's not actually needed when you're doing a backfill,

277
0:17:09.040 --> 0:17:14.120
because during backfill process, copying the data,

278
0:17:14.120 --> 0:17:17.920
it doesn't actually need to be transactional.

279
0:17:17.920 --> 0:17:23.920
And it's only a single node that does this, a single TIE DB node that orchestrates it.

280
0:17:23.920 --> 0:17:26.360
I'm not going to go deep into this.

281
0:17:26.360 --> 0:17:31.920
It basically just shows how you're creating a command in one TIE DB node.

282
0:17:31.920 --> 0:17:33.360
And it goes into a table.

283
0:17:33.360 --> 0:17:36.560
A TIE DB owner will do it, go through the different steps,

284
0:17:36.560 --> 0:17:40.720
and do the data migrations and data copying.

285
0:17:40.720 --> 0:17:47.720
So what we did first was create a feature with this feature flag.

286
0:17:47.720 --> 0:17:51.520
It uses this lightning, the import tool technology.

287
0:17:51.520 --> 0:17:55.920
It's completely built in in TIE DB cluster, so it's not the external tool.

288
0:17:55.920 --> 0:18:01.920
But it reads the data, and then it creates these SSD files for ingestion in Rokslib.

289
0:18:01.920 --> 0:18:03.920
So it's not a single node.

290
0:18:03.920 --> 0:18:05.320
It's a single node.

291
0:18:05.320 --> 0:18:06.720
And it's a single node.

292
0:18:06.720 --> 0:18:10.720
It creates these SSD files for ingestion in Rokslib.

293
0:18:10.720 --> 0:18:17.720
So it's very efficient load, and it has very low impact on the storage side.

294
0:18:17.720 --> 0:18:25.720
It just moves these files into the storage and takes them into the Rokslib levels.

295
0:18:25.720 --> 0:18:28.720
The result was around three times speed up,

296
0:18:28.720 --> 0:18:33.720
and of course, a lot less impact on normal load in the cluster.

297
0:18:33.720 --> 0:18:40.720
Even if you have a highly loaded cluster, you can do this almost without impacting it.

298
0:18:43.720 --> 0:18:50.720
And then we did a bit of analysis of where we could improve even more.

299
0:18:50.720 --> 0:18:56.720
And there was things like the scheduling could be improved just to shorten the time.

300
0:18:56.720 --> 0:19:01.720
Instead of reading directly from the key value store,

301
0:19:01.720 --> 0:19:10.720
we could use these co-processors for removing columns that's not needed, for example,

302
0:19:10.720 --> 0:19:15.720
for doing optimized scans, etc.

303
0:19:17.720 --> 0:19:21.720
We disconnected the read and write dependencies

304
0:19:21.720 --> 0:19:27.720
so they could run in parallel in synchronous, and a lot of other small optimization.

305
0:19:27.720 --> 0:19:32.720
And that created yet another three to five times speed up.

306
0:19:32.720 --> 0:19:39.720
So all in all, during the last year, we had done 10 times improvement in speed.

307
0:19:39.720 --> 0:19:43.720
While we're still only using a single TIDB node,

308
0:19:43.720 --> 0:19:50.720
and now we're three times faster than the baseline of the other implementations

309
0:19:50.720 --> 0:19:57.720
in Cockroach and Aurora that we have compared with.

310
0:19:57.720 --> 0:20:02.720
And there's a bit more to do, so we're currently looking into how we even can distribute this

311
0:20:02.720 --> 0:20:06.720
instead of running it on a single TIDB node,

312
0:20:06.720 --> 0:20:10.720
and also being able to autotune the priority.

313
0:20:10.720 --> 0:20:17.720
So if you have load that goes a bit up and a bit down, so the DDL work can adjust to that.

314
0:20:17.720 --> 0:20:20.720
And that is if you depend on a single time, if that breaks for any reason,

315
0:20:20.720 --> 0:20:23.720
then you basically just go back to the previous stages.

316
0:20:23.720 --> 0:20:29.720
Yeah, so we have a state state, so we go back a little bit,

317
0:20:29.720 --> 0:20:35.720
but you don't need to redo the whole feeling of the index or anything like that.

318
0:20:36.720 --> 0:20:40.720
And yeah, it's all on GitHub.

319
0:20:40.720 --> 0:20:43.720
If you're interested a bit in how it actually works,

320
0:20:43.720 --> 0:20:46.720
I would recommend go to OSS Insights.

321
0:20:46.720 --> 0:20:48.720
I would say it's a demo site.

322
0:20:48.720 --> 0:20:54.720
It runs TIDB in the background, and it's a simple web UI,

323
0:20:54.720 --> 0:20:59.720
well, quite nice UI on top, but it has all of the events from GitHub.

324
0:20:59.720 --> 0:21:04.720
Currently, it's 5.5 billion records, and we store it in a single table.

325
0:21:04.720 --> 0:21:07.720
It's a bit other things there as well.

326
0:21:07.720 --> 0:21:12.720
And you can compare for your own GitHub ID or your own project,

327
0:21:12.720 --> 0:21:18.720
your own repository, compare it and so on, and check some different frameworks, etc.

328
0:21:18.720 --> 0:21:21.720
It's quite cool, actually.

329
0:21:21.720 --> 0:21:27.720
Tie-up is very useful if you want to try it on your own laptop or in your own data center.

330
0:21:27.720 --> 0:21:30.720
Of course, you can go to TIDB Cloud as well,

331
0:21:30.720 --> 0:21:35.720
but I didn't mention that here because that's our commercial offer.

332
0:21:37.720 --> 0:21:41.720
Something else that we have that is not directly connected, it's Chaos Mesh.

333
0:21:41.720 --> 0:21:48.720
So if you have a system on Kubernetes and you want to see how it handles different errors,

334
0:21:48.720 --> 0:21:50.720
you can use that for injecting errors.

335
0:21:50.720 --> 0:21:55.720
That's something that we used for stabilizing and testing out the TIDB cluster.

336
0:21:58.720 --> 0:22:01.720
Then I think I'm out of time.

337
0:22:01.720 --> 0:22:05.720
Perfect timing, so you have time to answer questions.

338
0:22:05.720 --> 0:22:08.720
Yeah?

339
0:22:08.720 --> 0:22:09.720
Yeah?

340
0:22:09.720 --> 0:22:10.720
I have a bunch.

341
0:22:10.720 --> 0:22:18.720
First of all, I'm very interested in how do you organize the H step transitioning.

342
0:22:18.720 --> 0:22:26.720
I mean, you have both storages, and I missed the way you move the data from row into column of format.

343
0:22:26.720 --> 0:22:29.720
I believe you do a double copy.

344
0:22:29.720 --> 0:22:32.720
You have double copies of the data itself.

345
0:22:32.720 --> 0:22:38.720
So we always have the copy here,

346
0:22:38.720 --> 0:22:43.720
and the Raft leader of the group is always here.

347
0:22:43.720 --> 0:22:48.720
So you do have Raft leader and Raft follower in the TIDB,

348
0:22:48.720 --> 0:22:53.720
and then we extended the Raft protocol so we have learner states here,

349
0:22:53.720 --> 0:22:55.720
so they can never become leaders.

350
0:22:55.720 --> 0:22:59.720
So this is a must, and this is optional.

351
0:22:59.720 --> 0:23:04.720
Yeah?

352
0:23:04.720 --> 0:23:11.720
What about the optimizer model?

353
0:23:11.720 --> 0:23:15.720
How do you calculate the cost?

354
0:23:15.720 --> 0:23:16.720
Yes.

355
0:23:16.720 --> 0:23:21.720
To understand which storage format we use.

356
0:23:21.720 --> 0:23:25.720
Yeah, and it's also the influence of the volcano optimizer model,

357
0:23:25.720 --> 0:23:36.720
so that's how you more or less pipeline the different things and can move parts of the pipeline into an MPP framework that handles the column store.

358
0:23:36.720 --> 0:23:49.720
I wonder if this model and the optimizer are at least dispersed across the multiple portions of the TIDB or it's in a single.

359
0:23:49.720 --> 0:23:56.720
So the optimizer that's in the TIDB project, in the TIDB repository, so the SQL node.

360
0:23:56.720 --> 0:24:15.720
And when it executes, it's a push down this co-processor request and also through the MPP framework for pushing down query fragments or the co-processor request into either TIDB or a TICB or to TIFlash.

361
0:24:15.720 --> 0:24:26.720
So, for example, if you're doing a join where one part of the table can be resolved the fast by an index lookup, then it will go here for that part of the table.

362
0:24:26.720 --> 0:24:32.720
And for another table, it might be a big table scan or aggregation that will be faster here.

363
0:24:32.720 --> 0:24:44.720
So then it actually can combine that.

364
0:24:44.720 --> 0:25:02.720
I'm not sure I don't know the details deep enough for answer that.

365
0:25:02.720 --> 0:25:08.720
Yeah. So we are only we don't have on any own connectors or anything like that.

366
0:25:08.720 --> 0:25:17.720
We asked relying on MySQL connectors or MariaDB connectors and that's what we're using when we're testing.

367
0:25:17.720 --> 0:25:27.720
So you basically have the students that tries different kinds of queries and after the passes you understand that they are somehow equal.

368
0:25:27.720 --> 0:25:30.720
Yeah. And of course there are differences.

369
0:25:30.720 --> 0:25:35.720
But I would say the compatibility with the MySQL dialect.

370
0:25:35.720 --> 0:25:43.720
It's very, very high. But of course, like management commands for replication doesn't work because we don't do replication.

371
0:25:43.720 --> 0:25:51.720
We have internal replication or we use change data capture for transfer to another cluster.

372
0:25:51.720 --> 0:25:53.720
Thank you.

373
0:25:53.720 --> 0:25:55.720
Last question.

374
0:25:55.720 --> 0:26:04.720
What TIFlash does when there is high rate of single cell updates like how it handles this like the writing the whole file.

375
0:26:04.720 --> 0:26:09.720
So it's a it's a derivative of Clickhouse.

376
0:26:09.720 --> 0:26:20.720
So it's the caches the changes and then it updates it partially or rewrites the whole.

377
0:26:20.720 --> 0:26:27.720
It can. But if it's behind then it will more or less fall back here.

378
0:26:27.720 --> 0:26:39.720
You have some tweaking options. You can even do it as optimizer hints that you want to use either end in for example, etc.

379
0:26:39.720 --> 0:26:41.720
If there are more questions, I'm sure Mattias.

380
0:26:41.720 --> 0:26:45.720
Yeah, I'm here. Even Daniel is here as well.

381
0:26:45.720 --> 0:27:03.720
Thank you.

