WEBVTT

00:00.000 --> 00:07.000
The next session will be by Dami-en about the SDK with its end. So enjoy the session.

00:30.000 --> 00:38.000
We are facing granular. So that is one of the most often to come in the end of the project.

00:38.000 --> 00:45.000
It's been a very good resource. It's a pre-precision platform based on them.

00:45.000 --> 00:50.000
And we want to do better with what we have.

00:50.000 --> 01:01.000
So my focus is on the storage visualization part of the supervisor.

01:01.000 --> 01:09.000
And we currently have a very modernized approach to the storage platform.

01:09.000 --> 01:16.000
So if you can see on the left that we have a per-metal storage.

01:16.000 --> 01:26.000
And when we had the presentation from inside of the end, we lost 300% of the power of the end inside.

01:26.000 --> 01:37.000
What we want to do is to support the security given by the 10 providers, even the scratch team.

01:37.000 --> 01:50.000
We want to have less impact as possible in some user.

01:50.000 --> 01:58.000
So we want to minimize impact to users. So we want them to have to minimize things too much in the machine.

01:58.000 --> 02:08.000
So what we are proposing is to use... So first I have to introduce the problem.

02:08.000 --> 02:13.000
Then we have to take the pipeline and provide the units seen with storage.

02:13.000 --> 02:18.000
So the tutorials I've drawn first will be on the album.

02:18.000 --> 02:20.000
And to be able to use storage platform and network, we have to use the

02:48.000 --> 02:54.000
system to make sure that the storage is important.

02:54.000 --> 03:04.000
Because it has the responsibility of sharing the network and storage for everything.

03:04.000 --> 03:20.000
And we want to focus on the environment. So NVMe is a new storage protocol.

03:20.000 --> 03:35.000
So that's giving us a lot of preferences inside of the system.

03:35.000 --> 03:38.000
So we have all done zero virtual machines.

03:38.000 --> 03:45.000
That means we can share the storage for other virtual machines.

03:45.000 --> 03:51.000
And to do that, we have what we call a split driver.

03:51.000 --> 04:14.180
So we have a specialized protocol called VFX in the

04:14.180 --> 04:19.180
VFX system. So it's already in place since a long time.

04:19.180 --> 04:24.180
So it's an option in the VFX Linux. So it's not...

04:24.180 --> 04:30.180
And the backend, if you're interested in the XBNG, the backend is TARDIS.

04:30.180 --> 04:43.180
That is a new space program that takes multiple requests and gives you the IOT transfer results with non-zero plot error.

04:43.180 --> 04:52.180
In that we have a special interface to share memory between VMs that is used in the VFX protocol.

04:52.180 --> 04:59.180
We share plot requests from the return machines to the R.0.

04:59.180 --> 05:06.180
This interface is mediated by the new provider. So it's already in place.

05:06.180 --> 05:14.180
We have the return machine that we usually call W in the same time.

05:14.180 --> 05:23.180
Explicitly telling the improvisers that the memory will be granted access to by a 2.0.

05:23.180 --> 05:24.200
And the

05:24.200 --> 05:34.180
W0 need to ask the supervisor to map this memory into its own memory via W0 access.

05:34.180 --> 05:41.180
So I have already replaced TARDIS in our limitation because we want to use SDK in place of TARDIS.

05:41.180 --> 05:48.180
Be able to directly detect the current request and transmit it to the ID and the access.

05:48.180 --> 05:51.180
So to talk a little bit about more about TARDIS,

05:51.180 --> 05:54.180
TARDIS SDK is the storage performance of the backend kit.

05:54.180 --> 06:00.180
But it's originally created by W0, which is used by a user, by a user like Muten.

06:00.180 --> 06:02.600
It's a driver for the

06:02.600 --> 06:10.180
ambient devices running on the workspace in our case in terms of work.

06:10.180 --> 06:13.640
And it's used in the in-line ves

06:13.640 --> 06:19.180
So it's part of the same project as the TPG.

06:19.180 --> 06:21.180
But it's a great tip as ever.

06:21.180 --> 06:23.180
I'll take a look at the camera.

06:23.180 --> 06:26.180
Yeah, this is from the mobile device.

06:26.180 --> 06:28.180
Okay, thank you.

06:28.180 --> 06:32.180
So here we have the current state.

06:58.180 --> 07:24.180
So we have two times the block layers that need to be traversed for one request.

07:24.180 --> 07:32.180
It's one of the costs that is adding to the difference we have with the metal.

07:32.180 --> 07:43.180
So our proposal is to use SPDK to directly transmit BL carry requests from a virtual machine to a storage platform,

07:43.180 --> 07:56.180
a storage device, and to reduce the cost by bypassing the kernel in the in-dom zero.

07:56.180 --> 08:06.180
It's completely transparent for the virtual machine because we already use a lot of the infrastructure already present.

08:06.180 --> 08:17.180
So to take a BLK request, it's a simple part of structure in a shared memory in a ring.

08:17.180 --> 08:21.180
It's not very much different from virtual in this aspect.

08:21.180 --> 08:24.180
And we have multiple request types.

08:24.180 --> 08:26.180
So read, write or discard.

08:26.180 --> 08:35.180
Everything I'm going to focus on read, write because it's a minimum we need to put on to be able to share a request.

08:35.180 --> 08:41.180
And we just have to use it to transfer them to what SPDK to SPDK interface.

08:41.180 --> 08:49.180
So SPDK needs to use a special memory to be able to transmit it to the device.

08:49.180 --> 09:00.180
So we need to use the SPDK allocator, memory allocator, to be able to have a buffer that will be used to go from the device.

09:00.180 --> 09:08.180
So we need to have the data copied from the virtual machine to our DOM zero.

09:08.180 --> 09:10.180
Then we can transfer it to the disk.

09:10.180 --> 09:12.180
So it's pretty simple.

09:12.180 --> 09:13.180
We allocate memory.

09:13.180 --> 09:18.180
We copy the data using the grand stable interface into our memory.

09:18.180 --> 09:21.180
Then we just write it on the disk.

09:21.180 --> 09:26.180
SPDK will call the callbacks that we've given it when it is finished.

09:26.180 --> 09:31.180
And then we can do the same for the read requests.

09:31.180 --> 09:37.180
So it's working great for now.

09:37.180 --> 09:45.180
So as you can see, the read requests, the first and the second call and not read requests, is because the implementation is not finished.

09:45.180 --> 09:53.180
And in this case, I'm doing more grand calls to the epivisors that are on the read or the write requests.

09:53.180 --> 09:57.180
So it's a big cost of our implementation.

09:57.180 --> 10:04.180
But for now, it's done and we'll look into improving it.

10:04.180 --> 10:06.180
But we are doing better than that.

10:06.180 --> 10:07.180
That is on the right.

10:07.180 --> 10:11.180
And the blue column is that this is your current status.

10:11.180 --> 10:16.180
And our implementation is the read on the left.

10:16.180 --> 10:21.180
So same for block size and throughput.

10:21.180 --> 10:43.180
So we are able to improve the performance of our storage stack in a transparent manner for VM because they can use a normal VMs at boot today on the top disk in the current infrastructure of the stage and still make it work.

10:43.180 --> 10:56.180
The problem we have to use NVMe dedicated to the SPDK platform, but NVMe is pretty much everywhere nowadays, even in data centers, especially in data centers.

10:56.180 --> 11:11.180
We are still capable of using the security of the grand table because we keep the state of where the VM only share data that want to be returned to the disk with the back end in SPDK.

11:11.180 --> 11:17.180
And then is still doing the mediator for this.

11:17.180 --> 11:34.180
What we want to do is, of course, having the read request being better than the write request, than the tab disk, which since we are in some case, for example here at the same level with this optimization, I'm not very worried about that.

11:34.180 --> 11:55.180
But we want to be able to not have copy to copy data from VM into DOM0 then having it being handled by the NVMe and use directly the guest memory as source and destination for the email request from the NVMe drive.

11:55.180 --> 12:08.180
And we want to take a look at the grand table interface to see if it can be improved for modern days computing. So I'm finished.

12:08.180 --> 12:16.180
Thank you.

12:16.180 --> 12:26.180
I know from the cloud that it is very hard to install an implement.

12:26.180 --> 12:30.180
How hard is it to implement SPDK?

12:30.180 --> 12:33.180
This question is very quickly.

12:33.180 --> 12:39.180
Is your work somehow can be applied to a KVM virtualization tool?

12:39.180 --> 12:47.180
So the question is if SPDK is hard to use like DPDK and if it can be used in the KVM infrastructure.

12:47.180 --> 12:54.180
So it has always been used in the KVM infrastructure to be as a storage platform for VIT.io guest.

12:54.180 --> 13:00.180
And just that our case is crucial because of the different architecture between KVM and them.

13:00.180 --> 13:05.180
So it's already done by the SPDK community in this whole way.

13:05.180 --> 13:09.180
I would say that this is not very hard to install in our case.

13:09.180 --> 13:15.180
It would be given with the app advisor and the install of XAPNG.

13:15.180 --> 13:18.180
But it's not very hard to install.

13:18.180 --> 13:29.180
We just have to have a special configuration for our DOM0 because this is relying on super page.

13:29.180 --> 13:48.180
So we have to have the support and it's not available in the basic configuration of DOM0.

13:48.180 --> 13:49.180
Yes?

13:49.180 --> 13:55.180
Does your implementation survive the crash of the SPDK process on the host?

13:55.180 --> 14:03.180
The question is if the application is surviving the crash of the SPDK process.

14:03.180 --> 14:08.180
So the virtual machine would be able to survive SPDK not being available.

14:08.180 --> 14:14.180
We would lose the disk in the virtual machine.

14:14.180 --> 14:29.180
It would be hung in the virtual machine for the disk but the virtual machine would still be able to run with the problem.

14:29.180 --> 14:36.180
Any other questions?

14:36.180 --> 14:37.180
Thank you.

14:37.180 --> 14:44.180
Thank you.
