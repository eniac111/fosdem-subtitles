1
0:00:00.000 --> 0:00:07.000
You have image?

2
0:00:07.000 --> 0:00:14.000
Okay.

3
0:00:14.000 --> 0:00:17.000
Thank you. Okay.

4
0:00:17.000 --> 0:00:20.000
Thanks. I think the technical issues are now solved.

5
0:00:20.000 --> 0:00:25.000
So thanks again, everyone, for being here.

6
0:00:25.000 --> 0:00:27.000
So I'm going to talk today about Apache VIN.

7
0:00:27.000 --> 0:00:29.000
Apache VIN is a framework for data processing that runs on

8
0:00:29.000 --> 0:00:34.000
top of several platforms, and it's a special event for doing

9
0:00:34.000 --> 0:00:38.000
streaming analytics. So have you already introduced me?

10
0:00:38.000 --> 0:00:42.000
My name is Israel. So I work as a cloud data engineer in

11
0:00:42.000 --> 0:00:45.000
Google Cloud, helping customers doing data engineering on top

12
0:00:45.000 --> 0:00:48.000
of Google Cloud. A lot of the work that I do is actually

13
0:00:48.000 --> 0:00:52.000
helping customers with Apache VIN and particularly Dataflow,

14
0:00:52.000 --> 0:00:55.000
which is our runner for Apache VIN.

15
0:00:55.000 --> 0:01:00.000
So Apache VIN. What is it? Apache VIN is a framework for

16
0:01:00.000 --> 0:01:04.000
data processing. It allows to run data pipelines like

17
0:01:04.000 --> 0:01:08.000
Flink, like Spark, like so many other big data systems.

18
0:01:08.000 --> 0:01:13.000
It has two main features. The first one is that it's an

19
0:01:13.000 --> 0:01:17.000
unified computing model for batch and streaming.

20
0:01:17.000 --> 0:01:21.000
Any pipeline that you have written in Apache VIN for a batch

21
0:01:21.000 --> 0:01:25.000
use case, you may easily use it in streaming as well.

22
0:01:25.000 --> 0:01:29.000
So the same code, you use all that code and you have to add

23
0:01:29.000 --> 0:01:34.000
some small additions that we're going to talk about in a bit.

24
0:01:34.000 --> 0:01:39.000
And the other main feature is that it runs everywhere and you

25
0:01:39.000 --> 0:01:43.000
can run your pipeline in many different languages, for some

26
0:01:43.000 --> 0:01:47.000
definition of everywhere. So you can write your pipeline in

27
0:01:47.000 --> 0:01:54.000
Java, in Python, in Go. You may also run your pipeline in

28
0:01:54.000 --> 0:01:57.000
any of the programming languages of the Java Victor

29
0:01:57.000 --> 0:02:01.000
matching. For instance, I have here highlighted Scala because

30
0:02:01.000 --> 0:02:04.000
there is a framework called XIO, though don't buy Spotify,

31
0:02:04.000 --> 0:02:09.000
on top of Apache VIN for, let's say, Scala native development

32
0:02:09.000 --> 0:02:15.000
of pipeline. So you don't have to use like a Java looking

33
0:02:15.000 --> 0:02:22.000
code in Scala. So it's a functional code. There are lots

34
0:02:22.000 --> 0:02:26.000
of people using, for instance, Kotlin also on top of the

35
0:02:26.000 --> 0:02:31.000
Java Victor matching with a Java VIN SDK. So that's about the

36
0:02:31.000 --> 0:02:36.000
programming language that you may use. And you may run a

37
0:02:36.000 --> 0:02:40.000
VIN pipeline on top of runners. So there's a direct runner for

38
0:02:40.000 --> 0:02:45.000
local running and local testing of pipelines. It's not meant

39
0:02:45.000 --> 0:02:50.000
for, let's say, to be used with real world use cases. But then

40
0:02:50.000 --> 0:02:53.000
you can run your pipeline on top of Dataflow, on top of

41
0:02:53.000 --> 0:02:57.000
Flink, on top of Hazelcast, Spark, many different runners.

42
0:02:57.000 --> 0:03:00.000
So basically when you write a pipeline in Apache VIN, you are

43
0:03:00.000 --> 0:03:03.000
not tied to the platform where you're running. So you may move

44
0:03:03.000 --> 0:03:07.000
it to different platforms with some minor comments. So Apache

45
0:03:07.000 --> 0:03:10.000
VIN is a theoretical model for computing. Not all the runners

46
0:03:10.000 --> 0:03:15.000
implement the model, let's say, to the same degree of extent.

47
0:03:15.000 --> 0:03:19.000
So right now, as of now, I would say Dataflow and Flink are

48
0:03:19.000 --> 0:03:24.000
probably the ones that are covered like fully covered.

49
0:03:24.000 --> 0:03:29.000
Other runners may have some gaps. A silly example, Hadoop.

50
0:03:29.000 --> 0:03:32.000
You may run a Apache VIN pipeline on top of Hadoop also, but

51
0:03:32.000 --> 0:03:34.000
you cannot run streaming on top of Hadoop because it doesn't

52
0:03:34.000 --> 0:03:36.000
support the streaming. So it also depends on the capability

53
0:03:36.000 --> 0:03:39.000
of the runner. So what you're able to do with VIN, it depends

54
0:03:39.000 --> 0:03:41.000
on the capabilities of the runner. So there's no magic.

55
0:03:41.000 --> 0:03:44.000
So if VIN is batch and streaming, but if your platform doesn't

56
0:03:44.000 --> 0:03:47.000
support streaming, for instance, you cannot do streaming.

57
0:03:47.000 --> 0:03:51.000
So let's talk about streaming. What's the problem with streaming?

58
0:03:51.000 --> 0:03:53.000
It's extremely interesting. So in streaming, you are getting

59
0:03:53.000 --> 0:03:56.000
data, a lot of data, continuously. There is no beginning

60
0:03:56.000 --> 0:04:00.000
and there's no end. So you cannot know in advance where are

61
0:04:00.000 --> 0:04:05.000
the boundaries of your data. And it's coming continuously from

62
0:04:05.000 --> 0:04:08.000
many different places. Think, I don't know, like you are

63
0:04:08.000 --> 0:04:12.000
designing a mobile application for a game or whatever, and

64
0:04:12.000 --> 0:04:16.000
people are using it and sending events to your systems every

65
0:04:16.000 --> 0:04:22.000
once in a while. So because the application is deployed in the

66
0:04:22.000 --> 0:04:29.000
wild world, data will come. Who knows how? Okay. So it will

67
0:04:29.000 --> 0:04:32.000
come out of order. So some users will be, I don't know, in the

68
0:04:32.000 --> 0:04:36.000
underground and without the phone coverage, still try

69
0:04:36.000 --> 0:04:38.000
attempting to send in events and then they will send the

70
0:04:38.000 --> 0:04:42.000
events late. Like for instance here, let's see if I can put

71
0:04:42.000 --> 0:04:52.000
the pointer. So this is data that is supposed to be produced

72
0:04:52.000 --> 0:04:56.000
around 8 in the morning. Maybe the network latency and so on,

73
0:04:56.000 --> 0:05:00.000
but more or less you get it around 8 in the morning in your

74
0:05:00.000 --> 0:05:03.000
system. So this is the time where you are seeing the data

75
0:05:03.000 --> 0:05:06.000
in your system, but for whatever reason, you may also

76
0:05:06.000 --> 0:05:10.000
get very late data. And depending on what you want to do,

77
0:05:10.000 --> 0:05:15.000
you may want to process this data as it was produced, not as

78
0:05:15.000 --> 0:05:18.000
you are receiving it. So this is actually the problem with

79
0:05:18.000 --> 0:05:20.000
microvatching. So I remember when I started hearing about

80
0:05:20.000 --> 0:05:24.000
the streaming in the first days, many years ago, a lot of

81
0:05:24.000 --> 0:05:28.000
people said, ah, Spark, I don't like it because it does

82
0:05:28.000 --> 0:05:32.000
microvatching, it doesn't do real streaming. I had no clue

83
0:05:32.000 --> 0:05:34.000
what they meant. It's like, well, you have to group things,

84
0:05:34.000 --> 0:05:37.000
no, to process it somehow. So if the data is infinite, you

85
0:05:37.000 --> 0:05:40.000
will need to process it. The problem with microvatching,

86
0:05:40.000 --> 0:05:42.000
which is not happening in Spark anymore, so this was really

87
0:05:42.000 --> 0:05:45.000
like ancient times, the problem with microvatching is that

88
0:05:45.000 --> 0:05:48.000
you are doing the batches as you see the data, okay? And

89
0:05:48.000 --> 0:05:52.000
then you may have the data that belongs together in

90
0:05:52.000 --> 0:05:55.000
buckets that are separate. Like for instance, this was

91
0:05:55.000 --> 0:05:57.000
data that was produced at 8 in the morning. If you are

92
0:05:57.000 --> 0:06:00.000
doing buckets of one hour, so while you may capture here

93
0:06:00.000 --> 0:06:03.000
this message, but then if you have late data, you will

94
0:06:03.000 --> 0:06:08.000
capture it in a bucket that doesn't belong, that element

95
0:06:08.000 --> 0:06:10.000
doesn't belong with the rest of the elements there if you

96
0:06:10.000 --> 0:06:13.000
want to process them together. So you need to solve this

97
0:06:13.000 --> 0:06:17.000
problem of lack of order in streaming, okay? And this is

98
0:06:17.000 --> 0:06:21.000
what you can easily solve with Apache Bin. Let's talk

99
0:06:21.000 --> 0:06:25.000
about the watermark. There are many ways of doing stream

100
0:06:25.000 --> 0:06:30.000
processing. One of the most popular is in a concept of

101
0:06:30.000 --> 0:06:34.000
watermark. There is no one dimension of time. There are

102
0:06:34.000 --> 0:06:37.000
two dimensions of time, at least. That is the event

103
0:06:37.000 --> 0:06:41.000
time, the moment in which the data was produced, and that

104
0:06:41.000 --> 0:06:44.000
is the processing time, the moment in which you see the

105
0:06:44.000 --> 0:06:47.000
data. They will never be the same. They can be really

106
0:06:47.000 --> 0:06:51.000
close sometimes, but it's not, you cannot grant it how

107
0:06:51.000 --> 0:06:54.000
close or how far you are going to be from that moment,

108
0:06:54.000 --> 0:06:58.000
okay? So we put time in two dimensions, okay? So the

109
0:06:58.000 --> 0:07:02.000
ideal is, for instance, like this straight line in blue,

110
0:07:02.000 --> 0:07:05.000
for sure. This sun is impossible, okay? You cannot see

111
0:07:05.000 --> 0:07:09.000
data before it's produced, or not yet, at least, okay? So

112
0:07:09.000 --> 0:07:12.000
according to the laws of physics, okay? So, but then

113
0:07:12.000 --> 0:07:14.000
what's most likely what will happen is that you will

114
0:07:14.000 --> 0:07:17.000
have some delay, okay? Sometimes it will be closer to

115
0:07:17.000 --> 0:07:20.000
the ideal. Sometimes it will be farther from the ideal,

116
0:07:20.000 --> 0:07:26.000
okay? So, and you need to take this into account, okay?

117
0:07:26.000 --> 0:07:29.000
So let's see an example that might be a little bit more

118
0:07:29.000 --> 0:07:32.000
telling, okay? So Star Wars, okay? So have you ever,

119
0:07:32.000 --> 0:07:36.000
like, watched a Star Wars movie? So Star Wars were

120
0:07:36.000 --> 0:07:39.000
released out of order, out of order, okay? So the first

121
0:07:39.000 --> 0:07:42.000
movie was episode four. This is purely streaming. This

122
0:07:42.000 --> 0:07:44.000
is what happens in the streaming, okay? You are

123
0:07:44.000 --> 0:07:46.000
expecting events, okay, at the beginning of the session,

124
0:07:46.000 --> 0:07:48.000
in the middle of the session, the end of the session,

125
0:07:48.000 --> 0:07:51.000
okay? And then you get end of the session, middle of

126
0:07:51.000 --> 0:07:54.000
the session, beginning of the session. And you need to

127
0:07:54.000 --> 0:07:57.000
reorder things, okay? Depending on what you want to do,

128
0:07:57.000 --> 0:08:00.000
you may need to reorder, okay? If you don't care,

129
0:08:00.000 --> 0:08:03.000
look, I don't want to, I don't care. I just want to

130
0:08:03.000 --> 0:08:05.000
count how many movies per year were released, okay?

131
0:08:05.000 --> 0:08:07.000
Well, you don't need event time, okay? But if you

132
0:08:07.000 --> 0:08:12.000
want to reconstruct the story, okay? Who did what

133
0:08:12.000 --> 0:08:16.000
before or after? Who, what happened? So you need to

134
0:08:16.000 --> 0:08:19.000
actually be able to reconstruct that time, okay? So

135
0:08:19.000 --> 0:08:22.000
the time where the movies were released, it's processing

136
0:08:22.000 --> 0:08:25.000
time, event time is the time in which actually the events

137
0:08:25.000 --> 0:08:28.000
are happening, okay? So, and this is the kind of

138
0:08:28.000 --> 0:08:30.000
problems that we can solve using Apache VIN or Flink

139
0:08:30.000 --> 0:08:33.000
or many other streaming systems. Let's see how we can

140
0:08:33.000 --> 0:08:37.000
deal with this. The classical way is using windowing,

141
0:08:37.000 --> 0:08:41.000
okay? Windowing is grouping things based on temporal

142
0:08:41.000 --> 0:08:45.000
properties. When we do a data pipeline, we need to

143
0:08:45.000 --> 0:08:49.000
solve one question, which is what we are going to

144
0:08:49.000 --> 0:08:52.000
compute. But if you want to do this in the streaming

145
0:08:52.000 --> 0:08:55.000
and group things based on temporal properties, you need

146
0:08:55.000 --> 0:08:59.000
to answer three additional questions. Where, when, and

147
0:08:59.000 --> 0:09:02.000
how. Let's see some details. The what? This is easy.

148
0:09:02.000 --> 0:09:04.000
We are going to be aggregating, okay? So this is

149
0:09:04.000 --> 0:09:07.000
Java code and it's in Java here as an example. So it's

150
0:09:07.000 --> 0:09:11.000
Apache VIN API. I haven't entered into details.

151
0:09:11.000 --> 0:09:13.000
There's a link at the end with more details, okay? So

152
0:09:13.000 --> 0:09:15.000
don't mind the details right now. So we're aggregating

153
0:09:15.000 --> 0:09:18.000
things together. So this is what we are happening. We

154
0:09:18.000 --> 0:09:21.000
are not doing any kind of temporal based logic yet,

155
0:09:21.000 --> 0:09:23.000
okay? We're just aggregating stuff together, okay? So

156
0:09:23.000 --> 0:09:25.000
we are summing up all the numbers here, okay? So this

157
0:09:25.000 --> 0:09:27.000
is the operation that we are doing. Send us in batch.

158
0:09:27.000 --> 0:09:29.000
This is in batch, okay? So imagine that we are getting

159
0:09:29.000 --> 0:09:32.000
this batch, okay? The problems that when we are

160
0:09:32.000 --> 0:09:35.000
working in streaming, we don't see the full data at

161
0:09:35.000 --> 0:09:38.000
once, okay? And we need to produce output at some

162
0:09:38.000 --> 0:09:42.000
point so we cannot wait forever. So we need to

163
0:09:42.000 --> 0:09:44.000
decide how to group things together. So for instance

164
0:09:44.000 --> 0:09:48.000
here, we are going to group things in windows of two

165
0:09:48.000 --> 0:09:52.000
minutes, okay? But the windows of two minutes are not

166
0:09:52.000 --> 0:09:56.000
in processing time. They are in event time, okay? For

167
0:09:56.000 --> 0:10:01.000
instance, so here, this message here, so this message

168
0:10:01.000 --> 0:10:05.000
here, we sit around 12, okay? And we put it in the

169
0:10:05.000 --> 0:10:09.000
window of 12, okay? But this message over here, so

170
0:10:09.000 --> 0:10:14.000
this was received between 1208 and 1209, okay? And we

171
0:10:14.000 --> 0:10:17.000
are able still to attribute it to assign it to the

172
0:10:17.000 --> 0:10:21.000
window between 12 and 1202 in event time, okay? Because

173
0:10:21.000 --> 0:10:23.000
well, so we can wait for late data and put it in the

174
0:10:23.000 --> 0:10:27.000
right window despite the message being quite late

175
0:10:27.000 --> 0:10:30.000
compared to the processing time, okay? And same with

176
0:10:30.000 --> 0:10:33.000
the rest of windows. Now the question is, okay, good.

177
0:10:33.000 --> 0:10:37.000
So you are waiting until 1208, so what if your message

178
0:10:37.000 --> 0:10:41.000
shows up at 8 p.m.? What do you do? Like eight hours

179
0:10:41.000 --> 0:10:45.000
after. So we need to do another decision, okay? So we

180
0:10:45.000 --> 0:10:47.000
have already made the decision on how we are going to

181
0:10:47.000 --> 0:10:50.000
group things together. Here is with easy windows. There

182
0:10:50.000 --> 0:10:52.000
are more windows in Apache Bin, not entering two

183
0:10:52.000 --> 0:10:55.000
details right now. But now we need to decide how long do

184
0:10:55.000 --> 0:11:00.000
we wait? Okay? So we are going to wait until the

185
0:11:00.000 --> 0:11:05.000
watermark, okay? The watermark is this relationship

186
0:11:05.000 --> 0:11:08.000
between processing time and event time that in the case

187
0:11:08.000 --> 0:11:11.000
of Bin and depending on the runner is calculated and

188
0:11:11.000 --> 0:11:16.000
estimated on the fly as data goes through our pipeline

189
0:11:16.000 --> 0:11:20.000
and this is estimated and when you are, when you

190
0:11:20.000 --> 0:11:24.000
trespass the watermark, you have a certain degree of

191
0:11:24.000 --> 0:11:27.000
warranty that your data is complete, okay? A certain

192
0:11:27.000 --> 0:11:29.000
degree of warranty, okay? It cannot be granted because

193
0:11:29.000 --> 0:11:32.000
well, so the future cannot be known, okay? So we cannot

194
0:11:32.000 --> 0:11:35.000
travel in time, okay? So here, for instance, so we are

195
0:11:35.000 --> 0:11:38.000
processing data and the watermark and the nine, this

196
0:11:38.000 --> 0:11:43.000
number here that we were processing before, now it's

197
0:11:43.000 --> 0:11:46.000
left out of the window. So what does it mean if we are

198
0:11:46.000 --> 0:11:49.000
processing data? We were summing up numbers, that

199
0:11:49.000 --> 0:11:55.000
number, that nine, we are not counting it. As soon as

200
0:11:55.000 --> 0:11:58.000
we see it in our pipeline, it will be dropped, like

201
0:11:58.000 --> 0:12:03.000
lost, okay? So the pipeline will ignore it, okay? And

202
0:12:03.000 --> 0:12:07.000
it may make sense, okay? So you cannot wait forever.

203
0:12:07.000 --> 0:12:11.000
At some point you will have to stop and move on, okay?

204
0:12:11.000 --> 0:12:13.000
But maybe you want to take it into account, okay? So

205
0:12:13.000 --> 0:12:16.000
maybe you, I don't know, like this is a billion

206
0:12:16.000 --> 0:12:19.000
embarrassing thing and every penny counts, okay? So

207
0:12:19.000 --> 0:12:23.000
then you need to process it. Well, you have to take

208
0:12:23.000 --> 0:12:26.000
yet another decision. How we are going to wait for

209
0:12:26.000 --> 0:12:29.000
late data and how we are going to actually update the

210
0:12:29.000 --> 0:12:32.000
data, okay? Here I'm summing numbers. It's easy,

211
0:12:32.000 --> 0:12:35.000
commutative, associative, really no big deal, okay? So

212
0:12:35.000 --> 0:12:41.000
I can do it like say, I can do it like a monoid in

213
0:12:41.000 --> 0:12:43.000
big data processing. So I can just take the aggregation,

214
0:12:43.000 --> 0:12:46.000
the previous aggregation and keep aggregating. I don't

215
0:12:46.000 --> 0:12:49.000
need to keep all the numbers that I have seen so far.

216
0:12:49.000 --> 0:12:53.000
So it is easy. In other cases, for any non-associative,

217
0:12:53.000 --> 0:12:55.000
non-commutative operation, so you may need to have

218
0:12:55.000 --> 0:12:59.000
actually full data to produce an update, okay? And if

219
0:12:59.000 --> 0:13:02.000
you are working in streaming, maybe you don't want to

220
0:13:02.000 --> 0:13:05.000
accumulate all the data, okay? Because that will

221
0:13:05.000 --> 0:13:07.000
increase the amount of resources that you will need

222
0:13:07.000 --> 0:13:09.000
for your pie brand. It will have impact in performance,

223
0:13:09.000 --> 0:13:11.000
latency, and so on. So here we are accumulating because

224
0:13:11.000 --> 0:13:14.000
the operation allows it and we are actually waiting

225
0:13:14.000 --> 0:13:18.000
for late data, okay? So now we are waiting for late

226
0:13:18.000 --> 0:13:21.000
data, but we don't want to wait forever. We want to

227
0:13:21.000 --> 0:13:23.000
have some numbers, okay? So we are actually producing

228
0:13:23.000 --> 0:13:26.000
several outputs per window, okay? So like for instance

229
0:13:26.000 --> 0:13:29.000
here, continuing with the first, so when the watermark

230
0:13:29.000 --> 0:13:33.000
is trespassed, we produce an output, okay? And then when

231
0:13:33.000 --> 0:13:35.000
we see the new number, so we produce the output, we

232
0:13:35.000 --> 0:13:39.000
produce it really late, okay? But well, so this is, so

233
0:13:39.000 --> 0:13:41.000
we cannot make magic, okay? So this is when we see the

234
0:13:41.000 --> 0:13:45.000
data, so we cannot process it earlier than this, okay?

235
0:13:45.000 --> 0:13:48.000
We may actually decide to produce data, some output,

236
0:13:48.000 --> 0:13:51.000
even before the watermark, because the watermark can be

237
0:13:51.000 --> 0:13:54.000
really slow. It depends on the pace of the updates of

238
0:13:54.000 --> 0:13:57.000
the data. If for whatever reason users are sending

239
0:13:57.000 --> 0:14:00.000
your data with a lot of lateness, the watermark can

240
0:14:00.000 --> 0:14:04.000
progress really slowly, okay? And so the watermark, how

241
0:14:04.000 --> 0:14:07.000
you produce output is always a trade-off in the streaming

242
0:14:07.000 --> 0:14:09.000
between completeness and latency. You need to make a

243
0:14:09.000 --> 0:14:15.000
decision, okay? So here we put an early trigger, so

244
0:14:15.000 --> 0:14:20.000
we are producing output soon, low latency, but it's

245
0:14:20.000 --> 0:14:22.000
not, it's incomplete, because well, so later on we are

246
0:14:22.000 --> 0:14:26.000
going to keep seeing numbers until the watermark.

247
0:14:26.000 --> 0:14:30.000
Good. So basically this is streaming in Apache Bin in

248
0:14:30.000 --> 0:14:33.000
10 minutes. This is a lot of information explained very

249
0:14:33.000 --> 0:14:38.000
quickly. If you want to get deeper, if you want to get

250
0:14:38.000 --> 0:14:41.000
deeper, there is this example here, okay? So in Java and

251
0:14:41.000 --> 0:14:44.000
in Python, so it's available in the two languages, and

252
0:14:44.000 --> 0:14:47.000
you can see everything that we have seen in the previous

253
0:14:47.000 --> 0:14:51.000
slides with all details, okay? And you may run this

254
0:14:51.000 --> 0:14:55.000
locally if you want, so you don't have to have like an

255
0:14:55.000 --> 0:14:59.000
environment, so like a cloud environment, a cluster or a

256
0:14:59.000 --> 0:15:02.000
stream processor or anything like that, so it may run

257
0:15:02.000 --> 0:15:09.000
locally with some synthetic data, made up data, okay?

258
0:15:09.000 --> 0:15:12.000
Now, this is the classic way of doing streaming in Apache

259
0:15:12.000 --> 0:15:15.000
Bin. This has been around for years already, okay? So

260
0:15:15.000 --> 0:15:18.000
this is the same model that is implemented in Spark, it's

261
0:15:18.000 --> 0:15:21.000
the same model that is implemented in Flink, so they

262
0:15:21.000 --> 0:15:25.000
are all kind of similar. There are other things that you

263
0:15:25.000 --> 0:15:31.000
can also do in Apache Bin in streaming, like anything that

264
0:15:31.000 --> 0:15:33.000
you can do in Apache Bin, you can also do it in

265
0:15:33.000 --> 0:15:35.000
streaming, and I'm going to highlight here a couple of

266
0:15:35.000 --> 0:15:39.000
those, okay? I'm leaving out a lot of stuff, because well,

267
0:15:39.000 --> 0:15:43.000
so time is limited, and leave it out, for instance, SQL,

268
0:15:43.000 --> 0:15:47.000
so that was a great talk by Timo focusing on SQL, so you

269
0:15:47.000 --> 0:15:50.000
can also do SQL in Apache Bin if you want in streaming,

270
0:15:50.000 --> 0:15:54.000
okay? So similar examples to what Timo did, and you can

271
0:15:54.000 --> 0:15:57.000
actually run that on Flink if you want, okay? So it may

272
0:15:57.000 --> 0:15:59.000
make sense if you want, I don't know, if at some point

273
0:15:59.000 --> 0:16:02.000
you want to move away from Flink to Dataflow, you want

274
0:16:02.000 --> 0:16:05.000
to move away from Dataflow to Spark, so in order to have

275
0:16:05.000 --> 0:16:08.000
this portability. One thing that you can do in streaming

276
0:16:08.000 --> 0:16:11.000
is stateful functions, and stateful functions are very

277
0:16:11.000 --> 0:16:16.000
interesting for windowing between quotes that doesn't

278
0:16:16.000 --> 0:16:19.000
depend on time. Very typically, I work with customers,

279
0:16:19.000 --> 0:16:21.000
it's like all this windowing triggers things, it's super

280
0:16:21.000 --> 0:16:25.000
interesting, but look, whenever I see a message of this

281
0:16:25.000 --> 0:16:29.000
type, I want to have all the messages that I have seen so

282
0:16:29.000 --> 0:16:32.000
far in a group and do these calculations, and I don't

283
0:16:32.000 --> 0:16:35.000
care about time, okay? I don't care about grouping things

284
0:16:35.000 --> 0:16:38.000
in time. I want to group things by some logic, okay? I'm

285
0:16:38.000 --> 0:16:42.000
going to give you a predicate, you pass a message, if the

286
0:16:42.000 --> 0:16:45.000
message fulfills a condition, I want to close the previous

287
0:16:45.000 --> 0:16:48.000
window and start a new one. How can you do that in

288
0:16:48.000 --> 0:16:51.000
Apache Vint? You can do that with stateful functions, okay?

289
0:16:51.000 --> 0:16:55.000
Stateful functions, so here we have some input, here we

290
0:16:55.000 --> 0:16:58.000
have a map, it's called a part of Apache Vint, and we do

291
0:16:58.000 --> 0:17:01.000
some transformation, and we want to accumulate a state

292
0:17:01.000 --> 0:17:05.000
here, okay? So depending on what we see at some point, we

293
0:17:05.000 --> 0:17:09.000
do something else, and this is mutable state, okay? In a

294
0:17:09.000 --> 0:17:13.000
system like Dataflow, like Flink, like all the systems

295
0:17:13.000 --> 0:17:19.000
where Apache Vint runs, having a state, mutable state in

296
0:17:19.000 --> 0:17:24.000
streaming that is computed in a consistent way is extremely

297
0:17:24.000 --> 0:17:29.000
difficult, okay? One way to shoot yourself in your feet

298
0:17:29.000 --> 0:17:33.000
with systems like this in streaming is trying to keep

299
0:17:33.000 --> 0:17:36.000
accumulating a state using some kind of external system,

300
0:17:36.000 --> 0:17:42.000
okay? Because runners will have, sometimes will have

301
0:17:42.000 --> 0:17:45.000
issues that will be errors, that will be retries,

302
0:17:45.000 --> 0:17:49.000
infrastructure will die, you will have auto scaling, there

303
0:17:49.000 --> 0:17:54.000
are all kinds of situations that the runner may want to

304
0:17:54.000 --> 0:17:57.000
retract the computation and recompute again, okay? So,

305
0:17:57.000 --> 0:18:00.000
and then in all these kinds of situations, having any kind

306
0:18:00.000 --> 0:18:03.000
of external system for mutable state, it's complex, okay?

307
0:18:03.000 --> 0:18:06.000
It's doable, okay? You may have, and you will have with

308
0:18:06.000 --> 0:18:08.000
Apache Vint in any kind of the runners that you can run, you

309
0:18:08.000 --> 0:18:12.000
will have this end-to-end exactly once processing, but

310
0:18:12.000 --> 0:18:15.000
this end-to-end exactly once processing doesn't mean that

311
0:18:15.000 --> 0:18:18.000
your code is going to be executed exactly once. It may

312
0:18:18.000 --> 0:18:21.000
be executed more than once, okay? This is what makes

313
0:18:21.000 --> 0:18:24.000
maintaining external state to a pipeline complex. But if

314
0:18:24.000 --> 0:18:28.000
the state is internal to the pipeline, then, well, so the

315
0:18:28.000 --> 0:18:32.000
system itself can, let's say, take care of the problems of

316
0:18:32.000 --> 0:18:35.000
reprocessing and maintain a mutable state in a consistent

317
0:18:35.000 --> 0:18:39.000
way. So, this is what it's a stateful function in Apache

318
0:18:39.000 --> 0:18:44.000
Vint, and you can use it for use cases like this, okay?

319
0:18:44.000 --> 0:18:49.000
For instance, say that I want to produce windows between

320
0:18:49.000 --> 0:18:53.000
codes based on some kind of property. So, I keep seeing

321
0:18:53.000 --> 0:18:56.000
messages, okay, that I keep processing, okay, and then I

322
0:18:56.000 --> 0:19:00.000
keep accumulating the messages in some state, okay? I

323
0:19:00.000 --> 0:19:03.000
maintain a buffer, like I keep every single message that I

324
0:19:03.000 --> 0:19:06.000
see, and I count, okay? Because, well, the buffer cannot,

325
0:19:06.000 --> 0:19:10.000
so the buffer must have some boundaries, okay? So, because

326
0:19:10.000 --> 0:19:12.000
this is local state that is maintaining the machine in the

327
0:19:12.000 --> 0:19:15.000
worker, in the executor where you are running, and the

328
0:19:15.000 --> 0:19:18.000
executor will have limited resources. Maybe very large

329
0:19:18.000 --> 0:19:22.000
resources, but limited anyways, okay? So, you keep

330
0:19:22.000 --> 0:19:26.000
accumulating, and then you keep processing here, for

331
0:19:26.000 --> 0:19:28.000
instance. So, typically, you can use this, for instance, for

332
0:19:28.000 --> 0:19:30.000
batching to call in an external service, but you can also

333
0:19:30.000 --> 0:19:34.000
do here, whenever I see a specific type of message, I

334
0:19:34.000 --> 0:19:37.000
emit some output, okay? I emit some output, and then I have

335
0:19:37.000 --> 0:19:40.000
applied a window. All the messages that I have in the

336
0:19:40.000 --> 0:19:44.000
buffer, I type a new session ID, a new window ID, and then

337
0:19:44.000 --> 0:19:47.000
I emit them. I hold them for a while until I see the right

338
0:19:47.000 --> 0:19:50.000
message that I need, and then I emit them. There are two

339
0:19:50.000 --> 0:19:55.000
problems here. We want to, so customers always think that

340
0:19:55.000 --> 0:19:59.000
streaming is complex, and they want to get away of all the

341
0:19:59.000 --> 0:20:04.000
temporal base calculations, okay? So complex, so messy.

342
0:20:04.000 --> 0:20:08.000
Look, my algorithm is really much simpler, but it is not.

343
0:20:08.000 --> 0:20:11.000
So, you are in the streaming, so you cannot ignore time,

344
0:20:11.000 --> 0:20:14.000
okay? You have situations where you will see the messages

345
0:20:14.000 --> 0:20:17.000
out of order, and you will have situations where you

346
0:20:17.000 --> 0:20:21.000
will not see the messages for a while, okay? And then you

347
0:20:21.000 --> 0:20:23.000
need to decide what to do in these two cases, even if you

348
0:20:23.000 --> 0:20:26.000
don't want to, okay? What happens when I see out of

349
0:20:26.000 --> 0:20:29.000
order? You may say, I don't care, unlikely, but well, in

350
0:20:29.000 --> 0:20:33.000
some situations, it might be true, okay? Or you may have

351
0:20:33.000 --> 0:20:38.000
to wait, like, some timer in order to give room for late

352
0:20:38.000 --> 0:20:43.000
data to arrive into your code and actually produce the

353
0:20:43.000 --> 0:20:46.000
actual output, okay? So this will be an event time timer,

354
0:20:46.000 --> 0:20:53.000
okay? Look, in event time, you are going to see the messages

355
0:20:53.000 --> 0:20:57.000
and order, okay? So wait 30 seconds, 2 minutes, and so on.

356
0:20:57.000 --> 0:21:00.000
And then when you have seen all the messages, it's the moment

357
0:21:00.000 --> 0:21:02.000
in which you apply the session. That's called an event

358
0:21:02.000 --> 0:21:05.000
time timer. And you may have also problems of staleness,

359
0:21:05.000 --> 0:21:08.000
okay? I'm waiting for the end of my session, but I have not

360
0:21:08.000 --> 0:21:12.000
seen messages. I haven't seen messages in the last 5

361
0:21:12.000 --> 0:21:15.000
minutes in processing time, okay? The problem with event

362
0:21:15.000 --> 0:21:18.000
time is that depends on the progress of the watermark. But

363
0:21:18.000 --> 0:21:20.000
if you stop seeing messages, the watermark will stop

364
0:21:20.000 --> 0:21:25.000
advancing. The watermark is always estimated in being

365
0:21:25.000 --> 0:21:29.000
runners or normally estimated as the time stamp of the

366
0:21:29.000 --> 0:21:32.000
oldest message waiting to be processed, okay? So literally

367
0:21:32.000 --> 0:21:35.000
you may stop your pipeline waiting forever for some data

368
0:21:35.000 --> 0:21:39.000
that maybe it will never arrive, okay? So processing time

369
0:21:39.000 --> 0:21:42.000
timer solved this problem, okay? After 10 minutes, like,

370
0:21:42.000 --> 0:21:45.000
measure with a clock, if nothing comes, I don't care

371
0:21:45.000 --> 0:21:48.000
about the watermark. I don't want to keep going, okay? So

372
0:21:48.000 --> 0:21:51.000
keep going. So data has been lost for whatever reason, and

373
0:21:51.000 --> 0:21:55.000
we cannot wait forever. So this is a stateful function, and

374
0:21:55.000 --> 0:21:58.000
it's also very useful in streaming because it allows you

375
0:21:58.000 --> 0:22:01.000
to apply logic that goes beyond the temporal properties

376
0:22:01.000 --> 0:22:05.000
that we have seen in the previous slides. And here you

377
0:22:05.000 --> 0:22:08.000
have some examples and links. The slides are already

378
0:22:08.000 --> 0:22:11.000
available through the first time website, so I encourage

379
0:22:11.000 --> 0:22:15.000
you to have a look at these examples. What else can I do

380
0:22:15.000 --> 0:22:19.000
in streaming? Matched learning inference, okay? So there

381
0:22:19.000 --> 0:22:23.000
are many ways to do matched learning inference in

382
0:22:23.000 --> 0:22:26.000
streaming at a scale, okay? Many of those quite expensive.

383
0:22:26.000 --> 0:22:30.000
So you can deploy endpoints in cloud platforms, with

384
0:22:30.000 --> 0:22:35.000
GPUs, with a lot of stuff, okay? And normally, so well,

385
0:22:35.000 --> 0:22:38.000
so those are, those solve a lot of functionality for you,

386
0:22:38.000 --> 0:22:41.000
but they are expensive. So what if you want to apply

387
0:22:41.000 --> 0:22:45.000
matched learning inference in a pipeline in Apache Bin?

388
0:22:45.000 --> 0:22:47.000
Well, you could do that, okay? You could be thinking,

389
0:22:47.000 --> 0:22:49.000
well, I can do that, so I can, I don't know, like importance

390
0:22:49.000 --> 0:22:53.000
of flow, load the model, apply it, so you could do a lot

391
0:22:53.000 --> 0:22:56.000
of stuff, okay, yourself. But this is already solved for

392
0:22:56.000 --> 0:22:59.000
you in Apache Bin, okay? So you can run matched learning

393
0:22:59.000 --> 0:23:05.000
inference with the so-called run inference transform, okay?

394
0:23:05.000 --> 0:23:09.000
So we see it here. So right now, it has, let's say, out-of-the-box

395
0:23:09.000 --> 0:23:13.000
support for PyTorch, TensorFlow, and scikit-learn, with more

396
0:23:13.000 --> 0:23:17.000
coming. When you're running a distributed system, and you

397
0:23:17.000 --> 0:23:20.000
want to apply a model, each one of the workers in this

398
0:23:20.000 --> 0:23:24.000
distributed system will have its own memory. So the Apache

399
0:23:24.000 --> 0:23:27.000
Bin runs on top of shared nothing architecture, okay?

400
0:23:27.000 --> 0:23:30.000
Like Flink, Dataflow, Spark. Workers are independent of

401
0:23:30.000 --> 0:23:34.000
each other. They don't share any common state. The state

402
0:23:34.000 --> 0:23:38.000
that we have seen before is actually maintained per key

403
0:23:38.000 --> 0:23:41.000
and per window if we apply it. The per window is totally

404
0:23:41.000 --> 0:23:44.000
local for the worker, and two workers cannot share a state.

405
0:23:44.000 --> 0:23:47.000
But the model, we don't want to instantiate a model if we

406
0:23:47.000 --> 0:23:51.000
have 100 workers 100 times, because the model is going to

407
0:23:51.000 --> 0:23:54.000
be the same for every worker, right? So the model doesn't

408
0:23:54.000 --> 0:23:58.000
change. The model is actually read-only. Run inference

409
0:23:58.000 --> 0:24:01.000
solving this problem by having some state that is shared

410
0:24:01.000 --> 0:24:03.000
across all the workers, and it's transparent for you.

411
0:24:03.000 --> 0:24:05.000
So this is something that you can always implement in a

412
0:24:05.000 --> 0:24:08.000
distributed system, but it's complex. This is the problem

413
0:24:08.000 --> 0:24:11.000
that is solved with run inference. It's only one copy of

414
0:24:11.000 --> 0:24:14.000
the model per, let's say, machine where you're running,

415
0:24:14.000 --> 0:24:18.000
okay? So in memory, okay? Because what you need always

416
0:24:18.000 --> 0:24:21.000
to make an instance in memory to be able to apply it.

417
0:24:21.000 --> 0:24:24.000
But if you have, I don't know, like 100 threads, 100

418
0:24:24.000 --> 0:24:28.000
sub-workers, 100 CPUs inside the same machine, you will

419
0:24:28.000 --> 0:24:32.000
not have 100 copies of the model. Regardless of the

420
0:24:32.000 --> 0:24:35.000
computation model of how the runner is implemented on top

421
0:24:35.000 --> 0:24:37.000
of the machines. There will be only one copy in the

422
0:24:37.000 --> 0:24:40.000
memory of the machine. So this is the problem that is solved

423
0:24:40.000 --> 0:24:44.000
with run inference, okay? So if you want to apply a

424
0:24:44.000 --> 0:24:48.000
streaming inference, it's a very convenient way of doing

425
0:24:48.000 --> 0:24:52.000
this with very little code. And depending on the runner,

426
0:24:52.000 --> 0:24:55.000
this is a possibility in data flow. This is also a

427
0:24:55.000 --> 0:24:58.000
possibility if you are running on top of Kubernetes in a

428
0:24:58.000 --> 0:25:00.000
runner that supports Kubernetes, like, for instance,

429
0:25:00.000 --> 0:25:04.000
Flink. You can do also hinting of the resources that

430
0:25:04.000 --> 0:25:07.000
your transformation is going to need, okay? Hinting of

431
0:25:07.000 --> 0:25:09.000
transformations could be, look, this transformation is

432
0:25:09.000 --> 0:25:12.000
going to need this amount of memory, minimal, okay? But

433
0:25:12.000 --> 0:25:16.000
hinting could also be, look, this is a step that is

434
0:25:16.000 --> 0:25:20.000
running ML inference. So use a GPU for this step, okay?

435
0:25:20.000 --> 0:25:23.000
And then the runner will take care of making sure that

436
0:25:23.000 --> 0:25:26.000
the step that is running, the virtual machine, the same

437
0:25:26.000 --> 0:25:29.000
matches the infrastructure hints that you provide for

438
0:25:29.000 --> 0:25:32.000
the code, and you will have, let's say, different types

439
0:25:32.000 --> 0:25:35.000
of nodes for different types of transformations. One of

440
0:25:35.000 --> 0:25:37.000
the problems of shared-nath in architectures is that

441
0:25:37.000 --> 0:25:40.000
all the workers are alike. All the workers are the same.

442
0:25:40.000 --> 0:25:42.000
With this, you can have different types of workers for

443
0:25:42.000 --> 0:25:45.000
different kinds of transformations, which, let's

444
0:25:45.000 --> 0:25:50.000
say, in terms of cost, it's better. I don't have to

445
0:25:50.000 --> 0:25:53.000
say optimal, so maybe that's better alternatives. But

446
0:25:53.000 --> 0:25:56.000
basically, you use GPUs in the workers where you need to

447
0:25:56.000 --> 0:25:59.000
use GPUs. You don't use them in the workers where you

448
0:25:59.000 --> 0:26:01.000
don't need to use them. And you don't have to worry

449
0:26:01.000 --> 0:26:04.000
about assigning work to different workers. That's

450
0:26:04.000 --> 0:26:06.000
actually done by the runner automatically with these

451
0:26:06.000 --> 0:26:11.000
hints, okay? If you want to know more, here you have

452
0:26:11.000 --> 0:26:14.000
some links. So I have only five minutes left, so I'm

453
0:26:14.000 --> 0:26:17.000
leaving the best for the end of the presentation.

454
0:26:17.000 --> 0:26:21.000
Great. You, look, Israel, you showed some Java at the

455
0:26:21.000 --> 0:26:24.000
beginning. Now you tell me ML inference is so cool, but

456
0:26:24.000 --> 0:26:28.000
it's Python, right? So it's PyTorch, TensorFlow,

457
0:26:28.000 --> 0:26:31.000
Second-Land, it's all Python. One of the things that you

458
0:26:31.000 --> 0:26:35.000
can do in Apache VIN is in cross-language transforms.

459
0:26:35.000 --> 0:26:39.000
Anything that you have available in any of the SDKs,

460
0:26:39.000 --> 0:26:42.000
in any language, you may use it in any other language.

461
0:26:42.000 --> 0:26:45.000
As long as the runner supports this, okay? So it's not

462
0:26:45.000 --> 0:26:48.000
supported by all the runners, but it's supported by the

463
0:26:48.000 --> 0:26:52.000
main runners, okay? So basically, run inference may be

464
0:26:52.000 --> 0:26:56.000
used in Java. If you want to use any transformation from

465
0:26:56.000 --> 0:27:00.000
any language, you have to add some boilerplate code, not

466
0:27:00.000 --> 0:27:03.000
so much, but a little bit. This is already done, let's

467
0:27:03.000 --> 0:27:06.000
say, for the main transforms that are most popular or

468
0:27:06.000 --> 0:27:09.000
that they say that make more sense to be used in

469
0:27:09.000 --> 0:27:12.000
different languages, like a map. Well, using a map from

470
0:27:12.000 --> 0:27:16.000
Java in Python doesn't really make sense, okay? Using

471
0:27:16.000 --> 0:27:20.000
run inference makes using connectors, input-output

472
0:27:20.000 --> 0:27:23.000
connectors from another SDK in Python, for instance,

473
0:27:23.000 --> 0:27:27.000
makes sense because the amount of input-output

474
0:27:27.000 --> 0:27:31.000
connectors that you have per SDK is not the same. So you

475
0:27:31.000 --> 0:27:34.000
may write, I don't know, like the two databases in

476
0:27:34.000 --> 0:27:39.000
Java and two message queues in Python, but maybe you

477
0:27:39.000 --> 0:27:42.000
don't have the same functionalities in all the SDKs.

478
0:27:42.000 --> 0:27:45.000
You can use any connector from any SDK, and this makes

479
0:27:45.000 --> 0:27:48.000
it quite flexible, okay? So these are the so-called

480
0:27:48.000 --> 0:27:51.000
multi-language pipelines, and basically it means that

481
0:27:51.000 --> 0:27:55.000
you can run any transformation in any SDK, and this

482
0:27:55.000 --> 0:27:59.000
is implemented because the runner environment is

483
0:27:59.000 --> 0:28:01.000
containerized, okay? So there's a container per

484
0:28:01.000 --> 0:28:03.000
language, and there's some magic that makes, let's

485
0:28:03.000 --> 0:28:06.000
say, the container communicates between the

486
0:28:06.000 --> 0:28:09.000
Nsals, okay? And the serialization between

487
0:28:09.000 --> 0:28:12.000
programming languages. So this is part of the boilerplate

488
0:28:12.000 --> 0:28:15.000
that you need to take care of. If you use things like

489
0:28:15.000 --> 0:28:18.000
Apache B, the schemas that I haven't talked about in

490
0:28:18.000 --> 0:28:21.000
this talk, so it will be transparent for you, anything

491
0:28:21.000 --> 0:28:23.000
that you have in one schema in one language, you will

492
0:28:23.000 --> 0:28:25.000
be able to serialize it, and they serialize it to any

493
0:28:25.000 --> 0:28:28.000
other language. So if you follow, let's say, if you

494
0:28:28.000 --> 0:28:31.000
follow the Apache B in Capstone, it's quite

495
0:28:31.000 --> 0:28:34.000
straightforward to use these kind of things. Well,

496
0:28:34.000 --> 0:28:38.000
thanks, everyone, so far for your attention. So here

497
0:28:38.000 --> 0:28:41.000
and almost there, don't, here are some links that I

498
0:28:41.000 --> 0:28:44.000
recommend you to have a look if you want to learn

499
0:28:44.000 --> 0:28:48.000
more about Apache B. I have covered a lot of stuff in

500
0:28:48.000 --> 0:28:51.000
a very short time, okay? So there's a lot of things

501
0:28:51.000 --> 0:28:54.000
behind everything that I have explained here. If you

502
0:28:54.000 --> 0:28:57.000
want to know more about all the windowing, streaming,

503
0:28:57.000 --> 0:29:01.000
trigger, triggers, watermarks, and so on, I strongly

504
0:29:01.000 --> 0:29:04.000
recommend you this book. It was released some time

505
0:29:04.000 --> 0:29:06.000
ago. You may think that it's outdated. It's not

506
0:29:06.000 --> 0:29:09.000
outdated. So let's say this is the same model that is

507
0:29:09.000 --> 0:29:11.000
applied in many different streaming systems, and this

508
0:29:11.000 --> 0:29:14.000
is not a book about Apache B. It's a book about the

509
0:29:14.000 --> 0:29:17.000
streaming systems with lots of examples coming from

510
0:29:17.000 --> 0:29:21.000
Apache B. But also examples coming from Flink, Kafka,

511
0:29:21.000 --> 0:29:23.000
PubSub, and many other systems, and it's really, it's

512
0:29:23.000 --> 0:29:27.000
very interesting. It's my favorite book, one of my

513
0:29:27.000 --> 0:29:30.000
favorite books, the other one being the book actually

514
0:29:30.000 --> 0:29:34.000
from Martin Kledman about data intensive applications.

515
0:29:34.000 --> 0:29:37.000
And if you want to know more about BIN, so I recommend

516
0:29:37.000 --> 0:29:40.000
you, the BIN College. There are lots of videos with

517
0:29:40.000 --> 0:29:43.000
lots of details about the things that I have explained

518
0:29:43.000 --> 0:29:47.000
here in YouTube. Some of them are actually linked in

519
0:29:47.000 --> 0:29:50.000
the slides. For sure, the main site of Apache BIN and

520
0:29:50.000 --> 0:29:52.000
the programming guide and all the documentation that is

521
0:29:52.000 --> 0:29:57.000
there. And if you want to learn more about Apache BIN,

522
0:29:57.000 --> 0:29:59.000
there is also the videos of Apache BIN Summit, the

523
0:29:59.000 --> 0:30:02.000
previous editions. And if you want to participate, if

524
0:30:02.000 --> 0:30:04.000
you are here today, so you may be interested in

525
0:30:04.000 --> 0:30:07.000
streaming. So the call for papers is open until March

526
0:30:07.000 --> 0:30:12.000
20th. I think BIN Summit will be in June in New York.

527
0:30:12.000 --> 0:30:15.000
And well, I encourage you to submit talks.

528
0:30:15.000 --> 0:30:17.000
Well, this is all. So thanks all for your attention.

529
0:30:17.000 --> 0:30:19.000
It's time for questions.

530
0:30:19.000 --> 0:30:47.000
So what's the advantage of fishing BIN if you are

531
0:30:47.000 --> 0:30:51.000
already using Flink? So it's portability mainly. So if

532
0:30:51.000 --> 0:30:55.000
tomorrow you want to move away from Flink for whatever

533
0:30:55.000 --> 0:30:59.000
reason, so you should be able to move to other runners

534
0:30:59.000 --> 0:31:01.000
that have the same level of functionality, like for

535
0:31:01.000 --> 0:31:04.000
instance, Dataflow. I don't know. So we have one of

536
0:31:04.000 --> 0:31:07.000
the main committers here of Apache Flink. Say that he

537
0:31:07.000 --> 0:31:11.000
gets hit by a bus. We don't want that to happen. But

538
0:31:11.000 --> 0:31:16.000
that may happen. Everything may happen. So the way

539
0:31:16.000 --> 0:31:19.000
you know the world is really very uncertain. So

540
0:31:19.000 --> 0:31:22.000
basically you have portability.

541
0:31:22.000 --> 0:31:24.000
Thank you very much. Unfortunately we don't have

542
0:31:24.000 --> 0:31:27.000
time for more questions right now, but I'm sure you

543
0:31:27.000 --> 0:31:29.000
will be happy to answer it.

544
0:31:29.000 --> 0:31:47.000
Thanks.

