1
0:00:00.000 --> 0:00:10.800
Thanks very much. Thank you. So welcome everyone and I'm glad that you're here on Saturday

2
0:00:10.800 --> 0:00:16.320
early morning in this first session. So I would like to make it as easy as possible.

3
0:00:16.320 --> 0:00:24.320
Thanks for the O'Gallantas, Jervis and Yarmur for inviting me to talk today about stream processing.

4
0:00:24.320 --> 0:00:28.800
The fact is I don't know your background so I'm not sure exactly how much experience

5
0:00:28.800 --> 0:00:34.960
you have with real time stream processing. So if you see some concepts are easy, just get everyone

6
0:00:34.960 --> 0:00:40.160
after a few of these concepts. So I'll be talking today about real time stream processing on

7
0:00:40.160 --> 0:00:46.800
additive and there is one of the blocks. So that's what the main focus will be. Obviously this title

8
0:00:46.800 --> 0:00:55.600
as it is could be a startup company. So you would expect to have some ideas today where you can use

9
0:00:55.600 --> 0:01:01.600
some of these ideas in your world or in your experience or in your days of study, whatever

10
0:01:01.600 --> 0:01:08.960
you want. Whether you are a Java developer or data scientist or ML ops, so it doesn't matter.

11
0:01:08.960 --> 0:01:13.360
So there is something for everyone here today. So that's the main focus for this session.

12
0:01:14.000 --> 0:01:18.640
So anyone recognize these guys on the screen here?

13
0:01:18.640 --> 0:01:26.960
Right, so that's where I came from. I'm based in Liverpool in the UK and on the right side is the

14
0:01:28.160 --> 0:01:36.320
Liverpool for Coqola which is basically one of the top football teams in the UK. So I want

15
0:01:36.320 --> 0:01:41.040
just to highlight this screen here just to tell you that's real time stream processing is not

16
0:01:41.040 --> 0:01:48.880
in a specific domain, it could be in any domain. If you look at it, do you know how long it takes

17
0:01:48.880 --> 0:02:00.880
for example for an eye to blink? Come again. Yeah, so it takes over half one third of seconds.

18
0:02:00.880 --> 0:02:07.360
So that's pretty fast. So if you're thinking about like maybe minutes or hours, probably this is not

19
0:02:07.360 --> 0:02:13.040
the right discussion room for you. We're talking about sub-milliseconds today. So whether it's for

20
0:02:13.040 --> 0:02:18.480
example you use it in finance, whether you use it in IOD devices, smart devices, whether you use it

21
0:02:18.480 --> 0:02:24.080
in sports, hospitals or machine learning or what we're trying to do today for stream processing.

22
0:02:24.080 --> 0:02:30.080
So that's the main idea and obviously if you're working with real time stream processing you focus

23
0:02:30.080 --> 0:02:37.840
on the real time data right? And I've seen it so many times where platforms and tools focus on

24
0:02:37.840 --> 0:02:43.600
you know how much data you can process and you see these benchmarks everywhere on the internet

25
0:02:43.600 --> 0:02:50.880
and this is pretty cool I think but the key source and the secret source for this is to use something

26
0:02:50.880 --> 0:02:57.680
in combination between real time data and historical data. So the main reason for this is

27
0:02:57.680 --> 0:03:04.240
to look at context. So without you know knowing what's going on, you probably don't benefit much

28
0:03:04.240 --> 0:03:10.560
from the real time data you're processing. So what you want is always to go back and check what's

29
0:03:10.560 --> 0:03:17.760
going on with the context of this data. There is a problem in this script source obviously because

30
0:03:17.760 --> 0:03:23.280
what you're looking at is kind of like you know two different data types and you want to make

31
0:03:23.280 --> 0:03:29.360
sure that you process it at the same speed or you know very close to the same speed. Obviously it

32
0:03:29.360 --> 0:03:37.120
becomes really problem when you try to scale it. So if you have I don't know like maybe a few

33
0:03:37.120 --> 0:03:42.560
cases of data that you want to process probably it's not too much topic for you but when you

34
0:03:42.560 --> 0:03:47.520
start to scale it up it becomes really problem you know to understand how you want to scale it.

35
0:03:47.520 --> 0:03:53.600
So do you scale your data or do you scale your compute or do you scale both and at what speed.

36
0:03:54.160 --> 0:04:00.960
So we will discuss all these concepts today and if I ask you now how much did you process? Probably

37
0:04:02.000 --> 0:04:07.040
because in this room I would assume you have over million transactions per second or few

38
0:04:07.040 --> 0:04:12.400
millions or I don't know some of you might be processing billions of transactions per second.

39
0:04:12.400 --> 0:04:19.520
So that's pretty good and what we want today is to focus this domain into very

40
0:04:20.560 --> 0:04:26.240
specific area and this area essentially what we're trying to do today is to analyze traces. So

41
0:04:26.880 --> 0:04:31.200
it doesn't matter if it is like operating system traces or platform traces or it's

42
0:04:31.200 --> 0:04:35.920
like programming language traces what we want is to make sure that we have environment

43
0:04:35.920 --> 0:04:43.520
and within this environment you can scale your logs basically scale your processing

44
0:04:43.520 --> 0:04:49.440
and at the same time provide some kind of analytics right. So again if you look at how

45
0:04:49.440 --> 0:04:54.960
much data you can process the number by itself doesn't give you much what's going on here.

46
0:04:54.960 --> 0:04:59.200
So what you want is to find this specific information you're looking for

47
0:04:59.200 --> 0:05:06.400
or kind of like looking at finding the needles or finding the hidden areas within your data.

48
0:05:06.400 --> 0:05:14.800
So if you look at how much logs you process per day or per week and you store it somewhere

49
0:05:14.800 --> 0:05:02.680
on you know

50
0:05:15.440 --> 0:05:21.840
official hard drive or you store it in mori or you store it in the cloud. So what you want is

51
0:05:21.840 --> 0:05:29.440
to you know make sense of it and some companies do this process manually which means they run

52
0:05:29.440 --> 0:05:37.520
software and they go through their logs and this is kind of a patch service and they try to

53
0:05:37.520 --> 0:05:43.440
understand what's going on within the logs. So obviously this is a problem when you want to scale

54
0:05:43.440 --> 0:05:50.160
it and with the scaling you have different logs stored in different places and you want to make

55
0:05:50.160 --> 0:05:58.080
sure basically to have a platform where in this platform we kind of like looking at some kind of

56
0:05:58.080 --> 0:06:06.160
results. So for the sake of this discussion today we'll focus on two different solutions. So one of

57
0:06:06.160 --> 0:06:12.000
a kind of like to provide some kind of alerts and the other is provide some kind of trends within

58
0:06:12.000 --> 0:06:20.960
your data. Obviously I work for a company called Hezakast. So Hezakast as a platform allows you to

59
0:06:20.960 --> 0:06:27.200
do so but obviously you might have heard of some companies or you know they do some kind of stream

60
0:06:27.200 --> 0:06:33.520
processing. So this is kind of like you know overview what's going on with this domain at this

61
0:06:33.520 --> 0:06:40.880
time. Obviously you can split it depending on you're looking for open source solution or

62
0:06:40.880 --> 0:06:46.960
I don't know, hardware solution or you know some kind of management service and what you look at

63
0:06:46.960 --> 0:06:52.480
is which domain you are. So are you looking to capture your data or some kind of you know

64
0:06:52.480 --> 0:06:58.480
streaming your data or you want to do some kind of transformation on your data or do some kind

65
0:06:58.480 --> 0:07:05.760
of analytical machine learning. So you can see that you split it into 12 squares and within this

66
0:07:05.760 --> 0:07:12.800
where these like tools and platforms are you know spread it over. So obviously some tools exist on

67
0:07:12.800 --> 0:07:19.600
this screen for whatever reason but obviously this might give you some ideas but it's hard

68
0:07:19.600 --> 0:07:26.800
it's hard to decide which tool you want to go for. Simply because I think the distribution is not

69
0:07:26.800 --> 0:07:33.920
clear here so it tells you basically which tool is open source for example and where in process you

70
0:07:33.920 --> 0:07:38.800
can use it but it doesn't give you a full picture on you know how to do it in practical terms.

71
0:07:39.440 --> 0:07:45.600
And so this is where it might be easier to understand what we're talking about. So if you

72
0:07:45.600 --> 0:07:51.920
remember from my slide where I discussed the historical data and the new data. So today we're

73
0:07:51.920 --> 0:07:58.800
kind of like you know trying to split everything into two categories. So on one side you get like

74
0:07:58.800 --> 0:08:05.440
stream processing engines so these engines are pretty fast in you know streaming events

75
0:08:05.440 --> 0:08:11.840
and on this far right side you have some kind of fast data stores which are you know are pretty

76
0:08:11.840 --> 0:08:19.600
fast in handling data at speed. So again the solution for break time stream processing

77
0:08:20.560 --> 0:08:27.680
is kind of combination and you want to process data in this moment and at the same time you want

78
0:08:27.680 --> 0:08:34.960
to actually also access data stored somewhere. So that's where is a cast fit into this area here.

79
0:08:35.680 --> 0:08:41.520
So the platform itself obviously for those who don't know by the way we have one of the masterminds

80
0:08:41.520 --> 0:08:48.000
of Hezakast sitting in this room. So this is the platform so it's open source platform it doesn't

81
0:08:48.000 --> 0:08:53.840
matter where your shop is coming from whether it's a passion car or passion board. So IOT devices

82
0:08:53.840 --> 0:08:59.520
for example I don't know some kind of enterprise applications or even like within Hezakast or even

83
0:08:59.520 --> 0:09:05.760
you can write your own connector and you feed it into the platform. So platform historically used

84
0:09:05.760 --> 0:09:13.840
to be two different components so the IMPG and the JET engine and essentially now it's all packaged

85
0:09:13.840 --> 0:09:22.800
in one one jar file. As you see here it allows you to load your data from R-Biscuits into memory so

86
0:09:22.800 --> 0:09:31.840
you have access to external data and pretty much like instantaneously and this will where you know

87
0:09:31.840 --> 0:09:38.560
you can provide context what's going on with your data and at the same time you can actually do

88
0:09:38.560 --> 0:09:44.080
stream processing so that's what JET engine is. So from here you can do some kind I don't know

89
0:09:44.080 --> 0:09:49.840
maybe like data transformation or do some kind of stream processing as we will do today or even

90
0:09:49.840 --> 0:09:55.600
like define machine learning if you want to. You can connect it to some clients so these are some

91
0:09:55.600 --> 0:10:02.400
clients here so written in various languages. If you're from data science background which means

92
0:10:02.400 --> 0:10:07.360
your programming languages in general are not preferred for you so you might be considered

93
0:10:07.360 --> 0:10:14.640
using SQL to do what I'm planning today so this is another option you can do and once you process

94
0:10:14.640 --> 0:10:21.760
this data where you load it into memory for a JET file data and at the same time you have some

95
0:10:21.760 --> 0:10:26.720
kind of data coming in. In CASCAT topic for example and you do the combination or even

96
0:10:26.720 --> 0:10:33.440
you do transformation you can proceed to do some kind of visualization. So the good thing about

97
0:10:33.440 --> 0:10:40.480
HESECAS in general when it comes to scaling is it's partition aware so which means basically

98
0:10:40.480 --> 0:10:47.680
your compute your JET engine or your process essentially can be or can detect where your data

99
0:10:47.680 --> 0:10:54.320
is stored so this is like you know we are trying to have as low latency as possible when it comes

100
0:10:54.320 --> 0:11:00.400
to processing this data. So this is very important to understand because latency is your enemy when

101
0:11:00.400 --> 0:11:07.360
it comes to stream processing so what you want is kind of like having a platform where you avoid

102
0:11:07.360 --> 0:11:14.400
network folks for example you avoid IO to your hard disk you try to also avoid every time or

103
0:11:15.760 --> 0:11:20.400
context switching between threads so you want to avoid all of these but at the same time you want

104
0:11:20.400 --> 0:11:27.280
your process to be as close as possible to your data. You can apply some kind of you know machine

105
0:11:27.280 --> 0:11:34.960
learning on this and the scaling itself could be done in various ways so the main thing to take

106
0:11:34.960 --> 0:11:44.160
away from here is this no master worker relationship so all nodes basically are peers and we've done

107
0:11:44.160 --> 0:11:50.720
this study it's a bit outdated but it's kind of like 1 billion transactions per second on 45 nodes

108
0:11:51.280 --> 0:11:57.920
so what we're trying to do now is to add one zero into this number here and even though it's pretty

109
0:11:57.920 --> 0:12:04.320
impressive what is nice about it is the linear scaling which means more data you can add you

110
0:12:04.320 --> 0:12:11.280
know more nodes into it so that's the historical bit of this talk so let's just move to the technical

111
0:12:11.280 --> 0:12:17.120
part so for this demo what I wanted is kind of like you know show you some some ideas right

112
0:12:17.120 --> 0:12:22.000
so you should be able to take these ideas and apply it you know anywhere obviously the solution

113
0:12:22.000 --> 0:12:28.160
as itself would be like you know project by itself so feel free to edit and change it so all

114
0:12:28.160 --> 0:12:34.800
source code is available on github and the documentation as well so you can go through it

115
0:12:34.800 --> 0:12:41.280
so the main idea when it comes to analyzing or you know making sense out of your traces and

116
0:12:41.280 --> 0:12:49.200
logs is to store it somewhere close to you know you will compute first of all and it shouldn't

117
0:12:49.200 --> 0:12:54.000
be stored locally right so you want to store it first of all the first thing is to store on the

118
0:12:54.000 --> 0:13:00.480
cloud so for this for the sake of this demo what I'm doing is I'm storing everything onto the cloud

119
0:13:00.480 --> 0:13:05.040
so there is a solution called hazelgas hurricane which is kind of like service so you don't need

120
0:13:05.040 --> 0:13:10.960
to download jar run your relic you can simply click and play so you can create that cloud

121
0:13:10.960 --> 0:13:16.640
you run everything I'm discussing today so you create an instance of hazelgas and from there

122
0:13:16.640 --> 0:13:23.120
you can pretty much proceed to what I'm planning to do so the first option we were talking about

123
0:13:23.120 --> 0:13:29.040
is kind of like storing everything into the cloud so once we have all the data obviously we need

124
0:13:29.040 --> 0:13:37.120
some kind of you know trace message which makes sense so this trace message could be you know

125
0:13:37.120 --> 0:13:41.680
changed based on how you want to approach it right so for example if you're working with machine

126
0:13:41.680 --> 0:13:46.480
learning you probably look for some kind of I don't know classification solution for you

127
0:13:46.480 --> 0:13:53.120
you know for your tests or you could be looking for NLP if you don't want to work with machine

128
0:13:53.120 --> 0:13:58.880
learning you probably want to look for some kind of trends so you look for processing your data

129
0:13:58.880 --> 0:14:04.240
doesn't matter if you're using machine learning in this case you want to have some kind of data

130
0:14:04.240 --> 0:14:10.400
stored somewhere so it could be in JSON format or it could be like bar charge swing so it depends

131
0:14:10.400 --> 0:14:18.320
again how much the speed is important to you so the option first option is to go through the

132
0:14:18.320 --> 0:14:25.760
alerts so in alerts what we're trying to do here is to take everything and start in that cloud so

133
0:14:25.760 --> 0:14:31.600
obviously we don't store it on the cloud on our basis what we try to do is store it in memory

134
0:14:33.280 --> 0:14:38.880
my preference in this case is to use some kind of map structure so map structure allows you to

135
0:14:38.880 --> 0:14:46.720
essentially random access and rebalance between various nodes within your cluster and at the same

136
0:14:46.720 --> 0:14:53.280
time you want to have some key value so in order to know where this node is coming from so in this

137
0:14:53.280 --> 0:15:00.080
case could be like ID address for example so this is where and report number so as key and the value

138
0:15:00.080 --> 0:15:05.600
could be anything that makes sense to you so in this case for example you can track level of this

139
0:15:05.600 --> 0:15:12.240
error sorry this level and message for example if you want to do some kind of NLP processing

140
0:15:12.960 --> 0:15:19.360
and some kind of you know process or thread name on this obviously once you have your

141
0:15:19.360 --> 0:15:26.720
dm value what you can do is proceed and store it into memory so this is where you get this

142
0:15:26.720 --> 0:15:33.520
set to has a task and what we're trying to do is create the IMAP and once you have the IMAP

143
0:15:33.520 --> 0:15:39.360
means you should be able to store it you know access and do stream processing as I will show you

144
0:15:39.920 --> 0:15:46.720
so first message is to store it into the cloud store it in memory in this case i'm using

145
0:15:46.720 --> 0:15:53.280
gazakas and i'm using IMAP and second stage is to do the stream processing right so there are a

146
0:15:53.280 --> 0:16:00.080
couple of options here for you so first option is to use SQL so SQL is built within gazakas which

147
0:16:00.080 --> 0:16:04.800
means or on top of gazakas which means you should be able to work your data so you provide some kind

148
0:16:04.800 --> 0:16:10.800
of specific messages that you're looking for obviously depends on your input you can do some

149
0:16:10.800 --> 0:16:17.600
kind of SQL so whether it's in your joy for example or search and so on or the other option is to do

150
0:16:17.600 --> 0:16:23.440
some kind of prediction so you're getting some logs you don't know exactly what's going on to

151
0:16:23.440 --> 0:16:29.920
happen next and you try to predict to provide some kind of you know alerts or trends so we need to

152
0:16:29.920 --> 0:16:36.160
build the trends so in order to do this what I did is kind of like use the same key but for my

153
0:16:37.280 --> 0:16:45.760
value what I'm using is some kind of log score so log score is not important what I'm saying here

154
0:16:45.760 --> 0:16:54.240
is I want to give value for is every single message or every single log message so this could be for

155
0:16:54.240 --> 0:17:02.240
example how you know how important this specific message is for you or could be for example how

156
0:17:02.240 --> 0:17:09.840
serious or how you know dangerous the message is so as levels in logs you can define score so

157
0:17:09.840 --> 0:17:16.160
instead of having four levels for example you can spread it either from one one to one hundred

158
0:17:16.160 --> 0:17:21.040
so this should give you some kind of predictions why because if you have for example 10 messages

159
0:17:21.040 --> 0:17:29.440
10 local messages or for example warning you don't know exactly if the 11 matches will be warning or

160
0:17:29.440 --> 0:17:35.600
not if you want to predict it obviously it doesn't give you how much will be warning or not whereas

161
0:17:35.600 --> 0:17:44.640
if you use some kind of numerical value you can get as close as possible to this so we get key

162
0:17:44.640 --> 0:17:53.040
from there we get score from here and what we do next is to do some kind of predictions on the logs

163
0:17:53.040 --> 0:17:59.600
so in this process what we have is exactly my key and the value which is like the score on each log

164
0:17:59.600 --> 0:18:07.200
message and I imported into Helical so Helical allows you to basically input and output from two

165
0:18:07.200 --> 0:18:15.120
different maps and do stream processing so we'll build the train based on previous logs

166
0:18:16.560 --> 0:18:23.440
scores and we use the prediction on top of it to provide some kind of alert so zero means don't

167
0:18:23.440 --> 0:18:31.520
alert one means alert and as you can see here the actual workflow kind of like you build you read

168
0:18:31.520 --> 0:18:39.600
from then you define trend map so which is like normal map and from there you can use it to predict

169
0:18:39.600 --> 0:18:44.880
what's going to happen next obviously do some kind of visualization so how does it look like

170
0:18:45.680 --> 0:18:54.000
so this is kind of the prediction part of it so we take the logs map and we build trend map out of

171
0:18:54.000 --> 0:19:01.120
it so trend map would start reading messages and the scores and build train for you and from this

172
0:19:01.120 --> 0:19:06.240
trend I can use some kind of machine learning it doesn't have to be machine learning in this case

173
0:19:06.240 --> 0:19:12.160
it's linear regression but it could be anything to be honest and we check the values and we try to

174
0:19:12.160 --> 0:19:19.200
use some kind of prediction based on the previous values to decide if you want to send an alert or

175
0:19:19.200 --> 0:19:26.560
not and obviously this is kind of like describing the exact thing so when it there is a one on your

176
0:19:26.560 --> 0:19:34.560
values it's alert send alert when it's zero or not and here there are three ways to do it

177
0:19:35.280 --> 0:19:40.960
so this is where the stream processing comes into place so you could simply use SPL to read from

178
0:19:40.960 --> 0:19:47.520
them out and do some query if you want obviously this is batch which means it's not real time

179
0:19:47.520 --> 0:19:53.600
stream processing or you can even like create a pipeline or create a process and from this

180
0:19:53.600 --> 0:20:00.960
process you can read the logs and do the same thing so you can use either SQL or Java to do it

181
0:20:00.960 --> 0:20:07.440
but first two options are batch which means like you can process the data in real time you want to

182
0:20:07.440 --> 0:20:13.280
change this in real time so the third option is the journal map so journal map will track all

183
0:20:13.280 --> 0:20:19.680
changes so it's going to be honest so you have the logs stored and you have logs coming into Kafka

184
0:20:19.680 --> 0:20:26.720
topic and you can basically store both into journal map so we're on 5.2 version within

185
0:20:26.720 --> 0:20:33.440
hezakas 5.3 will have the SQL features on top of it which allows you if you're for example data

186
0:20:33.440 --> 0:20:40.160
scientist to just do the queries and change the data and obviously it's ring buffer so this is

187
0:20:40.160 --> 0:20:45.920
very important to understand so you can start processing your data from start or you know from

188
0:20:45.920 --> 0:20:52.640
the end and what you want is kind of like you know using this kind of alert to it so the first part

189
0:20:52.640 --> 0:20:59.120
is to read it so this is the actual map we built in the first option and from here you define the

190
0:20:59.120 --> 0:21:06.480
key for example and the value and you start for example to do some kind of filtering so this

191
0:21:06.480 --> 0:21:14.000
happens in real time on continuous and the map inset will allow you to basically track changes

192
0:21:14.000 --> 0:21:22.000
so to give you some takeaways and best practices so we just try to summarize everything we discussed

193
0:21:22.000 --> 0:21:27.760
today obviously there are more to discuss but this should give you something to go out and try

194
0:21:28.320 --> 0:21:33.680
so first of thing you need to store your logs into some kind of platform so in this case i'm

195
0:21:33.680 --> 0:21:40.080
using hezakas obviously you don't have to use hezakas the idea here is to do some kind of compute

196
0:21:40.080 --> 0:21:46.560
onto circle data to provide some context as well as real-time data and from there you need to store

197
0:21:46.560 --> 0:21:51.760
it on the cloud so you need to store it somewhere where you can access logs from multiple places

198
0:21:52.400 --> 0:21:58.160
obviously you have to be stored in memory and from there you need to choose format if you for

199
0:21:58.160 --> 0:22:03.360
example looking to provide some you know i don't know some predictions you probably need to use

200
0:22:03.360 --> 0:22:09.920
JSON format or for example if you want just to do some analysis you can sync and it's faster if you want to speed

201
0:22:10.720 --> 0:22:16.640
unit it is some kind of map structure obviously when you store it in memory because this will allow

202
0:22:17.840 --> 0:22:26.720
some kind of random access and also you need to consider how you empty your map so because

203
0:22:26.720 --> 0:22:32.320
you're limited on size obviously and finally you need to consider security so whatever you

204
0:22:32.320 --> 0:22:37.200
send to the cloud you need to make sure that you don't load something you know personally whatever

205
0:22:37.760 --> 0:22:43.440
so if you're interested in this topic we're running on conference next month so feel free

206
0:22:43.440 --> 0:22:49.040
to scan this code we provide training for this all free obviously and everything i mentioned today

207
0:22:49.040 --> 0:22:54.480
is up so you should be able to do everything i mentioned today i'll be sharing around here if

208
0:22:54.480 --> 0:22:59.360
you want to have a chat or if you want to discuss it a little bit more obviously within half an hour

209
0:22:59.360 --> 0:23:04.800
there's not much to give but hopefully you've got some ideas from mr and hopefully it will be also

210
0:23:04.800 --> 0:23:31.200
useful for you so with that being said thanks very much for listening open for questions

