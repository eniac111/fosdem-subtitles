1
0:00:00.000 --> 0:00:09.000
Okay, so yeah, thank you everyone for coming.

2
0:00:09.000 --> 0:00:17.000
The last one of the day after that you're free to be asked to know what ends where.

3
0:00:17.000 --> 0:00:23.000
For the last one of SuperPUR talk with Alex A, he is the CEO of ClickHouse.

4
0:00:23.000 --> 0:00:29.000
Yes, he's going to tell us how to build a real-time application with ClickHouse.

5
0:00:29.000 --> 0:00:36.000
Yeah, thank you.

6
0:00:36.000 --> 0:00:40.000
So, the title of my talk is very similar to the previous.

7
0:00:40.000 --> 0:00:45.000
So, let's see what will be the difference.

8
0:00:45.000 --> 0:00:52.000
I will try to build a small, simple analytical application just about right now.

9
0:00:52.000 --> 0:00:55.000
And how to build an analytical application.

10
0:00:55.000 --> 0:01:00.000
You have to figure out what to do, where to collect our data, how to prepare and clean our data,

11
0:01:00.000 --> 0:01:04.000
how to load it, and how to visualize it.

12
0:01:04.000 --> 0:01:12.000
And, here is the following technologies.

13
0:01:12.000 --> 0:00:57.000
Apache Flink, Apache Bloom, Apache Kafka, Apache Pizzare, Apache Spark, Apache Plunod, Stream

14
0:00:57.000 --> 0:01:25.000
It, the Bizoom, Apache Iceberg, Apache SuperSet.

15
0:01:25.000 --> 0:01:33.000
But, every time I'm playing Apache once again, I'm looking more and more stupid.

16
0:01:33.000 --> 0:01:37.000
So, maybe I don't actually have to use all of these technologies.

17
0:01:37.000 --> 0:01:43.000
Because if I do, at least I will have to be able to tell apart.

18
0:01:43.000 --> 0:01:48.000
What is the difference between Apache Kafka and Apache Pizzare?

19
0:01:48.000 --> 0:01:55.000
If you can't, don't even try to use these technologies.

20
0:01:55.000 --> 0:02:00.000
But what I want to do, actually I want just to analyze data.

21
0:02:00.000 --> 0:02:03.000
What are data? Give me some data.

22
0:02:03.000 --> 0:02:04.000
I want to analyze it.

23
0:02:04.000 --> 0:02:08.000
I have no idea what I will get in the result.

24
0:02:08.000 --> 0:02:17.000
I want some interesting data set with logs, metrics, time series data, events, clinics, whatever.

25
0:02:17.000 --> 0:02:20.000
So, where to find this data?

26
0:02:20.000 --> 0:02:28.000
If you want some data, there are plenty of sources of available public, updatable data sets.

27
0:02:28.000 --> 0:02:34.000
Like, Internet Archive or Zmodo.org, whatever it means.

28
0:02:34.000 --> 0:02:43.000
Or, ConanCrow, GitHub Archive, Wikipedia, blockchain data, from public blockchain, metrics, scans.

29
0:02:43.000 --> 0:02:48.000
Sometimes you can do metrics by yourself and get away with it.

30
0:02:48.000 --> 0:02:52.000
But there are plenty of downloads.

31
0:02:52.000 --> 0:02:57.000
So, maybe we will be surprised by my churns.

32
0:02:57.000 --> 0:03:00.000
But I selected the data from Wikipedia.

33
0:03:00.000 --> 0:03:02.000
Exactly. Almost exactly.

34
0:03:02.000 --> 0:03:04.000
As from previous talk.

35
0:03:04.000 --> 0:03:10.000
So, the data is available on bands.wikimiger.org.

36
0:03:10.000 --> 0:03:15.000
It is public domain. You can do whatever you want with it.

37
0:03:15.000 --> 0:03:20.000
It contains data dumps, edit history, and page view statistics.

38
0:03:20.000 --> 0:03:26.000
And I will summarize page view statistics.

39
0:03:26.000 --> 0:03:35.000
It is updated over here and represented by about 70,000 gzip files.

40
0:03:35.000 --> 0:03:37.000
Three and a half terabytes.

41
0:03:37.000 --> 0:03:39.000
What to do is three and a half terabytes.

42
0:03:39.000 --> 0:03:46.000
Download it.

43
0:03:46.000 --> 0:03:49.000
So, the data looks like this.

44
0:03:49.000 --> 0:03:51.000
It looks kind of raw.

45
0:03:51.000 --> 0:03:54.000
And I like it.

46
0:03:54.000 --> 0:03:57.000
And how to download it?

47
0:03:57.000 --> 0:04:02.000
With the shell script, it looks kind of raw.

48
0:04:02.000 --> 0:04:04.000
And I like it.

49
0:04:04.000 --> 0:04:07.000
So, what it is doing?

50
0:04:07.000 --> 0:04:12.000
It iterates by years, iterates by month.

51
0:04:12.000 --> 0:04:14.000
Collecting the list of links.

52
0:04:14.000 --> 0:04:20.000
And then simply downloading by parallel with wget and xarcs.

53
0:04:20.000 --> 0:04:26.000
It is rate limited to three concurrent requests, apparently.

54
0:04:26.000 --> 0:04:30.000
Actually, wget has a recursive mode.

55
0:04:30.000 --> 0:04:32.000
But it does not have parallelism.

56
0:04:32.000 --> 0:04:38.000
So, I decided to simply parallelize with xarcs.

57
0:04:38.000 --> 0:04:45.000
And after about three days, data is downloaded.

58
0:04:45.000 --> 0:04:48.000
Let's preview it.

59
0:04:48.000 --> 0:04:52.000
If you decompress just one file, it looks like this.

60
0:04:52.000 --> 0:04:54.000
It is kind of a strange format.

61
0:04:54.000 --> 0:05:00.000
It is not CSV, not TSV, not JSON.

62
0:05:00.000 --> 0:05:02.000
It does not look like protobuf.

63
0:05:02.000 --> 0:05:06.000
It is a wide space separated file.

64
0:05:06.000 --> 0:05:08.000
It was just a few fields.

65
0:05:08.000 --> 0:05:12.000
Title, project, subproject, the number of page views,

66
0:05:12.000 --> 0:05:17.000
and also zero for whatever, always zero field.

67
0:05:17.000 --> 0:05:19.000
How to load this data?

68
0:05:19.000 --> 0:05:25.000
And I like shell scripts, but I don't want to use set, oak, and parallel.

69
0:05:25.000 --> 0:05:33.000
Even despite I am on this open source conference, I will not use set, oak, and parallel.

70
0:05:33.000 --> 0:05:37.000
Instead, I will use clickhouse-local.

71
0:05:37.000 --> 0:05:39.000
What is clickhouse-local?

72
0:05:39.000 --> 0:05:45.000
It is a small tool for analytical data processing on local files

73
0:05:45.000 --> 0:05:49.000
or remote files without a server.

74
0:05:49.000 --> 0:05:53.000
You don't have to install clickhouse to use clickhouse-local.

75
0:05:53.000 --> 0:05:57.000
And it can process every data format.

76
0:05:57.000 --> 0:06:01.000
It can process external data from external data sources,

77
0:06:01.000 --> 0:06:07.000
data lakes, object storages, everything.

78
0:06:07.000 --> 0:06:11.000
And actually, clickhouse-local is not a unique tool.

79
0:06:11.000 --> 0:06:14.000
There are many tools for command line data processing.

80
0:06:14.000 --> 0:06:16.000
Here is a list.

81
0:06:16.000 --> 0:06:21.000
I will not pronounce this list because I like clickhouse-local.

82
0:06:21.000 --> 0:06:25.000
I don't like all these tools.

83
0:06:25.000 --> 0:06:28.000
Installing clickhouse-local is easy.

84
0:06:28.000 --> 0:06:29.000
Google.sh.

85
0:06:29.000 --> 0:06:38.000
It is also safe because, keep in mind, it is pipe sh, not pipe sudo sh.

86
0:06:38.000 --> 0:06:39.000
Right?

87
0:06:39.000 --> 0:06:42.000
It is also easy.

88
0:06:42.000 --> 0:06:44.000
And let's preview this data.

89
0:06:44.000 --> 0:06:46.000
It has interactive mode.

90
0:06:46.000 --> 0:06:48.000
Let's run clickhouse-local.

91
0:06:48.000 --> 0:06:52.000
And we can select directly from URL.

92
0:06:52.000 --> 0:06:54.000
What format to use?

93
0:06:54.000 --> 0:06:56.000
CSV does not work.

94
0:06:56.000 --> 0:06:57.000
CSV does not work.

95
0:06:57.000 --> 0:07:01.000
But there is a format pretty simple, named line as string.

96
0:07:01.000 --> 0:07:03.000
What is this format?

97
0:07:03.000 --> 0:07:09.000
It interprets a file as a table with a single column,

98
0:07:09.000 --> 0:07:13.000
named line with type string.

99
0:07:13.000 --> 0:07:17.000
So just a single column with all our data.

100
0:07:17.000 --> 0:07:21.000
We can use it for just filtering.

101
0:07:21.000 --> 0:07:26.000
We can also select from multiple files, as in this example.

102
0:07:26.000 --> 0:07:28.000
We can select the file name.

103
0:07:28.000 --> 0:07:32.000
We can filter by something.

104
0:07:32.000 --> 0:07:33.000
Okay.

105
0:07:33.000 --> 0:07:38.000
Now we have some idea what our data looks like.

106
0:07:38.000 --> 0:07:42.000
Now we have to clean up, prepare, structure our data,

107
0:07:42.000 --> 0:07:46.000
maybe convert it into another format.

108
0:07:46.000 --> 0:07:50.000
And I will do it with this select query.

109
0:07:50.000 --> 0:07:52.000
What it is doing?

110
0:07:52.000 --> 0:07:58.000
It is selecting from files all our 3 terabyte gzip files

111
0:07:58.000 --> 0:08:00.000
with line as a string.

112
0:08:00.000 --> 0:08:06.000
It will split the string by white space to some values.

113
0:08:06.000 --> 0:08:11.000
Represent it as array, select elements of this array

114
0:08:11.000 --> 0:08:14.000
as project, subproject, and path.

115
0:08:14.000 --> 0:08:18.000
Path can be URL encoded with percent encoding.

116
0:08:18.000 --> 0:08:22.000
I will use a function decodeURL component.

117
0:08:22.000 --> 0:08:26.000
I will also extract the date from the file name

118
0:08:26.000 --> 0:08:31.000
with a function path, date, time, best, effort.

119
0:08:31.000 --> 0:08:33.000
And it looks like this.

120
0:08:33.000 --> 0:08:35.000
It is not Russian Wikipedia.

121
0:08:35.000 --> 0:08:44.000
It is AB Wikipedia, whatever it means.

122
0:08:44.000 --> 0:08:46.000
And what is AA Wikipedia?

123
0:08:46.000 --> 0:08:48.000
I don't know.

124
0:08:48.000 --> 0:08:50.000
It will be pretty interesting.

125
0:08:50.000 --> 0:08:54.000
Also what I did with this 3.5 terabyte of files,

126
0:08:54.000 --> 0:08:57.000
I uploaded to my S3 bucket.

127
0:08:57.000 --> 0:09:00.000
And I just made this S3 bucket public,

128
0:09:00.000 --> 0:09:09.000
so until we have money, you will be able to download something.

129
0:09:09.000 --> 0:09:12.000
But please be kind.

130
0:09:12.000 --> 0:09:17.000
And you can select directly from this S3 bucket as well

131
0:09:17.000 --> 0:09:18.000
from all of these files.

132
0:09:18.000 --> 0:09:24.000
It works in the same way.

133
0:09:24.000 --> 0:09:30.000
Okay, so we just previewed our data.

134
0:09:30.000 --> 0:09:34.000
Now let's proceed to real data logic.

135
0:09:34.000 --> 0:09:39.000
Let's install a real Clickhouse server instead of Clickhouse Local.

136
0:09:39.000 --> 0:09:43.000
But actually there is no difference between Clickhouse Local

137
0:09:43.000 --> 0:09:46.000
and Clickhouse Client and Clickhouse Server.

138
0:09:46.000 --> 0:09:50.000
It is, well, everything in a single binary.

139
0:09:50.000 --> 0:09:56.000
You just rename it to Clickhouse Server and it automatically becomes a server.

140
0:09:56.000 --> 0:09:59.000
You can create a Simlink.

141
0:09:59.000 --> 0:10:02.000
You can take this binary and install it.

142
0:10:02.000 --> 0:10:09.000
And it will install into user bin, user etc, and vlog, and whatever.

143
0:10:09.000 --> 0:10:12.000
You can run it without installation.

144
0:10:12.000 --> 0:10:19.000
So let's start it and let's create a table.

145
0:10:19.000 --> 0:10:22.000
So here is a table structure.

146
0:10:22.000 --> 0:10:26.000
Five fields, date, time, because it is time series,

147
0:10:26.000 --> 0:10:34.000
project, subproject, page title, name it path, the number of page views, name it hits.

148
0:10:34.000 --> 0:10:43.000
I also enabled stronger compression with ZSTD, Z-standard, and low cardinality data types.

149
0:10:43.000 --> 0:10:46.000
And Z-standard is just a compression codec.

150
0:10:46.000 --> 0:10:50.000
I will also index it by path and time.

151
0:10:50.000 --> 0:10:55.000
So I will be able to quickly select for specific pages.

152
0:10:55.000 --> 0:10:59.000
And how to load data into this table.

153
0:10:59.000 --> 0:11:06.000
Let's use Kafka or Pulsar and automate with Airflow

154
0:11:06.000 --> 0:11:10.000
and do ETL with Airbyte or DBT.

155
0:11:10.000 --> 0:11:18.000
Actually, I don't know why DBT even exists because I can do everything without DBT.

156
0:11:18.000 --> 0:11:27.000
I will do it with just insert select, insert into Wikistat, my select query from S3.

157
0:11:27.000 --> 0:11:31.000
And I will wait while it finishes.

158
0:11:31.000 --> 0:11:34.000
Let's take a look. You don't see anything.

159
0:11:34.000 --> 0:11:38.000
Let's make a font slightly larger.

160
0:11:38.000 --> 0:11:42.000
I will make a font slightly larger.

161
0:11:42.000 --> 0:11:44.000
Okay.

162
0:11:44.000 --> 0:11:47.000
Now it started to load the data.

163
0:11:47.000 --> 0:11:58.000
0%, 57 CPU consumed, 2 gigabytes per second, and 50 million rows per second.

164
0:11:58.000 --> 0:12:01.000
50 million.

165
0:12:01.000 --> 0:12:03.000
I did not watch one of the previous talk.

166
0:12:03.000 --> 0:12:09.000
It was named loading more than a million records per second on a single server.

167
0:12:09.000 --> 0:12:17.000
So we are loading more than a million records per second on a single server.

168
0:12:17.000 --> 0:12:22.000
Okay. Let's take a look what is happening because just loading data is not enough.

169
0:12:22.000 --> 0:12:24.000
It will take a while.

170
0:12:24.000 --> 0:12:27.000
And what to do while it is loading?

171
0:12:27.000 --> 0:12:29.000
I will run Dstat.

172
0:12:29.000 --> 0:12:37.000
Dstat will show me the system usage and I see that it is bounded by I.O.

173
0:12:37.000 --> 0:12:40.000
500 megabytes per second, Britain.

174
0:12:40.000 --> 0:12:42.000
It is compressed data.

175
0:12:42.000 --> 0:12:45.000
I.O. wait 68%.

176
0:12:45.000 --> 0:12:48.000
CPU wait almost non-existing.

177
0:12:48.000 --> 0:12:51.000
I can also run top to see what is happening.

178
0:12:51.000 --> 0:12:55.000
CPU 16 cores and it works.

179
0:12:55.000 --> 0:12:58.000
And I.O. wait 70%.

180
0:12:58.000 --> 0:13:01.000
But for me it is not enough.

181
0:13:01.000 --> 0:13:05.000
For me it is not enough because I also run this tool.

182
0:13:05.000 --> 0:13:09.000
Perf top because I always profile my code.

183
0:13:09.000 --> 0:13:11.000
So what my code is doing?

184
0:13:11.000 --> 0:13:16.000
It is doing compression, sorting, nothing.

185
0:13:16.000 --> 0:13:31.000
Okay. And after eight hours my data is loaded.

186
0:13:31.000 --> 0:13:36.000
The table size on disk is just 700 gigabytes.

187
0:13:36.000 --> 0:13:40.000
Original was 3.5 terabytes.

188
0:13:40.000 --> 0:13:43.000
So it compressed like five times.

189
0:13:43.000 --> 0:13:50.000
It was in gzip, now it is in clique house with all the colon oriented compression.

190
0:13:50.000 --> 0:13:54.000
The speed was 50 million rows per second.

191
0:13:54.000 --> 0:14:00.000
But actually it was not true.

192
0:14:00.000 --> 0:14:06.000
Because after eight hours it degraded to just 14 million rows per second.

193
0:14:06.000 --> 0:14:08.000
Still not bad.

194
0:14:08.000 --> 0:14:14.000
It degraded because data has to be merged on disk and it takes right amplification.

195
0:14:14.000 --> 0:14:17.000
It takes additional I.O.

196
0:14:17.000 --> 0:14:21.000
So what is the size?

197
0:14:21.000 --> 0:14:26.000
3.8 billion records.

198
0:14:26.000 --> 0:14:29.000
0.3 trillion.

199
0:14:29.000 --> 0:14:40.000
The total page views on Wikipedia is just 1,300,000,000,000 page views.

200
0:14:40.000 --> 0:14:45.000
Nothing surprising Wikipedia is quite popular.

201
0:14:45.000 --> 0:14:48.000
And about my table.

202
0:14:48.000 --> 0:14:56.000
So every record took just 2.0 bytes compressed.

203
0:14:56.000 --> 0:15:01.000
All this title like Wikipedia main page.

204
0:15:01.000 --> 0:15:03.000
It was like 50 bytes.

205
0:15:03.000 --> 0:15:06.000
Now it is compressed to just 2 bytes.

206
0:15:06.000 --> 0:15:20.000
And if you look at compression ratio, actually path is compressed 270 times because we sorted by path.

207
0:15:20.000 --> 0:15:26.000
Okay, but so what? What to do with my data?

208
0:15:26.000 --> 0:15:28.000
I have loaded.

209
0:15:28.000 --> 0:15:35.000
It was 3.5 terabytes and I can be proud that I wasted eight hours loading this data.

210
0:15:35.000 --> 0:15:37.000
And it compressed so well.

211
0:15:37.000 --> 0:15:40.000
But what to do with this data?

212
0:15:40.000 --> 0:15:46.000
We need some actionable insights from this data.

213
0:15:46.000 --> 0:15:50.000
Let's make real-time dashboards.

214
0:15:50.000 --> 0:15:53.000
How to do real-time dashboards?

215
0:15:53.000 --> 0:16:01.000
We can use Grafana, SuperSets, Netabase, Tableau, Absorbable or even Stream Elite.

216
0:16:01.000 --> 0:16:04.000
I don't want to use Stream Elite.

217
0:16:04.000 --> 0:16:10.000
It looks too complex, too complicated in the previous talk.

218
0:16:10.000 --> 0:16:16.000
And actually there is no problem. I can use Grafana, SuperSets, Netabase with Clickhouse.

219
0:16:16.000 --> 0:16:21.000
It works perfectly, but I am an engineer.

220
0:16:21.000 --> 0:16:29.000
And why to use Grafana if I can write my own Grafana in a day?

221
0:16:29.000 --> 0:16:32.000
Let's do it just now.

222
0:16:32.000 --> 0:16:36.000
Let's decide what JavaScript framework to use.

223
0:16:36.000 --> 0:16:43.000
I can use React, Vue, Swelter. I don't know what is Swelter, but it is popular.

224
0:16:43.000 --> 0:16:50.000
You know if Rust were JavaScript framework, I will use Rust.

225
0:16:50.000 --> 0:16:57.000
Maybe I should use not JavaScript, but TypeScript.

226
0:16:57.000 --> 0:17:02.000
But no, I will use modern JavaScript.

227
0:17:02.000 --> 0:17:07.000
What is modern JavaScript?

228
0:17:07.000 --> 0:17:20.000
Modern JavaScript, it is when you simply open HTML file in Notepad or VI or whatever

229
0:17:20.000 --> 0:17:27.000
and write code without frameworks, without build systems, without dependencies.

230
0:17:27.000 --> 0:17:33.000
Actually I need one dependency, some charting library.

231
0:17:33.000 --> 0:17:44.000
And I just picked a random charting library from GitHub, named Uplot from Leona.

232
0:17:44.000 --> 0:17:48.000
The description Uplot is a fast, memory efficient library.

233
0:17:48.000 --> 0:17:55.000
Okay, solved. I will use it.

234
0:17:55.000 --> 0:17:59.000
Another question, how to query my database?

235
0:17:59.000 --> 0:18:03.000
Should I write a backend in Python?

236
0:18:03.000 --> 0:18:06.000
In Go? No.

237
0:18:06.000 --> 0:18:13.000
I will query my database directly from JavaScript, from modern JavaScript with Rust API.

238
0:18:13.000 --> 0:18:21.000
I will use async, await, fetch API, and post my query to the database

239
0:18:21.000 --> 0:18:26.000
and it will return the data in format JSON.

240
0:18:26.000 --> 0:18:30.000
Okay, enough modern JavaScript.

241
0:18:30.000 --> 0:18:36.000
So, Kliqhouse has a REST API embedded into the server.

242
0:18:36.000 --> 0:18:43.000
It has authentication, access control, rate limiting, quotas, query complexity limiting,

243
0:18:43.000 --> 0:18:46.000
parameterized queries, custom handlers.

244
0:18:46.000 --> 0:18:55.000
So you don't have to write select query, you can just define a handler like slash my report

245
0:18:55.000 --> 0:18:59.000
or slash insert my data.

246
0:18:59.000 --> 0:19:04.000
And you can actually open Kliqhouse to the internet and get away with that.

247
0:19:04.000 --> 0:19:10.000
I did that, it still works.

248
0:19:10.000 --> 0:19:17.000
Okay, here is a query for Wikipedia Trends that we will use for a dashboard.

249
0:19:17.000 --> 0:19:27.000
It will simply select this time series rounded to some time frame, to some page.

250
0:19:27.000 --> 0:19:30.000
And here is a parameterized query.

251
0:19:30.000 --> 0:19:33.000
It looks slightly different.

252
0:19:33.000 --> 0:19:35.000
It's not like question mark here.

253
0:19:35.000 --> 0:19:41.000
It is actually strictly typed substitution.

254
0:19:41.000 --> 0:19:46.000
Okay, and how long this query will take?

255
0:19:46.000 --> 0:19:50.000
Let me ask you, how long this query will take?

256
0:19:50.000 --> 0:19:53.000
What do you think?

257
0:19:53.000 --> 0:19:54.000
Eight days.

258
0:19:54.000 --> 0:19:56.000
Eight days, why eight days?

259
0:19:56.000 --> 0:20:03.000
It should work on a table with 0.3 trillion records.

260
0:20:03.000 --> 0:20:06.000
How long this query will take?

261
0:20:06.000 --> 0:20:11.000
20 milliseconds.

262
0:20:11.000 --> 0:20:16.000
Okay, let's experiment 9 milliseconds.

263
0:20:16.000 --> 0:20:18.000
So you are wrong.

264
0:20:18.000 --> 0:20:21.000
You are also wrong.

265
0:20:21.000 --> 0:20:25.000
I was crawling back and forth.

266
0:20:25.000 --> 0:20:30.000
So if we will maybe Kliqhouse is fast, what if I do MySQL?

267
0:20:30.000 --> 0:20:34.000
29 milliseconds, okay, closer.

268
0:20:34.000 --> 0:20:38.000
MariaDB 20 milliseconds.

269
0:20:38.000 --> 0:20:44.000
What if I will replace equality comparison to like and add percent?

270
0:20:44.000 --> 0:20:48.000
The same, because prefix also using index.

271
0:20:48.000 --> 0:20:51.000
But what if I will add percent on the front?

272
0:20:51.000 --> 0:20:56.000
Okay, now it started to do a full scan.

273
0:20:56.000 --> 0:21:01.000
And this full scan was quite fast, over 1 billion records per second,

274
0:21:01.000 --> 0:21:05.000
but still not fast enough for real time.

275
0:21:05.000 --> 0:21:09.000
But all the queries with exact matching was real time.

276
0:21:09.000 --> 0:21:17.000
Okay, let me show you this dashboard.

277
0:21:17.000 --> 0:21:21.000
It looks like this modern dashboard.

278
0:21:21.000 --> 0:21:26.000
It looks actually gorgeous, it has dark theme.

279
0:21:26.000 --> 0:21:32.000
And you can see it compares trends on Wikipedia for Kliqhouse.

280
0:21:32.000 --> 0:21:38.000
Kliqhouse is growing, spark is not growing.

281
0:21:38.000 --> 0:21:41.000
Green plum is not growing.

282
0:21:41.000 --> 0:21:42.000
What was there?

283
0:21:42.000 --> 0:21:44.000
Snowflake is quite okay.

284
0:21:44.000 --> 0:21:48.000
I don't like it.

285
0:21:48.000 --> 0:21:51.000
Let's see what is inside.

286
0:21:51.000 --> 0:21:54.000
Every chart is defined with parameterized query.

287
0:21:54.000 --> 0:21:58.000
You write select, actually it's not even parameterized.

288
0:21:58.000 --> 0:22:00.000
Okay, what about MongoDB?

289
0:22:00.000 --> 0:22:03.000
Here I define a new chart and here is Mongo.

290
0:22:03.000 --> 0:22:05.000
Okay, I did one mistake.

291
0:22:05.000 --> 0:22:11.000
It was filtered by outliers for Snowflake.

292
0:22:11.000 --> 0:22:13.000
Let's move.

293
0:22:13.000 --> 0:22:16.000
Okay, Mongo, no, Mongo is not doing great.

294
0:22:16.000 --> 0:22:18.000
Kliqhouse is doing great.

295
0:22:18.000 --> 0:22:23.000
By the way, what if you will just open a dashboard by default?

296
0:22:23.000 --> 0:22:27.000
It will present you observability dashboard for Kliqhouse.

297
0:22:27.000 --> 0:22:30.000
So you can see what the system is doing.

298
0:22:30.000 --> 0:22:34.000
It is actually the same code, the same dashboard,

299
0:22:34.000 --> 0:22:38.000
but different queries.

300
0:22:38.000 --> 0:22:41.000
You can use parameterized queries for these parameters,

301
0:22:41.000 --> 0:22:43.000
change parameters, change the time frame.

302
0:22:43.000 --> 0:22:48.000
It's not like Grafana.

303
0:22:48.000 --> 0:22:54.000
It does not have features, but it is nice.

304
0:22:54.000 --> 0:22:57.000
And you can see, yes, it is a single HTML page,

305
0:22:57.000 --> 0:23:02.000
and here is a proof.

306
0:23:02.000 --> 0:23:06.000
Okay.

307
0:23:06.000 --> 0:23:09.000
So what do we have?

308
0:23:09.000 --> 0:23:13.000
We have created real-time dashboard with Kliqhouse.

309
0:23:13.000 --> 0:23:18.000
We have loaded 0.3 trillion records of data

310
0:23:18.000 --> 0:23:21.000
from a public dataset.

311
0:23:21.000 --> 0:23:24.000
It works. It works fast. It looks great.

312
0:23:24.000 --> 0:23:26.000
And if you want to build...

313
0:23:26.000 --> 0:23:31.000
Actually, I don't insist you to use modern JavaScript.

314
0:23:31.000 --> 0:23:37.000
I don't insist you to query Kliqhouse directly from user browser.

315
0:23:37.000 --> 0:23:41.000
You can use Grafana superset, meta-base, stream,

316
0:23:41.000 --> 0:23:43.000
maybe I'm not sure.

317
0:23:43.000 --> 0:23:46.000
But you can also build these small applications.

318
0:23:46.000 --> 0:23:48.000
And I have built quite a few.

319
0:23:48.000 --> 0:23:51.000
There is Kliqhouse Playground,

320
0:23:51.000 --> 0:23:54.000
where you can explore some datasets.

321
0:23:54.000 --> 0:23:58.000
There is a web page for Kliqhouse testing infrastructure

322
0:23:58.000 --> 0:24:00.000
named r-tests-green-yet.

323
0:24:00.000 --> 0:24:02.000
You can try and check what it is.

324
0:24:02.000 --> 0:24:05.000
And the source code, dashboard-html,

325
0:24:05.000 --> 0:24:08.000
is located in our repository.

326
0:24:08.000 --> 0:24:14.000
And just to note, this service is not original.

327
0:24:14.000 --> 0:24:16.000
I have found multiple similar services.

328
0:24:16.000 --> 0:24:18.000
For example, Wikishark.

329
0:24:18.000 --> 0:24:21.000
For the same trends on Wikipedia.

330
0:24:21.000 --> 0:24:28.000
But on Wikishark, there is a description that the author...

331
0:24:28.000 --> 0:24:30.000
I did not remember.

332
0:24:30.000 --> 0:24:34.000
Maybe he made a PhD implementing a data structure,

333
0:24:34.000 --> 0:24:37.000
a custom data structure for this.

334
0:24:37.000 --> 0:24:42.000
But he can simply load the data into Kliqhouse.

335
0:24:42.000 --> 0:24:47.000
The experience of working with Kliqhouse was worth multiple PhDs.

336
0:24:47.000 --> 0:24:49.000
Okay.

337
0:24:49.000 --> 0:24:52.000
Thank you. That's it.

338
0:24:52.000 --> 0:25:02.000
APPLAUSE

339
0:25:02.000 --> 0:25:05.000
We do have time for multiple questions.

340
0:25:05.000 --> 0:25:09.000
About modern JavaScript, for example.

341
0:25:09.000 --> 0:25:14.000
Why is this super fast?

342
0:25:14.000 --> 0:25:16.000
Very easy.

343
0:25:16.000 --> 0:25:17.000
Repeat the question.

344
0:25:17.000 --> 0:25:20.000
Why this dashboard is fast?

345
0:25:20.000 --> 0:25:24.000
Because...

346
0:25:24.000 --> 0:25:26.000
Why it is inserting fast?

347
0:25:26.000 --> 0:25:28.000
Why it is selecting fast?

348
0:25:28.000 --> 0:25:30.000
Because I always profile it.

349
0:25:30.000 --> 0:25:35.000
You have seen, I always look at what is happening inside the code.

350
0:25:35.000 --> 0:25:37.000
What can be optimized?

351
0:25:37.000 --> 0:25:43.000
If I see that, like, some percent of time spent doing nothing for mem copy,

352
0:25:43.000 --> 0:25:47.000
I'm thinking, maybe I should optimize mem copy.

353
0:25:47.000 --> 0:25:54.000
Maybe I should remove mem copy.

354
0:25:54.000 --> 0:25:59.000
But actually, a very long list about everything.

355
0:25:59.000 --> 0:26:02.000
But still, we are talking about one machine.

356
0:26:02.000 --> 0:26:05.000
If one machine can present all the data.

357
0:26:05.000 --> 0:26:07.000
Yeah.

358
0:26:07.000 --> 0:26:13.000
I just created a machine on AWS with a GP2 EBS,

359
0:26:13.000 --> 0:26:17.000
three terabytes, just in case.

360
0:26:17.000 --> 0:26:19.000
That was in S3.

361
0:26:19.000 --> 0:26:20.000
I have uploaded.

362
0:26:20.000 --> 0:26:29.000
By the way, maybe we have time for some demos.

363
0:26:29.000 --> 0:26:41.000
But the resolution, the screen resolution is not...

364
0:26:41.000 --> 0:26:45.000
And Wi-Fi stopped to work.

365
0:26:45.000 --> 0:26:50.000
So probably no demos.

366
0:26:50.000 --> 0:26:53.000
But okay, next questions.

367
0:26:53.000 --> 0:26:54.000
Okay.

368
0:26:54.000 --> 0:26:55.000
Hello, thanks for the talk.

369
0:26:55.000 --> 0:26:57.000
You mentioned compression.

370
0:26:57.000 --> 0:27:00.000
Does that slow down select?

371
0:27:00.000 --> 0:27:01.000
Not quite.

372
0:27:01.000 --> 0:27:04.000
Actually, compression can even improve select queries.

373
0:27:04.000 --> 0:27:07.000
It is kind of paradoxical, but let me explain.

374
0:27:07.000 --> 0:27:11.000
First, because less amount of data will be read from disk.

375
0:27:11.000 --> 0:27:17.000
Second, because data read from disk is also cached in memory,

376
0:27:17.000 --> 0:27:19.000
in the page cache.

377
0:27:19.000 --> 0:27:22.000
And the page cache will contain compressed data.

378
0:27:22.000 --> 0:27:25.000
And when you process this data,

379
0:27:25.000 --> 0:27:29.000
you will decompress this data into CPU cache

380
0:27:29.000 --> 0:27:32.000
without using main memory.

381
0:27:32.000 --> 0:27:34.000
So even...

382
0:27:34.000 --> 0:27:42.000
Yeah, LZ4 from multiple threads can be faster than memory bandwidth.

383
0:27:42.000 --> 0:27:44.000
ZSTD, not always.

384
0:27:44.000 --> 0:27:51.000
But on servers like AMD Epic with 128 cores,

385
0:27:51.000 --> 0:27:55.000
if you run ZSTD, decompression, in every core,

386
0:27:55.000 --> 0:28:03.000
it has a chance to be faster than memory.

387
0:28:03.000 --> 0:28:08.000
So what did your total AWS build for this project?

388
0:28:08.000 --> 0:28:15.000
I prepared it yesterday and used also S3 prepared before that.

389
0:28:15.000 --> 0:28:19.000
So let's calculate S3 cost.

390
0:28:19.000 --> 0:28:23.000
I am storing the original data, 3.5 terabytes.

391
0:28:23.000 --> 0:28:27.000
And it should be like 23,

392
0:28:27.000 --> 0:28:31.000
but 23 is the least price per month for terabyte.

393
0:28:31.000 --> 0:28:39.000
So it will be like $70 per month

394
0:28:39.000 --> 0:28:43.000
if you don't have AWS discounts, but I do.

395
0:28:43.000 --> 0:28:46.000
LAUGHTER

396
0:28:46.000 --> 0:28:54.000
And for the server, the server was about $4 per hour

397
0:28:54.000 --> 0:28:58.000
for a server and something for GP2.

398
0:28:58.000 --> 0:29:03.000
So maybe something like $5 per hour, and it is still running.

399
0:29:03.000 --> 0:29:07.000
I started up it yesterday when I prepared the talk.

400
0:29:07.000 --> 0:29:14.000
And so 24 hours will be how many?

401
0:29:14.000 --> 0:29:17.000
Something like maybe $50.

402
0:29:17.000 --> 0:29:22.000
OK, I spent $50 for this talk.

403
0:29:22.000 --> 0:29:24.000
LAUGHTER

404
0:29:24.000 --> 0:29:30.000
Yeah, it is public.

405
0:29:30.000 --> 0:29:34.000
So keep in mind, if you will abuse it, we will simply turn it off.

406
0:29:34.000 --> 0:29:41.000
LAUGHTER

407
0:29:41.000 --> 0:29:43.000
Maybe you have a question about S3.

408
0:29:43.000 --> 0:29:46.000
You mentioned what type of connectors do you have for S3?

409
0:29:46.000 --> 0:29:47.000
Is it just for uploading?

410
0:29:47.000 --> 0:29:51.000
Or can you also use S3 for indexing and storing data as well?

411
0:29:51.000 --> 0:29:55.000
Yes, you can, and in multiple different ways.

412
0:29:55.000 --> 0:29:59.000
First, just a bunch of files on S3.

413
0:29:59.000 --> 0:30:01.000
Process them as is.

414
0:30:01.000 --> 0:30:07.000
Parquet, protobuf, Avro, Avro, it does not matter.

415
0:30:07.000 --> 0:30:09.000
Everything works.

416
0:30:09.000 --> 0:30:18.000
Second, you can process files in Apache Delta Lake or Apache Hoodie.

417
0:30:18.000 --> 0:30:23.000
And Apache Iceberg will be supported maybe in the next release.

418
0:30:23.000 --> 0:30:30.000
So you can prepare data in your Databricks or Spark.

419
0:30:30.000 --> 0:30:36.000
And process with Clickhouse because Clickhouse is better than Spark.

420
0:30:36.000 --> 0:30:37.000
LAUGHTER

421
0:30:37.000 --> 0:30:39.000
Third option.

422
0:30:39.000 --> 0:30:49.000
You can also plug in S3 as a virtual file system for merge tree tables.

423
0:30:49.000 --> 0:30:55.000
And it will be used not only for selects but also for inserts.

424
0:30:55.000 --> 0:30:58.000
And you can have your servers almost stateless.

425
0:30:58.000 --> 0:31:06.000
And the date will be in the object storage.

426
0:31:06.000 --> 0:31:16.000
Yeah, plenty of options.

427
0:31:16.000 --> 0:31:18.000
Sorry, sorry.

428
0:31:18.000 --> 0:31:21.000
One more question.

429
0:31:21.000 --> 0:31:23.000
It's a last talk anyway.

430
0:31:23.000 --> 0:31:28.000
Can you actually run Clickhouse in a cluster set up in distributed table?

431
0:31:28.000 --> 0:31:29.000
Yeah, for sure.

432
0:31:29.000 --> 0:31:31.000
You can use it in a cluster.

433
0:31:31.000 --> 0:31:37.000
You can set up an insert in distributed table and it will scale linearly.

434
0:31:37.000 --> 0:31:40.000
And these queries will also scale.

435
0:31:40.000 --> 0:31:47.000
The queries that take already like 9 milliseconds, 10 milliseconds will take not less.

436
0:31:47.000 --> 0:31:51.000
Maybe they will take even more, like 15 milliseconds.

437
0:31:51.000 --> 0:32:00.000
But the queries that took seconds, minutes, they will scale linearly.

438
0:32:00.000 --> 0:32:04.000
So, how do you limit how many nodes there can be in the cluster?

439
0:32:04.000 --> 0:32:06.000
Theoretically, no.

440
0:32:06.000 --> 0:32:15.000
But in practice, some companies are using Clickhouse on over 1,000 of nodes.

441
0:32:15.000 --> 0:32:21.000
Many companies are using Clickhouse on several hundreds of nodes.

442
0:32:21.000 --> 0:32:26.000
When you have to deal with clusters with hundreds and thousands of nodes,

443
0:32:26.000 --> 0:32:36.000
especially if it is geographically distributed, you will definitely have troubles.

444
0:32:36.000 --> 0:32:47.000
But with Clickhouse, it is totally possible to have these clusters and it will work.

445
0:32:47.000 --> 0:33:01.000
Apart from compression, which is of the source, what other building blocks are you using to build data structures or data storage that is readily available to compose the product?

446
0:33:01.000 --> 0:33:02.000
The question...

447
0:33:02.000 --> 0:33:10.000
Are there the way you make it so you data on this?

448
0:33:10.000 --> 0:33:17.000
Interesting question, because maybe you are asking about what are the data structures inside.

449
0:33:17.000 --> 0:33:25.000
Maybe you are asking, is Clickhouse based on some readily available data structures?

450
0:33:25.000 --> 0:33:29.000
The data format, Clickhouse-nort-3, is original.

451
0:33:29.000 --> 0:33:35.000
You can think that maybe it is somehow similar to Apache-Eisberg, maybe.

452
0:33:35.000 --> 0:33:37.000
But actually not.

453
0:33:37.000 --> 0:33:52.000
The colon format in memory and the network transfer format is also original, but it is very similar to Apache-Eiro, but slightly different.

454
0:33:52.000 --> 0:33:54.000
The algorithms...

455
0:33:54.000 --> 0:33:59.000
Actually, we took every good algorithm from everywhere.

456
0:33:59.000 --> 0:34:09.000
Someone writes a blog post on the Internet like, about, I have implemented the best hash table.

457
0:34:09.000 --> 0:34:22.000
Instantly, someone from my team will try and test it inside Clickhouse.

458
0:34:22.000 --> 0:34:25.000
Okay, looks like no more questions.

459
0:34:25.000 --> 0:34:26.000
Thank you.

460
0:34:26.000 --> 0:34:33.000
Thank you.

