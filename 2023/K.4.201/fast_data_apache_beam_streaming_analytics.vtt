WEBVTT

00:00.000 --> 00:07.000
You have image?

00:07.000 --> 00:14.000
Okay.

00:14.000 --> 00:17.000
Thank you. Okay.

00:17.000 --> 00:20.000
Thanks. I think the technical issues are now solved.

00:20.000 --> 00:25.000
So thanks again, everyone, for being here.

00:25.000 --> 00:27.000
So I'm going to talk today about Apache VIN.

00:27.000 --> 00:29.000
Apache VIN is a framework for data processing that runs on

00:29.000 --> 00:34.000
top of several platforms, and it's a special event for doing

00:34.000 --> 00:38.000
streaming analytics. So have you already introduced me?

00:38.000 --> 00:42.000
My name is Israel. So I work as a cloud data engineer in

00:42.000 --> 00:45.000
Google Cloud, helping customers doing data engineering on top

00:45.000 --> 00:48.000
of Google Cloud. A lot of the work that I do is actually

00:48.000 --> 00:52.000
helping customers with Apache VIN and particularly Dataflow,

00:52.000 --> 00:55.000
which is our runner for Apache VIN.

00:55.000 --> 01:00.000
So Apache VIN. What is it? Apache VIN is a framework for

01:00.000 --> 01:04.000
data processing. It allows to run data pipelines like

01:04.000 --> 01:08.000
Flink, like Spark, like so many other big data systems.

01:08.000 --> 01:13.000
It has two main features. The first one is that it's an

01:13.000 --> 01:17.000
unified computing model for batch and streaming.

01:17.000 --> 01:21.000
Any pipeline that you have written in Apache VIN for a batch

01:21.000 --> 01:25.000
use case, you may easily use it in streaming as well.

01:25.000 --> 01:29.000
So the same code, you use all that code and you have to add

01:29.000 --> 01:34.000
some small additions that we're going to talk about in a bit.

01:34.000 --> 01:39.000
And the other main feature is that it runs everywhere and you

01:39.000 --> 01:43.000
can run your pipeline in many different languages, for some

01:43.000 --> 01:47.000
definition of everywhere. So you can write your pipeline in

01:47.000 --> 01:54.000
Java, in Python, in Go. You may also run your pipeline in

01:54.000 --> 01:57.000
any of the programming languages of the Java Victor

01:57.000 --> 02:01.000
matching. For instance, I have here highlighted Scala because

02:01.000 --> 02:04.000
there is a framework called XIO, though don't buy Spotify,

02:04.000 --> 02:09.000
on top of Apache VIN for, let's say, Scala native development

02:09.000 --> 02:15.000
of pipeline. So you don't have to use like a Java looking

02:15.000 --> 02:22.000
code in Scala. So it's a functional code. There are lots

02:22.000 --> 02:26.000
of people using, for instance, Kotlin also on top of the

02:26.000 --> 02:31.000
Java Victor matching with a Java VIN SDK. So that's about the

02:31.000 --> 02:36.000
programming language that you may use. And you may run a

02:36.000 --> 02:40.000
VIN pipeline on top of runners. So there's a direct runner for

02:40.000 --> 02:45.000
local running and local testing of pipelines. It's not meant

02:45.000 --> 02:50.000
for, let's say, to be used with real world use cases. But then

02:50.000 --> 02:53.000
you can run your pipeline on top of Dataflow, on top of

02:53.000 --> 02:57.000
Flink, on top of Hazelcast, Spark, many different runners.

02:57.000 --> 03:00.000
So basically when you write a pipeline in Apache VIN, you are

03:00.000 --> 03:03.000
not tied to the platform where you're running. So you may move

03:03.000 --> 03:07.000
it to different platforms with some minor comments. So Apache

03:07.000 --> 03:10.000
VIN is a theoretical model for computing. Not all the runners

03:10.000 --> 03:15.000
implement the model, let's say, to the same degree of extent.

03:15.000 --> 03:19.000
So right now, as of now, I would say Dataflow and Flink are

03:19.000 --> 03:24.000
probably the ones that are covered like fully covered.

03:24.000 --> 03:29.000
Other runners may have some gaps. A silly example, Hadoop.

03:29.000 --> 03:32.000
You may run a Apache VIN pipeline on top of Hadoop also, but

03:32.000 --> 03:34.000
you cannot run streaming on top of Hadoop because it doesn't

03:34.000 --> 03:36.000
support the streaming. So it also depends on the capability

03:36.000 --> 03:39.000
of the runner. So what you're able to do with VIN, it depends

03:39.000 --> 03:41.000
on the capabilities of the runner. So there's no magic.

03:41.000 --> 03:44.000
So if VIN is batch and streaming, but if your platform doesn't

03:44.000 --> 03:47.000
support streaming, for instance, you cannot do streaming.

03:47.000 --> 03:51.000
So let's talk about streaming. What's the problem with streaming?

03:51.000 --> 03:53.000
It's extremely interesting. So in streaming, you are getting

03:53.000 --> 03:56.000
data, a lot of data, continuously. There is no beginning

03:56.000 --> 04:00.000
and there's no end. So you cannot know in advance where are

04:00.000 --> 04:05.000
the boundaries of your data. And it's coming continuously from

04:05.000 --> 04:08.000
many different places. Think, I don't know, like you are

04:08.000 --> 04:12.000
designing a mobile application for a game or whatever, and

04:12.000 --> 04:16.000
people are using it and sending events to your systems every

04:16.000 --> 04:22.000
once in a while. So because the application is deployed in the

04:22.000 --> 04:29.000
wild world, data will come. Who knows how? Okay. So it will

04:29.000 --> 04:32.000
come out of order. So some users will be, I don't know, in the

04:32.000 --> 04:36.000
underground and without the phone coverage, still try

04:36.000 --> 04:38.000
attempting to send in events and then they will send the

04:38.000 --> 04:42.000
events late. Like for instance here, let's see if I can put

04:42.000 --> 04:52.000
the pointer. So this is data that is supposed to be produced

04:52.000 --> 04:56.000
around 8 in the morning. Maybe the network latency and so on,

04:56.000 --> 05:00.000
but more or less you get it around 8 in the morning in your

05:00.000 --> 05:03.000
system. So this is the time where you are seeing the data

05:03.000 --> 05:06.000
in your system, but for whatever reason, you may also

05:06.000 --> 05:10.000
get very late data. And depending on what you want to do,

05:10.000 --> 05:15.000
you may want to process this data as it was produced, not as

05:15.000 --> 05:18.000
you are receiving it. So this is actually the problem with

05:18.000 --> 05:20.000
microvatching. So I remember when I started hearing about

05:20.000 --> 05:24.000
the streaming in the first days, many years ago, a lot of

05:24.000 --> 05:28.000
people said, ah, Spark, I don't like it because it does

05:28.000 --> 05:32.000
microvatching, it doesn't do real streaming. I had no clue

05:32.000 --> 05:34.000
what they meant. It's like, well, you have to group things,

05:34.000 --> 05:37.000
no, to process it somehow. So if the data is infinite, you

05:37.000 --> 05:40.000
will need to process it. The problem with microvatching,

05:40.000 --> 05:42.000
which is not happening in Spark anymore, so this was really

05:42.000 --> 05:45.000
like ancient times, the problem with microvatching is that

05:45.000 --> 05:48.000
you are doing the batches as you see the data, okay? And

05:48.000 --> 05:52.000
then you may have the data that belongs together in

05:52.000 --> 05:55.000
buckets that are separate. Like for instance, this was

05:55.000 --> 05:57.000
data that was produced at 8 in the morning. If you are

05:57.000 --> 06:00.000
doing buckets of one hour, so while you may capture here

06:00.000 --> 06:03.000
this message, but then if you have late data, you will

06:03.000 --> 06:08.000
capture it in a bucket that doesn't belong, that element

06:08.000 --> 06:10.000
doesn't belong with the rest of the elements there if you

06:10.000 --> 06:13.000
want to process them together. So you need to solve this

06:13.000 --> 06:17.000
problem of lack of order in streaming, okay? And this is

06:17.000 --> 06:21.000
what you can easily solve with Apache Bin. Let's talk

06:21.000 --> 06:25.000
about the watermark. There are many ways of doing stream

06:25.000 --> 06:30.000
processing. One of the most popular is in a concept of

06:30.000 --> 06:34.000
watermark. There is no one dimension of time. There are

06:34.000 --> 06:37.000
two dimensions of time, at least. That is the event

06:37.000 --> 06:41.000
time, the moment in which the data was produced, and that

06:41.000 --> 06:44.000
is the processing time, the moment in which you see the

06:44.000 --> 06:47.000
data. They will never be the same. They can be really

06:47.000 --> 06:51.000
close sometimes, but it's not, you cannot grant it how

06:51.000 --> 06:54.000
close or how far you are going to be from that moment,

06:54.000 --> 06:58.000
okay? So we put time in two dimensions, okay? So the

06:58.000 --> 07:02.000
ideal is, for instance, like this straight line in blue,

07:02.000 --> 07:05.000
for sure. This sun is impossible, okay? You cannot see

07:05.000 --> 07:09.000
data before it's produced, or not yet, at least, okay? So

07:09.000 --> 07:12.000
according to the laws of physics, okay? So, but then

07:12.000 --> 07:14.000
what's most likely what will happen is that you will

07:14.000 --> 07:17.000
have some delay, okay? Sometimes it will be closer to

07:17.000 --> 07:20.000
the ideal. Sometimes it will be farther from the ideal,

07:20.000 --> 07:26.000
okay? So, and you need to take this into account, okay?

07:26.000 --> 07:29.000
So let's see an example that might be a little bit more

07:29.000 --> 07:32.000
telling, okay? So Star Wars, okay? So have you ever,

07:32.000 --> 07:36.000
like, watched a Star Wars movie? So Star Wars were

07:36.000 --> 07:39.000
released out of order, out of order, okay? So the first

07:39.000 --> 07:42.000
movie was episode four. This is purely streaming. This

07:42.000 --> 07:44.000
is what happens in the streaming, okay? You are

07:44.000 --> 07:46.000
expecting events, okay, at the beginning of the session,

07:46.000 --> 07:48.000
in the middle of the session, the end of the session,

07:48.000 --> 07:51.000
okay? And then you get end of the session, middle of

07:51.000 --> 07:54.000
the session, beginning of the session. And you need to

07:54.000 --> 07:57.000
reorder things, okay? Depending on what you want to do,

07:57.000 --> 08:00.000
you may need to reorder, okay? If you don't care,

08:00.000 --> 08:03.000
look, I don't want to, I don't care. I just want to

08:03.000 --> 08:05.000
count how many movies per year were released, okay?

08:05.000 --> 08:07.000
Well, you don't need event time, okay? But if you

08:07.000 --> 08:12.000
want to reconstruct the story, okay? Who did what

08:12.000 --> 08:16.000
before or after? Who, what happened? So you need to

08:16.000 --> 08:19.000
actually be able to reconstruct that time, okay? So

08:19.000 --> 08:22.000
the time where the movies were released, it's processing

08:22.000 --> 08:25.000
time, event time is the time in which actually the events

08:25.000 --> 08:28.000
are happening, okay? So, and this is the kind of

08:28.000 --> 08:30.000
problems that we can solve using Apache VIN or Flink

08:30.000 --> 08:33.000
or many other streaming systems. Let's see how we can

08:33.000 --> 08:37.000
deal with this. The classical way is using windowing,

08:37.000 --> 08:41.000
okay? Windowing is grouping things based on temporal

08:41.000 --> 08:45.000
properties. When we do a data pipeline, we need to

08:45.000 --> 08:49.000
solve one question, which is what we are going to

08:49.000 --> 08:52.000
compute. But if you want to do this in the streaming

08:52.000 --> 08:55.000
and group things based on temporal properties, you need

08:55.000 --> 08:59.000
to answer three additional questions. Where, when, and

08:59.000 --> 09:02.000
how. Let's see some details. The what? This is easy.

09:02.000 --> 09:04.000
We are going to be aggregating, okay? So this is

09:04.000 --> 09:07.000
Java code and it's in Java here as an example. So it's

09:07.000 --> 09:11.000
Apache VIN API. I haven't entered into details.

09:11.000 --> 09:13.000
There's a link at the end with more details, okay? So

09:13.000 --> 09:15.000
don't mind the details right now. So we're aggregating

09:15.000 --> 09:18.000
things together. So this is what we are happening. We

09:18.000 --> 09:21.000
are not doing any kind of temporal based logic yet,

09:21.000 --> 09:23.000
okay? We're just aggregating stuff together, okay? So

09:23.000 --> 09:25.000
we are summing up all the numbers here, okay? So this

09:25.000 --> 09:27.000
is the operation that we are doing. Send us in batch.

09:27.000 --> 09:29.000
This is in batch, okay? So imagine that we are getting

09:29.000 --> 09:32.000
this batch, okay? The problems that when we are

09:32.000 --> 09:35.000
working in streaming, we don't see the full data at

09:35.000 --> 09:38.000
once, okay? And we need to produce output at some

09:38.000 --> 09:42.000
point so we cannot wait forever. So we need to

09:42.000 --> 09:44.000
decide how to group things together. So for instance

09:44.000 --> 09:48.000
here, we are going to group things in windows of two

09:48.000 --> 09:52.000
minutes, okay? But the windows of two minutes are not

09:52.000 --> 09:56.000
in processing time. They are in event time, okay? For

09:56.000 --> 10:01.000
instance, so here, this message here, so this message

10:01.000 --> 10:05.000
here, we sit around 12, okay? And we put it in the

10:05.000 --> 10:09.000
window of 12, okay? But this message over here, so

10:09.000 --> 10:14.000
this was received between 1208 and 1209, okay? And we

10:14.000 --> 10:17.000
are able still to attribute it to assign it to the

10:17.000 --> 10:21.000
window between 12 and 1202 in event time, okay? Because

10:21.000 --> 10:23.000
well, so we can wait for late data and put it in the

10:23.000 --> 10:27.000
right window despite the message being quite late

10:27.000 --> 10:30.000
compared to the processing time, okay? And same with

10:30.000 --> 10:33.000
the rest of windows. Now the question is, okay, good.

10:33.000 --> 10:37.000
So you are waiting until 1208, so what if your message

10:37.000 --> 10:41.000
shows up at 8 p.m.? What do you do? Like eight hours

10:41.000 --> 10:45.000
after. So we need to do another decision, okay? So we

10:45.000 --> 10:47.000
have already made the decision on how we are going to

10:47.000 --> 10:50.000
group things together. Here is with easy windows. There

10:50.000 --> 10:52.000
are more windows in Apache Bin, not entering two

10:52.000 --> 10:55.000
details right now. But now we need to decide how long do

10:55.000 --> 11:00.000
we wait? Okay? So we are going to wait until the

11:00.000 --> 11:05.000
watermark, okay? The watermark is this relationship

11:05.000 --> 11:08.000
between processing time and event time that in the case

11:08.000 --> 11:11.000
of Bin and depending on the runner is calculated and

11:11.000 --> 11:16.000
estimated on the fly as data goes through our pipeline

11:16.000 --> 11:20.000
and this is estimated and when you are, when you

11:20.000 --> 11:24.000
trespass the watermark, you have a certain degree of

11:24.000 --> 11:27.000
warranty that your data is complete, okay? A certain

11:27.000 --> 11:29.000
degree of warranty, okay? It cannot be granted because

11:29.000 --> 11:32.000
well, so the future cannot be known, okay? So we cannot

11:32.000 --> 11:35.000
travel in time, okay? So here, for instance, so we are

11:35.000 --> 11:38.000
processing data and the watermark and the nine, this

11:38.000 --> 11:43.000
number here that we were processing before, now it's

11:43.000 --> 11:46.000
left out of the window. So what does it mean if we are

11:46.000 --> 11:49.000
processing data? We were summing up numbers, that

11:49.000 --> 11:55.000
number, that nine, we are not counting it. As soon as

11:55.000 --> 11:58.000
we see it in our pipeline, it will be dropped, like

11:58.000 --> 12:03.000
lost, okay? So the pipeline will ignore it, okay? And

12:03.000 --> 12:07.000
it may make sense, okay? So you cannot wait forever.

12:07.000 --> 12:11.000
At some point you will have to stop and move on, okay?

12:11.000 --> 12:13.000
But maybe you want to take it into account, okay? So

12:13.000 --> 12:16.000
maybe you, I don't know, like this is a billion

12:16.000 --> 12:19.000
embarrassing thing and every penny counts, okay? So

12:19.000 --> 12:23.000
then you need to process it. Well, you have to take

12:23.000 --> 12:26.000
yet another decision. How we are going to wait for

12:26.000 --> 12:29.000
late data and how we are going to actually update the

12:29.000 --> 12:32.000
data, okay? Here I'm summing numbers. It's easy,

12:32.000 --> 12:35.000
commutative, associative, really no big deal, okay? So

12:35.000 --> 12:41.000
I can do it like say, I can do it like a monoid in

12:41.000 --> 12:43.000
big data processing. So I can just take the aggregation,

12:43.000 --> 12:46.000
the previous aggregation and keep aggregating. I don't

12:46.000 --> 12:49.000
need to keep all the numbers that I have seen so far.

12:49.000 --> 12:53.000
So it is easy. In other cases, for any non-associative,

12:53.000 --> 12:55.000
non-commutative operation, so you may need to have

12:55.000 --> 12:59.000
actually full data to produce an update, okay? And if

12:59.000 --> 13:02.000
you are working in streaming, maybe you don't want to

13:02.000 --> 13:05.000
accumulate all the data, okay? Because that will

13:05.000 --> 13:07.000
increase the amount of resources that you will need

13:07.000 --> 13:09.000
for your pie brand. It will have impact in performance,

13:09.000 --> 13:11.000
latency, and so on. So here we are accumulating because

13:11.000 --> 13:14.000
the operation allows it and we are actually waiting

13:14.000 --> 13:18.000
for late data, okay? So now we are waiting for late

13:18.000 --> 13:21.000
data, but we don't want to wait forever. We want to

13:21.000 --> 13:23.000
have some numbers, okay? So we are actually producing

13:23.000 --> 13:26.000
several outputs per window, okay? So like for instance

13:26.000 --> 13:29.000
here, continuing with the first, so when the watermark

13:29.000 --> 13:33.000
is trespassed, we produce an output, okay? And then when

13:33.000 --> 13:35.000
we see the new number, so we produce the output, we

13:35.000 --> 13:39.000
produce it really late, okay? But well, so this is, so

13:39.000 --> 13:41.000
we cannot make magic, okay? So this is when we see the

13:41.000 --> 13:45.000
data, so we cannot process it earlier than this, okay?

13:45.000 --> 13:48.000
We may actually decide to produce data, some output,

13:48.000 --> 13:51.000
even before the watermark, because the watermark can be

13:51.000 --> 13:54.000
really slow. It depends on the pace of the updates of

13:54.000 --> 13:57.000
the data. If for whatever reason users are sending

13:57.000 --> 14:00.000
your data with a lot of lateness, the watermark can

14:00.000 --> 14:04.000
progress really slowly, okay? And so the watermark, how

14:04.000 --> 14:07.000
you produce output is always a trade-off in the streaming

14:07.000 --> 14:09.000
between completeness and latency. You need to make a

14:09.000 --> 14:15.000
decision, okay? So here we put an early trigger, so

14:15.000 --> 14:20.000
we are producing output soon, low latency, but it's

14:20.000 --> 14:22.000
not, it's incomplete, because well, so later on we are

14:22.000 --> 14:26.000
going to keep seeing numbers until the watermark.

14:26.000 --> 14:30.000
Good. So basically this is streaming in Apache Bin in

14:30.000 --> 14:33.000
10 minutes. This is a lot of information explained very

14:33.000 --> 14:38.000
quickly. If you want to get deeper, if you want to get

14:38.000 --> 14:41.000
deeper, there is this example here, okay? So in Java and

14:41.000 --> 14:44.000
in Python, so it's available in the two languages, and

14:44.000 --> 14:47.000
you can see everything that we have seen in the previous

14:47.000 --> 14:51.000
slides with all details, okay? And you may run this

14:51.000 --> 14:55.000
locally if you want, so you don't have to have like an

14:55.000 --> 14:59.000
environment, so like a cloud environment, a cluster or a

14:59.000 --> 15:02.000
stream processor or anything like that, so it may run

15:02.000 --> 15:09.000
locally with some synthetic data, made up data, okay?

15:09.000 --> 15:12.000
Now, this is the classic way of doing streaming in Apache

15:12.000 --> 15:15.000
Bin. This has been around for years already, okay? So

15:15.000 --> 15:18.000
this is the same model that is implemented in Spark, it's

15:18.000 --> 15:21.000
the same model that is implemented in Flink, so they

15:21.000 --> 15:25.000
are all kind of similar. There are other things that you

15:25.000 --> 15:31.000
can also do in Apache Bin in streaming, like anything that

15:31.000 --> 15:33.000
you can do in Apache Bin, you can also do it in

15:33.000 --> 15:35.000
streaming, and I'm going to highlight here a couple of

15:35.000 --> 15:39.000
those, okay? I'm leaving out a lot of stuff, because well,

15:39.000 --> 15:43.000
so time is limited, and leave it out, for instance, SQL,

15:43.000 --> 15:47.000
so that was a great talk by Timo focusing on SQL, so you

15:47.000 --> 15:50.000
can also do SQL in Apache Bin if you want in streaming,

15:50.000 --> 15:54.000
okay? So similar examples to what Timo did, and you can

15:54.000 --> 15:57.000
actually run that on Flink if you want, okay? So it may

15:57.000 --> 15:59.000
make sense if you want, I don't know, if at some point

15:59.000 --> 16:02.000
you want to move away from Flink to Dataflow, you want

16:02.000 --> 16:05.000
to move away from Dataflow to Spark, so in order to have

16:05.000 --> 16:08.000
this portability. One thing that you can do in streaming

16:08.000 --> 16:11.000
is stateful functions, and stateful functions are very

16:11.000 --> 16:16.000
interesting for windowing between quotes that doesn't

16:16.000 --> 16:19.000
depend on time. Very typically, I work with customers,

16:19.000 --> 16:21.000
it's like all this windowing triggers things, it's super

16:21.000 --> 16:25.000
interesting, but look, whenever I see a message of this

16:25.000 --> 16:29.000
type, I want to have all the messages that I have seen so

16:29.000 --> 16:32.000
far in a group and do these calculations, and I don't

16:32.000 --> 16:35.000
care about time, okay? I don't care about grouping things

16:35.000 --> 16:38.000
in time. I want to group things by some logic, okay? I'm

16:38.000 --> 16:42.000
going to give you a predicate, you pass a message, if the

16:42.000 --> 16:45.000
message fulfills a condition, I want to close the previous

16:45.000 --> 16:48.000
window and start a new one. How can you do that in

16:48.000 --> 16:51.000
Apache Vint? You can do that with stateful functions, okay?

16:51.000 --> 16:55.000
Stateful functions, so here we have some input, here we

16:55.000 --> 16:58.000
have a map, it's called a part of Apache Vint, and we do

16:58.000 --> 17:01.000
some transformation, and we want to accumulate a state

17:01.000 --> 17:05.000
here, okay? So depending on what we see at some point, we

17:05.000 --> 17:09.000
do something else, and this is mutable state, okay? In a

17:09.000 --> 17:13.000
system like Dataflow, like Flink, like all the systems

17:13.000 --> 17:19.000
where Apache Vint runs, having a state, mutable state in

17:19.000 --> 17:24.000
streaming that is computed in a consistent way is extremely

17:24.000 --> 17:29.000
difficult, okay? One way to shoot yourself in your feet

17:29.000 --> 17:33.000
with systems like this in streaming is trying to keep

17:33.000 --> 17:36.000
accumulating a state using some kind of external system,

17:36.000 --> 17:42.000
okay? Because runners will have, sometimes will have

17:42.000 --> 17:45.000
issues that will be errors, that will be retries,

17:45.000 --> 17:49.000
infrastructure will die, you will have auto scaling, there

17:49.000 --> 17:54.000
are all kinds of situations that the runner may want to

17:54.000 --> 17:57.000
retract the computation and recompute again, okay? So,

17:57.000 --> 18:00.000
and then in all these kinds of situations, having any kind

18:00.000 --> 18:03.000
of external system for mutable state, it's complex, okay?

18:03.000 --> 18:06.000
It's doable, okay? You may have, and you will have with

18:06.000 --> 18:08.000
Apache Vint in any kind of the runners that you can run, you

18:08.000 --> 18:12.000
will have this end-to-end exactly once processing, but

18:12.000 --> 18:15.000
this end-to-end exactly once processing doesn't mean that

18:15.000 --> 18:18.000
your code is going to be executed exactly once. It may

18:18.000 --> 18:21.000
be executed more than once, okay? This is what makes

18:21.000 --> 18:24.000
maintaining external state to a pipeline complex. But if

18:24.000 --> 18:28.000
the state is internal to the pipeline, then, well, so the

18:28.000 --> 18:32.000
system itself can, let's say, take care of the problems of

18:32.000 --> 18:35.000
reprocessing and maintain a mutable state in a consistent

18:35.000 --> 18:39.000
way. So, this is what it's a stateful function in Apache

18:39.000 --> 18:44.000
Vint, and you can use it for use cases like this, okay?

18:44.000 --> 18:49.000
For instance, say that I want to produce windows between

18:49.000 --> 18:53.000
codes based on some kind of property. So, I keep seeing

18:53.000 --> 18:56.000
messages, okay, that I keep processing, okay, and then I

18:56.000 --> 19:00.000
keep accumulating the messages in some state, okay? I

19:00.000 --> 19:03.000
maintain a buffer, like I keep every single message that I

19:03.000 --> 19:06.000
see, and I count, okay? Because, well, the buffer cannot,

19:06.000 --> 19:10.000
so the buffer must have some boundaries, okay? So, because

19:10.000 --> 19:12.000
this is local state that is maintaining the machine in the

19:12.000 --> 19:15.000
worker, in the executor where you are running, and the

19:15.000 --> 19:18.000
executor will have limited resources. Maybe very large

19:18.000 --> 19:22.000
resources, but limited anyways, okay? So, you keep

19:22.000 --> 19:26.000
accumulating, and then you keep processing here, for

19:26.000 --> 19:28.000
instance. So, typically, you can use this, for instance, for

19:28.000 --> 19:30.000
batching to call in an external service, but you can also

19:30.000 --> 19:34.000
do here, whenever I see a specific type of message, I

19:34.000 --> 19:37.000
emit some output, okay? I emit some output, and then I have

19:37.000 --> 19:40.000
applied a window. All the messages that I have in the

19:40.000 --> 19:44.000
buffer, I type a new session ID, a new window ID, and then

19:44.000 --> 19:47.000
I emit them. I hold them for a while until I see the right

19:47.000 --> 19:50.000
message that I need, and then I emit them. There are two

19:50.000 --> 19:55.000
problems here. We want to, so customers always think that

19:55.000 --> 19:59.000
streaming is complex, and they want to get away of all the

19:59.000 --> 20:04.000
temporal base calculations, okay? So complex, so messy.

20:04.000 --> 20:08.000
Look, my algorithm is really much simpler, but it is not.

20:08.000 --> 20:11.000
So, you are in the streaming, so you cannot ignore time,

20:11.000 --> 20:14.000
okay? You have situations where you will see the messages

20:14.000 --> 20:17.000
out of order, and you will have situations where you

20:17.000 --> 20:21.000
will not see the messages for a while, okay? And then you

20:21.000 --> 20:23.000
need to decide what to do in these two cases, even if you

20:23.000 --> 20:26.000
don't want to, okay? What happens when I see out of

20:26.000 --> 20:29.000
order? You may say, I don't care, unlikely, but well, in

20:29.000 --> 20:33.000
some situations, it might be true, okay? Or you may have

20:33.000 --> 20:38.000
to wait, like, some timer in order to give room for late

20:38.000 --> 20:43.000
data to arrive into your code and actually produce the

20:43.000 --> 20:46.000
actual output, okay? So this will be an event time timer,

20:46.000 --> 20:53.000
okay? Look, in event time, you are going to see the messages

20:53.000 --> 20:57.000
and order, okay? So wait 30 seconds, 2 minutes, and so on.

20:57.000 --> 21:00.000
And then when you have seen all the messages, it's the moment

21:00.000 --> 21:02.000
in which you apply the session. That's called an event

21:02.000 --> 21:05.000
time timer. And you may have also problems of staleness,

21:05.000 --> 21:08.000
okay? I'm waiting for the end of my session, but I have not

21:08.000 --> 21:12.000
seen messages. I haven't seen messages in the last 5

21:12.000 --> 21:15.000
minutes in processing time, okay? The problem with event

21:15.000 --> 21:18.000
time is that depends on the progress of the watermark. But

21:18.000 --> 21:20.000
if you stop seeing messages, the watermark will stop

21:20.000 --> 21:25.000
advancing. The watermark is always estimated in being

21:25.000 --> 21:29.000
runners or normally estimated as the time stamp of the

21:29.000 --> 21:32.000
oldest message waiting to be processed, okay? So literally

21:32.000 --> 21:35.000
you may stop your pipeline waiting forever for some data

21:35.000 --> 21:39.000
that maybe it will never arrive, okay? So processing time

21:39.000 --> 21:42.000
timer solved this problem, okay? After 10 minutes, like,

21:42.000 --> 21:45.000
measure with a clock, if nothing comes, I don't care

21:45.000 --> 21:48.000
about the watermark. I don't want to keep going, okay? So

21:48.000 --> 21:51.000
keep going. So data has been lost for whatever reason, and

21:51.000 --> 21:55.000
we cannot wait forever. So this is a stateful function, and

21:55.000 --> 21:58.000
it's also very useful in streaming because it allows you

21:58.000 --> 22:01.000
to apply logic that goes beyond the temporal properties

22:01.000 --> 22:05.000
that we have seen in the previous slides. And here you

22:05.000 --> 22:08.000
have some examples and links. The slides are already

22:08.000 --> 22:11.000
available through the first time website, so I encourage

22:11.000 --> 22:15.000
you to have a look at these examples. What else can I do

22:15.000 --> 22:19.000
in streaming? Matched learning inference, okay? So there

22:19.000 --> 22:23.000
are many ways to do matched learning inference in

22:23.000 --> 22:26.000
streaming at a scale, okay? Many of those quite expensive.

22:26.000 --> 22:30.000
So you can deploy endpoints in cloud platforms, with

22:30.000 --> 22:35.000
GPUs, with a lot of stuff, okay? And normally, so well,

22:35.000 --> 22:38.000
so those are, those solve a lot of functionality for you,

22:38.000 --> 22:41.000
but they are expensive. So what if you want to apply

22:41.000 --> 22:45.000
matched learning inference in a pipeline in Apache Bin?

22:45.000 --> 22:47.000
Well, you could do that, okay? You could be thinking,

22:47.000 --> 22:49.000
well, I can do that, so I can, I don't know, like importance

22:49.000 --> 22:53.000
of flow, load the model, apply it, so you could do a lot

22:53.000 --> 22:56.000
of stuff, okay, yourself. But this is already solved for

22:56.000 --> 22:59.000
you in Apache Bin, okay? So you can run matched learning

22:59.000 --> 23:05.000
inference with the so-called run inference transform, okay?

23:05.000 --> 23:09.000
So we see it here. So right now, it has, let's say, out-of-the-box

23:09.000 --> 23:13.000
support for PyTorch, TensorFlow, and scikit-learn, with more

23:13.000 --> 23:17.000
coming. When you're running a distributed system, and you

23:17.000 --> 23:20.000
want to apply a model, each one of the workers in this

23:20.000 --> 23:24.000
distributed system will have its own memory. So the Apache

23:24.000 --> 23:27.000
Bin runs on top of shared nothing architecture, okay?

23:27.000 --> 23:30.000
Like Flink, Dataflow, Spark. Workers are independent of

23:30.000 --> 23:34.000
each other. They don't share any common state. The state

23:34.000 --> 23:38.000
that we have seen before is actually maintained per key

23:38.000 --> 23:41.000
and per window if we apply it. The per window is totally

23:41.000 --> 23:44.000
local for the worker, and two workers cannot share a state.

23:44.000 --> 23:47.000
But the model, we don't want to instantiate a model if we

23:47.000 --> 23:51.000
have 100 workers 100 times, because the model is going to

23:51.000 --> 23:54.000
be the same for every worker, right? So the model doesn't

23:54.000 --> 23:58.000
change. The model is actually read-only. Run inference

23:58.000 --> 24:01.000
solving this problem by having some state that is shared

24:01.000 --> 24:03.000
across all the workers, and it's transparent for you.

24:03.000 --> 24:05.000
So this is something that you can always implement in a

24:05.000 --> 24:08.000
distributed system, but it's complex. This is the problem

24:08.000 --> 24:11.000
that is solved with run inference. It's only one copy of

24:11.000 --> 24:14.000
the model per, let's say, machine where you're running,

24:14.000 --> 24:18.000
okay? So in memory, okay? Because what you need always

24:18.000 --> 24:21.000
to make an instance in memory to be able to apply it.

24:21.000 --> 24:24.000
But if you have, I don't know, like 100 threads, 100

24:24.000 --> 24:28.000
sub-workers, 100 CPUs inside the same machine, you will

24:28.000 --> 24:32.000
not have 100 copies of the model. Regardless of the

24:32.000 --> 24:35.000
computation model of how the runner is implemented on top

24:35.000 --> 24:37.000
of the machines. There will be only one copy in the

24:37.000 --> 24:40.000
memory of the machine. So this is the problem that is solved

24:40.000 --> 24:44.000
with run inference, okay? So if you want to apply a

24:44.000 --> 24:48.000
streaming inference, it's a very convenient way of doing

24:48.000 --> 24:52.000
this with very little code. And depending on the runner,

24:52.000 --> 24:55.000
this is a possibility in data flow. This is also a

24:55.000 --> 24:58.000
possibility if you are running on top of Kubernetes in a

24:58.000 --> 25:00.000
runner that supports Kubernetes, like, for instance,

25:00.000 --> 25:04.000
Flink. You can do also hinting of the resources that

25:04.000 --> 25:07.000
your transformation is going to need, okay? Hinting of

25:07.000 --> 25:09.000
transformations could be, look, this transformation is

25:09.000 --> 25:12.000
going to need this amount of memory, minimal, okay? But

25:12.000 --> 25:16.000
hinting could also be, look, this is a step that is

25:16.000 --> 25:20.000
running ML inference. So use a GPU for this step, okay?

25:20.000 --> 25:23.000
And then the runner will take care of making sure that

25:23.000 --> 25:26.000
the step that is running, the virtual machine, the same

25:26.000 --> 25:29.000
matches the infrastructure hints that you provide for

25:29.000 --> 25:32.000
the code, and you will have, let's say, different types

25:32.000 --> 25:35.000
of nodes for different types of transformations. One of

25:35.000 --> 25:37.000
the problems of shared-nath in architectures is that

25:37.000 --> 25:40.000
all the workers are alike. All the workers are the same.

25:40.000 --> 25:42.000
With this, you can have different types of workers for

25:42.000 --> 25:45.000
different kinds of transformations, which, let's

25:45.000 --> 25:50.000
say, in terms of cost, it's better. I don't have to

25:50.000 --> 25:53.000
say optimal, so maybe that's better alternatives. But

25:53.000 --> 25:56.000
basically, you use GPUs in the workers where you need to

25:56.000 --> 25:59.000
use GPUs. You don't use them in the workers where you

25:59.000 --> 26:01.000
don't need to use them. And you don't have to worry

26:01.000 --> 26:04.000
about assigning work to different workers. That's

26:04.000 --> 26:06.000
actually done by the runner automatically with these

26:06.000 --> 26:11.000
hints, okay? If you want to know more, here you have

26:11.000 --> 26:14.000
some links. So I have only five minutes left, so I'm

26:14.000 --> 26:17.000
leaving the best for the end of the presentation.

26:17.000 --> 26:21.000
Great. You, look, Israel, you showed some Java at the

26:21.000 --> 26:24.000
beginning. Now you tell me ML inference is so cool, but

26:24.000 --> 26:28.000
it's Python, right? So it's PyTorch, TensorFlow,

26:28.000 --> 26:31.000
Second-Land, it's all Python. One of the things that you

26:31.000 --> 26:35.000
can do in Apache VIN is in cross-language transforms.

26:35.000 --> 26:39.000
Anything that you have available in any of the SDKs,

26:39.000 --> 26:42.000
in any language, you may use it in any other language.

26:42.000 --> 26:45.000
As long as the runner supports this, okay? So it's not

26:45.000 --> 26:48.000
supported by all the runners, but it's supported by the

26:48.000 --> 26:52.000
main runners, okay? So basically, run inference may be

26:52.000 --> 26:56.000
used in Java. If you want to use any transformation from

26:56.000 --> 27:00.000
any language, you have to add some boilerplate code, not

27:00.000 --> 27:03.000
so much, but a little bit. This is already done, let's

27:03.000 --> 27:06.000
say, for the main transforms that are most popular or

27:06.000 --> 27:09.000
that they say that make more sense to be used in

27:09.000 --> 27:12.000
different languages, like a map. Well, using a map from

27:12.000 --> 27:16.000
Java in Python doesn't really make sense, okay? Using

27:16.000 --> 27:20.000
run inference makes using connectors, input-output

27:20.000 --> 27:23.000
connectors from another SDK in Python, for instance,

27:23.000 --> 27:27.000
makes sense because the amount of input-output

27:27.000 --> 27:31.000
connectors that you have per SDK is not the same. So you

27:31.000 --> 27:34.000
may write, I don't know, like the two databases in

27:34.000 --> 27:39.000
Java and two message queues in Python, but maybe you

27:39.000 --> 27:42.000
don't have the same functionalities in all the SDKs.

27:42.000 --> 27:45.000
You can use any connector from any SDK, and this makes

27:45.000 --> 27:48.000
it quite flexible, okay? So these are the so-called

27:48.000 --> 27:51.000
multi-language pipelines, and basically it means that

27:51.000 --> 27:55.000
you can run any transformation in any SDK, and this

27:55.000 --> 27:59.000
is implemented because the runner environment is

27:59.000 --> 28:01.000
containerized, okay? So there's a container per

28:01.000 --> 28:03.000
language, and there's some magic that makes, let's

28:03.000 --> 28:06.000
say, the container communicates between the

28:06.000 --> 28:09.000
Nsals, okay? And the serialization between

28:09.000 --> 28:12.000
programming languages. So this is part of the boilerplate

28:12.000 --> 28:15.000
that you need to take care of. If you use things like

28:15.000 --> 28:18.000
Apache B, the schemas that I haven't talked about in

28:18.000 --> 28:21.000
this talk, so it will be transparent for you, anything

28:21.000 --> 28:23.000
that you have in one schema in one language, you will

28:23.000 --> 28:25.000
be able to serialize it, and they serialize it to any

28:25.000 --> 28:28.000
other language. So if you follow, let's say, if you

28:28.000 --> 28:31.000
follow the Apache B in Capstone, it's quite

28:31.000 --> 28:34.000
straightforward to use these kind of things. Well,

28:34.000 --> 28:38.000
thanks, everyone, so far for your attention. So here

28:38.000 --> 28:41.000
and almost there, don't, here are some links that I

28:41.000 --> 28:44.000
recommend you to have a look if you want to learn

28:44.000 --> 28:48.000
more about Apache B. I have covered a lot of stuff in

28:48.000 --> 28:51.000
a very short time, okay? So there's a lot of things

28:51.000 --> 28:54.000
behind everything that I have explained here. If you

28:54.000 --> 28:57.000
want to know more about all the windowing, streaming,

28:57.000 --> 29:01.000
trigger, triggers, watermarks, and so on, I strongly

29:01.000 --> 29:04.000
recommend you this book. It was released some time

29:04.000 --> 29:06.000
ago. You may think that it's outdated. It's not

29:06.000 --> 29:09.000
outdated. So let's say this is the same model that is

29:09.000 --> 29:11.000
applied in many different streaming systems, and this

29:11.000 --> 29:14.000
is not a book about Apache B. It's a book about the

29:14.000 --> 29:17.000
streaming systems with lots of examples coming from

29:17.000 --> 29:21.000
Apache B. But also examples coming from Flink, Kafka,

29:21.000 --> 29:23.000
PubSub, and many other systems, and it's really, it's

29:23.000 --> 29:27.000
very interesting. It's my favorite book, one of my

29:27.000 --> 29:30.000
favorite books, the other one being the book actually

29:30.000 --> 29:34.000
from Martin Kledman about data intensive applications.

29:34.000 --> 29:37.000
And if you want to know more about BIN, so I recommend

29:37.000 --> 29:40.000
you, the BIN College. There are lots of videos with

29:40.000 --> 29:43.000
lots of details about the things that I have explained

29:43.000 --> 29:47.000
here in YouTube. Some of them are actually linked in

29:47.000 --> 29:50.000
the slides. For sure, the main site of Apache BIN and

29:50.000 --> 29:52.000
the programming guide and all the documentation that is

29:52.000 --> 29:57.000
there. And if you want to learn more about Apache BIN,

29:57.000 --> 29:59.000
there is also the videos of Apache BIN Summit, the

29:59.000 --> 30:02.000
previous editions. And if you want to participate, if

30:02.000 --> 30:04.000
you are here today, so you may be interested in

30:04.000 --> 30:07.000
streaming. So the call for papers is open until March

30:07.000 --> 30:12.000
20th. I think BIN Summit will be in June in New York.

30:12.000 --> 30:15.000
And well, I encourage you to submit talks.

30:15.000 --> 30:17.000
Well, this is all. So thanks all for your attention.

30:17.000 --> 30:19.000
It's time for questions.

30:19.000 --> 30:47.000
So what's the advantage of fishing BIN if you are

30:47.000 --> 30:51.000
already using Flink? So it's portability mainly. So if

30:51.000 --> 30:55.000
tomorrow you want to move away from Flink for whatever

30:55.000 --> 30:59.000
reason, so you should be able to move to other runners

30:59.000 --> 31:01.000
that have the same level of functionality, like for

31:01.000 --> 31:04.000
instance, Dataflow. I don't know. So we have one of

31:04.000 --> 31:07.000
the main committers here of Apache Flink. Say that he

31:07.000 --> 31:11.000
gets hit by a bus. We don't want that to happen. But

31:11.000 --> 31:16.000
that may happen. Everything may happen. So the way

31:16.000 --> 31:19.000
you know the world is really very uncertain. So

31:19.000 --> 31:22.000
basically you have portability.

31:22.000 --> 31:24.000
Thank you very much. Unfortunately we don't have

31:24.000 --> 31:27.000
time for more questions right now, but I'm sure you

31:27.000 --> 31:29.000
will be happy to answer it.

31:29.000 --> 31:47.000
Thanks.
