1
0:00:00.000 --> 0:00:10.440
So, the thing about QuestDB, apart from being open source, we want people to know us because

2
0:00:10.440 --> 0:00:14.720
we try to be very performant, but specifically in small machines.

3
0:00:14.720 --> 0:00:22.140
Like perform very well in 120 CPUs and 200 gigs of RAM.

4
0:00:22.140 --> 0:00:23.640
It's okay.

5
0:00:23.640 --> 0:00:29.200
Performing very well in 4 CPUs and 16 of RAM, 16 gigs, is more difficult.

6
0:00:29.200 --> 0:00:31.800
So we try to optimize for that.

7
0:00:31.800 --> 0:00:37.320
Actually in the past we were optimizing for the larger instance use case and then we realized

8
0:00:37.320 --> 0:00:40.440
not everybody has a super large instance at home.

9
0:00:40.440 --> 0:00:42.160
We try to be better at that.

10
0:00:42.160 --> 0:00:46.560
We also try to be very good with developer experience that you get performance out of

11
0:00:46.560 --> 0:00:47.560
the box.

12
0:00:47.560 --> 0:00:49.440
There are many things you can tweak in QuestDB.

13
0:00:49.440 --> 0:00:54.920
In every other database, every other system, a lot of configuration.

14
0:00:54.920 --> 0:01:02.080
The memory, page size, the buffers and whatnot, with CPUs, the what, blah, blah, blah, blah.

15
0:01:02.080 --> 0:01:05.320
By default, if you don't touch anything, we should perform well.

16
0:01:05.320 --> 0:01:08.280
And then if you have expert tolerance, you might fine tune.

17
0:01:08.280 --> 0:01:15.160
But we try hard to make developer experience simple and that's why we choose SQL also for

18
0:01:15.160 --> 0:01:16.160
querying data.

19
0:01:16.160 --> 0:01:20.200
So another time series database make the tradeoff.

20
0:01:20.200 --> 0:01:21.200
We want to perform.

21
0:01:21.200 --> 0:01:26.240
We need to use a different language, which is cool because, you know, I get it.

22
0:01:26.240 --> 0:01:31.880
We choose SQL because we want developers to have an easy way learning QuestDB.

23
0:01:31.880 --> 0:01:36.400
For ingesting data, you can use SQL but we also offer a different protocol which is faster.

24
0:01:36.400 --> 0:01:40.720
That's why we have library so you don't have to go low level to be performant.

25
0:01:40.720 --> 0:01:41.720
But that's the idea.

26
0:01:41.720 --> 0:01:44.440
And we are open source, very proud about being open source.

27
0:01:44.440 --> 0:01:46.640
But why we are building another database?

28
0:01:46.640 --> 0:01:48.360
There are a lot of databases.

29
0:01:48.360 --> 0:01:52.800
If you walk around, you're going to read about every type of database out there.

30
0:01:52.800 --> 0:01:55.880
Just today here, I saw MongoDB, I saw Clickhouse.

31
0:01:55.880 --> 0:02:00.400
There's a SQL about MariaDB.

32
0:02:00.400 --> 0:02:01.400
Why you need another database?

33
0:02:01.400 --> 0:02:03.160
Another open source database?

34
0:02:03.160 --> 0:02:06.920
Because different data look different and can have different problems.

35
0:02:06.920 --> 0:02:11.000
And in our case, we are specialized on time series.

36
0:02:11.000 --> 0:02:12.000
We don't do anything else.

37
0:02:12.000 --> 0:02:19.440
I mean, if you try to use QuestDB for full text analytics, we are truly the worst database

38
0:02:19.440 --> 0:02:21.120
ever for that.

39
0:02:21.120 --> 0:02:27.840
If you try to use QuestDB for geospatial queries, we support some geospatial things kind of

40
0:02:27.840 --> 0:02:28.840
a bit.

41
0:02:28.840 --> 0:02:33.040
We have a specific data type about geohasses.

42
0:02:33.040 --> 0:02:34.760
So we have a type about that.

43
0:02:34.760 --> 0:02:39.760
But we are not good for geospatial unless it is part of time series plus geo.

44
0:02:39.760 --> 0:02:40.760
That's kind of the idea.

45
0:02:40.760 --> 0:02:46.760
And we specialize only on time series analytics, on data which is changing over time and you

46
0:02:46.760 --> 0:02:49.160
want to monitor and track those changes.

47
0:02:49.160 --> 0:02:50.160
That's the idea.

48
0:02:50.160 --> 0:02:51.960
We are not good for anything else.

49
0:02:51.960 --> 0:02:56.520
If you try to use QuestDB for everything, boy, what a disappointment we are going to be.

50
0:02:56.520 --> 0:02:59.680
But if you try for time series database, this will be one of the good ones.

51
0:02:59.680 --> 0:03:00.680
That's kind of the idea.

52
0:03:00.680 --> 0:03:04.360
And that's why we are building QuestDB because there are a lot of time series data out there.

53
0:03:04.360 --> 0:03:06.840
And how do you know if you have a time series stolen?

54
0:03:06.840 --> 0:03:08.320
I have here a lot of things.

55
0:03:08.320 --> 0:03:10.600
I want to just read a couple of them.

56
0:03:10.600 --> 0:03:17.320
But basically, if most of the time you are reading data on a slice of time, tell me which

57
0:03:17.320 --> 0:03:20.960
energy consumption I have over the last minute.

58
0:03:20.960 --> 0:03:25.760
Tell me how is the nuclear reactor doing in the past 10 microseconds.

59
0:03:25.760 --> 0:03:31.640
Tell me what is the conversion for this user in the past week.

60
0:03:31.640 --> 0:03:37.560
Let me know for all the data I have a moving vehicle, which was the last position I saw

61
0:03:37.560 --> 0:03:42.000
it and what was the sensor in this particular point in time.

62
0:03:42.000 --> 0:03:44.920
So if you have that, time series can be interesting.

63
0:03:44.920 --> 0:03:47.880
Also with time series you have other types of tolerance.

64
0:03:47.880 --> 0:03:52.520
Data tends to be inserting faster than it reads.

65
0:03:52.520 --> 0:03:55.360
Databases, historically, have been optimized for reads.

66
0:03:55.360 --> 0:03:59.000
They try every trick in the book for making reads super fast.

67
0:03:59.000 --> 0:04:02.040
When you insert data, you need to define indexes.

68
0:04:02.040 --> 0:04:04.640
And they are going to index by many different things.

69
0:04:04.640 --> 0:04:08.000
And they keep caching memory for a lot of things and blah, blah, blah, blah.

70
0:04:08.000 --> 0:04:10.200
So reading is the key thing.

71
0:04:10.200 --> 0:04:12.800
Because usually you read data much more than you write.

72
0:04:12.800 --> 0:04:15.040
Not so intense databases.

73
0:04:15.040 --> 0:04:17.440
We can support heavy reads on top of that.

74
0:04:17.440 --> 0:04:21.040
But we need to support heavy inserts and keep perform of that.

75
0:04:21.040 --> 0:04:22.760
We don't use indexes.

76
0:04:22.760 --> 0:04:25.080
The performance we see today is with no indexes.

77
0:04:25.080 --> 0:04:26.640
We don't need them.

78
0:04:26.640 --> 0:04:27.920
We don't want them.

79
0:04:27.920 --> 0:04:30.520
Because having an index slows down ingestion.

80
0:04:30.520 --> 0:04:32.240
It's a luxury we cannot have.

81
0:04:32.240 --> 0:04:34.720
So we have some kind of indexing, but we don't have indexes.

82
0:04:34.720 --> 0:04:35.720
Not as you know them.

83
0:04:35.720 --> 0:04:37.720
That's kind of the idea here.

84
0:04:37.720 --> 0:04:38.720
So it's slightly different.

85
0:04:38.720 --> 0:04:44.040
And when you have data that you are writing very often, that data is going to grow.

86
0:04:44.040 --> 0:04:46.520
And it can grow fast.

87
0:04:46.520 --> 0:04:50.920
And you need to have some way of loading or deleting that data.

88
0:04:50.920 --> 0:04:56.160
On a traditional database, you just don't say, oh, I have, I don't know, I'm Amazon.

89
0:04:56.160 --> 0:04:57.160
And I'm getting users.

90
0:04:57.160 --> 0:04:59.680
It's like, oh, I already have a million users.

91
0:04:59.680 --> 0:05:00.680
A million won.

92
0:05:00.680 --> 0:05:01.680
I'm going to delay the old users.

93
0:05:01.680 --> 0:05:02.680
But you don't do that.

94
0:05:02.680 --> 0:05:03.680
I mean, sometimes you do.

95
0:05:03.680 --> 0:05:04.680
But you don't do that.

96
0:05:04.680 --> 0:05:06.880
You don't really do that on your databases.

97
0:05:06.880 --> 0:05:12.040
On time series database, almost all of them have some mechanism to deal with historical

98
0:05:12.040 --> 0:05:14.280
data and do something with that.

99
0:05:14.280 --> 0:05:16.040
In our case, you can mount partitions.

100
0:05:16.040 --> 0:05:17.800
You can mount to cheaper storage.

101
0:05:17.800 --> 0:05:18.800
Those kind of things.

102
0:05:18.800 --> 0:05:21.560
But we have the command Sunday.

103
0:05:21.560 --> 0:05:22.840
It is designed for that kind of thing.

104
0:05:22.840 --> 0:05:23.840
That kind of area.

105
0:05:23.840 --> 0:05:27.240
Many other things about how you have a time series stolen.

106
0:05:27.240 --> 0:05:28.680
But that kind of the area.

107
0:05:28.680 --> 0:05:33.840
But better than me just telling you, I'm going to show you some queries on top of demo data

108
0:05:33.840 --> 0:05:34.840
sets.

109
0:05:34.840 --> 0:05:38.320
So you get the feeling of what a time series database might be interesting.

110
0:05:38.320 --> 0:05:42.080
And then we'll go into details about the ingesting data and about all those things.

111
0:05:42.080 --> 0:05:43.480
Does that sound good so far?

112
0:05:43.480 --> 0:05:44.480
Yeah.

113
0:05:44.480 --> 0:05:45.480
Do you have any questions?

114
0:05:45.480 --> 0:05:47.480
I'm happy to take them during the talk, by the way.

115
0:05:47.480 --> 0:05:49.240
Not only at the end.

116
0:05:49.240 --> 0:05:52.120
So we have a live demo.

117
0:05:52.120 --> 0:05:58.120
Demo.questdb.io.

118
0:05:58.120 --> 0:05:59.760
This is running on a large machine on AWS.

119
0:05:59.760 --> 0:06:01.520
We don't need all the power.

120
0:06:01.520 --> 0:06:03.080
But since it's open to the public.

121
0:06:03.080 --> 0:06:06.880
And here we have a few different data sets.

122
0:06:06.880 --> 0:06:07.880
That is one.

123
0:06:07.880 --> 0:06:10.280
You are in a big data room.

124
0:06:10.280 --> 0:06:12.880
So you're truly familiar with the taxi rides.

125
0:06:12.880 --> 0:06:15.240
New York City taxi rides data set.

126
0:06:15.240 --> 0:06:19.960
It's the city of New York has a data set which is very cool for machine learning and for

127
0:06:19.960 --> 0:06:21.480
big data.

128
0:06:21.480 --> 0:06:24.680
Which is taxi rides in the city of New York.

129
0:06:24.680 --> 0:06:28.360
And the rides started when it finished.

130
0:06:28.360 --> 0:06:30.320
Also the coordinate and a few things.

131
0:06:30.320 --> 0:06:32.320
Like the tip and the amount of the fur.

132
0:06:32.320 --> 0:06:33.920
How many people blah blah blah.

133
0:06:33.920 --> 0:06:39.080
So we took that open data set and we just put it here on QuestDB.

134
0:06:39.080 --> 0:06:40.560
A few years of the data set.

135
0:06:40.560 --> 0:06:41.560
Yes, you know.

136
0:06:41.560 --> 0:06:43.280
Lot of columns here.

137
0:06:43.280 --> 0:06:47.200
So let me just show you how big this is.

138
0:06:47.200 --> 0:06:49.840
This is right now.

139
0:06:49.840 --> 0:06:52.560
Is the size okay?

140
0:06:52.560 --> 0:06:58.160
Maybe not.

141
0:06:58.160 --> 0:07:00.080
So it's 1.6 billion rows.

142
0:07:00.080 --> 0:07:01.080
Which is not huge.

143
0:07:01.080 --> 0:07:08.280
I mean if you have a relational database, 1.6 billion rows.

144
0:07:08.280 --> 0:07:10.720
Relational databases today they are great.

145
0:07:10.720 --> 0:07:15.480
But 1.6 billion rows is like yeah, I couldn't work with that.

146
0:07:15.480 --> 0:07:17.080
I'm not super comfortable.

147
0:07:17.080 --> 0:07:18.360
For us it's cute.

148
0:07:18.360 --> 0:07:23.440
It's like it's a data set which is respectable but not really cute.

149
0:07:23.440 --> 0:07:26.120
But 1.6 billion rows.

150
0:07:26.120 --> 0:07:29.240
So now what if I want to do something like for example, I don't know.

151
0:07:29.240 --> 0:07:35.760
I want to calculate the average of whichever, this for example.

152
0:07:35.760 --> 0:07:36.760
This number.

153
0:07:36.760 --> 0:07:42.800
I want to average the fair amount over 1.6 billion trips.

154
0:07:42.800 --> 0:07:48.200
How long do you expect your database to take to go do a full scan over

155
0:07:48.200 --> 0:07:51.520
1.6 billion rows and compute the average?

156
0:07:51.520 --> 0:07:53.640
No indexes, no anything.

157
0:07:53.640 --> 0:07:54.640
How long would you say?

158
0:07:54.640 --> 0:07:55.640
More or less.

159
0:07:55.640 --> 0:07:56.640
Ball park.

160
0:07:56.640 --> 0:07:57.640
1.6 billion rows.

161
0:07:57.640 --> 0:07:58.640
No one?

162
0:07:58.640 --> 0:07:59.640
Sorry?

163
0:07:59.640 --> 0:08:00.640
How is the size in gigabytes?

164
0:08:00.640 --> 0:08:01.640
Megabytes.

165
0:08:01.640 --> 0:08:06.840
I don't know for the whole data set but this is a double.

166
0:08:06.840 --> 0:08:08.880
I mean I'm really just no.

167
0:08:08.880 --> 0:08:09.880
It's big.

168
0:08:09.880 --> 0:08:10.880
It's big.

169
0:08:10.880 --> 0:08:17.520
When you download the CSV, it's about 600 megabytes and you have several of those.

170
0:08:17.520 --> 0:08:20.680
This is in the, you know, it's laggish.

171
0:08:20.680 --> 0:08:21.680
But anyway.

172
0:08:21.680 --> 0:08:26.680
Well, actually it was slower than I thought.

173
0:08:26.680 --> 0:08:29.280
It took, usually it takes half a second.

174
0:08:29.280 --> 0:08:32.000
This time it took 0.6 seconds.

175
0:08:32.000 --> 0:08:33.360
I know it's slow.

176
0:08:33.360 --> 0:08:35.520
I know but this was a reason.

177
0:08:35.520 --> 0:08:36.520
Sorry about that.

178
0:08:36.520 --> 0:08:39.260
But I told you, I told you we are a time series database.

179
0:08:39.260 --> 0:08:40.760
We are super slow for other things.

180
0:08:40.760 --> 0:08:42.400
This is not a time series query.

181
0:08:42.400 --> 0:08:44.000
Did you see any time stamp here?

182
0:08:44.000 --> 0:08:45.000
I didn't see anything.

183
0:08:45.000 --> 0:08:47.640
This is just a full scan.

184
0:08:47.640 --> 0:08:48.640
We parallelize.

185
0:08:48.640 --> 0:08:50.520
We read data and we are slow.

186
0:08:50.520 --> 0:08:55.640
We take almost over half a second to go over only 1.6 billion rows.

187
0:08:55.640 --> 0:08:56.640
Unforgivable.

188
0:08:56.640 --> 0:08:57.640
Sorry about that.

189
0:08:57.640 --> 0:08:58.640
But there will be here.

190
0:08:58.640 --> 0:08:59.640
No, that's okay.

191
0:08:59.640 --> 0:09:03.160
I mean I'm kind of half kidding but not really.

192
0:09:03.160 --> 0:09:06.680
But wait until I put a time dimension.

193
0:09:06.680 --> 0:09:08.000
Now yes.

194
0:09:08.000 --> 0:09:15.240
I want only, for example, I want only one year of data.

195
0:09:15.240 --> 0:09:19.760
I'm going to just also add another computation because I know that person.

196
0:09:19.760 --> 0:09:23.100
It's just counting data which is super fast.

197
0:09:23.100 --> 0:09:24.520
So I'm going to add another computation.

198
0:09:24.520 --> 0:09:29.320
So I'm going to count the data and only for 2016.

199
0:09:29.320 --> 0:09:30.320
And this is better.

200
0:09:30.320 --> 0:09:36.960
This is already 100 milliseconds because we are going only over a few rows.

201
0:09:36.960 --> 0:09:39.440
We are going only about...

202
0:09:39.440 --> 0:09:44.120
Yeah, it's only 146 million rows.

203
0:09:44.120 --> 0:09:45.120
This is much more manageable.

204
0:09:45.120 --> 0:09:47.600
So only 140 million rows.

205
0:09:47.600 --> 0:09:48.600
That's better.

206
0:09:48.600 --> 0:09:51.240
So we can go actually very fast on this.

207
0:09:51.240 --> 0:09:59.720
And then if you keep going down, oh no, I want only one month of data which is, I don't

208
0:09:59.720 --> 0:10:03.920
know, still, yeah, 12 million rows.

209
0:10:03.920 --> 0:10:06.840
So a month of data is 60 milliseconds.

210
0:10:06.840 --> 0:10:10.280
For one day of data, of course, is way faster.

211
0:10:10.280 --> 0:10:12.480
This is already 50 milliseconds.

212
0:10:12.480 --> 0:10:19.680
If I go to one specific hour and minute, it will be, you know, kind of not much faster

213
0:10:19.680 --> 0:10:20.680
because...

214
0:10:20.680 --> 0:10:21.680
What?

215
0:10:21.680 --> 0:10:22.680
It's under one millisecond.

216
0:10:22.680 --> 0:10:26.360
Oh yeah, it's under one millisecond actually.

217
0:10:26.360 --> 0:10:27.360
Thank you for that.

218
0:10:27.360 --> 0:10:30.240
But still, like, you know, we have partitions.

219
0:10:30.240 --> 0:10:36.440
So basically, one thing we do, we only go to the partition where the data is stored.

220
0:10:36.440 --> 0:10:37.840
So we only attack that part of the data.

221
0:10:37.840 --> 0:10:39.320
But that's another thing.

222
0:10:39.320 --> 0:10:43.480
For when you have like that time component, we are quite fast, or fairly fast.

223
0:10:43.480 --> 0:10:46.280
That's kind of the beauty for a time series database.

224
0:10:46.280 --> 0:10:49.280
And we can do also interesting, other interesting things.

225
0:10:49.280 --> 0:10:56.040
If I go to the same table and I saw you, what this looked like, you can see that for the

226
0:10:56.040 --> 0:11:00.760
same second, I have many trips because this is New York baby.

227
0:11:00.760 --> 0:11:04.560
And in New York, you know, the city that never leaves.

228
0:11:04.560 --> 0:11:06.960
You can't get back in every corner.

229
0:11:06.960 --> 0:11:08.680
You get rich when you land in New York.

230
0:11:08.680 --> 0:11:10.000
I spent one year there.

231
0:11:10.000 --> 0:11:11.160
It's not like that.

232
0:11:11.160 --> 0:11:17.160
Anyway, so in every particular second, even at midnight, you have always a few trips,

233
0:11:17.160 --> 0:11:18.160
at least.

234
0:11:18.160 --> 0:11:19.160
Okay?

235
0:11:19.160 --> 0:11:20.160
So actually, you could do that.

236
0:11:20.160 --> 0:11:26.840
You could do something like, I want to know the, I want to, if I want to do something

237
0:11:26.840 --> 0:11:41.600
like give me the daytime on how many trips are ending where this daytime is in, for example,

238
0:11:41.600 --> 0:11:43.600
June 21st.

239
0:11:43.600 --> 0:11:47.760
Siri, what are you doing there, man?

240
0:11:47.760 --> 0:11:51.520
I didn't even know I had Siri here.

241
0:11:51.520 --> 0:11:52.520
Okay.

242
0:11:52.520 --> 0:11:54.320
So, I don't know.

243
0:11:54.320 --> 0:12:01.480
For example, in this particular minute, in one particular day, I want to sample in one

244
0:12:01.480 --> 0:12:05.960
second interval and know how many trips I have for every particular second.

245
0:12:05.960 --> 0:12:08.400
So that's another thing you can do in a time series database.

246
0:12:08.400 --> 0:12:12.480
Rather than grouping by columns that you can also do, you can group by time.

247
0:12:12.480 --> 0:12:14.480
You call the sample by.

248
0:12:14.480 --> 0:12:22.240
So we can sample by any, we go from microsecond to year, I guess.

249
0:12:22.240 --> 0:12:23.480
Yeah, microsecond to year.

250
0:12:23.480 --> 0:12:27.760
So you can group by microsecond, millisecond, second, year, day, whatever.

251
0:12:27.760 --> 0:12:31.960
So in this case, I'm saying, okay, in this particular second, I have six trips and five

252
0:12:31.960 --> 0:12:33.800
trips and blah, blah, blah.

253
0:12:33.800 --> 0:12:34.800
You get there, yeah?

254
0:12:34.800 --> 0:12:40.280
So something I wanted to show you, which is another cool one, it's, I have this data set

255
0:12:40.280 --> 0:12:42.160
with several trips every second.

256
0:12:42.160 --> 0:12:48.480
I have another data set also with data from Manhattan is the weather data set.

257
0:12:48.480 --> 0:12:53.440
So maybe it could be interesting to know, to join those two data sets.

258
0:12:53.440 --> 0:12:58.960
It will be cool to know the weather that they had for a particular trip, because maybe that

259
0:12:58.960 --> 0:12:59.960
gives me some insight.

260
0:12:59.960 --> 0:13:01.040
I don't know.

261
0:13:01.040 --> 0:13:04.880
The challenge is this data set, of course, is real life.

262
0:13:04.880 --> 0:13:06.840
It's a different open data set.

263
0:13:06.840 --> 0:13:08.600
It's not at the same resolution.

264
0:13:08.600 --> 0:13:12.280
We don't have weather changes every second.

265
0:13:12.280 --> 0:13:14.040
In my hometown, sometimes that happens.

266
0:13:14.040 --> 0:13:17.560
And when I was living in London, that was crazy.

267
0:13:17.560 --> 0:13:23.680
But in real life, we don't measure, we don't store weather changes every second.

268
0:13:23.680 --> 0:13:28.920
In this particular data set, we have about two or three records every hour.

269
0:13:28.920 --> 0:13:35.680
So now if I want to join a data set with subsecond resolution, a data set with subhour resolution,

270
0:13:35.680 --> 0:13:40.480
and I want to do a join, if I want to do it in other databases, I could do it.

271
0:13:40.480 --> 0:13:42.440
It will take me a while.

272
0:13:42.440 --> 0:13:44.280
Then I will think I have it, and I wouldn't.

273
0:13:44.280 --> 0:13:47.280
And then it will be like, yeah, this makes sense or not really.

274
0:13:47.280 --> 0:13:48.840
And a week later, I will be crying.

275
0:13:48.840 --> 0:13:49.920
I don't know that.

276
0:13:49.920 --> 0:13:51.880
So I still know.

277
0:13:51.880 --> 0:13:56.360
So one thing we have here, we have a demo set, some example points.

278
0:13:56.360 --> 0:14:00.960
I'm going to move on to another thing really quickly, because otherwise.

279
0:14:00.960 --> 0:14:07.760
But this one I really like, we have a special type of join, which we call an as of join,

280
0:14:07.760 --> 0:14:09.680
which basically does this.

281
0:14:09.680 --> 0:14:14.320
I'm going to select the data from the table I told you already for one particular day

282
0:14:14.320 --> 0:14:15.320
in time.

283
0:14:15.320 --> 0:14:21.000
And then I'm going to do what we call an as of join, which basically says, this table

284
0:14:21.000 --> 0:14:22.920
has a timestamp.

285
0:14:22.920 --> 0:14:24.400
We call it the designated timestamp.

286
0:14:24.400 --> 0:14:26.760
You decide which is the column.

287
0:14:26.760 --> 0:14:27.760
You have several.

288
0:14:27.760 --> 0:14:30.440
So we have the designated timestamp in one.

289
0:14:30.440 --> 0:14:34.840
The timestamp in the other joined by the ones that are closer to each other.

290
0:14:34.840 --> 0:14:38.800
In this case, as of means the one which is exactly the same or immediately before me,

291
0:14:38.800 --> 0:14:40.120
the one which is closer to me.

292
0:14:40.120 --> 0:14:45.440
So what happened before we have also the one strictly behind me before me cannot be the

293
0:14:45.440 --> 0:14:46.440
same.

294
0:14:46.440 --> 0:14:47.440
But that's the idea.

295
0:14:47.440 --> 0:14:50.680
So in this case, for getting to different data sets, I can just do that.

296
0:14:50.680 --> 0:14:54.920
Also, I want to add here the timestamp for the other table.

297
0:14:54.920 --> 0:14:56.320
So it's clear.

298
0:14:56.320 --> 0:15:03.960
So if I run this query, now here I can see for each record on the New York taxi rides,

299
0:15:03.960 --> 0:15:09.320
I'm always getting the same timestamp in the weather data set, because I have only one

300
0:15:09.320 --> 0:15:12.680
entry every 40 or 45 minutes.

301
0:15:12.680 --> 0:15:19.320
If I move to a different point in the day to this day, but instead of at 12, at 1255,

302
0:15:19.320 --> 0:15:25.880
for example, I should see already that I'm matching to a different entry on this table.

303
0:15:25.880 --> 0:15:26.880
So that's it.

304
0:15:26.880 --> 0:15:28.360
I have different resolutions.

305
0:15:28.360 --> 0:15:31.160
I don't care which one we joined by time, because we're about time.

306
0:15:31.160 --> 0:15:32.160
That's kind of the idea.

307
0:15:32.160 --> 0:15:36.860
That's what I think is I have more interesting queries, but maybe for a different date.

308
0:15:36.860 --> 0:15:38.840
So that's the first thing.

309
0:15:38.840 --> 0:15:42.880
So I told you, okay, now you get the idea why tensile is coming to the extent that kind

310
0:15:42.880 --> 0:15:47.160
of things we can do down sampling, all those things in machine learning is very important.

311
0:15:47.160 --> 0:15:51.800
You have data maybe every second, and then you want to do a forecasting and doesn't make

312
0:15:51.800 --> 0:15:54.600
sense to train a model with every second data.

313
0:15:54.600 --> 0:15:59.600
In many cases, maybe you want to down sample to 15 minutes interval with this trick, you

314
0:15:59.600 --> 0:16:00.760
can do it easily.

315
0:16:00.760 --> 0:16:01.760
So that's kind of the idea.

316
0:16:01.760 --> 0:16:03.680
But I was speaking about ingesting data.

317
0:16:03.680 --> 0:16:09.000
So ingesting over 1 million times three years per second on a single instance.

318
0:16:09.000 --> 0:16:10.840
It's interesting.

319
0:16:10.840 --> 0:16:16.200
But ingesting over 1 million records per second, a single instance, it's easy, actually.

320
0:16:16.200 --> 0:16:20.920
I could just write to a file appending lines, and that will be it.

321
0:16:20.920 --> 0:16:25.480
The interesting bit is actually being able to ingest data while you are able to put it

322
0:16:25.480 --> 0:16:27.880
in real time, the same data you're ingesting.

323
0:16:27.880 --> 0:16:28.880
That's the trick.

324
0:16:28.880 --> 0:16:32.600
Because you're ingesting, I mean, you put it there, and you're like, why ingesting 1

325
0:16:32.600 --> 0:16:33.600
million records?

326
0:16:33.600 --> 0:16:36.800
When you think about this, like, well, wait, but how long do I have to wait to put in the

327
0:16:36.800 --> 0:16:37.800
data?

328
0:16:37.800 --> 0:16:40.720
So the idea is you can put in the data at the same time.

329
0:16:40.720 --> 0:16:45.240
All benchmarks are lies, of course, on the same benchmark that I want to tell you, other

330
0:16:45.240 --> 0:16:48.280
people will tell you the contrary, and I'm totally fine with that.

331
0:16:48.280 --> 0:16:55.040
A couple of years ago, we published an article saying, hey, we can ingest now at 1.4 million.

332
0:16:55.040 --> 0:16:59.320
The slides are linked already on the first page, by the way.

333
0:16:59.320 --> 0:17:01.320
Thank you.

334
0:17:01.320 --> 0:17:07.000
So our CTO posted about how we were ingesting 1.4 million records per second.

335
0:17:07.000 --> 0:17:12.760
These records were, they had like 20 columns, 10 dimensions, 10 strings, and 10 metrics,

336
0:17:12.760 --> 0:17:13.760
10 numbers.

337
0:17:13.760 --> 0:17:19.680
We're pulling guest records of 20 columns with 10 strings and 10 numbers, 1.4 million

338
0:17:19.680 --> 0:17:24.260
records per second, while running queries, which is the other bit.

339
0:17:24.260 --> 0:17:29.920
So we were able to scan over 4 million, 4 billion records per second, at the same time,

340
0:17:29.920 --> 0:17:32.840
in relatively small machines, relatively small.

341
0:17:32.840 --> 0:17:35.400
So that's kind of the idea.

342
0:17:35.400 --> 0:17:37.160
And these benchmarks, we didn't write it.

343
0:17:37.160 --> 0:17:40.840
There is a benchmark specifically for time series databases.

344
0:17:40.840 --> 0:17:45.440
As I told you earlier, if you load data in QuestDB, you can load relational data into

345
0:17:45.440 --> 0:17:47.280
QuestDB, and you can run queries.

346
0:17:47.280 --> 0:17:52.240
If you try to run a conventional benchmark on QuestDB, it's going to be super slow.

347
0:17:52.240 --> 0:17:54.000
We are not designed for full-text search.

348
0:17:54.000 --> 0:18:01.400
We are not designed for just operations within individual records or doing updating data.

349
0:18:01.400 --> 0:18:02.480
We are not designed for that.

350
0:18:02.480 --> 0:18:05.320
We can do it, but we are not designed for that.

351
0:18:05.320 --> 0:18:08.320
So there is also the other time series databases.

352
0:18:08.320 --> 0:18:13.600
So InfluxDB, another open source database, created this benchmark, the TSVS benchmark,

353
0:18:13.600 --> 0:18:16.920
which is specifically about time series databases.

354
0:18:16.920 --> 0:18:20.400
So the queries and the ingestion patterns matches what you would expect from a time

355
0:18:20.400 --> 0:18:21.800
series database.

356
0:18:21.800 --> 0:18:27.360
Now it's maintained by time scale, which is another open source database on top of Postgres.

357
0:18:27.360 --> 0:18:32.480
And we have our own, you know, the recent adapter for running that on top of QuestDB.

358
0:18:32.480 --> 0:18:36.640
And with that benchmark, it's with the one that we are getting those results.

359
0:18:36.640 --> 0:18:39.440
So with that particular benchmark is the one giving the results.

360
0:18:39.440 --> 0:18:42.960
So you know, your management might vary also depending on the hardware.

361
0:18:42.960 --> 0:18:47.240
If you try to run the benchmark in the cloud, it's going to be slower, always.

362
0:18:47.240 --> 0:18:53.320
Because in the cloud, by default, you use on AWS, you use CVS, on well cloud, you use

363
0:18:53.320 --> 0:18:57.600
the attached storage, it's networking storage, has on latency, because they are not local

364
0:18:57.600 --> 0:18:58.600
disk.

365
0:18:58.600 --> 0:18:59.920
They are super cool, but they are not local.

366
0:18:59.920 --> 0:19:01.840
It's going to be always slower.

367
0:19:01.840 --> 0:19:08.240
You want to get this on Google Cloud or AWS, you can do it, you have to use NVMe disk,

368
0:19:08.240 --> 0:19:13.160
which are local disk without attached to the instance, but they disappear when they when

369
0:19:13.160 --> 0:19:14.640
you close the instance.

370
0:19:14.640 --> 0:19:18.320
But with those disks, you will be getting the same benchmark.

371
0:19:18.320 --> 0:19:20.680
So hardware is also important with the benchmark.

372
0:19:20.680 --> 0:19:24.160
But that's the idea, you know, that that's how how we did it.

373
0:19:24.160 --> 0:19:28.120
Before I tell you a bit about the technical decisions that they will not have super time,

374
0:19:28.120 --> 0:19:33.400
but I want to show you how we are doing this ingestion.

375
0:19:33.400 --> 0:19:37.160
So let me ask if I can move this out of the way.

376
0:19:37.160 --> 0:19:38.720
So this is a scraping go.

377
0:19:38.720 --> 0:19:40.440
I don't know any go at all.

378
0:19:40.440 --> 0:19:43.440
But I know to run this so you know, and I don't want to advocate.

379
0:19:43.440 --> 0:19:48.560
I mean, I couldn't tell you that I know a lot of go but I have no idea.

380
0:19:48.560 --> 0:19:50.440
So go long, it's a language.

381
0:19:50.440 --> 0:19:54.440
So yeah, we have our it's pretty cool.

382
0:19:54.440 --> 0:19:59.600
So we have this library or package or whatever they call it in go, which is our official

383
0:19:59.600 --> 0:20:00.600
package.

384
0:20:00.600 --> 0:20:01.600
Caro, whatever.

385
0:20:01.600 --> 0:20:03.320
I don't know.

386
0:20:03.320 --> 0:20:04.920
So this is I'm missing languages here.

387
0:20:04.920 --> 0:20:06.400
Anyway, it's not that happy.

388
0:20:06.400 --> 0:20:07.400
Thank you.

389
0:20:07.400 --> 0:20:09.360
So yeah, this is our theme.

390
0:20:09.360 --> 0:20:13.120
I'm connecting to local host to the default port in QuestDB.

391
0:20:13.120 --> 0:20:15.360
I'm going to be simulating data.

392
0:20:15.360 --> 0:20:17.680
So I'm simulating IoT data.

393
0:20:17.680 --> 0:20:23.240
And I'm going to be putting a device type which can be red or blue or green or yellow.

394
0:20:23.240 --> 0:20:30.200
I'm going to be outputting duration, latitude, longitude, speed, and time stamping nanoseconds.

395
0:20:30.200 --> 0:20:35.760
And I'm going to do this in chunks of in batches of 50,000 records.

396
0:20:35.760 --> 0:20:37.720
I want to do this 200 times.

397
0:20:37.720 --> 0:20:40.360
50,000 records 200 times, 10 million records.

398
0:20:40.360 --> 0:20:45.440
I want to be inserting 10 million records on a database on a table that does not exist.

399
0:20:45.440 --> 0:20:48.800
QuestDB will create it automatically when it starts saving data.

400
0:20:48.800 --> 0:20:52.920
So if I run this is scraping go, which you run doing go run, well, don't go.

401
0:20:52.920 --> 0:20:59.760
So go run is ingesting data, it should take less than 10 seconds because we are guessing

402
0:20:59.760 --> 0:21:01.240
10 million.

403
0:21:01.240 --> 0:21:02.280
And that's finished.

404
0:21:02.280 --> 0:21:05.600
So let me just go to my local host here.

405
0:21:05.600 --> 0:21:08.000
Let me just sell it.

406
0:21:08.000 --> 0:21:10.560
Sell it.

407
0:21:10.560 --> 0:21:15.680
How many records did we ingest it for?

408
0:21:15.680 --> 0:21:17.320
I have to refresh the tables.

409
0:21:17.320 --> 0:21:18.320
Okay.

410
0:21:18.320 --> 0:21:21.920
How many records I ingested 10 million records.

411
0:21:21.920 --> 0:21:23.400
That's good.

412
0:21:23.400 --> 0:21:29.960
Can you tell me the terrible so I can see what happened here.

413
0:21:29.960 --> 0:21:33.120
Sample by one second.

414
0:21:33.120 --> 0:21:38.040
And he's telling me, yeah, you know, in the first second, only half a million because

415
0:21:38.040 --> 0:21:40.640
we we didn't start it at the top of the second.

416
0:21:40.640 --> 0:21:42.680
It was totally at second or something.

417
0:21:42.680 --> 0:21:46.240
But after that, 1 million, 1 million, 1 million.

418
0:21:46.240 --> 0:21:48.880
Hey, you see, you see the idea.

419
0:21:48.880 --> 0:21:50.200
Okay, that's not too bad.

420
0:21:50.200 --> 0:21:52.680
I can do this slightly better.

421
0:21:52.680 --> 0:22:01.680
I can run this script actually twice ingesting in the same instant to two different tables.

422
0:22:01.680 --> 0:22:06.760
So now if I refresh, I should see I have two tables, not only one.

423
0:22:06.760 --> 0:22:10.580
So I have two tables here, same hardware and everything.

424
0:22:10.580 --> 0:22:16.140
If I run again, I'm going to select only the last 10 rows.

425
0:22:16.140 --> 0:22:18.240
So we only see the latest run.

426
0:22:18.240 --> 0:22:22.920
So you can see it's lower now, I was actually ingesting to two tables, selling guest in

427
0:22:22.920 --> 0:22:26.600
only 700,000 per second, something like that.

428
0:22:26.600 --> 0:22:33.000
But if I go to the same time to the other table, I can just do a union.

429
0:22:33.000 --> 0:22:40.840
If I go to the table here, you should see that at the same time in the yeah, I cannot

430
0:22:40.840 --> 0:22:42.840
apply limit here.

431
0:22:42.840 --> 0:22:43.840
Sorry.

432
0:22:43.840 --> 0:22:48.320
I can't apply the union, so I should see that, you know, even if I was going to slower, the

433
0:22:48.320 --> 0:22:50.400
other table was reading data.

434
0:22:50.400 --> 0:22:52.960
And in this format, you cannot see it very well.

435
0:22:52.960 --> 0:22:58.240
But we can do something I told you earlier, I can just rather than do a join, I can just

436
0:22:58.240 --> 0:23:05.360
do something like as of join the first query with the second.

437
0:23:05.360 --> 0:23:09.760
So I should be able to do this.

438
0:23:09.760 --> 0:23:16.800
Now I have in the first run, we were running only one instance of sending data.

439
0:23:16.800 --> 0:23:19.800
And this one is the one in which I was running to.

440
0:23:19.800 --> 0:23:25.800
So you can see for this particular second, we were ingesting 700,000 records in one,

441
0:23:25.800 --> 0:23:28.360
700,000 records in the other same time.

442
0:23:28.360 --> 0:23:33.720
So about 1.4 something million in total, because we're in different tables out of the box.

443
0:23:33.720 --> 0:23:40.200
If I configure the writers and how many threads I have for processing things, I can get it

444
0:23:40.200 --> 0:23:41.840
slightly faster than this.

445
0:23:41.840 --> 0:23:43.160
But that's good enough.

446
0:23:43.160 --> 0:23:46.240
On a local M1 laptop, SSD, it's fast.

447
0:23:46.240 --> 0:23:47.240
But that's the idea.

448
0:23:47.240 --> 0:23:48.760
So that's the one million there.

449
0:23:48.760 --> 0:23:49.760
I wasn't lying.

450
0:23:49.760 --> 0:23:51.240
I was just telling you things.

451
0:23:51.240 --> 0:23:53.160
I have only a few minutes, but that's cool.

452
0:23:53.160 --> 0:23:55.120
How we got here?

453
0:23:55.120 --> 0:23:58.600
First, we can do a lot of essentials about the data.

454
0:23:58.600 --> 0:24:00.000
This is time serious.

455
0:24:00.000 --> 0:24:06.980
So we know people usually want to get not individual rows, but computations of a rows.

456
0:24:06.980 --> 0:24:12.880
We know people mostly want to group by things that are in the data, like a string, like

457
0:24:12.880 --> 0:24:16.880
the country name or the device name or the brand or whatever.

458
0:24:16.880 --> 0:24:21.760
So instead of storing strings, we have a special symbol, which is called a special type, which

459
0:24:21.760 --> 0:24:22.760
is called a symbol.

460
0:24:22.760 --> 0:24:27.560
If you give me a string, we convert into a number and we do look up automatically those

461
0:24:27.560 --> 0:24:28.560
things.

462
0:24:28.560 --> 0:24:33.520
So we make a lot of assumptions because we hyper-specialize on one particular use case.

463
0:24:33.520 --> 0:24:34.560
We optimize the storage.

464
0:24:34.560 --> 0:24:39.520
We don't use indexes because we store everything always in incremental order per partition.

465
0:24:39.520 --> 0:24:45.040
If we get data out of order, we have to regrade the partitions, but we don't need indexes because

466
0:24:45.040 --> 0:24:49.920
we always have the data physically in order so we can scan super quickly back and forth.

467
0:24:49.920 --> 0:24:50.920
That's kind of the idea.

468
0:24:50.920 --> 0:24:55.120
We also parallelize as much as we can using different things.

469
0:24:55.120 --> 0:24:57.400
This is written in Java, and it's from scratch.

470
0:24:57.400 --> 0:25:02.920
You will see some databases, which I love, like MongoDB, excellent device for content.

471
0:25:02.920 --> 0:25:04.800
They have a time series module.

472
0:25:04.800 --> 0:25:08.400
We use the same MongoDB collections for doing time series.

473
0:25:08.400 --> 0:25:14.080
They cannot be as fast because they are using exactly what they are using for content.

474
0:25:14.080 --> 0:25:15.080
It's very convenient.

475
0:25:15.080 --> 0:25:19.440
I can do everything, but same thing with all the engines that are building on top of other

476
0:25:19.440 --> 0:25:20.440
things.

477
0:25:20.440 --> 0:25:22.160
We don't have any dependencies.

478
0:25:22.160 --> 0:25:24.400
Everything is built for scratch.

479
0:25:24.400 --> 0:25:31.240
Usually we are writing some of the libraries in Java, like strings and loggers and so on,

480
0:25:31.240 --> 0:25:32.240
to avoid conversions.

481
0:25:32.240 --> 0:25:35.760
There are things that we don't use, so we don't use them.

482
0:25:35.760 --> 0:25:38.120
We have libraries for rings.

483
0:25:38.120 --> 0:25:40.720
We have libraries for memory management.

484
0:25:40.720 --> 0:25:45.320
We have libraries for absolutely everything, drag-written in our own version.

485
0:25:45.320 --> 0:25:50.040
We have our own Just-In-Time compiler because the original Just-In-Time compiler in Java

486
0:25:50.040 --> 0:25:54.360
was not performing enough for some of the parallelization in what it wanted to do.

487
0:25:54.360 --> 0:25:55.360
We wrote everything.

488
0:25:55.360 --> 0:25:57.360
Our Java is kind of weird.

489
0:25:57.360 --> 0:26:00.040
Jeremy can tell you more about that.

490
0:26:00.040 --> 0:26:03.160
It's super weird Java, but it's still Java.

491
0:26:03.160 --> 0:26:04.360
That's kind of the idea.

492
0:26:04.360 --> 0:26:07.880
We even wrote our own input output functions.

493
0:26:07.880 --> 0:26:08.880
That's kind of a thing.

494
0:26:08.880 --> 0:26:09.880
Why?

495
0:26:09.880 --> 0:26:12.080
Because we can get nanoseconds faster.

496
0:26:12.080 --> 0:26:13.080
This is log4j.

497
0:26:13.080 --> 0:26:14.080
Log4j.

498
0:26:14.080 --> 0:26:18.360
We don't speak about log4j, which is awesome.

499
0:26:18.360 --> 0:26:19.360
This is log4j.

500
0:26:19.360 --> 0:26:28.760
This is the nanoseconds, the operations you can do in each nanosecond.

501
0:26:28.760 --> 0:26:35.800
With log4j, log in, integer, you can do 82 operations per nanosecond.

502
0:26:35.800 --> 0:26:41.360
We can do 800 operations per nanosecond, which is, do you have to go down to the nanosecond?

503
0:26:41.360 --> 0:26:45.520
If you are doing a crude application, probably not.

504
0:26:45.520 --> 0:26:47.240
It really depends what you are building.

505
0:26:47.240 --> 0:26:50.520
That's kind of why we are writing things from scratch.

506
0:26:50.520 --> 0:26:55.520
Basically the approach of QSDBT performance, you know, this is like, I don't know who you

507
0:26:55.520 --> 0:27:00.760
are, but I don't know you, but I will find you and I will kill you.

508
0:27:00.760 --> 0:27:03.080
That's kind of the same approach I see on QSDBT.

509
0:27:03.080 --> 0:27:09.040
They are like, I don't know, we are so we can get faster at some obscure thing here.

510
0:27:09.040 --> 0:27:10.040
That's kind of the idea.

511
0:27:10.040 --> 0:27:13.400
We try to be a good team player.

512
0:27:13.400 --> 0:27:18.160
Jeremy here has contributed himself only alone.

513
0:27:18.160 --> 0:27:22.280
The connectors for Kafka Connect, connectors for Apache Flink.

514
0:27:22.280 --> 0:27:25.960
So we try to integrate with the rest of the ecosystem.

515
0:27:25.960 --> 0:27:28.120
We love it if you try QSDBT.

516
0:27:28.120 --> 0:27:30.000
You are open source geeks.

517
0:27:30.000 --> 0:27:31.560
You like GitHub stars.

518
0:27:31.560 --> 0:27:33.200
We like GitHub stars.

519
0:27:33.200 --> 0:27:34.200
Please contribute.

520
0:27:34.200 --> 0:27:37.440
Please start on GitHub if you like it.

521
0:27:37.440 --> 0:27:39.640
We have a contributor Slack channel.

522
0:27:39.640 --> 0:27:41.360
We are quite friendly.

523
0:27:41.360 --> 0:27:42.480
We are fast.

524
0:27:42.480 --> 0:27:44.360
We work with interesting problems.

525
0:27:44.360 --> 0:27:49.560
If you like interesting problems, if you like weird Java, we would love to have you here.

526
0:27:49.560 --> 0:27:50.560
So thank you very much.

527
0:27:50.560 --> 0:27:52.560
I can't take any questions outside.

528
0:27:52.560 --> 0:27:56.560
One question from the chat.

529
0:27:56.560 --> 0:27:58.560
Thank you.

530
0:27:58.560 --> 0:28:08.320
Someone is asking if QSDBT can work with GPS data.

531
0:28:08.320 --> 0:28:16.080
Yes, you can work with GPS data.

532
0:28:16.080 --> 0:28:17.080
We have double.

533
0:28:17.080 --> 0:28:18.840
You can use for that.

534
0:28:18.840 --> 0:28:21.280
We don't have a lot of geospatial functions.

535
0:28:21.280 --> 0:28:29.480
We have geohashes which basically allow you to define in which a different resolution

536
0:28:29.480 --> 0:28:32.760
in which a square in the world something is.

537
0:28:32.760 --> 0:28:37.600
So if you're telling us about finding where a point is in the world, a particular point

538
0:28:37.600 --> 0:28:39.920
in time, QSDBT is very cool.

539
0:28:39.920 --> 0:28:45.360
If you need to do other things with support on Mac libraries, COS and all those things

540
0:28:45.360 --> 0:28:47.440
to do your own calculations.

541
0:28:47.440 --> 0:28:49.600
But yeah, it can be used for GPS.

542
0:28:49.600 --> 0:28:54.080
Some people are doing asset tracking with QSDBT.

543
0:28:54.080 --> 0:28:55.080
Thank you.

544
0:28:55.080 --> 0:28:58.440
Mr.

