WEBVTT

00:00.000 --> 00:10.440
So, the thing about QuestDB, apart from being open source, we want people to know us because

00:10.440 --> 00:14.720
we try to be very performant, but specifically in small machines.

00:14.720 --> 00:22.140
Like perform very well in 120 CPUs and 200 gigs of RAM.

00:22.140 --> 00:23.640
It's okay.

00:23.640 --> 00:29.200
Performing very well in 4 CPUs and 16 of RAM, 16 gigs, is more difficult.

00:29.200 --> 00:31.800
So we try to optimize for that.

00:31.800 --> 00:37.320
Actually in the past we were optimizing for the larger instance use case and then we realized

00:37.320 --> 00:40.440
not everybody has a super large instance at home.

00:40.440 --> 00:42.160
We try to be better at that.

00:42.160 --> 00:46.560
We also try to be very good with developer experience that you get performance out of

00:46.560 --> 00:47.560
the box.

00:47.560 --> 00:49.440
There are many things you can tweak in QuestDB.

00:49.440 --> 00:54.920
In every other database, every other system, a lot of configuration.

00:54.920 --> 01:02.080
The memory, page size, the buffers and whatnot, with CPUs, the what, blah, blah, blah, blah.

01:02.080 --> 01:05.320
By default, if you don't touch anything, we should perform well.

01:05.320 --> 01:08.280
And then if you have expert tolerance, you might fine tune.

01:08.280 --> 01:15.160
But we try hard to make developer experience simple and that's why we choose SQL also for

01:15.160 --> 01:16.160
querying data.

01:16.160 --> 01:20.200
So another time series database make the tradeoff.

01:20.200 --> 01:21.200
We want to perform.

01:21.200 --> 01:26.240
We need to use a different language, which is cool because, you know, I get it.

01:26.240 --> 01:31.880
We choose SQL because we want developers to have an easy way learning QuestDB.

01:31.880 --> 01:36.400
For ingesting data, you can use SQL but we also offer a different protocol which is faster.

01:36.400 --> 01:40.720
That's why we have library so you don't have to go low level to be performant.

01:40.720 --> 01:41.720
But that's the idea.

01:41.720 --> 01:44.440
And we are open source, very proud about being open source.

01:44.440 --> 01:46.640
But why we are building another database?

01:46.640 --> 01:48.360
There are a lot of databases.

01:48.360 --> 01:52.800
If you walk around, you're going to read about every type of database out there.

01:52.800 --> 01:55.880
Just today here, I saw MongoDB, I saw Clickhouse.

01:55.880 --> 02:00.400
There's a SQL about MariaDB.

02:00.400 --> 02:01.400
Why you need another database?

02:01.400 --> 02:03.160
Another open source database?

02:03.160 --> 02:06.920
Because different data look different and can have different problems.

02:06.920 --> 02:11.000
And in our case, we are specialized on time series.

02:11.000 --> 02:12.000
We don't do anything else.

02:12.000 --> 02:19.440
I mean, if you try to use QuestDB for full text analytics, we are truly the worst database

02:19.440 --> 02:21.120
ever for that.

02:21.120 --> 02:27.840
If you try to use QuestDB for geospatial queries, we support some geospatial things kind of

02:27.840 --> 02:28.840
a bit.

02:28.840 --> 02:33.040
We have a specific data type about geohasses.

02:33.040 --> 02:34.760
So we have a type about that.

02:34.760 --> 02:39.760
But we are not good for geospatial unless it is part of time series plus geo.

02:39.760 --> 02:40.760
That's kind of the idea.

02:40.760 --> 02:46.760
And we specialize only on time series analytics, on data which is changing over time and you

02:46.760 --> 02:49.160
want to monitor and track those changes.

02:49.160 --> 02:50.160
That's the idea.

02:50.160 --> 02:51.960
We are not good for anything else.

02:51.960 --> 02:56.520
If you try to use QuestDB for everything, boy, what a disappointment we are going to be.

02:56.520 --> 02:59.680
But if you try for time series database, this will be one of the good ones.

02:59.680 --> 03:00.680
That's kind of the idea.

03:00.680 --> 03:04.360
And that's why we are building QuestDB because there are a lot of time series data out there.

03:04.360 --> 03:06.840
And how do you know if you have a time series stolen?

03:06.840 --> 03:08.320
I have here a lot of things.

03:08.320 --> 03:10.600
I want to just read a couple of them.

03:10.600 --> 03:17.320
But basically, if most of the time you are reading data on a slice of time, tell me which

03:17.320 --> 03:20.960
energy consumption I have over the last minute.

03:20.960 --> 03:25.760
Tell me how is the nuclear reactor doing in the past 10 microseconds.

03:25.760 --> 03:31.640
Tell me what is the conversion for this user in the past week.

03:31.640 --> 03:37.560
Let me know for all the data I have a moving vehicle, which was the last position I saw

03:37.560 --> 03:42.000
it and what was the sensor in this particular point in time.

03:42.000 --> 03:44.920
So if you have that, time series can be interesting.

03:44.920 --> 03:47.880
Also with time series you have other types of tolerance.

03:47.880 --> 03:52.520
Data tends to be inserting faster than it reads.

03:52.520 --> 03:55.360
Databases, historically, have been optimized for reads.

03:55.360 --> 03:59.000
They try every trick in the book for making reads super fast.

03:59.000 --> 04:02.040
When you insert data, you need to define indexes.

04:02.040 --> 04:04.640
And they are going to index by many different things.

04:04.640 --> 04:08.000
And they keep caching memory for a lot of things and blah, blah, blah, blah.

04:08.000 --> 04:10.200
So reading is the key thing.

04:10.200 --> 04:12.800
Because usually you read data much more than you write.

04:12.800 --> 04:15.040
Not so intense databases.

04:15.040 --> 04:17.440
We can support heavy reads on top of that.

04:17.440 --> 04:21.040
But we need to support heavy inserts and keep perform of that.

04:21.040 --> 04:22.760
We don't use indexes.

04:22.760 --> 04:25.080
The performance we see today is with no indexes.

04:25.080 --> 04:26.640
We don't need them.

04:26.640 --> 04:27.920
We don't want them.

04:27.920 --> 04:30.520
Because having an index slows down ingestion.

04:30.520 --> 04:32.240
It's a luxury we cannot have.

04:32.240 --> 04:34.720
So we have some kind of indexing, but we don't have indexes.

04:34.720 --> 04:35.720
Not as you know them.

04:35.720 --> 04:37.720
That's kind of the idea here.

04:37.720 --> 04:38.720
So it's slightly different.

04:38.720 --> 04:44.040
And when you have data that you are writing very often, that data is going to grow.

04:44.040 --> 04:46.520
And it can grow fast.

04:46.520 --> 04:50.920
And you need to have some way of loading or deleting that data.

04:50.920 --> 04:56.160
On a traditional database, you just don't say, oh, I have, I don't know, I'm Amazon.

04:56.160 --> 04:57.160
And I'm getting users.

04:57.160 --> 04:59.680
It's like, oh, I already have a million users.

04:59.680 --> 05:00.680
A million won.

05:00.680 --> 05:01.680
I'm going to delay the old users.

05:01.680 --> 05:02.680
But you don't do that.

05:02.680 --> 05:03.680
I mean, sometimes you do.

05:03.680 --> 05:04.680
But you don't do that.

05:04.680 --> 05:06.880
You don't really do that on your databases.

05:06.880 --> 05:12.040
On time series database, almost all of them have some mechanism to deal with historical

05:12.040 --> 05:14.280
data and do something with that.

05:14.280 --> 05:16.040
In our case, you can mount partitions.

05:16.040 --> 05:17.800
You can mount to cheaper storage.

05:17.800 --> 05:18.800
Those kind of things.

05:18.800 --> 05:21.560
But we have the command Sunday.

05:21.560 --> 05:22.840
It is designed for that kind of thing.

05:22.840 --> 05:23.840
That kind of area.

05:23.840 --> 05:27.240
Many other things about how you have a time series stolen.

05:27.240 --> 05:28.680
But that kind of the area.

05:28.680 --> 05:33.840
But better than me just telling you, I'm going to show you some queries on top of demo data

05:33.840 --> 05:34.840
sets.

05:34.840 --> 05:38.320
So you get the feeling of what a time series database might be interesting.

05:38.320 --> 05:42.080
And then we'll go into details about the ingesting data and about all those things.

05:42.080 --> 05:43.480
Does that sound good so far?

05:43.480 --> 05:44.480
Yeah.

05:44.480 --> 05:45.480
Do you have any questions?

05:45.480 --> 05:47.480
I'm happy to take them during the talk, by the way.

05:47.480 --> 05:49.240
Not only at the end.

05:49.240 --> 05:52.120
So we have a live demo.

05:52.120 --> 05:58.120
Demo.questdb.io.

05:58.120 --> 05:59.760
This is running on a large machine on AWS.

05:59.760 --> 06:01.520
We don't need all the power.

06:01.520 --> 06:03.080
But since it's open to the public.

06:03.080 --> 06:06.880
And here we have a few different data sets.

06:06.880 --> 06:07.880
That is one.

06:07.880 --> 06:10.280
You are in a big data room.

06:10.280 --> 06:12.880
So you're truly familiar with the taxi rides.

06:12.880 --> 06:15.240
New York City taxi rides data set.

06:15.240 --> 06:19.960
It's the city of New York has a data set which is very cool for machine learning and for

06:19.960 --> 06:21.480
big data.

06:21.480 --> 06:24.680
Which is taxi rides in the city of New York.

06:24.680 --> 06:28.360
And the rides started when it finished.

06:28.360 --> 06:30.320
Also the coordinate and a few things.

06:30.320 --> 06:32.320
Like the tip and the amount of the fur.

06:32.320 --> 06:33.920
How many people blah blah blah.

06:33.920 --> 06:39.080
So we took that open data set and we just put it here on QuestDB.

06:39.080 --> 06:40.560
A few years of the data set.

06:40.560 --> 06:41.560
Yes, you know.

06:41.560 --> 06:43.280
Lot of columns here.

06:43.280 --> 06:47.200
So let me just show you how big this is.

06:47.200 --> 06:49.840
This is right now.

06:49.840 --> 06:52.560
Is the size okay?

06:52.560 --> 06:58.160
Maybe not.

06:58.160 --> 07:00.080
So it's 1.6 billion rows.

07:00.080 --> 07:01.080
Which is not huge.

07:01.080 --> 07:08.280
I mean if you have a relational database, 1.6 billion rows.

07:08.280 --> 07:10.720
Relational databases today they are great.

07:10.720 --> 07:15.480
But 1.6 billion rows is like yeah, I couldn't work with that.

07:15.480 --> 07:17.080
I'm not super comfortable.

07:17.080 --> 07:18.360
For us it's cute.

07:18.360 --> 07:23.440
It's like it's a data set which is respectable but not really cute.

07:23.440 --> 07:26.120
But 1.6 billion rows.

07:26.120 --> 07:29.240
So now what if I want to do something like for example, I don't know.

07:29.240 --> 07:35.760
I want to calculate the average of whichever, this for example.

07:35.760 --> 07:36.760
This number.

07:36.760 --> 07:42.800
I want to average the fair amount over 1.6 billion trips.

07:42.800 --> 07:48.200
How long do you expect your database to take to go do a full scan over

07:48.200 --> 07:51.520
1.6 billion rows and compute the average?

07:51.520 --> 07:53.640
No indexes, no anything.

07:53.640 --> 07:54.640
How long would you say?

07:54.640 --> 07:55.640
More or less.

07:55.640 --> 07:56.640
Ball park.

07:56.640 --> 07:57.640
1.6 billion rows.

07:57.640 --> 07:58.640
No one?

07:58.640 --> 07:59.640
Sorry?

07:59.640 --> 08:00.640
How is the size in gigabytes?

08:00.640 --> 08:01.640
Megabytes.

08:01.640 --> 08:06.840
I don't know for the whole data set but this is a double.

08:06.840 --> 08:08.880
I mean I'm really just no.

08:08.880 --> 08:09.880
It's big.

08:09.880 --> 08:10.880
It's big.

08:10.880 --> 08:17.520
When you download the CSV, it's about 600 megabytes and you have several of those.

08:17.520 --> 08:20.680
This is in the, you know, it's laggish.

08:20.680 --> 08:21.680
But anyway.

08:21.680 --> 08:26.680
Well, actually it was slower than I thought.

08:26.680 --> 08:29.280
It took, usually it takes half a second.

08:29.280 --> 08:32.000
This time it took 0.6 seconds.

08:32.000 --> 08:33.360
I know it's slow.

08:33.360 --> 08:35.520
I know but this was a reason.

08:35.520 --> 08:36.520
Sorry about that.

08:36.520 --> 08:39.260
But I told you, I told you we are a time series database.

08:39.260 --> 08:40.760
We are super slow for other things.

08:40.760 --> 08:42.400
This is not a time series query.

08:42.400 --> 08:44.000
Did you see any time stamp here?

08:44.000 --> 08:45.000
I didn't see anything.

08:45.000 --> 08:47.640
This is just a full scan.

08:47.640 --> 08:48.640
We parallelize.

08:48.640 --> 08:50.520
We read data and we are slow.

08:50.520 --> 08:55.640
We take almost over half a second to go over only 1.6 billion rows.

08:55.640 --> 08:56.640
Unforgivable.

08:56.640 --> 08:57.640
Sorry about that.

08:57.640 --> 08:58.640
But there will be here.

08:58.640 --> 08:59.640
No, that's okay.

08:59.640 --> 09:03.160
I mean I'm kind of half kidding but not really.

09:03.160 --> 09:06.680
But wait until I put a time dimension.

09:06.680 --> 09:08.000
Now yes.

09:08.000 --> 09:15.240
I want only, for example, I want only one year of data.

09:15.240 --> 09:19.760
I'm going to just also add another computation because I know that person.

09:19.760 --> 09:23.100
It's just counting data which is super fast.

09:23.100 --> 09:24.520
So I'm going to add another computation.

09:24.520 --> 09:29.320
So I'm going to count the data and only for 2016.

09:29.320 --> 09:30.320
And this is better.

09:30.320 --> 09:36.960
This is already 100 milliseconds because we are going only over a few rows.

09:36.960 --> 09:39.440
We are going only about...

09:39.440 --> 09:44.120
Yeah, it's only 146 million rows.

09:44.120 --> 09:45.120
This is much more manageable.

09:45.120 --> 09:47.600
So only 140 million rows.

09:47.600 --> 09:48.600
That's better.

09:48.600 --> 09:51.240
So we can go actually very fast on this.

09:51.240 --> 09:59.720
And then if you keep going down, oh no, I want only one month of data which is, I don't

09:59.720 --> 10:03.920
know, still, yeah, 12 million rows.

10:03.920 --> 10:06.840
So a month of data is 60 milliseconds.

10:06.840 --> 10:10.280
For one day of data, of course, is way faster.

10:10.280 --> 10:12.480
This is already 50 milliseconds.

10:12.480 --> 10:19.680
If I go to one specific hour and minute, it will be, you know, kind of not much faster

10:19.680 --> 10:20.680
because...

10:20.680 --> 10:21.680
What?

10:21.680 --> 10:22.680
It's under one millisecond.

10:22.680 --> 10:26.360
Oh yeah, it's under one millisecond actually.

10:26.360 --> 10:27.360
Thank you for that.

10:27.360 --> 10:30.240
But still, like, you know, we have partitions.

10:30.240 --> 10:36.440
So basically, one thing we do, we only go to the partition where the data is stored.

10:36.440 --> 10:37.840
So we only attack that part of the data.

10:37.840 --> 10:39.320
But that's another thing.

10:39.320 --> 10:43.480
For when you have like that time component, we are quite fast, or fairly fast.

10:43.480 --> 10:46.280
That's kind of the beauty for a time series database.

10:46.280 --> 10:49.280
And we can do also interesting, other interesting things.

10:49.280 --> 10:56.040
If I go to the same table and I saw you, what this looked like, you can see that for the

10:56.040 --> 11:00.760
same second, I have many trips because this is New York baby.

11:00.760 --> 11:04.560
And in New York, you know, the city that never leaves.

11:04.560 --> 11:06.960
You can't get back in every corner.

11:06.960 --> 11:08.680
You get rich when you land in New York.

11:08.680 --> 11:10.000
I spent one year there.

11:10.000 --> 11:11.160
It's not like that.

11:11.160 --> 11:17.160
Anyway, so in every particular second, even at midnight, you have always a few trips,

11:17.160 --> 11:18.160
at least.

11:18.160 --> 11:19.160
Okay?

11:19.160 --> 11:20.160
So actually, you could do that.

11:20.160 --> 11:26.840
You could do something like, I want to know the, I want to, if I want to do something

11:26.840 --> 11:41.600
like give me the daytime on how many trips are ending where this daytime is in, for example,

11:41.600 --> 11:43.600
June 21st.

11:43.600 --> 11:47.760
Siri, what are you doing there, man?

11:47.760 --> 11:51.520
I didn't even know I had Siri here.

11:51.520 --> 11:52.520
Okay.

11:52.520 --> 11:54.320
So, I don't know.

11:54.320 --> 12:01.480
For example, in this particular minute, in one particular day, I want to sample in one

12:01.480 --> 12:05.960
second interval and know how many trips I have for every particular second.

12:05.960 --> 12:08.400
So that's another thing you can do in a time series database.

12:08.400 --> 12:12.480
Rather than grouping by columns that you can also do, you can group by time.

12:12.480 --> 12:14.480
You call the sample by.

12:14.480 --> 12:22.240
So we can sample by any, we go from microsecond to year, I guess.

12:22.240 --> 12:23.480
Yeah, microsecond to year.

12:23.480 --> 12:27.760
So you can group by microsecond, millisecond, second, year, day, whatever.

12:27.760 --> 12:31.960
So in this case, I'm saying, okay, in this particular second, I have six trips and five

12:31.960 --> 12:33.800
trips and blah, blah, blah.

12:33.800 --> 12:34.800
You get there, yeah?

12:34.800 --> 12:40.280
So something I wanted to show you, which is another cool one, it's, I have this data set

12:40.280 --> 12:42.160
with several trips every second.

12:42.160 --> 12:48.480
I have another data set also with data from Manhattan is the weather data set.

12:48.480 --> 12:53.440
So maybe it could be interesting to know, to join those two data sets.

12:53.440 --> 12:58.960
It will be cool to know the weather that they had for a particular trip, because maybe that

12:58.960 --> 12:59.960
gives me some insight.

12:59.960 --> 13:01.040
I don't know.

13:01.040 --> 13:04.880
The challenge is this data set, of course, is real life.

13:04.880 --> 13:06.840
It's a different open data set.

13:06.840 --> 13:08.600
It's not at the same resolution.

13:08.600 --> 13:12.280
We don't have weather changes every second.

13:12.280 --> 13:14.040
In my hometown, sometimes that happens.

13:14.040 --> 13:17.560
And when I was living in London, that was crazy.

13:17.560 --> 13:23.680
But in real life, we don't measure, we don't store weather changes every second.

13:23.680 --> 13:28.920
In this particular data set, we have about two or three records every hour.

13:28.920 --> 13:35.680
So now if I want to join a data set with subsecond resolution, a data set with subhour resolution,

13:35.680 --> 13:40.480
and I want to do a join, if I want to do it in other databases, I could do it.

13:40.480 --> 13:42.440
It will take me a while.

13:42.440 --> 13:44.280
Then I will think I have it, and I wouldn't.

13:44.280 --> 13:47.280
And then it will be like, yeah, this makes sense or not really.

13:47.280 --> 13:48.840
And a week later, I will be crying.

13:48.840 --> 13:49.920
I don't know that.

13:49.920 --> 13:51.880
So I still know.

13:51.880 --> 13:56.360
So one thing we have here, we have a demo set, some example points.

13:56.360 --> 14:00.960
I'm going to move on to another thing really quickly, because otherwise.

14:00.960 --> 14:07.760
But this one I really like, we have a special type of join, which we call an as of join,

14:07.760 --> 14:09.680
which basically does this.

14:09.680 --> 14:14.320
I'm going to select the data from the table I told you already for one particular day

14:14.320 --> 14:15.320
in time.

14:15.320 --> 14:21.000
And then I'm going to do what we call an as of join, which basically says, this table

14:21.000 --> 14:22.920
has a timestamp.

14:22.920 --> 14:24.400
We call it the designated timestamp.

14:24.400 --> 14:26.760
You decide which is the column.

14:26.760 --> 14:27.760
You have several.

14:27.760 --> 14:30.440
So we have the designated timestamp in one.

14:30.440 --> 14:34.840
The timestamp in the other joined by the ones that are closer to each other.

14:34.840 --> 14:38.800
In this case, as of means the one which is exactly the same or immediately before me,

14:38.800 --> 14:40.120
the one which is closer to me.

14:40.120 --> 14:45.440
So what happened before we have also the one strictly behind me before me cannot be the

14:45.440 --> 14:46.440
same.

14:46.440 --> 14:47.440
But that's the idea.

14:47.440 --> 14:50.680
So in this case, for getting to different data sets, I can just do that.

14:50.680 --> 14:54.920
Also, I want to add here the timestamp for the other table.

14:54.920 --> 14:56.320
So it's clear.

14:56.320 --> 15:03.960
So if I run this query, now here I can see for each record on the New York taxi rides,

15:03.960 --> 15:09.320
I'm always getting the same timestamp in the weather data set, because I have only one

15:09.320 --> 15:12.680
entry every 40 or 45 minutes.

15:12.680 --> 15:19.320
If I move to a different point in the day to this day, but instead of at 12, at 1255,

15:19.320 --> 15:25.880
for example, I should see already that I'm matching to a different entry on this table.

15:25.880 --> 15:26.880
So that's it.

15:26.880 --> 15:28.360
I have different resolutions.

15:28.360 --> 15:31.160
I don't care which one we joined by time, because we're about time.

15:31.160 --> 15:32.160
That's kind of the idea.

15:32.160 --> 15:36.860
That's what I think is I have more interesting queries, but maybe for a different date.

15:36.860 --> 15:38.840
So that's the first thing.

15:38.840 --> 15:42.880
So I told you, okay, now you get the idea why tensile is coming to the extent that kind

15:42.880 --> 15:47.160
of things we can do down sampling, all those things in machine learning is very important.

15:47.160 --> 15:51.800
You have data maybe every second, and then you want to do a forecasting and doesn't make

15:51.800 --> 15:54.600
sense to train a model with every second data.

15:54.600 --> 15:59.600
In many cases, maybe you want to down sample to 15 minutes interval with this trick, you

15:59.600 --> 16:00.760
can do it easily.

16:00.760 --> 16:01.760
So that's kind of the idea.

16:01.760 --> 16:03.680
But I was speaking about ingesting data.

16:03.680 --> 16:09.000
So ingesting over 1 million times three years per second on a single instance.

16:09.000 --> 16:10.840
It's interesting.

16:10.840 --> 16:16.200
But ingesting over 1 million records per second, a single instance, it's easy, actually.

16:16.200 --> 16:20.920
I could just write to a file appending lines, and that will be it.

16:20.920 --> 16:25.480
The interesting bit is actually being able to ingest data while you are able to put it

16:25.480 --> 16:27.880
in real time, the same data you're ingesting.

16:27.880 --> 16:28.880
That's the trick.

16:28.880 --> 16:32.600
Because you're ingesting, I mean, you put it there, and you're like, why ingesting 1

16:32.600 --> 16:33.600
million records?

16:33.600 --> 16:36.800
When you think about this, like, well, wait, but how long do I have to wait to put in the

16:36.800 --> 16:37.800
data?

16:37.800 --> 16:40.720
So the idea is you can put in the data at the same time.

16:40.720 --> 16:45.240
All benchmarks are lies, of course, on the same benchmark that I want to tell you, other

16:45.240 --> 16:48.280
people will tell you the contrary, and I'm totally fine with that.

16:48.280 --> 16:55.040
A couple of years ago, we published an article saying, hey, we can ingest now at 1.4 million.

16:55.040 --> 16:59.320
The slides are linked already on the first page, by the way.

16:59.320 --> 17:01.320
Thank you.

17:01.320 --> 17:07.000
So our CTO posted about how we were ingesting 1.4 million records per second.

17:07.000 --> 17:12.760
These records were, they had like 20 columns, 10 dimensions, 10 strings, and 10 metrics,

17:12.760 --> 17:13.760
10 numbers.

17:13.760 --> 17:19.680
We're pulling guest records of 20 columns with 10 strings and 10 numbers, 1.4 million

17:19.680 --> 17:24.260
records per second, while running queries, which is the other bit.

17:24.260 --> 17:29.920
So we were able to scan over 4 million, 4 billion records per second, at the same time,

17:29.920 --> 17:32.840
in relatively small machines, relatively small.

17:32.840 --> 17:35.400
So that's kind of the idea.

17:35.400 --> 17:37.160
And these benchmarks, we didn't write it.

17:37.160 --> 17:40.840
There is a benchmark specifically for time series databases.

17:40.840 --> 17:45.440
As I told you earlier, if you load data in QuestDB, you can load relational data into

17:45.440 --> 17:47.280
QuestDB, and you can run queries.

17:47.280 --> 17:52.240
If you try to run a conventional benchmark on QuestDB, it's going to be super slow.

17:52.240 --> 17:54.000
We are not designed for full-text search.

17:54.000 --> 18:01.400
We are not designed for just operations within individual records or doing updating data.

18:01.400 --> 18:02.480
We are not designed for that.

18:02.480 --> 18:05.320
We can do it, but we are not designed for that.

18:05.320 --> 18:08.320
So there is also the other time series databases.

18:08.320 --> 18:13.600
So InfluxDB, another open source database, created this benchmark, the TSVS benchmark,

18:13.600 --> 18:16.920
which is specifically about time series databases.

18:16.920 --> 18:20.400
So the queries and the ingestion patterns matches what you would expect from a time

18:20.400 --> 18:21.800
series database.

18:21.800 --> 18:27.360
Now it's maintained by time scale, which is another open source database on top of Postgres.

18:27.360 --> 18:32.480
And we have our own, you know, the recent adapter for running that on top of QuestDB.

18:32.480 --> 18:36.640
And with that benchmark, it's with the one that we are getting those results.

18:36.640 --> 18:39.440
So with that particular benchmark is the one giving the results.

18:39.440 --> 18:42.960
So you know, your management might vary also depending on the hardware.

18:42.960 --> 18:47.240
If you try to run the benchmark in the cloud, it's going to be slower, always.

18:47.240 --> 18:53.320
Because in the cloud, by default, you use on AWS, you use CVS, on well cloud, you use

18:53.320 --> 18:57.600
the attached storage, it's networking storage, has on latency, because they are not local

18:57.600 --> 18:58.600
disk.

18:58.600 --> 18:59.920
They are super cool, but they are not local.

18:59.920 --> 19:01.840
It's going to be always slower.

19:01.840 --> 19:08.240
You want to get this on Google Cloud or AWS, you can do it, you have to use NVMe disk,

19:08.240 --> 19:13.160
which are local disk without attached to the instance, but they disappear when they when

19:13.160 --> 19:14.640
you close the instance.

19:14.640 --> 19:18.320
But with those disks, you will be getting the same benchmark.

19:18.320 --> 19:20.680
So hardware is also important with the benchmark.

19:20.680 --> 19:24.160
But that's the idea, you know, that that's how how we did it.

19:24.160 --> 19:28.120
Before I tell you a bit about the technical decisions that they will not have super time,

19:28.120 --> 19:33.400
but I want to show you how we are doing this ingestion.

19:33.400 --> 19:37.160
So let me ask if I can move this out of the way.

19:37.160 --> 19:38.720
So this is a scraping go.

19:38.720 --> 19:40.440
I don't know any go at all.

19:40.440 --> 19:43.440
But I know to run this so you know, and I don't want to advocate.

19:43.440 --> 19:48.560
I mean, I couldn't tell you that I know a lot of go but I have no idea.

19:48.560 --> 19:50.440
So go long, it's a language.

19:50.440 --> 19:54.440
So yeah, we have our it's pretty cool.

19:54.440 --> 19:59.600
So we have this library or package or whatever they call it in go, which is our official

19:59.600 --> 20:00.600
package.

20:00.600 --> 20:01.600
Caro, whatever.

20:01.600 --> 20:03.320
I don't know.

20:03.320 --> 20:04.920
So this is I'm missing languages here.

20:04.920 --> 20:06.400
Anyway, it's not that happy.

20:06.400 --> 20:07.400
Thank you.

20:07.400 --> 20:09.360
So yeah, this is our theme.

20:09.360 --> 20:13.120
I'm connecting to local host to the default port in QuestDB.

20:13.120 --> 20:15.360
I'm going to be simulating data.

20:15.360 --> 20:17.680
So I'm simulating IoT data.

20:17.680 --> 20:23.240
And I'm going to be putting a device type which can be red or blue or green or yellow.

20:23.240 --> 20:30.200
I'm going to be outputting duration, latitude, longitude, speed, and time stamping nanoseconds.

20:30.200 --> 20:35.760
And I'm going to do this in chunks of in batches of 50,000 records.

20:35.760 --> 20:37.720
I want to do this 200 times.

20:37.720 --> 20:40.360
50,000 records 200 times, 10 million records.

20:40.360 --> 20:45.440
I want to be inserting 10 million records on a database on a table that does not exist.

20:45.440 --> 20:48.800
QuestDB will create it automatically when it starts saving data.

20:48.800 --> 20:52.920
So if I run this is scraping go, which you run doing go run, well, don't go.

20:52.920 --> 20:59.760
So go run is ingesting data, it should take less than 10 seconds because we are guessing

20:59.760 --> 21:01.240
10 million.

21:01.240 --> 21:02.280
And that's finished.

21:02.280 --> 21:05.600
So let me just go to my local host here.

21:05.600 --> 21:08.000
Let me just sell it.

21:08.000 --> 21:10.560
Sell it.

21:10.560 --> 21:15.680
How many records did we ingest it for?

21:15.680 --> 21:17.320
I have to refresh the tables.

21:17.320 --> 21:18.320
Okay.

21:18.320 --> 21:21.920
How many records I ingested 10 million records.

21:21.920 --> 21:23.400
That's good.

21:23.400 --> 21:29.960
Can you tell me the terrible so I can see what happened here.

21:29.960 --> 21:33.120
Sample by one second.

21:33.120 --> 21:38.040
And he's telling me, yeah, you know, in the first second, only half a million because

21:38.040 --> 21:40.640
we we didn't start it at the top of the second.

21:40.640 --> 21:42.680
It was totally at second or something.

21:42.680 --> 21:46.240
But after that, 1 million, 1 million, 1 million.

21:46.240 --> 21:48.880
Hey, you see, you see the idea.

21:48.880 --> 21:50.200
Okay, that's not too bad.

21:50.200 --> 21:52.680
I can do this slightly better.

21:52.680 --> 22:01.680
I can run this script actually twice ingesting in the same instant to two different tables.

22:01.680 --> 22:06.760
So now if I refresh, I should see I have two tables, not only one.

22:06.760 --> 22:10.580
So I have two tables here, same hardware and everything.

22:10.580 --> 22:16.140
If I run again, I'm going to select only the last 10 rows.

22:16.140 --> 22:18.240
So we only see the latest run.

22:18.240 --> 22:22.920
So you can see it's lower now, I was actually ingesting to two tables, selling guest in

22:22.920 --> 22:26.600
only 700,000 per second, something like that.

22:26.600 --> 22:33.000
But if I go to the same time to the other table, I can just do a union.

22:33.000 --> 22:40.840
If I go to the table here, you should see that at the same time in the yeah, I cannot

22:40.840 --> 22:42.840
apply limit here.

22:42.840 --> 22:43.840
Sorry.

22:43.840 --> 22:48.320
I can't apply the union, so I should see that, you know, even if I was going to slower, the

22:48.320 --> 22:50.400
other table was reading data.

22:50.400 --> 22:52.960
And in this format, you cannot see it very well.

22:52.960 --> 22:58.240
But we can do something I told you earlier, I can just rather than do a join, I can just

22:58.240 --> 23:05.360
do something like as of join the first query with the second.

23:05.360 --> 23:09.760
So I should be able to do this.

23:09.760 --> 23:16.800
Now I have in the first run, we were running only one instance of sending data.

23:16.800 --> 23:19.800
And this one is the one in which I was running to.

23:19.800 --> 23:25.800
So you can see for this particular second, we were ingesting 700,000 records in one,

23:25.800 --> 23:28.360
700,000 records in the other same time.

23:28.360 --> 23:33.720
So about 1.4 something million in total, because we're in different tables out of the box.

23:33.720 --> 23:40.200
If I configure the writers and how many threads I have for processing things, I can get it

23:40.200 --> 23:41.840
slightly faster than this.

23:41.840 --> 23:43.160
But that's good enough.

23:43.160 --> 23:46.240
On a local M1 laptop, SSD, it's fast.

23:46.240 --> 23:47.240
But that's the idea.

23:47.240 --> 23:48.760
So that's the one million there.

23:48.760 --> 23:49.760
I wasn't lying.

23:49.760 --> 23:51.240
I was just telling you things.

23:51.240 --> 23:53.160
I have only a few minutes, but that's cool.

23:53.160 --> 23:55.120
How we got here?

23:55.120 --> 23:58.600
First, we can do a lot of essentials about the data.

23:58.600 --> 24:00.000
This is time serious.

24:00.000 --> 24:06.980
So we know people usually want to get not individual rows, but computations of a rows.

24:06.980 --> 24:12.880
We know people mostly want to group by things that are in the data, like a string, like

24:12.880 --> 24:16.880
the country name or the device name or the brand or whatever.

24:16.880 --> 24:21.760
So instead of storing strings, we have a special symbol, which is called a special type, which

24:21.760 --> 24:22.760
is called a symbol.

24:22.760 --> 24:27.560
If you give me a string, we convert into a number and we do look up automatically those

24:27.560 --> 24:28.560
things.

24:28.560 --> 24:33.520
So we make a lot of assumptions because we hyper-specialize on one particular use case.

24:33.520 --> 24:34.560
We optimize the storage.

24:34.560 --> 24:39.520
We don't use indexes because we store everything always in incremental order per partition.

24:39.520 --> 24:45.040
If we get data out of order, we have to regrade the partitions, but we don't need indexes because

24:45.040 --> 24:49.920
we always have the data physically in order so we can scan super quickly back and forth.

24:49.920 --> 24:50.920
That's kind of the idea.

24:50.920 --> 24:55.120
We also parallelize as much as we can using different things.

24:55.120 --> 24:57.400
This is written in Java, and it's from scratch.

24:57.400 --> 25:02.920
You will see some databases, which I love, like MongoDB, excellent device for content.

25:02.920 --> 25:04.800
They have a time series module.

25:04.800 --> 25:08.400
We use the same MongoDB collections for doing time series.

25:08.400 --> 25:14.080
They cannot be as fast because they are using exactly what they are using for content.

25:14.080 --> 25:15.080
It's very convenient.

25:15.080 --> 25:19.440
I can do everything, but same thing with all the engines that are building on top of other

25:19.440 --> 25:20.440
things.

25:20.440 --> 25:22.160
We don't have any dependencies.

25:22.160 --> 25:24.400
Everything is built for scratch.

25:24.400 --> 25:31.240
Usually we are writing some of the libraries in Java, like strings and loggers and so on,

25:31.240 --> 25:32.240
to avoid conversions.

25:32.240 --> 25:35.760
There are things that we don't use, so we don't use them.

25:35.760 --> 25:38.120
We have libraries for rings.

25:38.120 --> 25:40.720
We have libraries for memory management.

25:40.720 --> 25:45.320
We have libraries for absolutely everything, drag-written in our own version.

25:45.320 --> 25:50.040
We have our own Just-In-Time compiler because the original Just-In-Time compiler in Java

25:50.040 --> 25:54.360
was not performing enough for some of the parallelization in what it wanted to do.

25:54.360 --> 25:55.360
We wrote everything.

25:55.360 --> 25:57.360
Our Java is kind of weird.

25:57.360 --> 26:00.040
Jeremy can tell you more about that.

26:00.040 --> 26:03.160
It's super weird Java, but it's still Java.

26:03.160 --> 26:04.360
That's kind of the idea.

26:04.360 --> 26:07.880
We even wrote our own input output functions.

26:07.880 --> 26:08.880
That's kind of a thing.

26:08.880 --> 26:09.880
Why?

26:09.880 --> 26:12.080
Because we can get nanoseconds faster.

26:12.080 --> 26:13.080
This is log4j.

26:13.080 --> 26:14.080
Log4j.

26:14.080 --> 26:18.360
We don't speak about log4j, which is awesome.

26:18.360 --> 26:19.360
This is log4j.

26:19.360 --> 26:28.760
This is the nanoseconds, the operations you can do in each nanosecond.

26:28.760 --> 26:35.800
With log4j, log in, integer, you can do 82 operations per nanosecond.

26:35.800 --> 26:41.360
We can do 800 operations per nanosecond, which is, do you have to go down to the nanosecond?

26:41.360 --> 26:45.520
If you are doing a crude application, probably not.

26:45.520 --> 26:47.240
It really depends what you are building.

26:47.240 --> 26:50.520
That's kind of why we are writing things from scratch.

26:50.520 --> 26:55.520
Basically the approach of QSDBT performance, you know, this is like, I don't know who you

26:55.520 --> 27:00.760
are, but I don't know you, but I will find you and I will kill you.

27:00.760 --> 27:03.080
That's kind of the same approach I see on QSDBT.

27:03.080 --> 27:09.040
They are like, I don't know, we are so we can get faster at some obscure thing here.

27:09.040 --> 27:10.040
That's kind of the idea.

27:10.040 --> 27:13.400
We try to be a good team player.

27:13.400 --> 27:18.160
Jeremy here has contributed himself only alone.

27:18.160 --> 27:22.280
The connectors for Kafka Connect, connectors for Apache Flink.

27:22.280 --> 27:25.960
So we try to integrate with the rest of the ecosystem.

27:25.960 --> 27:28.120
We love it if you try QSDBT.

27:28.120 --> 27:30.000
You are open source geeks.

27:30.000 --> 27:31.560
You like GitHub stars.

27:31.560 --> 27:33.200
We like GitHub stars.

27:33.200 --> 27:34.200
Please contribute.

27:34.200 --> 27:37.440
Please start on GitHub if you like it.

27:37.440 --> 27:39.640
We have a contributor Slack channel.

27:39.640 --> 27:41.360
We are quite friendly.

27:41.360 --> 27:42.480
We are fast.

27:42.480 --> 27:44.360
We work with interesting problems.

27:44.360 --> 27:49.560
If you like interesting problems, if you like weird Java, we would love to have you here.

27:49.560 --> 27:50.560
So thank you very much.

27:50.560 --> 27:52.560
I can't take any questions outside.

27:52.560 --> 27:56.560
One question from the chat.

27:56.560 --> 27:58.560
Thank you.

27:58.560 --> 28:08.320
Someone is asking if QSDBT can work with GPS data.

28:08.320 --> 28:16.080
Yes, you can work with GPS data.

28:16.080 --> 28:17.080
We have double.

28:17.080 --> 28:18.840
You can use for that.

28:18.840 --> 28:21.280
We don't have a lot of geospatial functions.

28:21.280 --> 28:29.480
We have geohashes which basically allow you to define in which a different resolution

28:29.480 --> 28:32.760
in which a square in the world something is.

28:32.760 --> 28:37.600
So if you're telling us about finding where a point is in the world, a particular point

28:37.600 --> 28:39.920
in time, QSDBT is very cool.

28:39.920 --> 28:45.360
If you need to do other things with support on Mac libraries, COS and all those things

28:45.360 --> 28:47.440
to do your own calculations.

28:47.440 --> 28:49.600
But yeah, it can be used for GPS.

28:49.600 --> 28:54.080
Some people are doing asset tracking with QSDBT.

28:54.080 --> 28:55.080
Thank you.

28:55.080 --> 28:58.440
Mr.
