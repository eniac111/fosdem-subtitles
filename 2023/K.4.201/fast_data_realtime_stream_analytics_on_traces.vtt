WEBVTT

00:00.000 --> 00:10.800
Thanks very much. Thank you. So welcome everyone and I'm glad that you're here on Saturday

00:10.800 --> 00:16.320
early morning in this first session. So I would like to make it as easy as possible.

00:16.320 --> 00:24.320
Thanks for the O'Gallantas, Jervis and Yarmur for inviting me to talk today about stream processing.

00:24.320 --> 00:28.800
The fact is I don't know your background so I'm not sure exactly how much experience

00:28.800 --> 00:34.960
you have with real time stream processing. So if you see some concepts are easy, just get everyone

00:34.960 --> 00:40.160
after a few of these concepts. So I'll be talking today about real time stream processing on

00:40.160 --> 00:46.800
additive and there is one of the blocks. So that's what the main focus will be. Obviously this title

00:46.800 --> 00:55.600
as it is could be a startup company. So you would expect to have some ideas today where you can use

00:55.600 --> 01:01.600
some of these ideas in your world or in your experience or in your days of study, whatever

01:01.600 --> 01:08.960
you want. Whether you are a Java developer or data scientist or ML ops, so it doesn't matter.

01:08.960 --> 01:13.360
So there is something for everyone here today. So that's the main focus for this session.

01:14.000 --> 01:18.640
So anyone recognize these guys on the screen here?

01:18.640 --> 01:26.960
Right, so that's where I came from. I'm based in Liverpool in the UK and on the right side is the

01:28.160 --> 01:36.320
Liverpool for Coqola which is basically one of the top football teams in the UK. So I want

01:36.320 --> 01:41.040
just to highlight this screen here just to tell you that's real time stream processing is not

01:41.040 --> 01:48.880
in a specific domain, it could be in any domain. If you look at it, do you know how long it takes

01:48.880 --> 02:00.880
for example for an eye to blink? Come again. Yeah, so it takes over half one third of seconds.

02:00.880 --> 02:07.360
So that's pretty fast. So if you're thinking about like maybe minutes or hours, probably this is not

02:07.360 --> 02:13.040
the right discussion room for you. We're talking about sub-milliseconds today. So whether it's for

02:13.040 --> 02:18.480
example you use it in finance, whether you use it in IOD devices, smart devices, whether you use it

02:18.480 --> 02:24.080
in sports, hospitals or machine learning or what we're trying to do today for stream processing.

02:24.080 --> 02:30.080
So that's the main idea and obviously if you're working with real time stream processing you focus

02:30.080 --> 02:37.840
on the real time data right? And I've seen it so many times where platforms and tools focus on

02:37.840 --> 02:43.600
you know how much data you can process and you see these benchmarks everywhere on the internet

02:43.600 --> 02:50.880
and this is pretty cool I think but the key source and the secret source for this is to use something

02:50.880 --> 02:57.680
in combination between real time data and historical data. So the main reason for this is

02:57.680 --> 03:04.240
to look at context. So without you know knowing what's going on, you probably don't benefit much

03:04.240 --> 03:10.560
from the real time data you're processing. So what you want is always to go back and check what's

03:10.560 --> 03:17.760
going on with the context of this data. There is a problem in this script source obviously because

03:17.760 --> 03:23.280
what you're looking at is kind of like you know two different data types and you want to make

03:23.280 --> 03:29.360
sure that you process it at the same speed or you know very close to the same speed. Obviously it

03:29.360 --> 03:37.120
becomes really problem when you try to scale it. So if you have I don't know like maybe a few

03:37.120 --> 03:42.560
cases of data that you want to process probably it's not too much topic for you but when you

03:42.560 --> 03:47.520
start to scale it up it becomes really problem you know to understand how you want to scale it.

03:47.520 --> 03:53.600
So do you scale your data or do you scale your compute or do you scale both and at what speed.

03:54.160 --> 04:00.960
So we will discuss all these concepts today and if I ask you now how much did you process? Probably

04:02.000 --> 04:07.040
because in this room I would assume you have over million transactions per second or few

04:07.040 --> 04:12.400
millions or I don't know some of you might be processing billions of transactions per second.

04:12.400 --> 04:19.520
So that's pretty good and what we want today is to focus this domain into very

04:20.560 --> 04:26.240
specific area and this area essentially what we're trying to do today is to analyze traces. So

04:26.880 --> 04:31.200
it doesn't matter if it is like operating system traces or platform traces or it's

04:31.200 --> 04:35.920
like programming language traces what we want is to make sure that we have environment

04:35.920 --> 04:43.520
and within this environment you can scale your logs basically scale your processing

04:43.520 --> 04:49.440
and at the same time provide some kind of analytics right. So again if you look at how

04:49.440 --> 04:54.960
much data you can process the number by itself doesn't give you much what's going on here.

04:54.960 --> 04:59.200
So what you want is to find this specific information you're looking for

04:59.200 --> 05:06.400
or kind of like looking at finding the needles or finding the hidden areas within your data.

05:06.400 --> 05:14.800
So if you look at how much logs you process per day or per week and you store it somewhere

05:14.800 --> 05:15.440
on you know

05:15.440 --> 05:21.840
official hard drive or you store it in mori or you store it in the cloud. So what you want is

05:21.840 --> 05:29.440
to you know make sense of it and some companies do this process manually which means they run

05:29.440 --> 05:37.520
software and they go through their logs and this is kind of a patch service and they try to

05:37.520 --> 05:43.440
understand what's going on within the logs. So obviously this is a problem when you want to scale

05:43.440 --> 05:50.160
it and with the scaling you have different logs stored in different places and you want to make

05:50.160 --> 05:58.080
sure basically to have a platform where in this platform we kind of like looking at some kind of

05:58.080 --> 06:06.160
results. So for the sake of this discussion today we'll focus on two different solutions. So one of

06:06.160 --> 06:12.000
a kind of like to provide some kind of alerts and the other is provide some kind of trends within

06:12.000 --> 06:20.960
your data. Obviously I work for a company called Hezakast. So Hezakast as a platform allows you to

06:20.960 --> 06:27.200
do so but obviously you might have heard of some companies or you know they do some kind of stream

06:27.200 --> 06:33.520
processing. So this is kind of like you know overview what's going on with this domain at this

06:33.520 --> 06:40.880
time. Obviously you can split it depending on you're looking for open source solution or

06:40.880 --> 06:46.960
I don't know, hardware solution or you know some kind of management service and what you look at

06:46.960 --> 06:52.480
is which domain you are. So are you looking to capture your data or some kind of you know

06:52.480 --> 06:58.480
streaming your data or you want to do some kind of transformation on your data or do some kind

06:58.480 --> 07:05.760
of analytical machine learning. So you can see that you split it into 12 squares and within this

07:05.760 --> 07:12.800
where these like tools and platforms are you know spread it over. So obviously some tools exist on

07:12.800 --> 07:19.600
this screen for whatever reason but obviously this might give you some ideas but it's hard

07:19.600 --> 07:26.800
it's hard to decide which tool you want to go for. Simply because I think the distribution is not

07:26.800 --> 07:33.920
clear here so it tells you basically which tool is open source for example and where in process you

07:33.920 --> 07:38.800
can use it but it doesn't give you a full picture on you know how to do it in practical terms.

07:39.440 --> 07:45.600
And so this is where it might be easier to understand what we're talking about. So if you

07:45.600 --> 07:51.920
remember from my slide where I discussed the historical data and the new data. So today we're

07:51.920 --> 07:58.800
kind of like you know trying to split everything into two categories. So on one side you get like

07:58.800 --> 08:05.440
stream processing engines so these engines are pretty fast in you know streaming events

08:05.440 --> 08:11.840
and on this far right side you have some kind of fast data stores which are you know are pretty

08:11.840 --> 08:19.600
fast in handling data at speed. So again the solution for break time stream processing

08:20.560 --> 08:27.680
is kind of combination and you want to process data in this moment and at the same time you want

08:27.680 --> 08:34.960
to actually also access data stored somewhere. So that's where is a cast fit into this area here.

08:35.680 --> 08:41.520
So the platform itself obviously for those who don't know by the way we have one of the masterminds

08:41.520 --> 08:48.000
of Hezakast sitting in this room. So this is the platform so it's open source platform it doesn't

08:48.000 --> 08:53.840
matter where your shop is coming from whether it's a passion car or passion board. So IOT devices

08:53.840 --> 08:59.520
for example I don't know some kind of enterprise applications or even like within Hezakast or even

08:59.520 --> 09:05.760
you can write your own connector and you feed it into the platform. So platform historically used

09:05.760 --> 09:13.840
to be two different components so the IMPG and the JET engine and essentially now it's all packaged

09:13.840 --> 09:22.800
in one one jar file. As you see here it allows you to load your data from R-Biscuits into memory so

09:22.800 --> 09:31.840
you have access to external data and pretty much like instantaneously and this will where you know

09:31.840 --> 09:38.560
you can provide context what's going on with your data and at the same time you can actually do

09:38.560 --> 09:44.080
stream processing so that's what JET engine is. So from here you can do some kind I don't know

09:44.080 --> 09:49.840
maybe like data transformation or do some kind of stream processing as we will do today or even

09:49.840 --> 09:55.600
like define machine learning if you want to. You can connect it to some clients so these are some

09:55.600 --> 10:02.400
clients here so written in various languages. If you're from data science background which means

10:02.400 --> 10:07.360
your programming languages in general are not preferred for you so you might be considered

10:07.360 --> 10:14.640
using SQL to do what I'm planning today so this is another option you can do and once you process

10:14.640 --> 10:21.760
this data where you load it into memory for a JET file data and at the same time you have some

10:21.760 --> 10:26.720
kind of data coming in. In CASCAT topic for example and you do the combination or even

10:26.720 --> 10:33.440
you do transformation you can proceed to do some kind of visualization. So the good thing about

10:33.440 --> 10:40.480
HESECAS in general when it comes to scaling is it's partition aware so which means basically

10:40.480 --> 10:47.680
your compute your JET engine or your process essentially can be or can detect where your data

10:47.680 --> 10:54.320
is stored so this is like you know we are trying to have as low latency as possible when it comes

10:54.320 --> 11:00.400
to processing this data. So this is very important to understand because latency is your enemy when

11:00.400 --> 11:07.360
it comes to stream processing so what you want is kind of like having a platform where you avoid

11:07.360 --> 11:14.400
network folks for example you avoid IO to your hard disk you try to also avoid every time or

11:15.760 --> 11:20.400
context switching between threads so you want to avoid all of these but at the same time you want

11:20.400 --> 11:27.280
your process to be as close as possible to your data. You can apply some kind of you know machine

11:27.280 --> 11:34.960
learning on this and the scaling itself could be done in various ways so the main thing to take

11:34.960 --> 11:44.160
away from here is this no master worker relationship so all nodes basically are peers and we've done

11:44.160 --> 11:50.720
this study it's a bit outdated but it's kind of like 1 billion transactions per second on 45 nodes

11:51.280 --> 11:57.920
so what we're trying to do now is to add one zero into this number here and even though it's pretty

11:57.920 --> 12:04.320
impressive what is nice about it is the linear scaling which means more data you can add you

12:04.320 --> 12:11.280
know more nodes into it so that's the historical bit of this talk so let's just move to the technical

12:11.280 --> 12:17.120
part so for this demo what I wanted is kind of like you know show you some some ideas right

12:17.120 --> 12:22.000
so you should be able to take these ideas and apply it you know anywhere obviously the solution

12:22.000 --> 12:28.160
as itself would be like you know project by itself so feel free to edit and change it so all

12:28.160 --> 12:34.800
source code is available on github and the documentation as well so you can go through it

12:34.800 --> 12:41.280
so the main idea when it comes to analyzing or you know making sense out of your traces and

12:41.280 --> 12:49.200
logs is to store it somewhere close to you know you will compute first of all and it shouldn't

12:49.200 --> 12:54.000
be stored locally right so you want to store it first of all the first thing is to store on the

12:54.000 --> 13:00.480
cloud so for this for the sake of this demo what I'm doing is I'm storing everything onto the cloud

13:00.480 --> 13:05.040
so there is a solution called hazelgas hurricane which is kind of like service so you don't need

13:05.040 --> 13:10.960
to download jar run your relic you can simply click and play so you can create that cloud

13:10.960 --> 13:16.640
you run everything I'm discussing today so you create an instance of hazelgas and from there

13:16.640 --> 13:23.120
you can pretty much proceed to what I'm planning to do so the first option we were talking about

13:23.120 --> 13:29.040
is kind of like storing everything into the cloud so once we have all the data obviously we need

13:29.040 --> 13:37.120
some kind of you know trace message which makes sense so this trace message could be you know

13:37.120 --> 13:41.680
changed based on how you want to approach it right so for example if you're working with machine

13:41.680 --> 13:46.480
learning you probably look for some kind of I don't know classification solution for you

13:46.480 --> 13:53.120
you know for your tests or you could be looking for NLP if you don't want to work with machine

13:53.120 --> 13:58.880
learning you probably want to look for some kind of trends so you look for processing your data

13:58.880 --> 14:04.240
doesn't matter if you're using machine learning in this case you want to have some kind of data

14:04.240 --> 14:10.400
stored somewhere so it could be in JSON format or it could be like bar charge swing so it depends

14:10.400 --> 14:18.320
again how much the speed is important to you so the option first option is to go through the

14:18.320 --> 14:25.760
alerts so in alerts what we're trying to do here is to take everything and start in that cloud so

14:25.760 --> 14:31.600
obviously we don't store it on the cloud on our basis what we try to do is store it in memory

14:33.280 --> 14:38.880
my preference in this case is to use some kind of map structure so map structure allows you to

14:38.880 --> 14:46.720
essentially random access and rebalance between various nodes within your cluster and at the same

14:46.720 --> 14:53.280
time you want to have some key value so in order to know where this node is coming from so in this

14:53.280 --> 15:00.080
case could be like ID address for example so this is where and report number so as key and the value

15:00.080 --> 15:05.600
could be anything that makes sense to you so in this case for example you can track level of this

15:05.600 --> 15:12.240
error sorry this level and message for example if you want to do some kind of NLP processing

15:12.960 --> 15:19.360
and some kind of you know process or thread name on this obviously once you have your

15:19.360 --> 15:26.720
dm value what you can do is proceed and store it into memory so this is where you get this

15:26.720 --> 15:33.520
set to has a task and what we're trying to do is create the IMAP and once you have the IMAP

15:33.520 --> 15:39.360
means you should be able to store it you know access and do stream processing as I will show you

15:39.920 --> 15:46.720
so first message is to store it into the cloud store it in memory in this case i'm using

15:46.720 --> 15:53.280
gazakas and i'm using IMAP and second stage is to do the stream processing right so there are a

15:53.280 --> 16:00.080
couple of options here for you so first option is to use SQL so SQL is built within gazakas which

16:00.080 --> 16:04.800
means or on top of gazakas which means you should be able to work your data so you provide some kind

16:04.800 --> 16:10.800
of specific messages that you're looking for obviously depends on your input you can do some

16:10.800 --> 16:17.600
kind of SQL so whether it's in your joy for example or search and so on or the other option is to do

16:17.600 --> 16:23.440
some kind of prediction so you're getting some logs you don't know exactly what's going on to

16:23.440 --> 16:29.920
happen next and you try to predict to provide some kind of you know alerts or trends so we need to

16:29.920 --> 16:36.160
build the trends so in order to do this what I did is kind of like use the same key but for my

16:37.280 --> 16:45.760
value what I'm using is some kind of log score so log score is not important what I'm saying here

16:45.760 --> 16:54.240
is I want to give value for is every single message or every single log message so this could be for

16:54.240 --> 17:02.240
example how you know how important this specific message is for you or could be for example how

17:02.240 --> 17:09.840
serious or how you know dangerous the message is so as levels in logs you can define score so

17:09.840 --> 17:16.160
instead of having four levels for example you can spread it either from one one to one hundred

17:16.160 --> 17:21.040
so this should give you some kind of predictions why because if you have for example 10 messages

17:21.040 --> 17:29.440
10 local messages or for example warning you don't know exactly if the 11 matches will be warning or

17:29.440 --> 17:35.600
not if you want to predict it obviously it doesn't give you how much will be warning or not whereas

17:35.600 --> 17:44.640
if you use some kind of numerical value you can get as close as possible to this so we get key

17:44.640 --> 17:53.040
from there we get score from here and what we do next is to do some kind of predictions on the logs

17:53.040 --> 17:59.600
so in this process what we have is exactly my key and the value which is like the score on each log

17:59.600 --> 18:07.200
message and I imported into Helical so Helical allows you to basically input and output from two

18:07.200 --> 18:15.120
different maps and do stream processing so we'll build the train based on previous logs

18:16.560 --> 18:23.440
scores and we use the prediction on top of it to provide some kind of alert so zero means don't

18:23.440 --> 18:31.520
alert one means alert and as you can see here the actual workflow kind of like you build you read

18:31.520 --> 18:39.600
from then you define trend map so which is like normal map and from there you can use it to predict

18:39.600 --> 18:44.880
what's going to happen next obviously do some kind of visualization so how does it look like

18:45.680 --> 18:54.000
so this is kind of the prediction part of it so we take the logs map and we build trend map out of

18:54.000 --> 19:01.120
it so trend map would start reading messages and the scores and build train for you and from this

19:01.120 --> 19:06.240
trend I can use some kind of machine learning it doesn't have to be machine learning in this case

19:06.240 --> 19:12.160
it's linear regression but it could be anything to be honest and we check the values and we try to

19:12.160 --> 19:19.200
use some kind of prediction based on the previous values to decide if you want to send an alert or

19:19.200 --> 19:26.560
not and obviously this is kind of like describing the exact thing so when it there is a one on your

19:26.560 --> 19:34.560
values it's alert send alert when it's zero or not and here there are three ways to do it

19:35.280 --> 19:40.960
so this is where the stream processing comes into place so you could simply use SPL to read from

19:40.960 --> 19:47.520
them out and do some query if you want obviously this is batch which means it's not real time

19:47.520 --> 19:53.600
stream processing or you can even like create a pipeline or create a process and from this

19:53.600 --> 20:00.960
process you can read the logs and do the same thing so you can use either SQL or Java to do it

20:00.960 --> 20:07.440
but first two options are batch which means like you can process the data in real time you want to

20:07.440 --> 20:13.280
change this in real time so the third option is the journal map so journal map will track all

20:13.280 --> 20:19.680
changes so it's going to be honest so you have the logs stored and you have logs coming into Kafka

20:19.680 --> 20:26.720
topic and you can basically store both into journal map so we're on 5.2 version within

20:26.720 --> 20:33.440
hezakas 5.3 will have the SQL features on top of it which allows you if you're for example data

20:33.440 --> 20:40.160
scientist to just do the queries and change the data and obviously it's ring buffer so this is

20:40.160 --> 20:45.920
very important to understand so you can start processing your data from start or you know from

20:45.920 --> 20:52.640
the end and what you want is kind of like you know using this kind of alert to it so the first part

20:52.640 --> 20:59.120
is to read it so this is the actual map we built in the first option and from here you define the

20:59.120 --> 21:06.480
key for example and the value and you start for example to do some kind of filtering so this

21:06.480 --> 21:14.000
happens in real time on continuous and the map inset will allow you to basically track changes

21:14.000 --> 21:22.000
so to give you some takeaways and best practices so we just try to summarize everything we discussed

21:22.000 --> 21:27.760
today obviously there are more to discuss but this should give you something to go out and try

21:28.320 --> 21:33.680
so first of thing you need to store your logs into some kind of platform so in this case i'm

21:33.680 --> 21:40.080
using hezakas obviously you don't have to use hezakas the idea here is to do some kind of compute

21:40.080 --> 21:46.560
onto circle data to provide some context as well as real-time data and from there you need to store

21:46.560 --> 21:51.760
it on the cloud so you need to store it somewhere where you can access logs from multiple places

21:52.400 --> 21:58.160
obviously you have to be stored in memory and from there you need to choose format if you for

21:58.160 --> 22:03.360
example looking to provide some you know i don't know some predictions you probably need to use

22:03.360 --> 22:09.920
JSON format or for example if you want just to do some analysis you can sync and it's faster if you want to speed

22:10.720 --> 22:16.640
unit it is some kind of map structure obviously when you store it in memory because this will allow

22:17.840 --> 22:26.720
some kind of random access and also you need to consider how you empty your map so because

22:26.720 --> 22:32.320
you're limited on size obviously and finally you need to consider security so whatever you

22:32.320 --> 22:37.200
send to the cloud you need to make sure that you don't load something you know personally whatever

22:37.760 --> 22:43.440
so if you're interested in this topic we're running on conference next month so feel free

22:43.440 --> 22:49.040
to scan this code we provide training for this all free obviously and everything i mentioned today

22:49.040 --> 22:54.480
is up so you should be able to do everything i mentioned today i'll be sharing around here if

22:54.480 --> 22:59.360
you want to have a chat or if you want to discuss it a little bit more obviously within half an hour

22:59.360 --> 23:04.800
there's not much to give but hopefully you've got some ideas from mr and hopefully it will be also

23:04.800 --> 23:31.200
useful for you so with that being said thanks very much for listening open for questions
